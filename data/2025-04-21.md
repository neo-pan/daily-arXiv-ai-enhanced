<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.CV](#cs.CV) [Total: 25]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.CL](#cs.CL) [Total: 15]
- [math.OC](#math.OC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.NE](#cs.NE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
*Timo Aukusti Laine*

Main category: cs.AI

TL;DR: 本文基于量子启发框架，澄清LLM语义表示的核心假设，阐述六个关键原则，并讨论量子计算的潜力。


<details>
  <summary>Details</summary>
Motivation: 证明量子启发框架是研究LLM语义空间的有效方法，并探索其信息处理和响应生成的洞见。

Method: 详细阐述六个基于量子力学类比的关键原则。

Result: 提供了LLM信息处理和响应生成的宝贵洞见，并讨论了利用量子计算开发更强大高效LLM的潜力。

Conclusion: 量子启发框架有效，并有望通过量子计算提升LLM性能。

Abstract: In the previous article, we presented a quantum-inspired framework for
modeling semantic representation and processing in Large Language Models
(LLMs), drawing upon mathematical tools and conceptual analogies from quantum
mechanics to offer a new perspective on these complex systems. In this paper,
we clarify the core assumptions of this model, providing a detailed exposition
of six key principles that govern semantic representation, interaction, and
dynamics within LLMs. The goal is to justify that a quantum-inspired framework
is a valid approach to studying semantic spaces. This framework offers valuable
insights into their information processing and response generation, and we
further discuss the potential of leveraging quantum computing to develop
significantly more powerful and efficient LLMs based on these principles.

</details>


### [2] [Graphical Models for Decision-Making: Integrating Causality and Game Theory](https://arxiv.org/abs/2504.13210)
*Maarten C. Vonk,Mauricio Gonzalez Soto,Anna V. Kononova*

Main category: cs.AI

TL;DR: 这篇论文通过概率图形模型澄清博弈论和因果性的关键概念，以支持其实践应用并鼓励更广泛采用。


<details>
  <summary>Details</summary>
Motivation: 尽管因果性和博弈论的整合在理论上取得了进展，但实际应用尚未充分探索，需要澄清概念以促进实施。

Method: 通过严格检查这些概念并用直观、一致的例子进行说明，聚焦于概率图形模型。

Result: 澄清了实施模型所需的输入，提供从业者关于应用和选择的见解，并引用了现有研究。

Conclusion: 希望这项工作鼓励这些模型在现实世界的更广泛采用。

Abstract: Causality and game theory are two influential fields that contribute
significantly to decision-making in various domains. Causality defines and
models causal relationships in complex policy problems, while game theory
provides insights into strategic interactions among stakeholders with competing
interests. Integrating these frameworks has led to significant theoretical
advancements with the potential to improve decision-making processes. However,
practical applications of these developments remain underexplored. To support
efforts toward implementation, this paper clarifies key concepts in game theory
and causality that are essential to their intersection, particularly within the
context of probabilistic graphical models. By rigorously examining these
concepts and illustrating them with intuitive, consistent examples, we clarify
the required inputs for implementing these models, provide practitioners with
insights into their application and selection across different scenarios, and
reference existing research that supports their implementation. We hope this
work encourages broader adoption of these models in real-world scenarios.

</details>


### [3] [Causal-Copilot: An Autonomous Causal Analysis Agent](https://arxiv.org/abs/2504.13263)
*Xinyue Wang,Kun Zhou,Wenyi Wu,Har Simrat Singh,Fang Nan,Songyao Jin,Aryan Philip,Saloni Patnaik,Hou Zhu,Shivam Singh,Parjanya Prashant,Qian Shen,Biwei Huang*

Main category: cs.AI

TL;DR: 本文引入Causal-Copilot，一个基于大语言模型的AI代理，自动化因果分析流程，使其对非专家更易访问，并桥接理论与实际应用。


<details>
  <summary>Details</summary>
Motivation: 因果分析在科学发现和决策中至关重要，但其概念和算法复杂，导致领域专家无法利用最新进展，研究者也缺乏实际测试。

Method: 开发Causal-Copilot自治代理，自动化处理表格和时间序列数据的因果发现、推理、算法选择、超参数优化、结果解释和行动洞见生成，支持自然语言交互。

Result: 实证评估显示Causal-Copilot性能优于现有基准，可靠、可扩展。

Conclusion: Causal-Copilot桥接因果分析理论与实际应用，促进专家访问和方法进步，形成良性循环。

Abstract: Causal analysis plays a foundational role in scientific discovery and
reliable decision-making, yet it remains largely inaccessible to domain experts
due to its conceptual and algorithmic complexity. This disconnect between
causal methodology and practical usability presents a dual challenge: domain
experts are unable to leverage recent advances in causal learning, while causal
researchers lack broad, real-world deployment to test and refine their methods.
To address this, we introduce Causal-Copilot, an autonomous agent that
operationalizes expert-level causal analysis within a large language model
framework. Causal-Copilot automates the full pipeline of causal analysis for
both tabular and time-series data -- including causal discovery, causal
inference, algorithm selection, hyperparameter optimization, result
interpretation, and generation of actionable insights. It supports interactive
refinement through natural language, lowering the barrier for non-specialists
while preserving methodological rigor. By integrating over 20 state-of-the-art
causal analysis techniques, our system fosters a virtuous cycle -- expanding
access to advanced causal methods for domain experts while generating rich,
real-world applications that inform and advance causal theory. Empirical
evaluations demonstrate that Causal-Copilot achieves superior performance
compared to existing baselines, offering a reliable, scalable, and extensible
solution that bridges the gap between theoretical sophistication and real-world
applicability in causal analysis.

</details>


### [4] [On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management](https://arxiv.org/abs/2504.13314)
*Timothy Tjhay,Ricardo J. Bessa,Jose Paulos*

Main category: cs.AI

TL;DR: 本文提出一个框架，使用Grid2Op环境模拟干扰，评估强化学习代理在拥堵管理中的鲁棒性和弹性，并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案虽定义了高风险领域的鲁棒性、弹性和安全要求，但缺乏详细评估方法，本文旨在填补这一空白。

Method: 引入一个新框架，利用Grid2Op数字环境，通过扰动代理模拟干扰，评估AI性能；鲁棒性通过稳定性和奖励影响指标衡量，弹性通过恢复性能量化。

Result: 框架有效识别漏洞，并提升AI的鲁棒性和弹性。

Conclusion: 该框架对关键应用中的AI系统改进具有重要意义。

Abstract: The European Union's Artificial Intelligence (AI) Act defines robustness,
resilience, and security requirements for high-risk sectors but lacks detailed
methodologies for assessment. This paper introduces a novel framework for
quantitatively evaluating the robustness and resilience of reinforcement
learning agents in congestion management. Using the AI-friendly digital
environment Grid2Op, perturbation agents simulate natural and adversarial
disruptions by perturbing the input of AI systems without altering the actual
state of the environment, enabling the assessment of AI performance under
various scenarios. Robustness is measured through stability and reward impact
metrics, while resilience quantifies recovery from performance degradation. The
results demonstrate the framework's effectiveness in identifying
vulnerabilities and improving AI robustness and resilience for critical
applications.

</details>


### [5] [Cost-of-Pass: An Economic Framework for Evaluating Language Models](https://arxiv.org/abs/2504.13359)
*Mehmet Hamza Erol,Batu El,Mirac Suzgun,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文提出一个基于生产理论的框架，评估AI语言模型的准确性和推理成本权衡，揭示模型创新驱动成本效率进步。


<details>
  <summary>Details</summary>
Motivation: AI系统经济采用需经济价值超过推理成本，因此需指标兼顾性能和成本。

Method: 引入'cost-of-pass'和'frontier cost-of-pass'指标，分析模型成本效率，包括反事实边界和推理时技术评估。

Result: 轻量模型适基本量化任务，大模型适知识密集任务，推理模型适复杂量化问题；成本显著下降，模型创新为主导。

Conclusion: 模型级创新是成本效率关键驱动力，该框架可衡量进步并指导部署。

Abstract: The widespread adoption of AI systems in the economy hinges on their ability
to generate economic value that outweighs their inference costs. Evaluating
this tradeoff requires metrics that account for both performance and costs. We
propose a framework grounded in production theory for evaluating language
models by combining accuracy and inference cost. We introduce "cost-of-pass",
the expected monetary cost of generating a correct solution. We then define the
"frontier cost-of-pass" as the minimum cost-of-pass achievable across available
models or the "human-expert, using the approximate cost of hiring an expert.
Our analysis reveals distinct economic insights. First, lightweight models are
most cost-effective for basic quantitative tasks, large models for
knowledge-intensive ones, and reasoning models for complex quantitative
problems, despite higher per-token costs. Second, tracking this frontier
cost-of-pass over the past year reveals significant progress, particularly for
complex quantitative tasks where the cost has roughly halved every few months.
Third, to trace key innovations driving this progress, we examine
counterfactual frontiers: estimates of cost-efficiency without specific model
classes. We find that innovations in lightweight, large, and reasoning models
have been essential for pushing the frontier in basic quantitative,
knowledge-intensive, and complex quantitative tasks, respectively. Finally, we
assess the cost-reductions afforded by common inference-time techniques like
majority voting and self-refinement, finding that their marginal accuracy gains
rarely justify their costs. Our findings underscore that complementary
model-level innovations are the primary drivers of cost-efficiency, and our
economic framework provides a principled tool for measuring this progress and
guiding deployment.

</details>


### [6] [In between myth and reality: AI for math -- a case study in category theory](https://arxiv.org/abs/2504.13360)
*Răzvan Diaconescu*

Main category: cs.AI

TL;DR: 本论文通过实验探讨AI系统在数学研究中的作用，并为AI开发者提供改进建议。


<details>
  <summary>Details</summary>
Motivation: 由于对AI系统解决数学问题性能的日益兴趣，以及现有测试结果不一。

Method: 进行了使用两个主要当代AI系统的实验。

Result: 实验结果讨论了AI如何辅助数学研究，并提供了改进方向。

Conclusion: 强调AI在数学研究中的潜力，并建议开发者改进AI系统。

Abstract: Recently, there is an increasing interest in understanding the performance of
AI systems in solving math problems. A multitude of tests have been performed,
with mixed conclusions. In this paper we discuss an experiment we have made in
the direction of mathematical research, with two of the most prominent
contemporary AI systems. One of the objective of this experiment is to get an
understanding of how AI systems can assist mathematical research. Another
objective is to support the AI systems developers by formulating suggestions
for directions of improvement.

</details>


### [7] [Trust, but verify](https://arxiv.org/abs/2504.13443)
*Michael J. Yuan,Carlos Campoy,Sydney Lai,James Snewin,Ju Long*

Main category: cs.AI

TL;DR: 本文探讨了在去中心化AI网络如Gaia中，通过同行共识和财务激励检测未授权LLM节点的方法。


<details>
  <summary>Details</summary>
Motivation: 为了维护服务质量，确保节点运行指定的LLM，避免未授权或错误模型。

Method: 使用社会共识算法检测不诚实节点，并通过EigenLayer AVS的主体间验证系统引入财务激励和惩罚。

Result: 在主要诚实节点集群中证明了检测有效性，并提供了Gaia网络的实验数据。

Conclusion: 通过财务机制鼓励诚实行为，提高网络整体可靠性。

Abstract: Decentralized AI agent networks, such as Gaia, allows individuals to run
customized LLMs on their own computers and then provide services to the public.
However, in order to maintain service quality, the network must verify that
individual nodes are running their designated LLMs. In this paper, we
demonstrate that in a cluster of mostly honest nodes, we can detect nodes that
run unauthorized or incorrect LLM through social consensus of its peers. We
will discuss the algorithm and experimental data from the Gaia network. We will
also discuss the intersubjective validation system, implemented as an
EigenLayer AVS to introduce financial incentives and penalties to encourage
honest behavior from LLM nodes.

</details>


### [8] [Optimizing Electric Vehicle Charging Station Locations: A Data-driven System with Multi-source Fusion](https://arxiv.org/abs/2504.13517)
*Lihuan Li,Du Yin,Hao Xue,David Lillo-Trynes,Flora Salim*

Main category: cs.AI

TL;DR: 这篇论文开发了一个基于数据的系统，用于优化澳大利亚新南威尔士州的电动汽车充电站位置，考虑旅行数据、地理因素和风险。


<details>
  <summary>Details</summary>
Motivation: 动机是解决电动汽车充电需求增长带来的挑战，如长途旅行中的里程焦虑和住宅充电站分布不足。

Method: 方法是通过整合EV旅行数据、地理数据（如路线和LGA边界）、火灾洪水风险以及兴趣点，并通过案例研究进行可视化和评估。

Result: 结果是提供了一个多源融合系统，推荐充电站位置，并通过可视化展示其可行性。

Conclusion: 结论是这项工作可为讨论提供平台，指导未来充电站的部署。

Abstract: With the growing electric vehicles (EVs) charging demand, urban planners face
the challenges of providing charging infrastructure at optimal locations. For
example, range anxiety during long-distance travel and the inadequate
distribution of residential charging stations are the major issues many cities
face. To achieve reasonable estimation and deployment of the charging demand,
we develop a data-driven system based on existing EV trips in New South Wales
(NSW) state, Australia, incorporating multiple factors that enhance the
geographical feasibility of recommended charging stations. Our system
integrates data sources including EV trip data, geographical data such as route
data and Local Government Area (LGA) boundaries, as well as features like fire
and flood risks, and Points of Interest (POIs). We visualize our results to
intuitively demonstrate the findings from our data-driven, multi-source fusion
system, and evaluate them through case studies. The outcome of this work can
provide a platform for discussion to develop new insights that could be used to
give guidance on where to position future EV charging stations.

</details>


### [9] [Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning](https://arxiv.org/abs/2504.13554)
*Xin Tang,Qian Chen,Wenjie Weng,Chao Jin,Zhang Liu,Jiacheng Wang,Geng Sun,Xiaohuan Li,Dusit Niyato*

Main category: cs.AI

TL;DR: 论文提出UAV、GER、HAP合作框架和HG-MADDPG算法，优化任务分配和探索，减少完成时间和能量消耗，确保系统稳定性。


<details>
  <summary>Details</summary>
Motivation: AI驱动的CNN在UAV和GCN中计算需求高，导致系统不稳定，地面资源有限且动态。

Method: 提出合作框架，使用Lyapunov优化和HG-MADDPG算法（结合Hungarian算法和GDM-based MADDPG），优化任务分配和探索。

Result: 模拟结果显示，提高了任务卸载效率，减少了延迟，提升了系统稳定性。

Conclusion: 方法有效，显著改善了UAV任务处理性能和稳定性。

Abstract: Artificial Intelligence (AI)-driven convolutional neural networks enhance
rescue, inspection, and surveillance tasks performed by low-altitude uncrewed
aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown
environments. However, their high computational demands often exceed a single
UAV's capacity, leading to system instability, further exacerbated by the
limited and dynamic resources of GCNs. To address these challenges, this paper
proposes a novel cooperation framework involving UAVs, ground-embedded robots
(GERs), and high-altitude platforms (HAPs), which enable resource pooling
through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide
computing services for UAV offloaded tasks. Specifically, we formulate the
multi-objective optimization problem of task assignment and exploration
optimization in UAVs as a dynamic long-term optimization problem. Our objective
is to minimize task completion time and energy consumption while ensuring
system stability over time. To achieve this, we first employ the Lyapunov
optimization technique to transform the original problem, with stability
constraints, into a per-slot deterministic problem. We then propose an
algorithm named HG-MADDPG, which combines the Hungarian algorithm with a
generative diffusion model (GDM)-based multi-agent deep deterministic policy
gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a
method for exploration area selection, enhancing UAV efficiency in interacting
with the environment. We then innovatively integrate the GDM and multi-agent
deep deterministic policy gradient (MADDPG) to optimize task assignment
decisions, such as task offloading and resource allocation. Simulation results
demonstrate the effectiveness of the proposed approach, with significant
improvements in task offloading efficiency, latency reduction, and system
stability compared to baseline methods.

</details>


### [10] [Multi-modal Knowledge Graph Generation with Semantics-enriched Prompts](https://arxiv.org/abs/2504.13631)
*Yajing Xu,Zhiqiang Liu,Jiaoyan Chen,Mingchen Tu,Zhuo Chen,Jeff Z. Pan,Yichi Zhang,Yushan Zhu,Wen Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 本文提出框架从传统知识图谱构建多模态知识图谱，并设计VSNS方法提升图像质量，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱数量不足，构建面临挑战，特别是选择高质量、上下文相关的图像。

Method: 提出构建框架和VSNS方法，包括VNS过滤难可视化关系，SNS选择结构特征邻居。

Result: 在MKG-Y和DB15K数据集上评估，结果显示VSNS生成图像质量更高、更相关。

Conclusion: VSNS方法有效改善图像选择，提升多模态知识图谱构建质量。

Abstract: Multi-modal Knowledge Graphs (MMKGs) have been widely applied across various
domains for knowledge representation. However, the existing MMKGs are
significantly fewer than required, and their construction faces numerous
challenges, particularly in ensuring the selection of high-quality,
contextually relevant images for knowledge graph enrichment. To address these
challenges, we present a framework for constructing MMKGs from conventional
KGs. Furthermore, to generate higher-quality images that are more relevant to
the context in the given knowledge graph, we designed a neighbor selection
method called Visualizable Structural Neighbor Selection (VSNS). This method
consists of two modules: Visualizable Neighbor Selection (VNS) and Structural
Neighbor Selection (SNS). The VNS module filters relations that are difficult
to visualize, while the SNS module selects neighbors that most effectively
capture the structural characteristics of the entity. To evaluate the quality
of the generated images, we performed qualitative and quantitative evaluations
on two datasets, MKG-Y and DB15K. The experimental results indicate that using
the VSNS method to select neighbors results in higher-quality images that are
more relevant to the knowledge graph.

</details>


### [11] [Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs](https://arxiv.org/abs/2504.13644)
*Gabriel Freedman,Francesca Toni*

Main category: cs.AI

TL;DR: 这篇论文显示大型语言模型在概率推理方面存在缺陷，无法提供合理连贯的概率信念，并引入新数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 为了确保LLM在信息检索和自动决策系统中可靠、可解释和有效，需要实现对概率推理的忠实表示。

Method: 引入一个包含不确定真值声明的新数据集，并应用不确定性量化技术来评估LLM是否遵守概率推理的基本属性。

Result: 发现当前LLM缺乏提供合理和连贯概率信念的能力。

Conclusion: 强调了LLM在概率推理方面的局限性，暗示需要进一步改进。

Abstract: Advances in the general capabilities of large language models (LLMs) have led
to their use for information retrieval, and as components in automated decision
systems. A faithful representation of probabilistic reasoning in these models
may be essential to ensure trustworthy, explainable and effective performance
in these tasks. Despite previous work suggesting that LLMs can perform complex
reasoning and well-calibrated uncertainty quantification, we find that current
versions of this class of model lack the ability to provide rational and
coherent representations of probabilistic beliefs. To demonstrate this, we
introduce a novel dataset of claims with indeterminate truth values and apply a
number of well-established techniques for uncertainty quantification to measure
the ability of LLM's to adhere to fundamental properties of probabilistic
reasoning.

</details>


### [12] [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
*Yichen Wu,Xudong Pan,Geng Hong,Min Yang*

Main category: cs.AI

TL;DR: 本论文引入OpenDeception框架评估LLM的欺骗风险，发现意图和成功率高，需要加强监管。


<details>
  <summary>Details</summary>
Motivation: LLM能力提升和代理应用普及，欺骗风险需系统评估和有效监督。

Method: 构建OpenDeception框架，使用开放式场景数据集和代理模拟，检查内部推理过程评估欺骗意图和能力。

Result: 评估11个主流LLM，欺骗意图比例超80%，成功率超50%，能力强模型风险更高。

Conclusion: 强调需紧急解决LLM代理欺骗风险，并加强抑制欺骗行为的对齐努力。

Abstract: As the general capabilities of large language models (LLMs) improve and agent
applications become more widespread, the underlying deception risks urgently
require systematic evaluation and effective oversight. Unlike existing
evaluation which uses simulated games or presents limited choices, we introduce
OpenDeception, a novel deception evaluation framework with an open-ended
scenario dataset. OpenDeception jointly evaluates both the deception intention
and capabilities of LLM-based agents by inspecting their internal reasoning
process. Specifically, we construct five types of common use cases where LLMs
intensively interact with the user, each consisting of ten diverse, concrete
scenarios from the real world. To avoid ethical concerns and costs of high-risk
deceptive interactions with human testers, we propose to simulate the
multi-turn dialogue via agent simulation. Extensive evaluation of eleven
mainstream LLMs on OpenDeception highlights the urgent need to address
deception risks and security concerns in LLM-based agents: the deception
intention ratio across the models exceeds 80%, while the deception success rate
surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do
exhibit a higher risk of deception, which calls for more alignment efforts on
inhibiting deceptive behaviors.

</details>


### [13] [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
*Yang Yue,Zhiqi Chen,Rui Lu,Andrew Zhao,Zhaokai Wang,Yang Yue,Shiji Song,Gao Huang*

Main category: cs.AI

TL;DR: RLVR 并未带来根本性的新推理能力，而是通过偏向高奖励路径提高了采样效率，但缩小了推理能力边界。


<details>
  <summary>Details</summary>
Motivation: 重新审视 RLVR 是否能让大型语言模型持续自提升并获得新推理能力。

Method: 使用大 k 值下的 pass@k 指标测量不同模型和基准的推理能力边界，比较 RL 训练模型和基础模型，分析推理路径，并扩展到视觉推理任务和知识蒸馏。

Result: 发现 RL 没有引发新推理模式；基础模型在大 k 时表现更好；RL 提高了效率但缩小了边界；蒸馏能引入新知识。

Conclusion: 突显 RLVR 在提升 LLM 推理能力方面的局限性，需要重新思考 RL 训练的影响和开发更好范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning capabilities of LLMs,
particularly in mathematics and programming tasks. It is widely believed that
RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning
abilities that exceed corresponding base models' capacity. In this study,
however, we critically re-examines this assumption by measuring the
pass@\textit{k} metric with large values of \textit{k} to explore the reasoning
capability boundary of the models across a wide range of model families and
benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally
new reasoning patterns. While RL-trained models outperform their base models at
smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or
even higher pass@$k$ score compared to their RL counterparts at large $k$
values. The reasoning paths generated by RL-trained models are already included
in the base models' sampling distribution, suggesting that most reasoning
abilities manifested in RL-trained models are already obtained by base models.
Further analysis shows that RL training boosts the performance by biasing the
model's output distribution toward paths that are more likely to yield rewards,
therefore sampling correct responses more efficiently. But this also results in
a narrower reasoning capability boundary compared to base models. Similar
results are observed in visual reasoning tasks trained with RLVR. Moreover, we
find that distillation can genuinely introduce new knowledge into the model,
different from RLVR. These findings underscore a critical limitation of RLVR in
advancing LLM reasoning abilities which requires us to fundamentally rethink
the impact of RL training in reasoning LLMs and the need of a better paradigm.
Project Page: https://limit-of-RLVR.github.io

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Harmony: A Unified Framework for Modality Incremental Learning](https://arxiv.org/abs/2504.13218)
*Yaguang Song,Xiaoshan Yang,Dongmei Jiang,Yaowei Wang,Changsheng Xu*

Main category: cs.LG

TL;DR: 本文提出模态增量学习（MIL）范式和Harmony框架，实现跨不同模态的增量学习。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据可能来自全新的模态，而现有研究多关注一致模态的增量学习。

Method: 提出Harmony框架，使用自适应兼容特征调制和累积模态桥接来实现模态对齐和知识保留。

Result: 实验显示在MIL任务上显著优于现有增量学习方法。

Conclusion: 验证了开发统一模型在连续演化模态序列上的增量学习可行性，并证明了Harmony框架的有效性。

Abstract: Incremental learning aims to enable models to continuously acquire knowledge
from evolving data streams while preserving previously learned capabilities.
While current research predominantly focuses on unimodal incremental learning
and multimodal incremental learning where the modalities are consistent,
real-world scenarios often present data from entirely new modalities, posing
additional challenges. This paper investigates the feasibility of developing a
unified model capable of incremental learning across continuously evolving
modal sequences. To this end, we introduce a novel paradigm called Modality
Incremental Learning (MIL), where each learning stage involves data from
distinct modalities. To address this task, we propose a novel framework named
Harmony, designed to achieve modal alignment and knowledge retention, enabling
the model to reduce the modal discrepancy and learn from a sequence of distinct
modalities, ultimately completing tasks across multiple modalities within a
unified framework. Our approach introduces the adaptive compatible feature
modulation and cumulative modal bridging. Through constructing historical modal
features and performing modal knowledge accumulation and alignment, the
proposed components collaboratively bridge modal differences and maintain
knowledge retention, even with solely unimodal data available at each learning
phase.These components work in concert to establish effective modality
connections and maintain knowledge retention, even when only unimodal data is
available at each learning stage. Extensive experiments on the MIL task
demonstrate that our proposed method significantly outperforms existing
incremental learning methods, validating its effectiveness in MIL scenarios.

</details>


### [15] [Scaling Laws for Data-Efficient Visual Transfer Learning](https://arxiv.org/abs/2504.13219)
*Wenxuan Yang,Qingqu Wei,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.LG

TL;DR: 本文建立了视觉转移学习的數據高效缩放定律框架，探讨了数据有限条件下知识蒸馏的有效性，并通过经验验证了性能转折点。


<details>
  <summary>Details</summary>
Motivation: 当前视觉AI模型缩放定律主要关注大规模预训练，但忽略了数据受限的下游任务，本文旨在填补这一空白，研究数据有限时的缩放行为和知识蒸馏效能。

Method: 提出数据高效缩放定律框架和蒸馏边界理论，通过分析1K-1M样本范围的视觉任务，并对不同模型规模（2.5M至38M参数）进行经验验证。

Result: 在数据稀缺时，蒸馏模型优于非蒸馏模型；数据增加超过临界点时，非蒸馏模型表现更好，错误差异曲线显示转折点，证实理论预测。

Conclusion: 重新定义数据有限条件下的缩放定律，桥接大规模预训练与下游适应的知识鸿沟，有助于优化计算资源分配。

Abstract: Current scaling laws for visual AI models focus predominantly on large-scale
pretraining, leaving a critical gap in understanding how performance scales for
data-constrained downstream tasks. To address this limitation, this paper
establishes the first practical framework for data-efficient scaling laws in
visual transfer learning, addressing two fundamental questions: 1) How do
scaling behaviors shift when downstream tasks operate with limited data? 2)
What governs the efficacy of knowledge distillation under such constraints?
Through systematic analysis of vision tasks across data regimes (1K-1M
samples), we propose the distillation boundary theory, revealing a critical
turning point in distillation efficiency: 1) Distillation superiority: In
data-scarce conditions, distilled models significantly outperform their
non-distillation counterparts, efficiently leveraging inherited knowledge to
compensate for limited training samples. 2) Pre-training dominance: As
pre-training data increases beyond a critical threshold, non-distilled models
gradually surpass distilled versions, suggesting diminishing returns from
knowledge inheritance when sufficient task-specific data becomes available.
Empirical validation across various model scales (2.5M to 38M parameters) and
data volumes demonstrate these performance inflection points, with error
difference curves transitioning from positive to negative values at critical
data thresholds, confirming our theoretical predictions. This work redefines
scaling laws for data-limited regimes, bridging the knowledge gap between
large-scale pretraining and practical downstream adaptation, addressing a
critical barrier to understanding vision model scaling behaviors and optimizing
computational resource allocation.

</details>


### [16] [Modelling Mean-Field Games with Neural Ordinary Differential Equations](https://arxiv.org/abs/2504.13228)
*Anna C. M. Thöni,Yoram Bachrach,Tal Kachman*

Main category: cs.LG

TL;DR: 本论文结合均值场博弈理论和神经ODE，开发数据驱动模型处理复杂战略互动。


<details>
  <summary>Details</summary>
Motivation: 均值场博弈理论依赖模型近似，可能导致解决方案问题和偏差，动机是使其更数据驱动和鲁棒。

Method: 使用神经常微分方程结合均值场博弈理论，基于自动微分创建轻量级模型。

Result: 成功解决三种不同复杂度的均值场博弈，展示了模型灵活性、高效性和少量数据学习能力。

Conclusion: 模型减少模型依赖，提高了客观性和鲁棒性。

Abstract: Mean-field game theory relies on approximating games that would otherwise
have been intractable to model. While the games can be solved analytically via
the associated system of partial derivatives, this approach is not model-free,
can lead to the loss of the existence or uniqueness of solutions and may suffer
from modelling bias. To reduce the dependency between the model and the game,
we combine mean-field game theory with deep learning in the form of neural
ordinary differential equations. The resulting model is data-driven,
lightweight and can learn extensive strategic interactions that are hard to
capture using mean-field theory alone. In addition, the model is based on
automatic differentiation, making it more robust and objective than approaches
based on finite differences. We highlight the efficiency and flexibility of our
approach by solving three mean-field games that vary in their complexity,
observability and the presence of noise. Using these results, we show that the
model is flexible, lightweight and requires few observations to learn the
distribution underlying the data.

</details>


### [17] [PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning](https://arxiv.org/abs/2504.13229)
*Yifei Wang,Qi Liu,Fuli Min,Honghao Wang*

Main category: cs.LG

TL;DR: 这篇论文提出PSG-MAE框架，通过自监督学习提升深度神经网络在多导睡眠图信号分析中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在PSG数据分析中数据集有限、任务单一、转移能力和鲁棒性差的问题。

Method: 提出基于掩码自动编码器的PSG-MAE框架，使用自监督学习、互补掩码、多通道重建和通道间对比学习策略。

Result: 实验显示，睡眠分期准确率83.7%，阻塞性睡眠呼吸暂停检测准确率90.45%。

Conclusion: PSG-MAE框架有效捕获PSG信号时序和通道间信息，提升模型鲁棒性和广泛适用性。

Abstract: Polysomnography (PSG) signals are essential for studying sleep processes and
diagnosing sleep disorders. Analyzing PSG data through deep neural networks
(DNNs) for automated sleep monitoring has become increasingly feasible.
However, the limited availability of datasets for certain sleep events often
leads to DNNs focusing on a single task with a single-sourced training dataset.
As a result, these models struggle to transfer to new sleep events and lack
robustness when applied to new datasets. To address these challenges, we
propose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By
performing self-supervised learning on a large volume of unlabeled PSG data,
PSG-MAE develops a robust feature extraction network that can be broadly
applied to various sleep event monitoring tasks. Unlike conventional MAEs,
PSG-MAE generates complementary masks across PSG channels, integrates a
multichannel signal reconstruction method, and employs a self-supervised
inter-channel contrastive learning (ICCL) strategy. This approach enables the
encoder to capture temporal features from each channel while simultaneously
learning latent relationships between channels, thereby enhancing the
utilization of multichannel information. Experimental results show that PSG-MAE
effectively captures both temporal details and inter-channel information from
PSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with
downstream feature decomposition networks, it achieves an accuracy of 83.7% for
sleep staging and 90.45% for detecting obstructive sleep apnea, which
highlights the framework's robustness and broad applicability.

</details>


### [18] [Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms](https://arxiv.org/abs/2504.13233)
*Alireza Rafiei,Gari D. Clifford,Nasim Katebi*

Main category: cs.LG

TL;DR: 本论文提出Auto-FEDUS模型，通过生成多普勒超声波信号解决胎儿健康监测数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 缺乏广泛DUS数据集和数据不平衡限制了机器学习在胎儿健康评估中的应用。

Method: 使用基于扩张因果卷积的神经时间网络，将FECG信号映射到DUS波形。

Result: Auto-FEDUS在时域和频域优于传统模型，生成信号逼真，心率估计误差限为4.5次/分。

Conclusion: 此模型缓解数据 scarcity，提升DUS-based胎儿健康模型的训练和泛化能力。

Abstract: Fetal health monitoring through one-dimensional Doppler ultrasound (DUS)
signals offers a cost-effective and accessible approach that is increasingly
gaining interest. Despite its potential, the development of machine learning
based techniques to assess the health condition of mothers and fetuses using
DUS signals remains limited. This scarcity is primarily due to the lack of
extensive DUS datasets with a reliable reference for interpretation and data
imbalance across different gestational ages. In response, we introduce a novel
autoregressive generative model designed to map fetal electrocardiogram (FECG)
signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural
temporal network based on dilated causal convolutions that operate directly on
the waveform level, the model effectively captures both short and long-range
dependencies within the signals, preserving the integrity of generated data.
Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional
generative architectures across both time and frequency domain evaluations,
producing DUS signals that closely resemble the morphology of their real
counterparts. The realism of these synthesized signals was further gauged using
a quality assessment model, which classified all as good quality, and a heart
rate estimation model, which produced comparable results for generated and real
data, with a Bland-Altman limit of 4.5 beats per minute. This advancement
offers a promising solution for mitigating limited data availability and
enhancing the training of DUS-based fetal models, making them more effective
and generalizable.

</details>


### [19] [Non-Uniform Class-Wise Coreset Selection: Characterizing Category Difficulty for Data-Efficient Transfer Learning](https://arxiv.org/abs/2504.13234)
*Hanyu Zhang,Zhen Xing,Wenxuan Yang,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.LG

TL;DR: 本文提出NUCS方法，通过考虑类别级难度改善核心集选择，提高转移学习的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着转移学习模型和数据集的增长，需要高效的适应和存储优化。现有的方法忽略类别级特征，导致少数类 underrepresented。

Method: 提出非均匀类别-wise核心集选择框架NUCS，自动根据类别难度分配数据预算，并自适应选择样本。

Result: 在14个数据集上实验，NUCS在保留30%样本时匹配全数据准确率，并减少计算时间60%。

Conclusion: 强调表征类别难度的必要性，提供了一个鲁棒的数据高效解决方案。

Abstract: As transfer learning models and datasets grow larger, efficient adaptation
and storage optimization have become critical needs. Coreset selection
addresses these challenges by identifying and retaining the most informative
samples, constructing a compact subset for target domain training. However,
current methods primarily rely on instance-level difficulty assessments,
overlooking crucial category-level characteristics and consequently
under-representing minority classes. To overcome this limitation, we propose
Non-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that
integrates both class-level and instance-level criteria. NUCS automatically
allocates data selection budgets for each class based on intrinsic category
difficulty and adaptively selects samples within optimal difficulty ranges. By
explicitly incorporating category-specific insights, our approach achieves a
more balanced and representative coreset, addressing key shortcomings of prior
methods. Comprehensive theoretical analysis validates the rationale behind
adaptive budget allocation and sample selection, while extensive experiments
across 14 diverse datasets and model architectures demonstrate NUCS's
consistent improvements over state-of-the-art methods, achieving superior
accuracy and computational efficiency. Notably, on CIFAR100 and Food101, NUCS
matches full-data training accuracy while retaining just 30% of samples and
reducing computation time by 60%. Our work highlights the importance of
characterizing category difficulty in coreset selection, offering a robust and
data-efficient solution for transfer learning.

</details>


### [20] [NNTile: a machine learning framework capable of training extremely large GPT language models on a single node](https://arxiv.org/abs/2504.13236)
*Aleksandr Mikhalev,Aleksandr Katrutsa,Konstantin Sozykin,Ivan Oseledets*

Main category: cs.LG

TL;DR: 这项研究提出NNTile框架，用于在异构集群中训练大型深度神经网络。它基于StarPU库，实现任务-based并行，并自动调度任务到CPU和GPU上。性能通过大量数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 动机是减轻人类在决定计算位置和通信时机上的负担，转而由自动调度器（如贪婪启发式或AI-based软件）处理，提高效率。

Method: 方法是使用StarPU库实现任务-based并行，允许操作在任何CPU核心或GPU设备上执行，根据自动调度决策。

Result: 结果展示了该工具在训练大型语言模型时的性能，通过广泛的数值实验证明其有效性。

Conclusion: 结论是这种自动调度方法有效地提高了神经网络训练的效率和灵活性。

Abstract: This study presents an NNTile framework for training large deep neural
networks in heterogeneous clusters. The NNTile is based on a StarPU library,
which implements task-based parallelism and schedules all provided tasks onto
all available processing units (CPUs and GPUs). It means that a particular
operation, necessary to train a large neural network, can be performed on any
of the CPU cores or GPU devices, depending on automatic scheduling decisions.
Such an approach shifts the burden of deciding where to compute and when to
communicate from a human being to an automatic decision maker, whether a simple
greedy heuristic or a complex AI-based software. The performance of the
presented tool for training large language models is demonstrated in extensive
numerical experiments.

</details>


### [21] [Recursive Deep Inverse Reinforcement Learning](https://arxiv.org/abs/2504.13241)
*Paul Ghanem,Michael Potter,Owen Howell,Pau Closas,Alireza Ramezani,Deniz Erdogmus,Robert Platt,Tales Imbiriba*

Main category: cs.LG

TL;DR: 这篇论文提出了一种在线递归深度逆强化学习（RDIRL）方法，用于快速推断对手的目标和成本函数，并在基准任务中表现出优于现有IRL算法。


<details>
  <summary>Details</summary>
Motivation: 推断对手目标对网络安全、军事和策略游戏等领域的反规划和非合作多代理系统至关重要，但现有深度IRL方法通常是离线的，需要大批量数据和一阶更新，限制了实时应用。

Method: 提出RDIRL方法，通过最小化Guided Cost Learning（GCL）目标的上界，使用类似于扩展卡尔曼滤波器的顺序二阶牛顿更新来恢复对手的成本函数。

Result: RDIRL能够在标准和对抗性基准任务中成功恢复专家代理的成本和奖励函数，并优于多个领先的IRL算法。

Conclusion: 该方法实现了快速收敛的在线学习，提高了在实时场景中推断对手目标的适用性。

Abstract: Inferring an adversary's goals from exhibited behavior is crucial for
counterplanning and non-cooperative multi-agent systems in domains like
cybersecurity, military, and strategy games. Deep Inverse Reinforcement
Learning (IRL) methods based on maximum entropy principles show promise in
recovering adversaries' goals but are typically offline, require large batch
sizes with gradient descent, and rely on first-order updates, limiting their
applicability in real-time scenarios. We propose an online Recursive Deep
Inverse Reinforcement Learning (RDIRL) approach to recover the cost function
governing the adversary actions and goals. Specifically, we minimize an upper
bound on the standard Guided Cost Learning (GCL) objective using sequential
second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading
to a fast (in terms of convergence) learning algorithm. We demonstrate that
RDIRL is able to recover cost and reward functions of expert agents in standard
and adversarial benchmark tasks. Experiments on benchmark tasks show that our
proposed approach outperforms several leading IRL algorithms.

</details>


### [22] [Graph Learning at Scale: Characterizing and Optimizing Pre-Propagation GNNs](https://arxiv.org/abs/2504.13266)
*Zichao Yue,Chenhui Deng,Zhiru Zhang*

Main category: cs.LG

TL;DR: 本文分析了PP-GNNs，与基于采样的GNNs比较，识别瓶颈并提出优化方案，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 探索PP-GNNs的实际优势和系统优化，这些模型理论上解决了邻居爆炸问题，但实践上未充分研究。

Method: 全面表征PP-GNNs，与图采样方法比较，并提出优化的数据加载方案和定制训练方法。

Result: PP-GNNs实现可比准确性；优化后，训练吞吐量平均提升15倍，相比采样GNNs最高可达100倍加速。

Conclusion: 通过优化，PP-GNNs在大型图训练中提供更好的可伸缩性和效率。

Abstract: Graph neural networks (GNNs) are widely used for learning node embeddings in
graphs, typically adopting a message-passing scheme. This approach, however,
leads to the neighbor explosion problem, with exponentially growing
computational and memory demands as layers increase. Graph sampling has become
the predominant method for scaling GNNs to large graphs, mitigating but not
fully solving the issue. Pre-propagation GNNs (PP-GNNs) represent a new class
of models that decouple feature propagation from training through
pre-processing, addressing neighbor explosion in theory. Yet, their practical
advantages and system-level optimizations remain underexplored. This paper
provides a comprehensive characterization of PP-GNNs, comparing them with
graph-sampling-based methods in training efficiency, scalability, and accuracy.
While PP-GNNs achieve comparable accuracy, we identify data loading as the key
bottleneck for training efficiency and input expansion as a major scalability
challenge. To address these issues, we propose optimized data loading schemes
and tailored training methods that improve PP-GNN training throughput by an
average of 15$\times$ over the PP-GNN baselines, with speedup of up to 2 orders
of magnitude compared to sampling-based GNNs on large graph benchmarks. Our
implementation is publicly available at
https://github.com/cornell-zhang/preprop-gnn.

</details>


### [23] [Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model](https://arxiv.org/abs/2504.13292)
*Zhiwei Xu,Zhiyu Ni,Yixin Wang,Wei Hu*

Main category: cs.LG

TL;DR: 本论文提出GrokTransfer方法，通过转移嵌入加速神经网络泛化，消除Grokking中的延迟现象。


<details>
  <summary>Details</summary>
Motivation: 解决Grokking延迟泛化的不可预测性和低效率问题，实现直接泛化。

Method: 训练较小弱模型到非最优性能，提取其输入嵌入初始化目标强模型。

Result: 在XOR任务上证明无延迟泛化，并在多种任务中实证验证其有效性。

Conclusion: GrokTransfer能有效加速泛化和优化训练动态。

Abstract: ''Grokking'' is a phenomenon where a neural network first memorizes training
data and generalizes poorly, but then suddenly transitions to near-perfect
generalization after prolonged training. While intriguing, this delayed
generalization phenomenon compromises predictability and efficiency. Ideally,
models should generalize directly without delay. To this end, this paper
proposes GrokTransfer, a simple and principled method for accelerating grokking
in training neural networks, based on the key observation that data embedding
plays a crucial role in determining whether generalization is delayed.
GrokTransfer first trains a smaller, weaker model to reach a nontrivial (but
far from optimal) test performance. Then, the learned input embedding from this
weaker model is extracted and used to initialize the embedding in the target,
stronger model. We rigorously prove that, on a synthetic XOR task where delayed
generalization always occurs in normal training, GrokTransfer enables the
target model to generalize directly without delay. Moreover, we demonstrate
that, across empirical studies of different tasks, GrokTransfer effectively
reshapes the training dynamics and eliminates delayed generalization, for both
fully-connected neural networks and Transformers.

</details>


### [24] [Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis](https://arxiv.org/abs/2504.13296)
*Ganesh Sundaram,Jonas Ulmen,Daniel Görges*

Main category: cs.LG

TL;DR: 本篇论文提出一种组件感知剪枝策略，用于多组件神经架构，保持功能完整性，实现更高稀疏度和更少性能损失。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在资源受限环境下的部署挑战，以及在剪枝多组件神经架构时可能破坏网络完整性的风险。

Method: 引入组件感知剪枝策略，通过扩展依赖图隔离单个组件和组件间流，创建更小、针对性的剪枝组。

Result: 在控制任务上演示，该方法实现了更大稀疏度和减少性能退化。

Conclusion: 该方法为高效优化复杂多组件深度神经网络开辟了新路径。

Abstract: Deep neural networks (DNNs) deliver outstanding performance, but their
complexity often prohibits deployment in resource-constrained settings.
Comprehensive structured pruning frameworks based on parameter dependency
analysis reduce model size with specific regard to computational performance.
When applying them to Multi-Component Neural Architectures (MCNAs), they risk
network integrity by removing large parameter groups. We introduce a
component-aware pruning strategy, extending dependency graphs to isolate
individual components and inter-component flows. This creates smaller, targeted
pruning groups that conserve functional integrity. Demonstrated effectively on
a control task, our approach achieves greater sparsity and reduced performance
degradation, opening a path for optimizing complex, multi-component DNNs
efficiently.

</details>


### [25] [A Model-Based Approach to Imitation Learning through Multi-Step Predictions](https://arxiv.org/abs/2504.13413)
*Haldun Balim,Yang Hu,Yuyang Zhang,Na Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型预测控制的模仿学习框架，使用多步状态预测提升鲁棒性，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法存在累积错误、泛化能力有限、错误修正困难和分布偏移问题。

Method: 引入新型模型-based模仿学习框架，通过多步状态预测整合预测建模。

Result: 在数值基准测试中优于传统行为克隆，对分布偏移和测量噪声具有更强鲁棒性。

Conclusion: 提供了样本复杂度、错误界和收敛性的理论保证。

Abstract: Imitation learning is a widely used approach for training agents to replicate
expert behavior in complex decision-making tasks. However, existing methods
often struggle with compounding errors and limited generalization, due to the
inherent challenge of error correction and the distribution shift between
training and deployment. In this paper, we present a novel model-based
imitation learning framework inspired by model predictive control, which
addresses these limitations by integrating predictive modeling through
multi-step state predictions. Our method outperforms traditional behavior
cloning numerical benchmarks, demonstrating superior robustness to distribution
shift and measurement noise both in available data and during execution.
Furthermore, we provide theoretical guarantees on the sample complexity and
error bounds of our method, offering insights into its convergence properties.

</details>


### [26] [Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR](https://arxiv.org/abs/2504.13302)
*Ibrahim Emirahmetoglu,David E. Stewart*

Main category: cs.LG

TL;DR: 本文提出改进Hessian-free优化，使用LSMR方法和新小批量选择算法，加速深度自编码器训练并提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 通过减少训练数据量和改进优化方法，加速Hessian-free在深度自编码器训练中的应用。

Method: 采用LSMR代替共轭梯度算法，结合改进预处理条件器，并引入基于方差估计和验证集性能的动态小批量选择算法。

Result: 实验显示，该随机Hessian-free优化方法实现快速训练，并改善深度自编码器的泛化错误。

Conclusion: 该方法有效提升了训练效率和模型泛化能力。

Abstract: Hessian-free (HF) optimization has been shown to effectively train deep
autoencoders (Martens, 2010). In this paper, we aim to accelerate HF training
of autoencoders by reducing the amount of data used in training. HF utilizes
the conjugate gradient algorithm to estimate update directions. Instead, we
propose using the LSMR method, which is known for effectively solving large
sparse linear systems. We also incorporate Chapelle & Erhan (2011)'s improved
preconditioner for HF optimization. In addition, we introduce a new mini-batch
selection algorithm to mitigate overfitting. Our algorithm starts with a small
subset of the training data and gradually increases the mini-batch size based
on (i) variance estimates obtained during the computation of a mini-batch
gradient (Byrd et al., 2012) and (ii) the relative decrease in objective value
for the validation data. Our experimental results demonstrate that our
stochastic Hessian-free optimization, using the LSMR method and the new sample
selection algorithm, leads to rapid training of deep autoencoders with improved
generalization error.

</details>


### [27] [Wearable-Derived Behavioral and Physiological Biomarkers for Classifying Unipolar and Bipolar Depression Severity](https://arxiv.org/abs/2504.13331)
*Yassine Ouzar,Clémence Nineuil,Fouad Boutaleb,Emery Pierson,Ali Amad,Mohamed Daoudi*

Main category: cs.LG

TL;DR: 本研究使用可穿戴设备预测抑郁症亚型（单相和双相），引入CALYPSO数据集，通过生理和行为信号进行非侵入式检测，并使用机器学习方法建立基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用二元分类，无法捕捉抑郁症的异质性，因此旨在识别独特生物标志物，提高诊断精度和个性化治疗。

Method: 引入CALYPSO数据集，包含血容量脉搏、电皮肤活动、体温和三轴加速度信号，使用标准特征和机器学习方法进行基准测试。

Result: 初步结果显示，物理活动特征准确率达96.77%，温度特征达93.55%，在区分抑郁症亚型中表现突出。

Conclusion: 这些发现强调生理和行为监测可改善抑郁症分类，推动更个性化的临床干预。

Abstract: Depression is a complex mental disorder characterized by a diverse range of
observable and measurable indicators that go beyond traditional subjective
assessments. Recent research has increasingly focused on objective, passive,
and continuous monitoring using wearable devices to gain more precise insights
into the physiological and behavioral aspects of depression. However, most
existing studies primarily distinguish between healthy and depressed
individuals, adopting a binary classification that fails to capture the
heterogeneity of depressive disorders. In this study, we leverage wearable
devices to predict depression subtypes-specifically unipolar and bipolar
depression-aiming to identify distinctive biomarkers that could enhance
diagnostic precision and support personalized treatment strategies. To this
end, we introduce the CALYPSO dataset, designed for non-invasive detection of
depression subtypes and symptomatology through physiological and behavioral
signals, including blood volume pulse, electrodermal activity, body
temperature, and three-axis acceleration. Additionally, we establish a
benchmark on the dataset using well-known features and standard machine
learning methods. Preliminary results indicate that features related to
physical activity, extracted from accelerometer data, are the most effective in
distinguishing between unipolar and bipolar depression, achieving an accuracy
of $96.77\%$. Temperature-based features also showed high discriminative power,
reaching an accuracy of $93.55\%$. These findings highlight the potential of
physiological and behavioral monitoring for improving the classification of
depressive subtypes, paving the way for more tailored clinical interventions.

</details>


### [28] [Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator](https://arxiv.org/abs/2504.13529)
*Zinuo You,John Cartlidge,Karen Elliott,Menghan Ge,Daniel Gold*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯优化框架，用于在有限观察下优化黑箱投资组合管理模型。


<details>
  <summary>Details</summary>
Motivation: 现有黑箱模型性能易变、评估成本高，优化性能同时控制风险是关键挑战。

Method: 提出自适应权重Lagrangian估计器，兼顾最大化性能和最小化观察方差。

Result: 实验在五个回测设置和三个黑箱模型上显示方法优越性，消融研究验证了估计器的有效性。

Conclusion: 该框架提高了黑箱模型优化性能并降低了风险。

Abstract: Existing portfolio management approaches are often black-box models due to
safety and commercial issues in the industry. However, their performance can
vary considerably whenever market conditions or internal trading strategies
change. Furthermore, evaluating these non-transparent systems is expensive,
where certain budgets limit observations of the systems. Therefore, optimizing
performance while controlling the potential risk of these financial systems has
become a critical challenge. This work presents a novel Bayesian optimization
framework to optimize black-box portfolio management models under limited
observations. In conventional Bayesian optimization settings, the objective
function is to maximize the expectation of performance metrics. However, simply
maximizing performance expectations leads to erratic optimization trajectories,
which exacerbate risk accumulation in portfolio management. Meanwhile, this can
lead to misalignment between the target distribution and the actual
distribution of the black-box model. To mitigate this problem, we propose an
adaptive weight Lagrangian estimator considering dual objective, which
incorporates maximizing model performance and minimizing variance of model
observations. Extensive experiments demonstrate the superiority of our approach
over five backtest settings with three black-box stock portfolio management
models. Ablation studies further verify the effectiveness of the proposed
estimator.

</details>


### [29] [Denoising and Reconstruction of Nonlinear Dynamics using Truncated Reservoir Computing](https://arxiv.org/abs/2504.13355)
*Omid Sedehi,Manish Yadav,Merten Stender,Sebastian Oberst*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的Reservoir Computing (RC) 方法，用于噪声过滤和重建非线性动力学，通过优化超参数提高了性能，并在Lorenz吸引子和AdEx系统中验证。


<details>
  <summary>Details</summary>
Motivation: 分布式物理系统的测量数据往往稀疏且噪声干扰大，基本动力学方程不可用，RC技术在噪声环境下的潜力未被充分探索。

Method: 提出新型RC方法，包括学习协议和超参数优化（如泄漏率、光谱半径等），通过截断冗余节点和边，在Lorenz吸引子和AdEx系统中测试噪声强度、频率和参数变化。

Result: RC显示出良好的去噪性能和泛化能力，与EKF相比在低信噪比和高频范围内具有竞争力。

Conclusion: RC框架通过优化提供有效的噪声过滤和动力学重建，展示了良好的泛化行为。

Abstract: Measurements acquired from distributed physical systems are often sparse and
noisy. Therefore, signal processing and system identification tools are
required to mitigate noise effects and reconstruct unobserved dynamics from
limited sensor data. However, this process is particularly challenging because
the fundamental equations governing the dynamics are largely unavailable in
practice. Reservoir Computing (RC) techniques have shown promise in efficiently
simulating dynamical systems through an unstructured and efficient computation
graph comprising a set of neurons with random connectivity. However, the
potential of RC to operate in noisy regimes and distinguish noise from the
primary dynamics of the system has not been fully explored. This paper presents
a novel RC method for noise filtering and reconstructing nonlinear dynamics,
offering a novel learning protocol associated with hyperparameter optimization.
The performance of the RC in terms of noise intensity, noise frequency content,
and drastic shifts in dynamical parameters are studied in two illustrative
examples involving the nonlinear dynamics of the Lorenz attractor and adaptive
exponential integrate-and-fire system (AdEx). It is shown that the denoising
performance improves via truncating redundant nodes and edges of the computing
reservoir, as well as properly optimizing the hyperparameters, e.g., the
leakage rate, the spectral radius, the input connectivity, and the ridge
regression parameter. Furthermore, the presented framework shows good
generalization behavior when tested for reconstructing unseen attractors from
the bifurcation diagram. Compared to the Extended Kalman Filter (EKF), the
presented RC framework yields competitive accuracy at low signal-to-noise
ratios (SNRs) and high-frequency ranges.

</details>


### [30] [An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning](https://arxiv.org/abs/2504.13368)
*Haoran Xu,Shuozhe Li,Harshit Sikchi,Scott Niekum,Amy Zhang*

Main category: cs.LG

TL;DR: IDRL 是一种新的迭代双重强化学习方法，通过优化访问分布比率来提升离线 RL 性能。


<details>
  <summary>Details</summary>
Motivation: 受实验启发，发现使用离线数据集加专家数据集训练鉴别器后进行加权行为克隆效果良好，但现有 Dual-RL 方法未正确估计访问分布比率，因此 IDRL 提出无额外专家数据集的迭代修正方法。

Method: IDRL 在每次迭代中，使用前一迭代学得的比率移除权重为零的次优转移，并对剩余子数据集运行 Dual-RL，形成改进的访问分布比率课程。

Result: 在 D4RL 和受损演示等离线数据集上验证，IDRL 在性能和稳定性上优于 Primal-RL 和 Dual-RL 基线。

Conclusion: IDRL 是一种有效的方法，能够显著提升离线强化学习的性能和稳定性。

Abstract: We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that
takes an optimal discriminator-weighted imitation view of solving RL. Our
method is motivated by a simple experiment in which we find training a
discriminator using the offline dataset plus an additional expert dataset and
then performing discriminator-weighted behavior cloning gives strong results on
various types of datasets. That optimal discriminator weight is quite similar
to the learned visitation distribution ratio in Dual-RL, however, we find that
current Dual-RL methods do not correctly estimate that ratio. In IDRL, we
propose a correction method to iteratively approach the optimal visitation
distribution ratio in the offline dataset given no addtional expert dataset.
During each iteration, IDRL removes zero-weight suboptimal transitions using
the learned ratio from the previous iteration and runs Dual-RL on the remaining
subdataset. This can be seen as replacing the behavior visitation distribution
with the optimized visitation distribution from the previous iteration, which
theoretically gives a curriculum of improved visitation distribution ratios
that are closer to the optimal discriminator weight. We verify the
effectiveness of IDRL on various kinds of offline datasets, including D4RL
datasets and more realistic corrupted demonstrations. IDRL beats strong
Primal-RL and Dual-RL baselines in terms of both performance and stability, on
all datasets.

</details>


### [31] [A mean teacher algorithm for unlearning of language models](https://arxiv.org/abs/2504.13388)
*Yegor Klochkov*

Main category: cs.LG

TL;DR: 这篇论文使用均值教师算法和新的NLUL损失函数来改进语言模型的遗忘，减少记忆而不损失实用性，并在MUSE基准上显示改进。


<details>
  <summary>Details</summary>
Motivation: 语言模型遗忘的目标是减少对选定文本实例的记忆，同时保留模型的一般能力。尽管有各种方法，但在大规模数据集上减少记忆而不显著降低模型效用仍然具有挑战性。

Method: 调查均值教师算法（一种来自持续学习文献的简单近端优化方法），它逐渐修改教师模型。我们展示了均值教师可以近似一个缓慢的自然梯度下降（NGD）的轨迹，并引入新的NLUL损失来避免梯度消失问题。

Result: 均值教师和NLUL的组合在MUSE基准上改善了一些指标。

Conclusion: 均值教师算法结合NLUL损失可以有效改进语言模型的遗忘过程，减少记忆而不显著降低模型效用。

Abstract: One of the goals of language model unlearning is to reduce memorization of
selected text instances while retaining the model's general abilities. Despite
various proposed methods, reducing memorization of large datasets without
noticeable degradation in model utility remains challenging. In this paper, we
investigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple
proximal optimization method from continual learning literature that gradually
modifies the teacher model. We show that the mean teacher can approximate a
trajectory of a slow natural gradient descent (NGD), which inherently seeks
low-curvature updates that are less likely to degrade the model utility. While
slow NGD can suffer from vanishing gradients, we introduce a new unlearning
loss called "negative log-unlikelihood" (NLUL) that avoids this problem. We
show that the combination of mean teacher and NLUL improves some metrics on the
MUSE benchmarks (Shi et al., 2024).

</details>


### [32] [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
*Saksham Rastogi,Pratyush Maini,Danish Pruthi*

Main category: cs.LG

TL;DR: 本文介绍了STAMP框架，用于检测数据集是否被用于大型语言模型的预训练，通过水印重述和统计测试。


<details>
  <summary>Details</summary>
Motivation: 数据创建者和基准测试策展人担心他们的专有数据未经授权或许可就被用于模型训练。

Method: 生成多个带有唯一密钥水印的重述版本，其中一个公开发布，其他保持私有，然后使用配对统计测试比较模型似然度来证明成员资格。

Result: 成功检测了在训练数据中仅出现一次且占总标记少于0.001%的基准测试污染，优于基线方法，并验证了数据语义和效用的保留，还应用于真实场景。

Conclusion: STAMP是一个有效的框架，用于检测数据集在LLM预训练语料中的成员资格。

Abstract: Given how large parts of publicly available text are crawled to pretrain
large language models (LLMs), data creators increasingly worry about the
inclusion of their proprietary data for model training without attribution or
licensing. Their concerns are also shared by benchmark curators whose test-sets
might be compromised. In this paper, we present STAMP, a framework for
detecting dataset membership-i.e., determining the inclusion of a dataset in
the pretraining corpora of LLMs. Given an original piece of content, our
proposal involves first generating multiple rephrases, each embedding a
watermark with a unique secret key. One version is to be released publicly,
while others are to be kept private. Subsequently, creators can compare model
likelihoods between public and private versions using paired statistical tests
to prove membership. We show that our framework can successfully detect
contamination across four benchmarks which appear only once in the training
data and constitute less than 0.001% of the total tokens, outperforming several
contamination detection and dataset inference baselines. We verify that STAMP
preserves both the semantic meaning and the utility of the original data in
comparing different models. We apply STAMP to two real-world scenarios to
confirm the inclusion of paper abstracts and blog articles in the pretraining
corpora.

</details>


### [33] [Equilibrium Conserving Neural Operators for Super-Resolution Learning](https://arxiv.org/abs/2504.13422)
*Vivek Oommen,Andreas E. Robertson,Daniel Diaz,Coleman Alleman,Zhen Zhang,Anthony D. Rollett,George E. Karniadakis,Rémi Dingreville*

Main category: cs.LG

TL;DR: 本论文引入ECO架构，用于固体力学问题的超分辨率学习框架，可用低分辨率数据训练高分辨率神经网络，减少数据成本。


<details>
  <summary>Details</summary>
Motivation: 神经代理求解器需大量高分辨率数据，本文旨在打破这一限制，提高效率。

Method: 开发Equilibrium Conserving Operator (ECO)架构，将物理守恒定律嵌入网络，使用低分辨率数据训练。

Result: 在孔隙矩阵和多晶材料示例中，数据成本降低两个数量级，并可推广到其他物理问题。

Conclusion: ECO框架提供资源高效的代理建模方法，消除对高保真数据的需求。

Abstract: Neural surrogate solvers can estimate solutions to partial differential
equations in physical problems more efficiently than standard numerical
methods, but require extensive high-resolution training data. In this paper, we
break this limitation; we introduce a framework for super-resolution learning
in solid mechanics problems. Our approach allows one to train a high-resolution
neural network using only low-resolution data. Our Equilibrium Conserving
Operator (ECO) architecture embeds known physics directly into the network to
make up for missing high-resolution information during training. We evaluate
this ECO-based super-resolution framework that strongly enforces
conservation-laws in the predicted solutions on two working examples: embedded
pores in a homogenized matrix and randomly textured polycrystalline materials.
ECO eliminates the reliance on high-fidelity data and reduces the upfront cost
of data collection by two orders of magnitude, offering a robust pathway for
resource-efficient surrogate modeling in materials modeling. ECO is readily
generalizable to other physics-based problems.

</details>


### [34] [Simplifying Graph Convolutional Networks with Redundancy-Free Neighbors](https://arxiv.org/abs/2504.13426)
*Jielong LuZhihao Wu,Zhiling Cai,Yueyang Pi,Shiping Wang*

Main category: cs.LG

TL;DR: 这篇论文通过分析GCN的消息传递机制，识别出过聚合是过平滑现象的根本原因。


<details>
  <summary>Details</summary>
Motivation: 动机是解决GCN浅层架构的限制，以及现有深度GCN改进方法效果有限的问题。

Method: 方法是分析GCN的内在消息传递机制，并识别出过聚合现象，即高阶邻居的消息必须通过低阶邻居传递导致的冗余。

Result: 结果显示过聚合不仅引入显著冗余，而且是过平滑的根本原因。

Conclusion: 结论是过聚合是GCN中需要解决的关键问题。

Abstract: In recent years, Graph Convolutional Networks (GCNs) have gained popularity
for their exceptional ability to process graph-structured data. Existing
GCN-based approaches typically employ a shallow model architecture due to the
over-smoothing phenomenon. Current approaches to mitigating over-smoothing
primarily involve adding supplementary components to GCN architectures, such as
residual connections and random edge-dropping strategies. However, these
improvements toward deep GCNs have achieved only limited success. In this work,
we analyze the intrinsic message passing mechanism of GCNs and identify a
critical issue: messages originating from high-order neighbors must traverse
through low-order neighbors to reach the target node. This repeated reliance on
low-order neighbors leads to redundant information aggregation, a phenomenon we
term over-aggregation. Our analysis demonstrates that over-aggregation not only
introduces significant redundancy but also serves as the fundamental cause of
over-smoothing in GCNs.

</details>


### [35] [Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs](https://arxiv.org/abs/2504.13429)
*Shenzhi Yang,Bin Liang,An Liu,Lin Gui,Xingkai Yao,Xiaofang Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出NODESAFE方法，改善图神经网络（GNNs）在节点级别检测异常分布（OOD）数据的性能，解决了GNNSAFE的局限性。


<details>
  <summary>Details</summary>
Motivation: 图形在现实应用中具有关键作用且安全需求高，亟需提升GNNs检测OOD数据的能力；研究发现GNNSAFE的分数聚合易受极端值影响，限制了检测准确性。

Method: 提出NODESAFE，通过添加两个优化项，使负能量分数有界并缓解logit偏移，减少极端分数的生成。

Result: 实验结果显示，NODESAFE显著提升了GNNs的OOD检测能力，例如在结构操纵诱导的OOD数据检测中，FPR95指标在无（有）OOD数据暴露场景下分别降低了28.4%（22.7%）。

Conclusion: 该方法极大地提高了GNNs在节点级别OOD数据检测的准确性。

Abstract: Given the critical role of graphs in real-world applications and their
high-security requirements, improving the ability of graph neural networks
(GNNs) to detect out-of-distribution (OOD) data is an urgent research problem.
The recent work GNNSAFE proposes a framework based on the aggregation of
negative energy scores that significantly improves the performance of GNNs to
detect node-level OOD data. However, our study finds that score aggregation
among nodes is susceptible to extreme values due to the unboundedness of the
negative energy scores and logit shifts, which severely limits the accuracy of
GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE:
reducing the generation of extreme scores of nodes by adding two optimization
terms that make the negative energy scores bounded and mitigate the logit
shift. Experimental results show that our approach dramatically improves the
ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD
data induced by Structure Manipulation, the metric of FPR95 (lower is better)
in scenarios without (with) OOD data exposure are reduced from the current SOTA
by 28.4% (22.7%).

</details>


### [36] [Using Machine Learning and Neural Networks to Analyze and Predict Chaos in Multi-Pendulum and Chaotic Systems](https://arxiv.org/abs/2504.13453)
*Vasista Ramachandruni,Sai Hruday Reddy Nara,Geo Lalu,Sabrina Yang,Mohit Ramesh Kumar,Aarjav Jain,Pratham Mehta,Hankyu Koo,Jason Damonte,Marx Akl*

Main category: cs.LG

TL;DR: 本研究评估了10种机器学习模型预测多摆混沌系统的性能，发现LSTM和GRU在不同场景下最佳。


<details>
  <summary>Details</summary>
Motivation: 混沌系统广泛存在于天气、疾病爆发和金融市场，预测它们对社会有益。

Method: 使用ODE-RK4生成数据，采用滑动窗口和时间步进方法，评估RMSE、R^2和Lyapunov指数。

Result: 双摆时LSTM最佳；三摆时滑动窗口下VRNN最佳，时间步进下GRU最佳，有摩擦时LSTM最佳。

Conclusion: LSTM和GRU等模型适合预测混沌系统，提供准确预测。

Abstract: A chaotic system is a highly volatile system characterized by its sensitive
dependence on initial conditions and outside factors. Chaotic systems are
prevalent throughout the world today: in weather patterns, disease outbreaks,
and even financial markets. Chaotic systems are seen in every field of science
and humanities, so being able to predict these systems is greatly beneficial to
society. In this study, we evaluate 10 different machine learning models and
neural networks [1] based on Root Mean Squared Error (RMSE) and R^2 values for
their ability to predict one of these systems, the multi-pendulum. We begin by
generating synthetic data representing the angles of the pendulum over time
using the Runge Kutta Method for solving 4th Order Differential Equations
(ODE-RK4) [2]. At first, we used the single-step sliding window approach,
predicting the 50st step after training for steps 0-49 and so forth. However,
to more accurately cover chaotic motion and behavior in these systems, we
transitioned to a time-step based approach. Here, we trained the model/network
on many initial angles and tested it on a completely new set of initial angles,
or 'in-between' to capture chaotic motion to its fullest extent. We also
evaluated the stability of the system using Lyapunov exponents. We concluded
that for a double pendulum, the best model was the Long Short Term Memory
Network (LSTM)[3] for the sliding window and time step approaches in both
friction and frictionless scenarios. For triple pendulum, the Vanilla Recurrent
Neural Network (VRNN)[4] was the best for the sliding window and Gated
Recurrent Network (GRU) [5] was the best for the time step approach, but for
friction, LSTM was the best.

</details>


### [37] [Stratify: Rethinking Federated Learning for Non-IID Data through Balanced Sampling](https://arxiv.org/abs/2504.13462)
*Hui Yeok Wong,Chee Kau Lim,Chee Seng Chan*

Main category: cs.LG

TL;DR: Stratify 是一种新联邦学习框架，通过分层采样和客户端选择处理非 IID 数据，性能与 IID 基线相当，并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非 IID 数据上的挑战，现有的方法仅对症状进行调整，而未解决根本问题。

Method: 引入 Stratify 框架，包括分层标签调度、基于标签的客户端选择、细粒度更新方案和使用同态加密的安全协议。

Result: 在 MNIST、CIFAR-10 等数据集上，性能与 IID 基线相当，收敛更快，并减少客户端计算。

Conclusion: 证明 Stratify 在实际联邦学习场景中具有实用有效性。

Abstract: Federated Learning (FL) on non-independently and identically distributed
(non-IID) data remains a critical challenge, as existing approaches struggle
with severe data heterogeneity. Current methods primarily address symptoms of
non-IID by applying incremental adjustments to Federated Averaging (FedAvg),
rather than directly resolving its inherent design limitations. Consequently,
performance significantly deteriorates under highly heterogeneous conditions,
as the fundamental issue of imbalanced exposure to diverse class and feature
distributions remains unresolved. This paper introduces Stratify, a novel FL
framework designed to systematically manage class and feature distributions
throughout training, effectively tackling the root cause of non-IID challenges.
Inspired by classical stratified sampling, our approach employs a Stratified
Label Schedule (SLS) to ensure balanced exposure across labels, significantly
reducing bias and variance in aggregated gradients. Complementing SLS, we
propose a label-aware client selection strategy, restricting participation
exclusively to clients possessing data relevant to scheduled labels.
Additionally, Stratify incorporates a fine-grained, high-frequency update
scheme, accelerating convergence and further mitigating data heterogeneity. To
uphold privacy, we implement a secure client selection protocol leveraging
homomorphic encryption, enabling precise global label statistics without
disclosing sensitive client information. Extensive evaluations on MNIST,
CIFAR-10, CIFAR-100, Tiny-ImageNet, COVTYPE, PACS, and Digits-DG demonstrate
that Stratify attains performance comparable to IID baselines, accelerates
convergence, and reduces client-side computation compared to state-of-the-art
methods, underscoring its practical effectiveness in realistic federated
learning scenarios.

</details>


### [38] [Are you SURE? Enhancing Multimodal Pretraining with Missing Modalities through Uncertainty Estimation](https://arxiv.org/abs/2504.13465)
*Duy A. Nguyen,Quan Huu Do,Khoa D. Doan,Minh N. Do*

Main category: cs.LG

TL;DR: 这篇论文介绍了SURE框架，通过重建缺失模态和估计不确定性，改进多模态学习在不完整数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习依赖所有模态可用，但实际应用中数据常不完整，现有的重建方法忽略了不确定性问题。

Method: SURE框架使用潜空间重建、不确定性估计、Pearson相关性损失函数，以及在深度网络中的统计误差传播。

Result: 在情感分析、流派分类和动作识别任务中，SURE实现了最先进性能，即使数据不完整也能保持鲁棒性。

Conclusion: SURE提升了预训练多模态模型的性能和可解释性，通过可靠的不确定性估计处理缺失模态问题。

Abstract: Multimodal learning has demonstrated incredible successes by integrating
diverse data sources, yet it often relies on the availability of all modalities
- an assumption that rarely holds in real-world applications. Pretrained
multimodal models, while effective, struggle when confronted with small-scale
and incomplete datasets (i.e., missing modalities), limiting their practical
applicability. Previous studies on reconstructing missing modalities have
overlooked the reconstruction's potential unreliability, which could compromise
the quality of the final outputs. We present SURE (Scalable Uncertainty and
Reconstruction Estimation), a novel framework that extends the capabilities of
pretrained multimodal models by introducing latent space reconstruction and
uncertainty estimation for both reconstructed modalities and downstream tasks.
Our method is architecture-agnostic, reconstructs missing modalities, and
delivers reliable uncertainty estimates, improving both interpretability and
performance. SURE introduces a unique Pearson Correlation-based loss and
applies statistical error propagation in deep networks for the first time,
allowing precise quantification of uncertainties from missing data and model
predictions. Extensive experiments across tasks such as sentiment analysis,
genre classification, and action recognition show that SURE consistently
achieves state-of-the-art performance, ensuring robust predictions even in the
presence of incomplete data.

</details>


### [39] [Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions](https://arxiv.org/abs/2504.13476)
*Jiadong Lou,Bingqing Liu,Yuanheng Xiong,Xiaodong Zhang,Xu Yuan*

Main category: cs.LG

TL;DR: 这项研究使用变分自动编码器（VAE）从高光谱遥感数据中检索浮游植物吸收系数和叶绿素a，与混合密度网络（MDN）相比表现出色，并证明AI可提升海洋颜色遥感的应用。


<details>
  <summary>Details</summary>
Motivation: 浮游植物改变水色但人类难以察觉，高光谱传感器和算法可改善对沿海水域浮游植物群落组成的表征，解决传统遥感挑战。

Method: 采用VAE模型作为骨干，结合创新设计，从高光谱遥感反射率中检索浮游植物参数，并与MDN方法比较，针对光复杂性高的河口-沿海水域。

Result: VAE模型在实验验证中显示高精度、低偏差，优于MDN，尤其在高维数据上，提供证据支持AI在高光谱任务中的优势。

Conclusion: 整合AI技术，EMIT、PACE及未来任务的高光谱数据将促进对水生生态系统浮游植物动态的理解。

Abstract: Phytoplankton absorb and scatter light in unique ways, subtly altering the
color of water, changes that are often minor for human eyes to detect but can
be captured by sensitive ocean color instruments onboard satellites from space.
Hyperspectral sensors, paired with advanced algorithms, are expected to
significantly enhance the characterization of phytoplankton community
composition, especially in coastal waters where ocean color remote sensing
applications have historically encountered significant challenges. This study
presents novel machine learning-based solutions for NASA's hyperspectral
missions, including EMIT and PACE, tackling high-fidelity retrievals of
phytoplankton absorption coefficient and chlorophyll a from their hyperspectral
remote sensing reflectance. Given that a single Rrs spectrum may correspond to
varied combinations of inherent optical properties and associated
concentrations, the Variational Autoencoder (VAE) is used as a backbone in this
study to handle such multi-distribution prediction problems. We first time
tailor the VAE model with innovative designs to achieve hyperspectral
retrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex
estuarine-coastal waters. Validation with extensive experimental observation
demonstrates superior performance of the VAE models with high precision and low
bias. The in-depth analysis of VAE's advanced model structures and learning
designs highlights the improvement and advantages of VAE-based solutions over
the mixture density network (MDN) approach, particularly on high-dimensional
data, such as PACE. Our study provides strong evidence that current EMIT and
PACE hyperspectral data as well as the upcoming Surface Biology Geology mission
will open new pathways toward a better understanding of phytoplankton community
dynamics in aquatic ecosystems when integrated with AI technologies.

</details>


### [40] [Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios](https://arxiv.org/abs/2504.13478)
*Vivian Lin,Ramneet Kaur,Yahan Yang,Souradeep Dutta,Yiannis Kantaros,Anirban Roy,Susmit Jha,Oleg Sokolsky,Insup Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种鲁棒的安全监控方法，使用自适应保形预测和增量学习来预测网络物理系统的安全违规，即使在分布外数据情况下。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅检测分布外数据但不能保证安全属性，本文旨在直接监控安全并对分布外数据鲁棒。

Method: 通过预测未来轨迹监控信号时序逻辑安全规范，结合自适应保形预测提供概率保证和增量学习减少保守性。

Result: 在F1Tenth汽车和赛车的案例研究中，方法在分布外设置下实现了高召回率、及时性和精度，优于其他方法。

Conclusion: 该方法在理论和实践上有效，确保了学习启用系统的安全。

Abstract: The safety of learning-enabled cyber-physical systems is compromised by the
well-known vulnerabilities of deep neural networks to out-of-distribution (OOD)
inputs. Existing literature has sought to monitor the safety of such systems by
detecting OOD data. However, such approaches have limited utility, as the
presence of an OOD input does not necessarily imply the violation of a desired
safety property. We instead propose to directly monitor safety in a manner that
is itself robust to OOD data. To this end, we predict violations of signal
temporal logic safety specifications based on predicted future trajectories.
Our safety monitor additionally uses a novel combination of adaptive conformal
prediction and incremental learning. The former obtains probabilistic
prediction guarantees even on OOD data, and the latter prevents overly
conservative predictions. We evaluate the efficacy of the proposed approach in
two case studies on safety monitoring: 1) predicting collisions of an F1Tenth
car with static obstacles, and 2) predicting collisions of a race car with
multiple dynamic obstacles. We find that adaptive conformal prediction obtains
theoretical guarantees where other uncertainty quantification methods fail to
do so. Additionally, combining adaptive conformal prediction and incremental
learning for safety monitoring achieves high recall and timeliness while
reducing loss in precision. We achieve these results even in OOD settings and
outperform alternative methods.

</details>


### [41] [Integrating Locality-Aware Attention with Transformers for General Geometry PDEs](https://arxiv.org/abs/2504.13480)
*Minsu Koh,Beom-Chul Park,Heejo Kong,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出LA2Former神经算子，通过局部感知注意力机制提升偏微分方程（PDE）建模的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经算子依赖均匀网格的局限性，以及Transformer方法忽略局部PDE行为的挑战。

Method: 使用K-最近邻动态分块和全局-局部注意力机制，结合线性注意力和成对注意力。

Result: 在六个基准数据集上，预测准确性较现有线性注意力方法提高超过50%，并在最优条件下优于全成对注意力。

Conclusion: 强调局部特征学习在推进Transformer-based神经算子用于复杂域PDE求解中的关键重要性。

Abstract: Neural operators have emerged as promising frameworks for learning mappings
governed by partial differential equations (PDEs), serving as data-driven
alternatives to traditional numerical methods. While methods such as the
Fourier neural operator (FNO) have demonstrated notable performance, their
reliance on uniform grids restricts their applicability to complex geometries
and irregular meshes. Recently, Transformer-based neural operators with linear
attention mechanisms have shown potential in overcoming these limitations for
large-scale PDE simulations. However, these approaches predominantly emphasize
global feature aggregation, often overlooking fine-scale dynamics and localized
PDE behaviors essential for accurate solutions. To address these challenges, we
propose the Locality-Aware Attention Transformer (LA2Former), which leverages
K-nearest neighbors for dynamic patchifying and integrates global-local
attention for enhanced PDE modeling. By combining linear attention for
efficient global context encoding with pairwise attention for capturing
intricate local interactions, LA2Former achieves an optimal balance between
computational efficiency and predictive accuracy. Extensive evaluations across
six benchmark datasets demonstrate that LA2Former improves predictive accuracy
by over 50% relative to existing linear attention methods, while also
outperforming full pairwise attention under optimal conditions. This work
underscores the critical importance of localized feature learning in advancing
Transformer-based neural operators for solving PDEs on complex and irregular
domains.

</details>


### [42] [Latent Tensor Factorization with Nonlinear PID Control for Missing Data Recovery in Non-Intrusive Load Monitoring](https://arxiv.org/abs/2504.13483)
*Yiran Wang,Tangtang Xie,Hao Wu*

Main category: cs.LG

TL;DR: 本论文提出NPIL模型，通过非线性PID和PSO改进NILM中缺失数据的估计，提高收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: NILM是智能电网关键技术，但数据缺失问题（如传感器故障）导致监控不准确，现有的SGD-based模型收敛慢。

Method: 提出NPIL模型：a) 使用非线性PID控制器重建学习错误，融入过去信息；b) 采用PSO算法适应增益参数，提高计算效率。

Result: 在真实NILM数据集实验中，NPIL模型在收敛速度和准确性上优于现有模型。

Conclusion: NPIL模型有效解决了NILM中缺失数据估计问题，提升了整体性能。

Abstract: Non-Intrusive Load Monitoring (NILM) has emerged as a key smart grid
technology, identifying electrical device and providing detailed energy
consumption data for precise demand response management. Nevertheless, NILM
data suffers from missing values due to inescapable factors like sensor
failure, leading to inaccuracies in non-intrusive load monitoring. A stochastic
gradient descent (SGD)-based latent factorization of tensors model has proven
to be effective in estimating missing data, however, it updates a latent factor
solely based on the current stochastic gradient, without considering past
information, which leads to slow convergence of anLFT model. To address this
issue, this paper proposes a Nonlinear Proportional-integral-derivative
(PID)-Incorporated Latent factorization of tensors (NPIL) model with two-fold
ideas: a) rebuilding the instant learning error according to the principle of a
nonlinear PID controller, thus, the past update information is efficiently
incorporated into the learning scheme, and b) implementing gain parameter
adaptation by utilizing particle swarm optimization (PSO) algorithm, hence, the
model computational efficiency is effectively improved. Experimental results on
real-world NILM datasets demonstrate that the proposed NPIL model surpasses
state-of-the-art models in convergence rate and accuracy when predicting the
missing NILM data.

</details>


### [43] [Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2504.13484)
*Vivian Lin,Insup Lee*

Main category: cs.LG

TL;DR: 论文提出'监控和恢复'范式来处理神经网络分布偏移问题，而不是传统的'检测和弃权'方法。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布偏移的易感性导致学习型网络物理系统的可靠性维护成为挑战，现有的方法在实际应用中受限。

Method: 提出'监控和恢复'范式，包括鲁棒安全监控和分布偏移恢复，并讨论最近工作的两个例子。

Result: 展示了该范式的可行性和前景，但未详细说明具体结果。

Conclusion: 建议'监控和恢复'范式作为未来研究的方向。

Abstract: With the known vulnerability of neural networks to distribution shift,
maintaining reliability in learning-enabled cyber-physical systems poses a
salient challenge. In response, many existing methods adopt a detect and
abstain methodology, aiming to detect distribution shift at inference time so
that the learning-enabled component can abstain from decision-making. This
approach, however, has limited use in real-world applications. We instead
propose a monitor and recover paradigm as a promising direction for future
research. This philosophy emphasizes 1) robust safety monitoring instead of
distribution shift detection and 2) distribution shift recovery instead of
abstention. We discuss two examples from our recent work.

</details>


### [44] [Deep Learning Models Meet Financial Data Modalities](https://arxiv.org/abs/2504.13521)
*Kasymkhan Khubiev,Michail Semenov*

Main category: cs.LG

TL;DR: 这篇论文探讨了将深度学习应用于金融数据以提升算法交易性能，特别通过限价订单簿分析达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在处理非结构化数据上取得成功，但应用于结构化金融数据仍面临挑战，本研究旨在提升交易策略和投资组合优化的预测性能。

Method: 开发嵌入技术，将限价订单簿的连续快照视为图像-based 输入，并作为不同的输入通道整合到深度学习模型中。

Result: 在高频交易算法中实现了最先进性能。

Conclusion: 突显了深度学习在金融应用中的有效性。

Abstract: Algorithmic trading relies on extracting meaningful signals from diverse
financial data sources, including candlestick charts, order statistics on put
and canceled orders, traded volume data, limit order books, and news flow.
While deep learning has demonstrated remarkable success in processing
unstructured data and has significantly advanced natural language processing,
its application to structured financial data remains an ongoing challenge. This
study investigates the integration of deep learning models with financial data
modalities, aiming to enhance predictive performance in trading strategies and
portfolio optimization. We present a novel approach to incorporating limit
order book analysis into algorithmic trading by developing embedding techniques
and treating sequential limit order book snapshots as distinct input channels
in an image-based representation. Our methodology for processing limit order
book data achieves state-of-the-art performance in high-frequency trading
algorithms, underscoring the effectiveness of deep learning in financial
applications.

</details>


### [45] [Cross-Modal Temporal Fusion for Financial Market Forecasting](https://arxiv.org/abs/2504.13522)
*Yunhua Pei,John Cartlidge,Anandadeep Mandal,Daniel Gold,Enrique Marcilio,Riccardo Mazzon*

Main category: cs.LG

TL;DR: 本论文提出Cross-Modal Temporal Fusion (CMTF)，一个基于Transformer的框架，用于整合异构金融数据，提高股票价格预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理不同数据模态的交互，需要一种整合多源数据的改进方法。

Method: CMTF使用注意力机制动态权重大小不同模态，并包含张量解释模块和自动训练方案以优化模型。

Result: 在实际金融数据集上，CMTF在股票价格预测中优于基线模型，并提供可扩展的解决方案。

Conclusion: CMTF为金融市场预测中的跨模态整合提供了有效且可扩展的方法。

Abstract: Accurate financial market forecasting requires diverse data sources,
including historical price trends, macroeconomic indicators, and financial
news, each contributing unique predictive signals. However, existing methods
often process these modalities independently or fail to effectively model their
interactions. In this paper, we introduce Cross-Modal Temporal Fusion (CMTF), a
novel transformer-based framework that integrates heterogeneous financial data
to improve predictive accuracy. Our approach employs attention mechanisms to
dynamically weight the contribution of different modalities, along with a
specialized tensor interpretation module for feature extraction. To facilitate
rapid model iteration in industry applications, we incorporate a mature
auto-training scheme that streamlines optimization. When applied to real-world
financial datasets, CMTF demonstrates improvements over baseline models in
forecasting stock price movements and provides a scalable and effective
solution for cross-modal integration in financial market prediction.

</details>


### [46] [Can Local Representation Alignment RNNs Solve Temporal Tasks?](https://arxiv.org/abs/2504.13531)
*Nikolay Manchev,Luis C. Garcia-Peraza-Herrera*

Main category: cs.LG

TL;DR: 本文提出LRA方法改进RNN训练，解决梯度消失问题，并通过梯度正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: RNN常用于实时处理和数据流，但BPTT易于梯度问题和生物不合理性，本文寻求使用局部更新提高稳定性。

Method: 使用局部表示对齐(LRA)，分析性能，实验归一化和误差函数，引入梯度正则化促进收敛。

Result: LRA模型易于梯度消失，正则化后性能提升，在时间顺序等任务上优于未正则化版本。

Conclusion: 改进了LRA方法，使RNN训练更稳定，提升了在自然语言处理等领域的实用性。

Abstract: Recurrent Neural Networks (RNNs) are commonly used for real-time processing,
streaming data, and cases where the amount of training samples is limited.
Backpropagation Through Time (BPTT) is the predominant algorithm for training
RNNs; however, it is frequently criticized for being prone to exploding and
vanishing gradients and being biologically implausible. In this paper, we
present and evaluate a target propagation-based method for RNNs, which uses
local updates and seeks to reduce the said instabilities. Having stable RNN
models increases their practical use in a wide range of fields such as natural
language processing, time-series forecasting, anomaly detection, control
systems, and robotics.
  The proposed solution uses local representation alignment (LRA). We
thoroughly analyze the performance of this method, experiment with
normalization and different local error functions, and invalidate certain
assumptions about the behavior of this type of learning. Namely, we demonstrate
that despite the decomposition of the network into sub-graphs, the model still
suffers from vanishing gradients. We also show that gradient clipping as
proposed in LRA has little to no effect on network performance. This results in
an LRA RNN model that is very difficult to train due to vanishing gradients. We
address this by introducing gradient regularization in the direction of the
update and demonstrate that this modification promotes gradient flow and
meaningfully impacts convergence. We compare and discuss the performance of the
algorithm, and we show that the regularized LRA RNN considerably outperforms
the unregularized version on three landmark tasks: temporal order, 3-bit
temporal order, and random permutation.

</details>


### [47] [Irregular Sampling of High-Dimensional Functions in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2504.13543)
*Armin Iske,Lennart Ohlsen*

Main category: cs.LG

TL;DR: 本论文开发了在再生核Hilbert空间中使用不规则样本的采样公式，针对张量积核，通过组合低维样本显著降低高维函数采样的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 动机是解决高维函数采样计算复杂度高的挑战，提高采样效率。

Method: 方法包括开发基于确定序列的不规则样本采样公式，并证明低维确定样本可张量积到高维。

Result: 结果是显著减少了高维函数采样的计算复杂度。

Conclusion: 结论是这种方法为高维函数采样提供了高效的计算框架。

Abstract: We develop sampling formulas for high-dimensional functions in reproducing
kernel Hilbert spaces, where we rely on irregular samples that are taken at
determining sequences of data points. We place particular emphasis on sampling
formulas for tensor product kernels, where we show that determining irregular
samples in lower dimensions can be composed to obtain a tensor of determining
irregular samples in higher dimensions. This in turn reduces the computational
complexity of sampling formulas for high-dimensional functions quite
significantly.

</details>


### [48] [Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective](https://arxiv.org/abs/2504.13558)
*Yuling Jiao,Yanming Lai,Yang Wang,Bokai Yan*

Main category: cs.LG

TL;DR: 本文证明简单Transformer可高效逼近Hölder连续函数，克服维数灾难，使用Kolmogorov-Arnold定理。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer在逼近复杂函数类时的能力，以克服维数灾难，因为其在机器学习中的广泛应用。

Method: 构建包含一个自注意力层和特定前馈层的Transformer，基于Kolmogorov-Arnold表示定理和翻译技术。

Result: 实现逼近精度ε时，仅需O(log(1/ε))层前馈层，宽度为O((1/ε^{2/β}) log(1/ε))，使用其他激活函数可使宽度常数。

Conclusion: 展示了Transformer的强大表达能力，并提供了直观的证明方法。

Abstract: The Transformer model is widely used in various application areas of machine
learning, such as natural language processing. This paper investigates the
approximation of the H\"older continuous function class
$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$
by Transformers and constructs several Transformers that can overcome the curse
of dimensionality. These Transformers consist of one self-attention layer with
one head and the softmax function as the activation function, along with
several feedforward layers. For example, to achieve an approximation accuracy
of $\epsilon$, if the activation functions of the feedforward layers in the
Transformer are ReLU and floor, only
$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$ layers of feedforward layers
are needed, with widths of these layers not exceeding
$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$.
If other activation functions are allowed in the feedforward layers, the width
of the feedforward layers can be further reduced to a constant. These results
demonstrate that Transformers have a strong expressive capability. The
construction in this paper is based on the Kolmogorov-Arnold Representation
Theorem and does not require the concept of contextual mapping, hence our proof
is more intuitively clear compared to previous Transformer approximation works.
Additionally, the translation technique proposed in this paper helps to apply
the previous approximation results of feedforward neural networks to
Transformer research.

</details>


### [49] [Bayesian continual learning and forgetting in neural networks](https://arxiv.org/abs/2504.13569)
*Djohan Bonnet,Kellian Cottart,Tifenn Hirtzlin,Tarcisius Januel,Thomas Dalgaty,Elisa Vianello,Damien Querlioz*

Main category: cs.LG

TL;DR: 本论文引入MESU框架，一个基于不确定性的贝叶斯方法，帮助神经网络平衡学习和遗忘，避免灾难性遗忘和记忆。


<details>
  <summary>Details</summary>
Motivation: 生物突触能轻松平衡记忆保留和灵活性，而人工神经网络在灾难性遗忘和记忆之间挣扎，因此需要一个新的框架来模仿生物学特性。

Method: 通过MESU框架，采用贝叶斯方法根据不确定性更新网络参数，结合学习和遗忘，适应流数据，并提供认识不确定性估计。

Result: 实验显示MESU在MNIST和CIFAR-100基准测试中，优于现有持续学习技术，在准确性、学习新任务和异常检测方面表现更好。

Conclusion: 该方法统一了元可塑性、贝叶斯推理和Hessian-based正则化的想法，提供了一个生物启发的路径，实现鲁棒的持续学习。

Abstract: Biological synapses effortlessly balance memory retention and flexibility,
yet artificial neural networks still struggle with the extremes of catastrophic
forgetting and catastrophic remembering. Here, we introduce Metaplasticity from
Synaptic Uncertainty (MESU), a Bayesian framework that updates network
parameters according their uncertainty. This approach allows a principled
combination of learning and forgetting that ensures that critical knowledge is
preserved while unused or outdated information is gradually released. Unlike
standard Bayesian approaches -- which risk becoming overly constrained, and
popular continual-learning methods that rely on explicit task boundaries, MESU
seamlessly adapts to streaming data. It further provides reliable epistemic
uncertainty estimates, allowing out-of-distribution detection, the only
computational cost being to sample the weights multiple times to provide proper
output statistics. Experiments on image-classification benchmarks demonstrate
that MESU mitigates catastrophic forgetting, while maintaining plasticity for
new tasks. When training 200 sequential permuted MNIST tasks, MESU outperforms
established continual learning techniques in terms of accuracy, capability to
learn additional tasks, and out-of-distribution data detection. Additionally,
due to its non-reliance on task boundaries, MESU outperforms conventional
learning techniques on the incremental training of CIFAR-100 tasks consistently
in a wide range of scenarios. Our results unify ideas from metaplasticity,
Bayesian inference, and Hessian-based regularization, offering a
biologically-inspired pathway to robust, perpetual learning.

</details>


### [50] [MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework](https://arxiv.org/abs/2504.13574)
*Zhenkai Qin,Feng Zhu,Huan Zeng,Xunyi Nong*

Main category: cs.LG

TL;DR: 本文提出MAAM轻量级注意力模块，用于资源受限环境下的图像分类，实现了高准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 资源受限场景下需要轻量模型，传统注意力机制计算复杂且结构刚性，限制了应用。

Method: 提出MAAM模块，使用三个并行代理分支、适应性权重融合和卷积压缩层，集成MindSpore框架。

Result: CIFAR-10数据集上准确率达87.0%，优于CNN（58.3%）和MLP（49.6%），训练效率提升30%。

Conclusion: 消融实验验证组件必要性，展示了硬件加速和低内存占用，提供实用部署方案。

Abstract: The demand for lightweight models in image classification tasks under
resource-constrained environments necessitates a balance between computational
efficiency and robust feature representation. Traditional attention mechanisms,
despite their strong feature modeling capability, often struggle with high
computational complexity and structural rigidity, limiting their applicability
in scenarios with limited computational resources (e.g., edge devices or
real-time systems). To address this, we propose the Multi-Agent Aggregation
Module (MAAM), a lightweight attention architecture integrated with the
MindSpore framework. MAAM employs three parallel agent branches with
independently parameterized operations to extract heterogeneous features,
adaptively fused via learnable scalar weights, and refined through a
convolutional compression layer. Leveraging MindSpore's dynamic computational
graph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10
dataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)
models, while improving training efficiency by 30%. Ablation studies confirm
the critical role of agent attention (accuracy drops to 32.0% if removed) and
compression modules (25.5% if omitted), validating their necessity for
maintaining discriminative feature learning. The framework's hardware
acceleration capabilities and minimal memory footprint further demonstrate its
practicality, offering a deployable solution for image classification in
resource-constrained scenarios without compromising accuracy.

</details>


### [51] [MSTIM: A MindSpore-Based Model for Traffic Flow Prediction](https://arxiv.org/abs/2504.13576)
*Weiqi Qin,Yuxin Liu,Dongze Wu,Zhenkai Qin,Qining Luo*

Main category: cs.LG

TL;DR: 本文提出MSTIM模型，结合LSTM、CNN和注意力机制，提高交通流量预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对传统模型处理多尺度时间特征和动态变化时准确率低、误差波动大的问题。

Method: 基于Mindspore框架，提出集成LSTM、CNN和注意力机制的MSTIM模型。

Result: 实验使用MITV数据集，在MAE、MSE和RMSE指标上优于LSTM-attention、CNN-attention和LSTM-CNN模型。

Conclusion: 显著提升了交通流量预测的准确性和稳定性。

Abstract: Aiming at the problems of low accuracy and large error fluctuation of
traditional traffic flow predictionmodels when dealing with multi-scale
temporal features and dynamic change patterns. this paperproposes a multi-scale
time series information modelling model MSTIM based on the Mindspore framework,
which integrates long and short-term memory networks (LSTMs), convolutional
neural networks (CNN), and the attention mechanism to improve the modelling
accuracy and stability. The Metropolitan Interstate Traffic Volume (MITV)
dataset was used for the experiments and compared and analysed with typical
LSTM-attention models, CNN-attention models and LSTM-CNN models. The
experimental results show that the MSTIM model achieves better results in the
metrics of Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean
Square Error (RMSE), which significantly improves the accuracy and stability of
the traffic volume prediction.

</details>


### [52] [How to Achieve Higher Accuracy with Less Training Points?](https://arxiv.org/abs/2504.13586)
*Jinghan Yang,Anupam Pani,Yunchao Zhang*

Main category: cs.LG

TL;DR: 本论文使用影响函数技术选择训练数据子集，在二元分类任务中仅用10%的数据即可达到与全数据集相当的性能，甚至用60%的数据时性能更高。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型训练中数据集使用导致的计算低效问题，通过识别信息丰富的子集来提高训练效率。

Method: 基于影响函数的技术来确定训练样本子集，并在逻辑回归的二元分类任务上进行实证评估。

Result: 使用10%的数据时性能与全数据集相当，使用60%的数据时准确率更高。

Conclusion: 该方法有效减少数据使用量，同时维持或提升模型性能，证明了其在提高训练效率方面的潜力。

Abstract: In the era of large-scale model training, the extensive use of available
datasets has resulted in significant computational inefficiencies. To tackle
this issue, we explore methods for identifying informative subsets of training
data that can achieve comparable or even superior model performance. We propose
a technique based on influence functions to determine which training samples
should be included in the training set. We conducted empirical evaluations of
our method on binary classification tasks utilizing logistic regression models.
Our approach demonstrates performance comparable to that of training on the
entire dataset while using only 10% of the data. Furthermore, we found that our
method achieved even higher accuracy when trained with just 60% of the data.

</details>


### [53] [Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data](https://arxiv.org/abs/2504.13598)
*Charalampos Kleitsikas,Nikolaos Korfiatis,Stefanos Leonardos,Carmine Ventre*

Main category: cs.LG

TL;DR: 简而言之，这篇论文使用NLP和ML技术分析区块链交易数据中的情感，以预测比特币和以太坊价格变动，并发现比特币在情感预测方面具有信息优势。


<details>
  <summary>Details</summary>
Motivation: 区块链用于存储任意内容可能影响价格，但当前分析方法有限，故采用NLP技术改进。

Method: 采用自然语言处理和机器学习技术，从区块链交易数据中提取情感并预测价格变动。

Result: 展示了情感数据对价格预测的效力，并发现比特币比以太坊更易预测。

Conclusion: 引入区块链情感分析作为新框架，提升加密货币市场预测。

Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment
systems, are increasingly used to store and share arbitrary content, such as
text messages and files. Although often non-financial, this hidden content can
impact price movements by conveying private information, shaping sentiment, and
influencing public opinion. However, current analyses of such data are limited
in scope and scalability, primarily relying on manual classification or
hand-crafted heuristics. In this work, we address these limitations by
employing Natural Language Processing techniques to analyze, detect patterns,
and extract public sentiment encoded within blockchain transactional data.
Using a variety of Machine Learning techniques, we showcase for the first time
the predictive power of blockchain-embedded sentiment in forecasting
cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our
findings shed light on a previously underexplored source of freely available,
transparent, and immutable data and introduce blockchain sentiment analysis as
a novel and robust framework for enhancing financial predictions in
cryptocurrency markets. Incidentally, we discover an asymmetry between
cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that
the sentiment embedded into transactional data is sufficient to predict its
price movement.

</details>


### [54] [Fairness and Robustness in Machine Unlearning](https://arxiv.org/abs/2504.13610)
*Khoa Tran,Simon S. Woo*

Main category: cs.LG

TL;DR: 本论文首次关注机器遗忘中的公平性和鲁棒性，通过实验证明公平性与鲁棒性的相关性，并提出改进评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法虽在准确性和效率上表现良好，但未实现精确遗忘，且忽略公平性和鲁棒性；本研究旨在解决这些问题。

Method: 提出基于方差-偏差权衡的公平性猜想，并在ResNet和ViT模型上进行实验验证。

Result: 实验显示公平性差距与模型敏感性相关，现有算法易受对抗攻击，且中间层和最后一层的遗忘即可降低时间和内存复杂度。

Conclusion: 建议使用公平性差距和鲁棒性指标评估遗忘算法，并采用中间层遗忘以提高效率。

Abstract: Machine unlearning poses the challenge of ``how to eliminate the influence of
specific data from a pretrained model'' in regard to privacy concerns. While
prior research on approximated unlearning has demonstrated accuracy and
efficiency in time complexity, we claim that it falls short of achieving exact
unlearning, and we are the first to focus on fairness and robustness in machine
unlearning algorithms. Our study presents fairness Conjectures for a
well-trained model, based on the variance-bias trade-off characteristic, and
considers their relevance to robustness. Our Conjectures are supported by
experiments conducted on the two most widely used model architectures, ResNet
and ViT, demonstrating the correlation between fairness and robustness:
\textit{the higher fairness-gap is, the more the model is sensitive and
vulnerable}. In addition, our experiments demonstrate the vulnerability of
current state-of-the-art approximated unlearning algorithms to adversarial
attacks, where their unlearned models suffer a significant drop in accuracy
compared to the exact-unlearned models. We claim that our fairness-gap
measurement and robustness metric should be used to evaluate the unlearning
algorithm. Furthermore, we demonstrate that unlearning in the intermediate and
last layers is sufficient and cost-effective for time and memory complexity.

</details>


### [55] [Entropic Time Schedulers for Generative Diffusion Models](https://arxiv.org/abs/2504.13612)
*Dejan Stancevic,Luca Ambrogioni*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵的时间调度器，用于改进扩散模型的生成性能，而不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型的性能依赖于噪声调度函数，本文旨在通过熵-based采样确保每个点贡献相同的信息量。

Method: 方法包括基于熵的时间重参数化，证明其独立于初始时间，提供精确的熵时间估计公式，并引入缩放熵时间。

Result: 实验在高斯混合分布和ImageNet上显示，使用（缩放）熵时间显著提升推理性能和图像质量指标，如FID和FD-DINO，尤其在少函数评估次数时。

Conclusion: 使用缩放熵时间重参数化可大幅提高预训练模型如EDM2的性能，而不增加额外开销。

Abstract: The practical performance of generative diffusion models depends on the
appropriate choice of the noise scheduling function, which can also be
equivalently expressed as a time reparameterization. In this paper, we present
a time scheduler that selects sampling points based on entropy rather than
uniform time spacing, ensuring that each point contributes an equal amount of
information to the final generation. We prove that this time reparameterization
does not depend on the initial choice of time. Furthermore, we provide a
tractable exact formula to estimate this \emph{entropic time} for a trained
model using the training loss without substantial overhead. Alongside the
entropic time, inspired by the optimality results, we introduce a rescaled
entropic time. In our experiments with mixtures of Gaussian distributions and
ImageNet, we show that using the (rescaled) entropic times greatly improves the
inference performance of trained models. In particular, we found that the image
quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can
be substantially increased by the rescaled entropic time reparameterization
without increasing the number of function evaluations, with greater
improvements in the few NFEs regime.

</details>


### [56] [Efficient algorithms for the Hadamard decomposition](https://arxiv.org/abs/2504.13633)
*Samuel Wertz,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种高效的Hadamard分解算法，通过交替优化和改进策略，提升矩阵压缩性能，并在实验中显示优势。


<details>
  <summary>Details</summary>
Motivation: 论文旨在改进Hadamard分解技术，以提高数据分析和矩阵压缩的效率。

Method: 方法包括交替优化分解为凸子问题、SVD启发的初始化策略、动量更新加速，以及扩展到多个低秩矩阵。

Result: 实验结果表明，该方法在各种数据集上优于现有的梯度下降方法和传统低秩逼近技术。

Conclusion: 结论是所提方法在Hadamard分解中表现出色，提供高效框架。

Abstract: The Hadamard decomposition is a powerful technique for data analysis and
matrix compression, which decomposes a given matrix into the element-wise
product of two or more low-rank matrices. In this paper, we develop an
efficient algorithm to solve this problem, leveraging an alternating
optimization approach that decomposes the global non-convex problem into a
series of convex sub-problems. To improve performance, we explore advanced
initialization strategies inspired by the singular value decomposition (SVD)
and incorporate acceleration techniques by introducing momentum-based updates.
Beyond optimizing the two-matrix case, we also extend the Hadamard
decomposition framework to support more than two low-rank matrices, enabling
approximations with higher effective ranks while preserving computational
efficiency. Finally, we conduct extensive experiments to compare our method
with the existing gradient descent-based approaches for the Hadamard
decomposition and with traditional low-rank approximation techniques. The
results highlight the effectiveness of our proposed method across diverse
datasets.

</details>


### [57] [MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting Mitigation in GFSCIL](https://arxiv.org/abs/2504.13691)
*Jinhui Pang,Changqing Lin,Hao Lin,Jinglin He,Zhengjun Li,Zhihui Zhang,Xiaoshuai Hao*

Main category: cs.LG

TL;DR: 本论文提出MEGA方法，解决GFSCIL中的灾难性遗忘问题，并在实验中取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有GFSCIL方法过度简化学习过程，无法整合GCL技术，且论文引入更严格的增量训练设置。

Method: 通过meta-training阶段计算增量二阶梯度，学习高质量先验以对齐meta-training和增量学习行为。

Result: 在四个主流图数据集上达到state-of-the-art性能，并提升各种GCL方法的有效性。

Conclusion: MEGA作为一个模型无关的GFSCIL范式，为未来研究提供新方向。

Abstract: Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to
continually learn from limited samples of novel tasks after initial training on
a large base dataset. Existing GFSCIL approaches typically utilize Prototypical
Networks (PNs) for metric-based class representations and fine-tune the model
during the incremental learning stage. However, these PN-based methods
oversimplify learning via novel query set fine-tuning and fail to integrate
Graph Continual Learning (GCL) techniques due to architectural constraints. To
address these challenges, we propose a more rigorous and practical setting for
GFSCIL that excludes query sets during the incremental training phase. Building
on this foundation, we introduce Model-Agnostic Meta Graph Continual Learning
(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.
Specifically, by calculating the incremental second-order gradient during the
meta-training stage, we endow the model to learn high-quality priors that
enhance incremental learning by aligning its behaviors across both the
meta-training and incremental learning stages. Extensive experiments on four
mainstream graph datasets demonstrate that MEGA achieves state-of-the-art
results and enhances the effectiveness of various GCL methods in GFSCIL. We
believe that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,
paving the way for future research.

</details>


### [58] [Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for Interpretable Heterogeneous Treatment Effects](https://arxiv.org/abs/2504.13733)
*Yichen Liu*

Main category: cs.LG

TL;DR: 本篇论文提出动态正则化因果提升决策树（CBDT），通过动态调整正则化来改进异质治疗效应估计。


<details>
  <summary>Details</summary>
Motivation: 现有树-based因果推断方法在观测数据上错误率高，无法捕捉复杂交互和依赖静态正则化。

Method: 提出CBDT框架，将方差正则化和平均治疗效应校准集成到梯度提升决策树的损失函数中，并动态更新正则化参数。

Result: 实验显示显著提高估计准确性、可靠覆盖，并在临床研究中识别可操作规则。

Conclusion: 动态正则化有效收紧错误边界，提升预测性能和可解释性。

Abstract: Heterogeneous treatment effect estimation in high-stakes applications demands
models that simultaneously optimize precision, interpretability, and
calibration. Many existing tree-based causal inference techniques, however,
exhibit high estimation errors when applied to observational data because they
struggle to capture complex interactions among factors and rely on static
regularization schemes. In this work, we propose Dynamic Regularized Causal
Boosted Decision Trees (CBDT), a novel framework that integrates variance
regularization and average treatment effect calibration into the loss function
of gradient boosted decision trees. Our approach dynamically updates the
regularization parameters using gradient statistics to better balance the
bias-variance tradeoff. Extensive experiments on standard benchmark datasets
and real-world clinical data demonstrate that the proposed method significantly
improves estimation accuracy while maintaining reliable coverage of true
treatment effects. In an intensive care unit patient triage study, the method
successfully identified clinically actionable rules and achieved high accuracy
in treatment effect estimation. The results validate that dynamic
regularization can effectively tighten error bounds and enhance both predictive
performance and model interpretability.

</details>


### [59] [Learning to Attribute with Attention](https://arxiv.org/abs/2504.13752)
*Benjamin Cohen-Wang,Yung-Sung Chuang,Aleksander Madry*

Main category: cs.LG

TL;DR: AT2方法使用注意力权重高效归因语言模型中令牌影响，与消融方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 减少令牌归因成本，因为直接消融昂贵且朴素注意力方法不可靠。

Method: 将不同注意力头的权重视为特征，利用消融信号学习归因，开发AT2方法。

Result: AT2性能与多消融方法相当，但更高效；在问答中修剪上下文改善答案质量。

Conclusion: AT2是可靠高效的归因工具，可优化模型性能，并提供开源代码。

Abstract: Given a sequence of tokens generated by a language model, we may want to
identify the preceding tokens that influence the model to generate this
sequence. Performing such token attribution is expensive; a common approach is
to ablate preceding tokens and directly measure their effects. To reduce the
cost of token attribution, we revisit attention weights as a heuristic for how
a language model uses previous tokens. Naive approaches to attribute model
behavior with attention (e.g., averaging attention weights across attention
heads to estimate a token's influence) have been found to be unreliable. To
attain faithful attributions, we propose treating the attention weights of
different attention heads as features. This way, we can learn how to
effectively leverage attention weights for attribution (using signal from
ablations). Our resulting method, Attribution with Attention (AT2), reliably
performs on par with approaches that involve many ablations, while being
significantly more efficient. To showcase the utility of AT2, we use it to
prune less important parts of a provided context in a question answering
setting, improving answer quality. We provide code for AT2 at
https://github.com/MadryLab/AT2 .

</details>


### [60] [Predictors of Childhood Vaccination Uptake in England: An Explainable Machine Learning Analysis of Longitudinal Regional Data (2021-2024)](https://arxiv.org/abs/2504.13755)
*Amin Noroozi,Sidratul Muntaha Esha,Mansoureh Ghari*

Main category: cs.LG

TL;DR: 本研究使用机器学习分析英格兰儿童疫苗接种覆盖率的差异，识别出关键的地理、人口和文化因素。


<details>
  <summary>Details</summary>
Motivation: 解决以往研究依赖横断面数据和传统方法，无法捕捉疫苗接种动态和多变量互动的局限性。

Method: 对2021-2024年英格兰150个地区的NHS数据进行纵向机器学习分析，使用层次聚类分组地区、CatBoost分类器预测覆盖率集群，并采用SHAP方法解释预测因素。

Result: 预测准确率高（92.1%、90.6%、86.3%）；地理、文化和人口变量如农村程度、英语熟练度、外来人口比例和民族组成是最重要预测因素，社会经济变量重要性较低；农村地区接种率更高，非英语母语、外国出生和少数民族群体覆盖率较低。

Conclusion: 强调地理和人口因素对疫苗接种覆盖率的影响，建议针对特定群体实施干预措施。

Abstract: Childhood vaccination is a cornerstone of public health, yet disparities in
vaccination coverage persist across England. These disparities are shaped by
complex interactions among various factors, including geographic, demographic,
socioeconomic, and cultural (GDSC) factors. Previous studies mostly rely on
cross-sectional data and traditional statistical approaches that assess
individual or limited sets of variables in isolation. Such methods may fall
short in capturing the dynamic and multivariate nature of vaccine uptake. In
this paper, we conducted a longitudinal machine learning analysis of childhood
vaccination coverage across 150 districts in England from 2021 to 2024. Using
vaccination data from NHS records, we applied hierarchical clustering to group
districts by vaccination coverage into low- and high-coverage clusters. A
CatBoost classifier was then trained to predict districts' vaccination clusters
using their GDSC data. Finally, the SHapley Additive exPlanations (SHAP) method
was used to interpret the predictors' importance. The classifier achieved high
accuracies of 92.1, 90.6, and 86.3 in predicting districts' vaccination
clusters for the years 2021-2022, 2022-2023, and 2023-2024, respectively. SHAP
revealed that geographic, cultural, and demographic variables, particularly
rurality, English language proficiency, the percentage of foreign-born
residents, and ethnic composition, were the most influential predictors of
vaccination coverage, whereas socioeconomic variables, such as deprivation and
employment, consistently showed lower importance, especially in 2023-2024.
Surprisingly, rural districts were significantly more likely to have higher
vaccination rates. Additionally, districts with lower vaccination coverage had
higher populations whose first language was not English, who were born outside
the UK, or who were from ethnic minority groups.

</details>


### [61] [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)
*Dmitrii Kharlapenko,Stepan Shabalin,Fazl Barez,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TL;DR: 本论文使用稀疏自编码器（SAEs）来理解大语言模型中的上下文学习（ICL），通过识别关键特征及其因果作用。


<details>
  <summary>Details</summary>
Motivation: 为了展示SAEs在解决可解释性问题中的有效性，特别是针对ICL机制的开放问题。

Method: 改编Marks et al. (2024)的稀疏特征电路方法，应用于更大的Gemma-1 2B模型，并通过电路发现识别任务检测特征。

Result: 识别了编码任务知识的特征，其潜变量可零样本诱导任务；任务向量被稀疏SAE潜变量和近似；发现任务检测特征在提示中更早激活，并通过注意力层和MLP层与执行特征因果相关。

Conclusion: SAEs有助于加深对ICL机制的理解，并证明其在模型可解释性中的实用性。

Abstract: Sparse autoencoders (SAEs) are a popular tool for interpreting large language
model activations, but their utility in addressing open questions in
interpretability remains unclear. In this work, we demonstrate their
effectiveness by using SAEs to deepen our understanding of the mechanism behind
in-context learning (ICL). We identify abstract SAE features that (i) encode
the model's knowledge of which task to execute and (ii) whose latent vectors
causally induce the task zero-shot. This aligns with prior work showing that
ICL is mediated by task vectors. We further demonstrate that these task vectors
are well approximated by a sparse sum of SAE latents, including these
task-execution features. To explore the ICL mechanism, we adapt the sparse
feature circuits methodology of Marks et al. (2024) to work for the much larger
Gemma-1 2B model, with 30 times as many parameters, and to the more complex
task of ICL. Through circuit finding, we discover task-detecting features with
corresponding SAE latents that activate earlier in the prompt, that detect when
tasks have been performed. They are causally linked with task-execution
features through the attention and MLP sublayers.

</details>


### [62] [Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems](https://arxiv.org/abs/2504.13768)
*Vinay Sharma,Rémi Tanguy Oddon,Pietro Tesini,Jens Ravesloot,Cees Taal,Olga Fink*

Main category: cs.LG

TL;DR: 提出Equi-Euler GraphNet，一种物理信息图神经网络，用于多体系统内部力和轨迹的实时预测，提高数字孪生效率。


<details>
  <summary>Details</summary>
Motivation: 准确实时建模多体动力学对数字孪生应用至关重要，联合预测内部负载和轨迹可用于故障检测和预测性维护。

Method: 提出Equi-Euler GraphNet，使用等变消息传递和Euler积分的时间更新机制，针对圆柱滚子轴承设计。

Result: 泛化能力强，优于现有GNN，速度提升200倍，准确性相当。

Conclusion: 作为高效降阶模型，适用于数字孪生、设计和维护。

Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for
enabling digital twin applications across industries. While many data-driven
approaches aim to learn system dynamics, jointly predicting internal loads and
system trajectories remains a key challenge. This dual prediction is especially
important for fault detection and predictive maintenance, where internal
loads-such as contact forces-act as early indicators of faults, reflecting wear
or misalignment before affecting motion. These forces also serve as inputs to
degradation models (e.g., crack growth), enabling damage prediction and
remaining useful life estimation. We propose Equi-Euler GraphNet, a
physics-informed graph neural network (GNN) that simultaneously predicts
internal forces and global trajectories in multi-body systems. In this
mesh-free framework, nodes represent system components and edges encode
interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an
equivariant message-passing scheme, interpreting edge messages as interaction
forces consistent under Euclidean transformations; and (2) a temporal-aware
iterative node update mechanism, based on Euler integration, to capture
influence of distant interactions over time. Tailored for cylindrical roller
bearings, it decouples ring dynamics from constrained motion of rolling
elements. Trained on high-fidelity multiphysics simulations, Equi-Euler
GraphNet generalizes beyond the training distribution, accurately predicting
loads and trajectories under unseen speeds, loads, and configurations. It
outperforms state-of-the-art GNNs focused on trajectory prediction, delivering
stable rollouts over thousands of time steps with minimal error accumulation.
Achieving up to a 200x speedup over conventional solvers while maintaining
comparable accuracy, it serves as an efficient reduced-order model for digital
twins, design, and maintenance.

</details>


### [63] [DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs](https://arxiv.org/abs/2504.13774)
*Tamim Al Mahmud,Najeeb Jebreel,Josep Domingo-Ferrer,David Sanchez*

Main category: cs.LG

TL;DR: 本论文提出DP2Unlearning框架，使用差分隐私训练LLM，实现高效unlearning并提供遗忘保证。


<details>
  <summary>Details</summary>
Motivation: LLM易记忆私有或版权信息，引发伦理问题，重新训练成本高，需要更高效的unlearning方法。

Method: 采用ε-差分隐私保护训练数据，启用后续高效unlearning过程。

Result: 实验显示unlearning后模型性能与重新训练相当，成本降低约一半，并优于近似unlearning方法。

Conclusion: DP2Unlearning在保持模型效用和实现遗忘方面表现出色，提供正式保证。

Abstract: Large language models (LLMs) have recently revolutionized language processing
tasks but have also brought ethical and legal issues. LLMs have a tendency to
memorize potentially private or copyrighted information present in the training
data, which might then be delivered to end users at inference time. When this
happens, a naive solution is to retrain the model from scratch after excluding
the undesired data. Although this guarantees that the target data have been
forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning
offers a more efficient alternative, as it consists of ex post modifications of
the trained model itself to prevent undesirable results, but it lacks
forgetting guarantees because it relies solely on empirical evidence. In this
work, we present DP2Unlearning, a novel LLM unlearning framework that offers
formal forgetting guarantees at a significantly lower cost than retraining from
scratch on the data to be retained. DP2Unlearning involves training LLMs on
textual data protected using {\epsilon}-differential privacy (DP), which later
enables efficient unlearning with the guarantees against disclosure associated
with the chosen {\epsilon}. Our experiments demonstrate that DP2Unlearning
achieves similar model performance post-unlearning, compared to an LLM
retraining from scratch on retained data -- the gold standard exact unlearning
-- but at approximately half the unlearning cost. In addition, with a
reasonable computational cost, it outperforms approximate unlearning methods at
both preserving the utility of the model post-unlearning and effectively
forgetting the targeted information.

</details>


### [64] [On the Relationship Between Robustness and Expressivity of Graph Neural Networks](https://arxiv.org/abs/2504.13786)
*Lorenz Kummer,Wilfried N. Gansterer,Nils M. Kriege*

Main category: cs.LG

TL;DR: 本论文通过分析框架研究图神经网络（GNNs）对位翻转攻击的脆弱性，重点探讨表达能力的影响因素。


<details>
  <summary>Details</summary>
Motivation: 动机是了解GNNs在位翻转攻击下的脆弱性，特别是表达能力的损失，以识别关键影响因素。

Method: 方法包括引入分析框架、建立正式标准、推导理论边界，并使用真实数据集进行实证验证。

Result: 结果显示，ReLU激活的GNNs在高同质图和低维特征下特别易受攻击，实证结果支持理论洞见。

Conclusion: 结论提供可操作策略，以减轻表达能力关键应用中的攻击风险。

Abstract: We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip
attacks (BFAs) by introducing an analytical framework to study the influence of
architectural features, graph properties, and their interaction.
  The expressivity of GNNs refers to their ability to distinguish
non-isomorphic graphs and depends on the encoding of node neighborhoods. We
examine the vulnerability of neural multiset functions commonly used for this
purpose and establish formal criteria to characterize a GNN's susceptibility to
losing expressivity due to BFAs. This enables an analysis of the impact of
homophily, graph structural variety, feature encoding, and activation functions
on GNN robustness. We derive theoretical bounds for the number of bit flips
required to degrade GNN expressivity on a dataset, identifying ReLU-activated
GNNs operating on highly homophilous graphs with low-dimensional or one-hot
encoded features as particularly susceptible. Empirical results using ten
real-world datasets confirm the statistical significance of our key theoretical
insights and offer actionable results to mitigate BFA risks in
expressivity-critical applications.

</details>


### [65] [Probabilistic Stability Guarantees for Feature Attributions](https://arxiv.org/abs/2504.13787)
*Helen Jin,Anton Xue,Weiqiu You,Surbhi Goel,Eric Wong*

Main category: cs.LG

TL;DR: 本论文引入软稳定性，并提出一种简单、模型无关且样本高效的稳定性认证算法（SCA），为特征归因提供非保守的保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖平滑分类器且保证保守，限制了应用，因此需要改进。

Method: 引入软稳定性，提出SCA算法；使用布尔函数分析表征平滑下的稳定性。

Result: 证明轻微平滑允许准确性和稳定性优雅权衡；在视觉和语言任务上评估SCA的有效性。

Conclusion: 软稳定性提升了解释方法的鲁棒性测量，提供更好保证。

Abstract: Stability guarantees are an emerging tool for evaluating feature
attributions, but existing certification methods rely on smoothed classifiers
and often yield conservative guarantees. To address these limitations, we
introduce soft stability and propose a simple, model-agnostic, and
sample-efficient stability certification algorithm (SCA) that provides
non-trivial and interpretable guarantees for any attribution. Moreover, we show
that mild smoothing enables a graceful tradeoff between accuracy and stability,
in contrast to prior certification methods that require a more aggressive
compromise. Using Boolean function analysis, we give a novel characterization
of stability under smoothing. We evaluate SCA on vision and language tasks, and
demonstrate the effectiveness of soft stability in measuring the robustness of
explanation methods.

</details>


### [66] [The Binary and Ternary Quantization Can Improve Feature Discrimination](https://arxiv.org/abs/2504.13792)
*Weizhi Lu,Mingrui Chen,Weiyu Li*

Main category: cs.LG

TL;DR: 这篇论文质疑了量化错误越高分类性能越差的假设，展示了二元和三元量化可以改善特征区分并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前对量化错误的研究缺乏理论基础，且与经验发现矛盾，因此需要直接研究量化数据的特征区分。

Method: 提出直接调查量化数据的特征区分，并通过在图像、语音和文本等各种数据类型上的分类实验进行验证。

Result: 发现二元和三元量化方法可以改善原数据的特征区分，并在分类实验中取得可比或优越的准确性。

Conclusion: 评估量化效果应关注特征区分而非错误，低位宽量化方法可提升性能。

Abstract: In machine learning, quantization is widely used to simplify data
representation and facilitate algorithm deployment on hardware. Given the
fundamental role of classification in machine learning, it is crucial to
investigate the impact of quantization on classification. Current research
primarily focuses on quantization errors, operating under the premise that
higher quantization errors generally result in lower classification
performance. However, this premise lacks a solid theoretical foundation and
often contradicts empirical findings. For instance, certain extremely low
bit-width quantization methods, such as $\{0,1\}$-binary quantization and $\{0,
\pm1\}$-ternary quantization, can achieve comparable or even superior
classification accuracy compared to the original non-quantized data, despite
exhibiting high quantization errors. To more accurately evaluate classification
performance, we propose to directly investigate the feature discrimination of
quantized data, instead of analyzing its quantization error. Interestingly, it
is found that both binary and ternary quantization methods can improve, rather
than degrade, the feature discrimination of the original data. This remarkable
performance is validated through classification experiments across various data
types, including images, speech, and texts.

</details>


### [67] [Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction](https://arxiv.org/abs/2504.13797)
*Yu Wang,Shujie Liu,Shuai Lv,Gengshuo Liu*

Main category: cs.LG

TL;DR: 本论文提出MKDPINN方法，通过元学习和物理信息神经网络在数据稀缺下预测旋转机械剩余寿命，提高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RUL预测方法在目标域数据稀缺和退化动态不明朗时表现不佳，需要整合数据驱动和物理-based方法。

Method: 使用隐状态映射器（HSM）将传感器数据映射到低维隐藏状态空间，物理引导调节器（PGR）学习未知非线性PDE，并通过元学习优化源域任务以适应新目标任务。

Result: 在工业数据和C-MAPSS基准测试中，MKDPINN在泛化能力和准确性上优于基线方法。

Conclusion: 证明了MKDPINN在数据稀缺条件下进行RUL预测的有效性。

Abstract: Predicting the remaining useful life (RUL) of rotating machinery is critical
for industrial safety and maintenance, but existing methods struggle with
scarce target-domain data and unclear degradation dynamics. We propose a
Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network
(MKDPINN) to address these challenges. The method first maps noisy sensor data
to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A
Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing
degradation evolution, embedding these physical constraints into the PINN
framework. This integrates data-driven and physics-based approaches. The
framework uses meta-learning, optimizing across source-domain meta-tasks to
enable few-shot adaptation to new target tasks. Experiments on industrial data
and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization
and accuracy, proving its effectiveness for RUL prediction under data scarcity

</details>


### [68] [Transformer Encoder and Multi-features Time2Vec for Financial Prediction](https://arxiv.org/abs/2504.13801)
*Nguyen Kim Hai Bui,Nguyen Duy Chien,Péter Kovács,Gergő Bognár*

Main category: cs.LG

TL;DR: 本论文提出了一种结合Time2Vec和Transformer Encoder的新神经网络架构，用于金融预测，通过相关性特征选择提高多股票价格预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 金融预测需处理短期波动和长期依赖，Transformer在时间序列分析中成功，但现有研究主要关注单个特征，忽略行业内股票的相关性。

Method: 整合Time2Vec与Transformer Encoder，提出相关性特征选择方法，并通过超参数微调和基准模型比较进行分析。

Result: 该方法优于位置编码等技术，选择相关性特征提升了多股票价格预测准确性。

Conclusion: 结论是，该方法优于现有技术，相关性特征选择显著提高了预测性能。

Abstract: Financial prediction is a complex and challenging task of time series
analysis and signal processing, expected to model both short-term fluctuations
and long-term temporal dependencies. Transformers have remarkable success
mostly in natural language processing using attention mechanism, which also
influenced the time series community. The ability to capture both short and
long-range dependencies helps to understand the financial market and to
recognize price patterns, leading to successful applications of Transformers in
stock prediction. Although, the previous research predominantly focuses on
individual features and singular predictions, that limits the model's ability
to understand broader market trends. In reality, within sectors such as finance
and technology, companies belonging to the same industry often exhibit
correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating
Time2Vec with the Encoder of the Transformer model. Based on the study of
different markets, we propose a novel correlation feature selection method.
Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a
comparative analysis of our results against benchmark models. We conclude that
our method outperforms other state-of-the-art encoding methods such as
positional encoding, and we also conclude that selecting correlation features
enhance the accuracy of predicting multiple stock prices.

</details>


### [69] [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
*Yixuan Even Xu,Yash Savani,Fei Fang,Zico Kolter*

Main category: cs.LG

TL;DR: RL在LLM中面临计算不对称问题，提出PODS框架并使用max-variance down-sampling方法，实验证明性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决RL推理阶段并行高效而策略更新阶段同步内存密集的不对称问题。

Method: 引入PODS框架，解耦rollouts生成和策略更新；开发max-variance down-sampling选择多样奖励信号的rollouts。

Result: 在GSM8K基准上，PODS方法优于标准GRPO。

Conclusion: 证明了方法的算法效率并实证了其优越性。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing
reasoning capabilities in large language models, but faces a fundamental
asymmetry in computation and memory requirements: inference is embarrassingly
parallel with a minimal memory footprint, while policy updates require
extensive synchronization and are memory-intensive. To address this asymmetry,
we introduce PODS (Policy Optimization with Down-Sampling), a framework that
strategically decouples these phases by generating numerous rollouts in
parallel but updating only on an informative subset. Within this framework, we
develop max-variance down-sampling, a theoretically motivated method that
selects rollouts with maximally diverse reward signals. We prove that this
approach has an efficient algorithmic solution, and empirically demonstrate
that GRPO with PODS using max-variance down-sampling achieves superior
performance over standard GRPO on the GSM8K benchmark.

</details>


### [70] [Parameter-Efficient Continual Fine-Tuning: A Survey](https://arxiv.org/abs/2504.13822)
*Eric Nuertey Coleman,Luigi Quarantiello,Ziyue Liu,Qinwen Yang,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 本调查论文探讨了参数高效连续微调（PECFT），结合连续学习（CL）和参数高效微调（PEFT），以解决AI模型在动态环境中的适应问题，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大型预训练模型对i.i.d.假设的依赖性，强调在真实世界动态环境中高效连续学习的必要性，以及CL和PEFT的协同作用。

Method: 方法包括概述CL算法和PEFT技术，审阅PECFT的最新进展，讨论各种方法、评估指标和未来研究方向。

Result: 结果突出了CL和PEFT的协同效应，提供了该领域的指导，并指出了潜在的未来研究方向。

Conclusion: 结论强调PECFT的重要性，旨在指导研究者并推动该领域的创新发展。

Abstract: The emergence of large pre-trained networks has revolutionized the AI field,
unlocking new possibilities and achieving unprecedented performance. However,
these models inherit a fundamental limitation from traditional Machine Learning
approaches: their strong dependence on the \textit{i.i.d.} assumption hinders
their adaptability to dynamic learning scenarios. We believe the next
breakthrough in AI lies in enabling efficient adaptation to evolving
environments -- such as the real world -- where new data and tasks arrive
sequentially. This challenge defines the field of Continual Learning (CL), a
Machine Learning paradigm focused on developing lifelong learning neural
models. One alternative to efficiently adapt these large-scale models is known
Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of
adapting the model to a particular data or scenario by performing small and
efficient modifications, achieving similar performance to full fine-tuning.
However, these techniques still lack the ability to adjust the model to
multiple tasks continually, as they suffer from the issue of Catastrophic
Forgetting. In this survey, we first provide an overview of CL algorithms and
PEFT methods before reviewing the state-of-the-art on Parameter-Efficient
Continual Fine-Tuning (PECFT). We examine various approaches, discuss
evaluation metrics, and explore potential future research directions. Our goal
is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,
guide researchers in this field, and pave the way for novel future research
directions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [71] [CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2504.13472)
*Xinchen Wang,Pengfei Gao,Chao Peng,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: 本文提出CodeVisionary框架，用于改进LLM在代码生成评估中的性能，解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在缺陷：人工方法耗时费力，基于指标的方法依赖参考答案，LLM-based方法缺乏多源知识和对复杂代码的理解。

Method: 提出CodeVisionary框架，包括多源知识分析阶段（制定逐步计划收集知识）和基于协商的评分阶段（多个判断者讨论达成共识）。

Result: 实验显示CodeVisionary优于基线方法，Pearson、Spearman和Kendall-Tau系数平均提升0.202、0.139和0.117，并提供详细评估报告。

Conclusion: CodeVisionary是一种高效的评估框架，可帮助开发者改进，并提供公开资源。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in code
generation, underscoring the critical need for rigorous and comprehensive
evaluation. Existing evaluation approaches fall into three categories,
including human-centered, metric-based, and LLM-based. Considering that
human-centered approaches are labour-intensive and metric-based ones overly
rely on reference answers, LLM-based approaches are gaining increasing
attention due to their stronger contextual understanding capabilities and
superior efficiency. However, the performance of LLM-based approaches remains
limited due to: (1) lack of multisource domain knowledge, and (2) insufficient
comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based
agent framework for evaluating LLMs in code generation. CodeVisionary consists
of two stages: (1) Multiscore knowledge analysis stage, which aims to gather
multisource and comprehensive domain knowledge by formulating and executing a
stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves
multiple judges engaging in discussions to better comprehend the complex code
and reach a consensus on the evaluation score. Extensive experiments
demonstrate that CodeVisionary achieves the best performance for evaluating
LLMs in code generation, outperforming the best baseline methods with average
improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau
coefficients, respectively. Besides, CodeVisionary provides detailed evaluation
reports, which assist developers in identifying shortcomings and making
improvements. The resources of CodeVisionary are available at
https://anonymous.4open.science/r/CodeVisionary.

</details>


### [72] [Large Language Models for Validating Network Protocol Parsers](https://arxiv.org/abs/2504.13515)
*Mingwei Zheng,Danning Xie,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 本论文提出PARVAL框架，使用大语言模型自动验证网络协议解析器与标准的一致性，通过统一表示形式比较，实验在BFD协议上显示低误报率并发现多个漏洞。


<details>
  <summary>Details</summary>
Motivation: 网络协议解析器错误可能导致严重安全问题，现有的方法需手动努力或忽略标准，因此需要更自动化的验证机制。

Method: 提出PARVAL多代理框架，利用大语言模型将协议标准和代码转化为统一'格式规范'，然后进行差异比较以检测不一致性。

Result: 在BFD协议评估中，误报率仅5.6%，发现七个独特错误，其中五个为新发现。

Conclusion: PARVAL框架证明了使用LLM自动验证协议实现的有效性，有助于减少解析器漏洞。

Abstract: Network protocol parsers are essential for enabling correct and secure
communication between devices. Bugs in these parsers can introduce critical
vulnerabilities, including memory corruption, information leakage, and
denial-of-service attacks. An intuitive way to assess parser correctness is to
compare the implementation with its official protocol standard. However, this
comparison is challenging because protocol standards are typically written in
natural language, whereas implementations are in source code. Existing methods
like model checking, fuzzing, and differential testing have been used to find
parsing bugs, but they either require significant manual effort or ignore the
protocol standards, limiting their ability to detect semantic violations. To
enable more automated validation of parser implementations against protocol
standards, we propose PARVAL, a multi-agent framework built on large language
models (LLMs). PARVAL leverages the capabilities of LLMs to understand both
natural language and code. It transforms both protocol standards and their
implementations into a unified intermediate representation, referred to as
format specifications, and performs a differential comparison to uncover
inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection
(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies
inconsistencies between the implementation and its RFC standard, achieving a
low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including
five previously unknown issues.

</details>


### [73] [Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of ChatGPT-Generated Code](https://arxiv.org/abs/2504.13656)
*Antonio Della Porta,Stefano Lambiase,Fabio Palomba*

Main category: cs.SE

TL;DR: 这篇论文研究提示模式对代码质量的影响，发现无显著差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中存在性能不一致和质量问题，需要探索提示工程以提升代码可维护性。

Method: 使用Dev-GPT数据集分析7583个代码文件，比较Zero-Shot、Zero-Shot with Chain-of-Thought和Few-Shot提示模式，并进行Kruskal-Wallis测试。

Result: Zero-Shot提示最常见，代码问题最小，测试显示不同提示模式在可维护性、安全性和可靠性上无显著差异。

Conclusion: 提示结构可能对ChatGPT辅助代码生成的质量指标影响不大。

Abstract: Large Language Models (LLMs) have rapidly transformed software development,
especially in code generation. However, their inconsistent performance, prone
to hallucinations and quality issues, complicates program comprehension and
hinders maintainability. Research indicates that prompt engineering-the
practice of designing inputs to direct LLMs toward generating relevant
outputs-may help address these challenges. In this regard, researchers have
introduced prompt patterns, structured templates intended to guide users in
formulating their requests. However, the influence of prompt patterns on code
quality has yet to be thoroughly investigated. An improved understanding of
this relationship would be essential to advancing our collective knowledge on
how to effectively use LLMs for code generation, thereby enhancing their
understandability in contemporary software development. This paper empirically
investigates the impact of prompt patterns on code quality, specifically
maintainability, security, and reliability, using the Dev-GPT dataset. Results
show that Zero-Shot prompting is most common, followed by Zero-Shot with
Chain-of-Thought and Few-Shot. Analysis of 7583 code files across quality
metrics revealed minimal issues, with Kruskal-Wallis tests indicating no
significant differences among patterns, suggesting that prompt structure may
not substantially impact these quality metrics in ChatGPT-assisted code
generation.

</details>


### [74] [A Survey for What Developers Require in AI-powered Tools that Aid in Component Selection in CBSD](https://arxiv.org/abs/2504.13751)
*Mahdi Jaberzadeh Ansari,Ann Barcomb*

Main category: cs.SE

TL;DR: 本论文通过对近100名CBSD从业者和研究者的混合方法调查，探讨组件选择问题、行业需求和AI工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管CBSD研究已有四十年历史，但缺乏被行业广泛接受的标准组件选择方法或工具，行业与学术之间存在鸿沟。

Method: 采用混合方法调查，针对从事CBSD实践或研究的近100人。

Result: 调查揭示了行业面临的问题、需求解决方案、当前最佳实践、组件选择质量标准的优先级，以及对AI驱动工具的看法。

Conclusion: 强调需要开发整合最新技术进步的组件选择工具，特别是AI驱动工具，以桥接行业与学术差距。

Abstract: Although it has been more than four decades that the first components-based
software development (CBSD) studies were conducted, there is still no standard
method or tool for component selection which is widely accepted by the
industry. The gulf between industry and academia contributes to the lack of an
accepted tool. We conducted a mixed methods survey of nearly 100 people engaged
in component-based software engineering practice or research to better
understand the problems facing industry, how these needs could be addressed,
and current best practices employed in component selection. We also sought to
identify and prioritize quality criteria for component selection from an
industry perspective. In response to the call for CBSD component selection
tools to incorporate recent technical advances, we also explored the
perceptions of professionals about AI-driven tools, present and envisioned.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [75] [Universal Representations for Classification-enhanced Lossy Compression](https://arxiv.org/abs/2504.13191)
*Nam Nguyen*

Main category: cs.CV

TL;DR: 本文探讨通用编码器在失真-感知和失真-分类压缩中的应用，避免重新训练编码器。实验在MNIST上显示感知任务性能损失小，但RDC设置中可能有显著失真增加。


<details>
  <summary>Details</summary>
Motivation: 动机是避免为每个特定失真-感知或失真-分类权衡点重新训练编码器。

Method: 方法包括开发单一通用编码器，支持多个解码目标，并在MNIST数据集上进行实验验证。

Result: 结果显示在MNIST实验中，通用编码器在感知压缩中性能下降最小，但在美国RDC设置中复用时失真显著增加。

Conclusion: 结论是通用编码器适用于感知任务，但在美国RDC中效果有限。

Abstract: In lossy compression, the classical tradeoff between compression rate and
reconstruction distortion has traditionally guided algorithm design. However,
Blau and Michaeli [5] introduced a generalized framework, known as the
rate-distortion-perception (RDP) function, incorporating perceptual quality as
an additional dimension of evaluation. More recently, the
rate-distortion-classification (RDC) function was investigated in [19],
evaluating compression performance by considering classification accuracy
alongside distortion. In this paper, we explore universal representations,
where a single encoder is developed to achieve multiple decoding objectives
across various distortion and classification (or perception) constraints. This
universality avoids retraining encoders for each specific operating point
within these tradeoffs. Our experimental validation on the MNIST dataset
indicates that a universal encoder incurs only minimal performance degradation
compared to individually optimized encoders for perceptual image compression
tasks, aligning with prior results from [23]. Nonetheless, we also identify
that in the RDC setting, reusing an encoder optimized for one specific
classification-distortion tradeoff leads to a significant distortion penalty
when applied to alternative points.

</details>


### [76] [Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems](https://arxiv.org/abs/2504.13648)
*Uthman Baroudi,Alala BaHamid,Yasser Elalfy,Ziad Al Alami*

Main category: cs.CV

TL;DR: TLDR：本论文使用YOLOv8-seg迁移学习检测道路坑洼，结合图像和深度图，提供全面表征，提升安全和维护。


<details>
  <summary>Details</summary>
Motivation: 动机：现有机器学习方法无法全面表征坑洼，且手动分析耗时费力。

Method: 方法：采用预训练YOLOv8-seg模型进行检测和分割，创建新数据集融合图像与深度图，提取坑洼详细信息。

Result: 结果：实现精确定位、面积计算和全面表征，有助于自主车辆导航和道路维护。

Conclusion: 结论：提升道路危险检测能力，辅助维护部门更有效地应对损坏。

Abstract: Road anomaly detection plays a crucial role in road maintenance and in
enhancing the safety of both drivers and vehicles. Recent machine learning
approaches for road anomaly detection have overcome the tedious and
time-consuming process of manual analysis and anomaly counting; however, they
often fall short in providing a complete characterization of road potholes. In
this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg
model for the automatic characterization of potholes using digital images
captured from a dashboard-mounted camera. Our work includes the creation of a
novel dataset, comprising both images and their corresponding depth maps,
collected from diverse road environments in Al-Khobar city and the KFUPM campus
in Saudi Arabia. Our approach performs pothole detection and segmentation to
precisely localize potholes and calculate their area. Subsequently, the
segmented image is merged with its depth map to extract detailed depth
information about the potholes. This integration of segmentation and depth data
offers a more comprehensive characterization compared to previous deep
learning-based road anomaly detection systems. Overall, this method not only
has the potential to significantly enhance autonomous vehicle navigation by
improving the detection and characterization of road hazards but also assists
road maintenance authorities in responding more effectively to road damage.

</details>


### [77] [Intelligent road crack detection and analysis based on improved YOLOv8](https://arxiv.org/abs/2504.13208)
*Haomin Zuo,Zhengyang Li,Jiangchuan Gong,Zhen Tian*

Main category: cs.CV

TL;DR: 这篇论文基于增强的YOLOv8深度学习框架，提出智能道路裂缝检测系统，通过注意力机制提高检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 城市化和交通流量增加导致路面损坏问题加剧，传统手动检测方法效率低下且成本高，需要更智能的解决方案。

Method: 提出基于增强YOLOv8的目标分割模型，使用4029张图像训练，结合ECA和CBAM注意力机制，识别并分析裂缝区域，包括计算最大最小宽度和位置。

Result: 实验结果显示，加入注意力机制显著提升了检测准确性和效率。

Conclusion: 该系统为道路维护和安全监测提供新颖解决方案。

Abstract: As urbanization speeds up and traffic flow increases, the issue of pavement
distress is becoming increasingly pronounced, posing a severe threat to road
safety and service life. Traditional methods of pothole detection rely on
manual inspection, which is not only inefficient but also costly. This paper
proposes an intelligent road crack detection and analysis system, based on the
enhanced YOLOv8 deep learning framework. A target segmentation model has been
developed through the training of 4029 images, capable of efficiently and
accurately recognizing and segmenting crack regions in roads. The model also
analyzes the segmented regions to precisely calculate the maximum and minimum
widths of cracks and their exact locations. Experimental results indicate that
the incorporation of ECA and CBAM attention mechanisms substantially enhances
the model's detection accuracy and efficiency, offering a novel solution for
road maintenance and safety monitoring.

</details>


### [78] [Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance](https://arxiv.org/abs/2504.13211)
*Subin Kim,Hoonrae Kim,Jihyun Lee,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CV

TL;DR: 本文提出多模态方法，使用面部线索提升AI治疗师处理客户抵抗的能力，在认知行为疗法中优于纯文本方法。


<details>
  <summary>Details</summary>
Motivation: 解决文本-based CBT模型处理客户抵抗不足的问题，这会削弱治疗联盟。

Method: 引入合成数据集Mirror，将客户陈述与面部图像配对，训练Vision-Language Models分析面部线索、推断情绪并生成移情响应。

Result: Mirror显著提升AI治疗师处理抵抗的能力，在治疗技能和治疗联盟方面优于现有文本-based CBT方法。

Conclusion: 多模态方法有效地改进了AI在心理治疗中的表现，特别是应对客户抵抗。

Abstract: Recent studies have explored the use of large language models (LLMs) in
psychotherapy; however, text-based cognitive behavioral therapy (CBT) models
often struggle with client resistance, which can weaken therapeutic alliance.
To address this, we propose a multimodal approach that incorporates nonverbal
cues, allowing the AI therapist to better align its responses with the client's
negative emotional state. Specifically, we introduce a new synthetic dataset,
Multimodal Interactive Rolling with Resistance (Mirror), which is a novel
synthetic dataset that pairs client statements with corresponding facial
images. Using this dataset, we train baseline Vision-Language Models (VLMs)
that can analyze facial cues, infer emotions, and generate empathetic responses
to effectively manage resistance. They are then evaluated in terms of both the
therapist's counseling skills and the strength of the therapeutic alliance in
the presence of client resistance. Our results demonstrate that Mirror
significantly enhances the AI therapist's ability to handle resistance, which
outperforms existing text-based CBT approaches.

</details>


### [79] [ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization](https://arxiv.org/abs/2504.13224)
*Fuwei Liu*

Main category: cs.CV

TL;DR: 这篇论文提出ICAS框架，用于高效可控的多主体风格转移。


<details>
  <summary>Details</summary>
Motivation: 多主体风格图像生成面临风格属性模糊和一致应用难题，现有的扩散模型方法计算成本高，难以维护语义保真度。

Method: ICAS通过自适应微调扩散模型内容注入分支，结合IP-Adapter风格注入和ControlNet结构条件化，实现高效风格转移。

Result: 实验显示ICAS在结构保留、风格一致性和推理效率上表现出色。

Conclusion: ICAS为多主体风格转移建立了新范式，适用于实际应用。

Abstract: Generating multi-subject stylized images remains a significant challenge due
to the ambiguity in defining style attributes (e.g., color, texture,
atmosphere, and structure) and the difficulty in consistently applying them
across multiple subjects. Although recent diffusion-based text-to-image models
have achieved remarkable progress, existing methods typically rely on
computationally expensive inversion procedures or large-scale stylized
datasets. Moreover, these methods often struggle with maintaining multi-subject
semantic fidelity and are limited by high inference costs. To address these
limitations, we propose ICAS (IP-Adapter and ControlNet-based Attention
Structure), a novel framework for efficient and controllable multi-subject
style transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only
the content injection branch of a pre-trained diffusion model, thereby
preserving identity-specific semantics while enhancing style controllability.
By combining IP-Adapter for adaptive style injection with ControlNet for
structural conditioning, our framework ensures faithful global layout
preservation alongside accurate local style synthesis. Furthermore, ICAS
introduces a cyclic multi-subject content embedding mechanism, which enables
effective style transfer under limited-data settings without the need for
extensive stylized corpora. Extensive experiments show that ICAS achieves
superior performance in structure preservation, style consistency, and
inference efficiency, establishing a new paradigm for multi-subject style
transfer in real-world applications.

</details>


### [80] [WildFireCan-MMD: A Multimodal dataset for Classification of User-generated Content During Wildfires in Canada](https://arxiv.org/abs/2504.13231)
*Braeden Sherritt,Isar Nejadgholi,Marzieh Amini*

Main category: cs.CV

TL;DR: 本论文引入WildFireCan-MMD数据集，用于社交媒体野火信息提取，显示训练模型比零样本提示更有效，提高至23%。


<details>
  <summary>Details</summary>
Motivation: 野火期间快速信息获取至关重要，传统数据源缓慢且昂贵，社交媒体提供实时更新，但提取相关洞见具有挑战。

Method: 提出WildFireCan-MMD多模态数据集，包含加拿大野火X帖子，标注13个关键主题；评估视觉语言模型和自定义训练分类器，比较零样本提示与训练模型。

Result: 训练模型在有标注数据时比零样本提示性能提升高达23%，即使简单模型也表现出色。

Conclusion: 强调定制化、区域化数据集的重要性，以及任务特定训练，以适应不同地区灾害响应需求。

Abstract: Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across 13 key themes. Evaluating both Vision Language Models and custom-trained
classifiers, we show that while zero-shot prompting offers quick deployment,
even simple trained models outperform them when labelled data is available, by
up to 23%. Our findings highlight the enduring importance of tailored datasets
and task-specific training. Importantly, such datasets should be localized, as
disaster response requirements vary across regions and contexts.

</details>


### [81] [SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling](https://arxiv.org/abs/2504.13310)
*Yasin Almalioglu,Andrzej Kucik,Geoffrey French,Dafni Antotsiou,Alexander Adam,Cedric Archambeau*

Main category: cs.CV

TL;DR: 这篇论文引入TRANSAR，一种自监督端到端视觉Transformer模型，用于SAR图像物体检测，通过掩码图像预训练和自适应采样技术解决数据稀缺和小物体检测挑战。


<details>
  <summary>Details</summary>
Motivation: SAR图像物体检测面临数据标注稀缺、低空间分辨率、噪声等问题，限制了其在城市监测和灾害响应中的应用。

Method: TRANSAR使用无标签SAR数据集的掩码图像预训练，结合辅助二元语义分割和基于课程学习及模型反馈的自适应采样调度器。

Result: 在基准SAR数据集评估中，TRANSAR优于传统模型如DeepLabv3、UNet和最先进自监督模型如DPT、SegFormer、UPerNet。

Conclusion: 该方法证明自监督学习在SAR物体检测中的有效性，特别是小物体检测，并为有限标注数据下的发展提供新路径。

Abstract: Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery
holds immense potential in tasks such as urban monitoring and disaster
response. However, the inherent complexities of SAR data and the scarcity of
annotations present significant challenges in the advancement of object
detection in this domain. Notably, the detection of small objects in
satellite-borne SAR images poses a particularly intricate problem, because of
the technology's relatively low spatial resolution and inherent noise.
Furthermore, the lack of large labelled SAR datasets hinders the development of
supervised deep learning-based object detection models. In this paper, we
introduce TRANSAR, a novel self-supervised end-to-end vision transformer-based
SAR object detection model that incorporates masked image pre-training on an
unlabeled SAR image dataset that spans more than $25,700$ km\textsuperscript{2}
ground area. Unlike traditional object detection formulation, our approach
capitalises on auxiliary binary semantic segmentation, designed to segregate
objects of interest during the post-tuning, especially the smaller ones, from
the background. In addition, to address the innate class imbalance due to the
disproportion of the object to the image size, we introduce an adaptive
sampling scheduler that dynamically adjusts the target class distribution
during training based on curriculum learning and model feedback. This approach
allows us to outperform conventional supervised architecture such as DeepLabv3
or UNet, and state-of-the-art self-supervised learning-based arhitectures such
as DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark
SAR datasets.

</details>


### [82] [VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture](https://arxiv.org/abs/2504.13365)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 简而言之：VLLFL 是一个轻量级联邦学习框架，使用视觉语言模型进行农业物体检测，提高性能、减少通信开销并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 动机：解决农业物体检测中数据收集规模大和隐私问题的挑战。

Method: 方法：提出 VLLFL 框架，利用视觉语言模型的泛化能力和联邦学习，训练紧凑提示生成器以提升性能并降低通信开销。

Result: 结果：实验显示 VLM 性能提升 14.53%，通信开销减少 99.3%，适用于水果识别和动物检测等任务。

Conclusion: 结论：VLLFL 提供高效、可扩展、隐私保护的农业解决方案。

Abstract: In modern smart agriculture, object detection plays a crucial role by
enabling automation, precision farming, and monitoring of resources. From
identifying crop health and pest infestations to optimizing harvesting
processes, accurate object detection enhances both productivity and
sustainability. However, training object detection models often requires
large-scale data collection and raises privacy concerns, particularly when
sensitive agricultural data is distributed across farms. To address these
challenges, we propose VLLFL, a vision-language model-based lightweight
federated learning framework (VLLFL). It harnesses the generalization and
context-aware detection capabilities of the vision-language model (VLM) and
leverages the privacy-preserving nature of federated learning. By training a
compact prompt generator to boost the performance of the VLM deployed across
different farms, VLLFL preserves privacy while reducing communication overhead.
Experimental results demonstrate that VLLFL achieves 14.53% improvement in the
performance of VLM while reducing 99.3% communication overhead. Spanning tasks
from identifying a wide variety of fruits to detecting harmful animals in
agriculture, the proposed framework offers an efficient, scalable, and
privacy-preserving solution specifically tailored to agricultural applications.

</details>


### [83] [Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety](https://arxiv.org/abs/2504.13399)
*Shashank Shriram,Srinivasa Perisetla,Aryan Keskar,Harsha Krishnaswamy,Tonko Emil Westerhof Bossen,Andreas Møgelmose,Ross Greer*

Main category: cs.CV

TL;DR: 这篇论文提出了一种多模态方法，用于零样本危险物检测，提高自动驾驶中的危险识别和解释。


<details>
  <summary>Details</summary>
Motivation: 检测视觉数据中的异常危险物是自动驾驶的关键挑战，现有的模型依赖预定义类别，难以处理不可预测的危险物。

Method: 整合视觉-语言模型(VLM)、大型语言模型(LLM)和CLIP进行零样本物体检测，创建扩展的COOOL数据集，使用余弦相似度评估语义相似性。

Result: 评估突出了视觉-语言方法的优势和局限性，并发布了工具和数据集。

Conclusion: 为未来自动危险检测系统的改进提供了见解，并开源了模型和数据。

Abstract: Detecting anomalous hazards in visual data, particularly in video streams, is
a critical challenge in autonomous driving. Existing models often struggle with
unpredictable, out-of-label hazards due to their reliance on predefined object
categories. In this paper, we propose a multimodal approach that integrates
vision-language reasoning with zero-shot object detection to improve hazard
identification and explanation. Our pipeline consists of a Vision-Language
Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects
within a traffic scene. We refine object detection by incorporating OpenAI's
CLIP model to match predicted hazards with bounding box annotations, improving
localization accuracy. To assess model performance, we create a ground truth
dataset by denoising and extending the foundational COOOL
(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete
natural language descriptions for hazard annotations. We define a means of
hazard detection and labeling evaluation on the extended dataset using cosine
similarity. This evaluation considers the semantic similarity between the
predicted hazard description and the annotated ground truth for each video.
Additionally, we release a set of tools for structuring and managing
large-scale hazard detection datasets. Our findings highlight the strengths and
limitations of current vision-language-based approaches, offering insights into
future improvements in autonomous hazard detection systems. Our models,
scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git

</details>


### [84] [LoRA-Based Continual Learning with Constraints on Critical Parameter Changes](https://arxiv.org/abs/2504.13407)
*Shimou Ling,Liang Zhang,Jiangwei Zhao,Lili Pan,Hongliang Li*

Main category: cs.CV

TL;DR: 本文提出冻结关键参数和LoRAC方法来提升持续学习性能，达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 正交LoRA调优虽能减少遗忘，但预任务关键参数仍会改变，因此需直接冻结这些参数。

Method: 冻结Vision Transformer中预任务的关键参数矩阵，并基于QR分解提出正交LoRA组合(LoRAC)。

Result: 在Split CIFAR-100数据集上，准确率提升6.35%，遗忘减少3.24%，并在多个基准上达到最先进性能。

Conclusion: 实验证明该方法有效，提供了代码支持。

Abstract: LoRA-based continual learning represents a promising avenue for leveraging
pre-trained models in downstream continual learning tasks. Recent studies have
shown that orthogonal LoRA tuning effectively mitigates forgetting. However,
this work unveils that under orthogonal LoRA tuning, the critical parameters
for pre-tasks still change notably after learning post-tasks. To address this
problem, we directly propose freezing the most critical parameter matrices in
the Vision Transformer (ViT) for pre-tasks before learning post-tasks. In
addition, building on orthogonal LoRA tuning, we propose orthogonal LoRA
composition (LoRAC) based on QR decomposition, which may further enhance the
plasticity of our method. Elaborate ablation studies and extensive comparisons
demonstrate the effectiveness of our proposed method. Our results indicate that
our method achieves state-of-the-art (SOTA) performance on several well-known
continual learning benchmarks. For instance, on the Split CIFAR-100 dataset,
our method shows a 6.35\% improvement in accuracy and a 3.24\% reduction in
forgetting compared to previous methods. Our code is available at
https://github.com/learninginvision/LoRAC-IPC.

</details>


### [85] [Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization](https://arxiv.org/abs/2504.13460)
*Hongwei Ji,Wulian Yun,Mengshi Qi,Huadong Ma*

Main category: cs.CV

TL;DR: 本论文提出一种使用链式思考文本推理的少样本时间动作定位方法，整合文本和视觉信息，提高性能，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有少样本TAL方法依赖视频信息而忽略文本语义支持，本文旨在通过文本推理提升动作定位的鲁棒性和泛化能力。

Method: 设计一个语义感知文本-视觉对齐模块和链式思考推理框架，使用VLM和LLM生成视频文本描述，以捕捉动作变异。

Result: 在ActivityNet1.3和THUMOS14数据集上实验，引入人类异常定位新数据集，方法在单多实例场景下显著优于现有方法。

Conclusion: 该方法有效提升了少样本TAL性能，并将发布代码、数据和基准，促进领域发展。

Abstract: Traditional temporal action localization (TAL) methods rely on large amounts
of detailed annotated data, whereas few-shot TAL reduces this dependence by
using only a few training samples to identify unseen action categories.
However, existing few-shot TAL methods typically focus solely on video-level
information, neglecting textual information, which can provide valuable
semantic support for the localization task. Therefore, we propose a new
few-shot temporal action localization method by Chain-of-Thought textual
reasoning to improve localization performance. Specifically, we design a novel
few-shot learning framework that leverages textual semantic information to
enhance the model's ability to capture action commonalities and variations,
which includes a semantic-aware text-visual alignment module designed to align
the query and support videos at different levels. Meanwhile, to better express
the temporal dependencies and causal relationships between actions at the
textual level to assist action localization, we design a Chain of Thought
(CoT)-like reasoning method that progressively guides the Vision Language Model
(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for
videos. The generated texts can capture more variance of action than visual
features. We conduct extensive experiments on the publicly available
ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named
Human-related Anomaly Localization and explore the application of the TAL task
in human anomaly detection. The experimental results demonstrate that our
proposed method significantly outperforms existing methods in single-instance
and multi-instance scenarios. We will release our code, data and benchmark.

</details>


### [86] [Beyond One-Hot Labels: Semantic Mixing for Model Calibration](https://arxiv.org/abs/2504.13548)
*Haoyang Luo,Linwei Tao,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 本论文提出校准感知语义混合（CSM）框架，使用扩散模型生成合成数据以提升模型校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法依赖one-hot标签，缺乏不确定性知识，需要合成带置信度标签的数据集。

Method: 引入CSM框架，通过扩散模型混合类特征并标注置信度分数，结合校准再标注和优化损失函数。

Result: 实验结果显示CSM在模型校准上优于现有方法。

Conclusion: CSM是一种有效的合成数据技术，能改善模型的置信度校准。

Abstract: Model calibration seeks to ensure that models produce confidence scores that
accurately reflect the true likelihood of their predictions being correct.
However, existing calibration approaches are fundamentally tied to datasets of
one-hot labels implicitly assuming full certainty in all the annotations. Such
datasets are effective for classification but provides insufficient knowledge
of uncertainty for model calibration, necessitating the curation of datasets
with numerically rich ground-truth confidence values. However, due to the
scarcity of uncertain visual examples, such samples are not easily available as
real datasets. In this paper, we introduce calibration-aware data augmentation
to create synthetic datasets of diverse samples and their ground-truth
uncertainty. Specifically, we present Calibration-aware Semantic Mixing (CSM),
a novel framework that generates training samples with mixed class
characteristics and annotates them with distinct confidence scores via
diffusion models. Based on this framework, we propose calibrated reannotation
to tackle the misalignment between the annotated confidence score and the
mixing ratio during the diffusion reverse process. Besides, we explore the loss
functions that better fit the new data representation paradigm. Experimental
results demonstrate that CSM achieves superior calibration compared to the
state-of-the-art calibration approaches. Code is available at
github.com/E-Galois/CSM.

</details>


### [87] [SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification](https://arxiv.org/abs/2504.13220)
*Ummay Maria Muna,Md. Mehedi Hasan Shawon,Md Jobayer,Sumaiya Akter,Saifur Rahman Sabuj*

Main category: cs.CV

TL;DR: 本篇论文提出了一种SSTAF Transformer，用于基于EEG的运动想象分类，实现了比现有方法更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决EEG信号非平稳性和主体间变异性带来的挑战，以开发鲁棒的跨主体分类模型。

Method: 引入SSTAF Transformer，包括空间、光谱和时间注意机制，并使用短时傅里叶变换提取时间-频率域特征。

Result: 在EEGMMIDB数据集上准确率达76.83%，在BCI Competition IV-2a上达68.30%，优于传统CNN-based和部分transformer-based方法。

Conclusion: SSTAF Transformer展示了改进的性能，为脑机接口应用提供有前景的解决方案。

Abstract: Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor
imagery classification offer promising solutions in neurorehabilitation and
assistive technologies by enabling communication between the brain and external
devices. However, the non-stationary nature of EEG signals and significant
inter-subject variability cause substantial challenges for developing robust
cross-subject classification models. This paper introduces a novel
Spatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically
designed for upper-limb motor imagery classification. Our architecture consists
of a spectral transformer and a spatial transformer, followed by a transformer
block and a classifier network. Each module is integrated with attention
mechanisms that dynamically attend to the most discriminative patterns across
multiple domains, such as spectral frequencies, spatial electrode locations,
and temporal dynamics. The short-time Fourier transform is incorporated to
extract features in the time-frequency domain to make it easier for the model
to obtain a better feature distinction. We evaluated our SSTAF Transformer
model on two publicly available datasets, the EEGMMIDB dataset, and BCI
Competition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30%
in the data sets, respectively, outperforms traditional CNN-based architectures
and a few existing transformer-based approaches.

</details>


### [88] [Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation](https://arxiv.org/abs/2504.13560)
*SoYoung Park,Hyewon Lee,Mingyu Choi,Seunghoon Han,Jong-Ryul Lee,Sungsu Lim,Tae-Ho Kim*

Main category: cs.CV

TL;DR: This paper introduces IAP-AS, a method that uses image tagging and LLMs to create dynamic prompts for improved anomaly segmentation in industrial applications, achieving up to 10% better F1-max scores.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided zero-shot anomaly segmentation models rely on fixed prompts, limiting adaptability in diverse industrial scenarios and highlighting the need for flexible, context-aware prompting strategies.

Method: Proposes IAP-AS, which generates dynamic, context-aware prompts by extracting object attributes from images using an image tagging model and a large language model (LLM).

Result: Improves the F1-max metric by up to 10%, demonstrating superior adaptability and generalization in experiments.

Conclusion: IAP-AS provides a scalable solution for anomaly segmentation across various industries.

Abstract: Anomaly segmentation is essential for industrial quality, maintenance, and
stability. Existing text-guided zero-shot anomaly segmentation models are
effective but rely on fixed prompts, limiting adaptability in diverse
industrial scenarios. This highlights the need for flexible, context-aware
prompting strategies. We propose Image-Aware Prompt Anomaly Segmentation
(IAP-AS), which enhances anomaly segmentation by generating dynamic,
context-aware prompts using an image tagging model and a large language model
(LLM). IAP-AS extracts object attributes from images to generate context-aware
prompts, improving adaptability and generalization in dynamic and unstructured
industrial environments. In our experiments, IAP-AS improves the F1-max metric
by up to 10%, demonstrating superior adaptability and generalization. It
provides a scalable solution for anomaly segmentation across industries

</details>


### [89] [LIFT+: Lightweight Fine-Tuning for Long-Tail Learning](https://arxiv.org/abs/2504.13282)
*Jiang-Xin Shi,Tong Wei,Yu-Feng Li*

Main category: cs.CV

TL;DR: 这篇论文揭示了在长尾学习中微调策略的误用，并提出LIFT+框架，通过轻量级微调等方法提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨微调策略对长尾学习性能的影响，指出现有方法的误用和改进潜力。

Method: 提出LIFT+框架，包括轻量级微调、语义感知初始化、最小化数据增强和测试时集成。

Result: LIFT+将训练周期减少到≤15个，并将学习参数减少到不足1%，在性能上显著优于现有方法。

Conclusion: LIFT+提供高效、准确的微调管道，促进快速收敛和模型紧凑性。

Abstract: The fine-tuning paradigm has emerged as a prominent approach for addressing
long-tail learning tasks in the era of foundation models. However, the impact
of fine-tuning strategies on long-tail learning performance remains unexplored.
In this work, we disclose that existing paradigms exhibit a profound misuse of
fine-tuning methods, leaving significant room for improvement in both
efficiency and accuracy. Specifically, we reveal that heavy fine-tuning
(fine-tuning a large proportion of model parameters) can lead to non-negligible
performance deterioration on tail classes, whereas lightweight fine-tuning
demonstrates superior effectiveness. Through comprehensive theoretical and
empirical validation, we identify this phenomenon as stemming from inconsistent
class conditional distributions induced by heavy fine-tuning. Building on this
insight, we propose LIFT+, an innovative lightweight fine-tuning framework to
optimize consistent class conditions. Furthermore, LIFT+ incorporates
semantic-aware initialization, minimalist data augmentation, and test-time
ensembling to enhance adaptation and generalization of foundation models. Our
framework provides an efficient and accurate pipeline that facilitates fast
convergence and model compactness. Extensive experiments demonstrate that LIFT+
significantly reduces both training epochs (from $\sim$100 to $\leq$15) and
learned parameters (less than 1%), while surpassing state-of-the-art approaches
by a considerable margin. The source code is available at
https://github.com/shijxcs/LIFT-plus.

</details>


### [90] [HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering](https://arxiv.org/abs/2504.13590)
*Alexander Rusnak,Frédéric Kaplan*

Main category: cs.CV

TL;DR: 本文提出HAEC方法，用于城市规模开放词汇3D场景理解，采用混合专家图变换器，在SensatUrban数据集上首次应用，并开发合成标注管道。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景理解依赖手动标注，现有开放词汇方法无法高效扩展到城市规模数据集。

Method: HAEC是一种基于超点图聚类的层次方法，使用新型混合专家图变换器作为骨干，并从原始点云中衍生合成标注管道。

Result: 首次在SensatUrban城市规模数据集上实现开放词汇场景理解，并展示了合成标注管道的有效性。

Conclusion: 该技术可解锁密集城市3D场景的复杂操作，并为数字孪生处理开辟新路径。

Abstract: Traditional 3D scene understanding techniques are generally predicated on
hand-annotated label sets, but in recent years a new class of open-vocabulary
3D scene understanding techniques has emerged. Despite the success of this
paradigm on small scenes, existing approaches cannot scale efficiently to
city-scale 3D datasets. In this paper, we present Hierarchical vocab-Agnostic
Expert Clustering (HAEC), after the latin word for 'these', a superpoint graph
clustering based approach which utilizes a novel mixture of experts graph
transformer for its backbone. We administer this highly scalable approach to
the first application of open-vocabulary scene understanding on the SensatUrban
city-scale dataset. We also demonstrate a synthetic labeling pipeline which is
derived entirely from the raw point clouds with no hand-annotation. Our
technique can help unlock complex operations on dense urban 3D scenes and open
a new path forward in the processing of digital twins.

</details>


### [91] [How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings](https://arxiv.org/abs/2504.13412)
*Samuel Audia,Soheil Feizi,Matthias Zwicker,Dinesh Manocha*

Main category: cs.CV

TL;DR: 这篇论文比较了傅里叶特征编码（FFE）和多网格参数编码（MPE）在低维映射神经网络中的性能，使用NTK分析证明MPE通过网格结构提升性能，并在图像和3D表面回归任务中实证验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 神经网络难以学习高频信息，本文比较FFE和MPE两种缓解谱偏差的技术，探究MPE在分辨率和细节上的优势。

Method: 使用神经切线核（NTK）通过核回归分析FFE和MPE，证明MPE通过网格结构提升最小特征值，并通过2D图像和3D隐式表面回归实验验证。

Result: MPE将最小特征值比基线提高8个数量级，比FFE提高2个数量级；在PSNR和MS-SSIM指标上，MPE比基线提高15 dB / 0.65，比FFE提高12 dB / 0.33。

Conclusion: MPE通过网格结构而非嵌入空间改善性能，与FFE机制不同；MPE在学习精细细节时无别名问题，并显示出更好性能。

Abstract: Neural networks that map between low dimensional spaces are ubiquitous in
computer graphics and scientific computing; however, in their naive
implementation, they are unable to learn high frequency information. We present
a comprehensive analysis comparing the two most common techniques for
mitigating this spectral bias: Fourier feature encodings (FFE) and multigrid
parametric encodings (MPE). FFEs are seen as the standard for low dimensional
mappings, but MPEs often outperform them and learn representations with higher
resolution and finer detail. FFE's roots in the Fourier transform, make it
susceptible to aliasing if pushed too far, while MPEs, which use a learned grid
structure, have no such limitation. To understand the difference in
performance, we use the neural tangent kernel (NTK) to evaluate these encodings
through the lens of an analogous kernel regression. By finding a lower bound on
the smallest eigenvalue of the NTK, we prove that MPEs improve a network's
performance through the structure of their grid and not their learnable
embedding. This mechanism is fundamentally different from FFEs, which rely
solely on their embedding space to improve performance. Results are empirically
validated on a 2D image regression task using images taken from 100 synonym
sets of ImageNet and 3D implicit surface regression on objects from the
Stanford graphics dataset. Using peak signal-to-noise ratio (PSNR) and
multiscale structural similarity (MS-SSIM) to evaluate how well fine details
are learned, we show that the MPE increases the minimum eigenvalue by 8 orders
of magnitude over the baseline and 2 orders of magnitude over the FFE. The
increase in spectrum corresponds to a 15 dB (PSNR) / 0.65 (MS-SSIM) increase
over baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the FFE.

</details>


### [92] [MicroFlow: Domain-Specific Optical Flow for Ground Deformation Estimation in Seismic Events](https://arxiv.org/abs/2504.13452)
*Juliette Bertrand,Sophie Giffard-Roisin,James Hollingsworth,Julien Mairal*

Main category: cs.CV

TL;DR: 本论文提出了一种新的深度学习模型，用于精确估计地表位移，改进了传统方法。


<details>
  <summary>Details</summary>
Motivation: 地表位移测量重要但难以直接获取，现有深度学习方法在缺乏真实地面真相、需要亚像素精度和时间变化等方面存在挑战。

Method: 提出使用迭代细化、显式扭曲层和不依赖相关性的主干网络的模型，并结合非凸总变差正则化以保持断层线清晰度。

Result: 在半合成基准上显著优于现有地球物理方法，并在真实世界中高分辨率场景中泛化良好。

Conclusion: 该模型提升了地表位移估计的精度和鲁棒性。

Abstract: Dense ground displacement measurements are crucial for geological studies but
are impractical to collect directly. Traditionally, displacement fields are
estimated using patch matching on optical satellite images from different
acquisition times. While deep learning-based optical flow models are promising,
their adoption in ground deformation analysis is hindered by challenges such as
the absence of real ground truth, the need for sub-pixel precision, and
temporal variations due to geological or anthropogenic changes. In particular,
we identify that deep learning models relying on explicit correlation layers
struggle at estimating small displacements in real-world conditions. Instead,
we propose a model that employs iterative refinements with explicit warping
layers and a correlation-independent backbone, enabling sub-pixel precision.
Additionally, a non-convex variant of Total Variation regularization preserves
fault-line sharpness while maintaining smoothness elsewhere. Our model
significantly outperforms widely used geophysics methods on semi-synthetic
benchmarks and generalizes well to challenging real-world scenarios captured by
both medium- and high-resolution sensors. Project page:
https://jbertrand89.github.io/microflow/.

</details>


### [93] [AnyTSR: Any-Scale Thermal Super-Resolution for UAV](https://arxiv.org/abs/2504.13682)
*Mengyuan Li,Changhong Fu,Ziyu Lu,Zijie Zhang,Haobo Zuo,Liangliang Yao*

Main category: cs.CV

TL;DR: 本论文提出一种新型任意比例热成像超分辨率方法AnyTSR，用于提升无人机在复杂环境中的应用性能。


<details>
  <summary>Details</summary>
Motivation: 热成像传感器分辨率低导致细节不足和边界模糊，现有超分辨率方法针对固定比例，计算开销大且不灵活。

Method: 提出AnyTSR方法，包括新图像编码器用于特征编码、任意比例上采样器用于减少伪影，并构建UAV-TSR数据集。

Result: 实验结果显示AnyTSR在所有缩放因子上优于现有方法，生成更准确和详细的高分辨率图像。

Conclusion: 该方法有效解决了热成像在无人机中的低分辨率问题，提高了应用灵活性和性能。

Abstract: Thermal imaging can greatly enhance the application of intelligent unmanned
aerial vehicles (UAV) in challenging environments. However, the inherent low
resolution of thermal sensors leads to insufficient details and blurred
boundaries. Super-resolution (SR) offers a promising solution to address this
issue, while most existing SR methods are designed for fixed-scale SR. They are
computationally expensive and inflexible in practical applications. To address
above issues, this work proposes a novel any-scale thermal SR method (AnyTSR)
for UAV within a single model. Specifically, a new image encoder is proposed to
explicitly assign specific feature code to enable more accurate and flexible
representation. Additionally, by effectively embedding coordinate offset
information into the local feature ensemble, an innovative any-scale upsampler
is proposed to better understand spatial relationships and reduce artifacts.
Moreover, a novel dataset (UAV-TSR), covering both land and water scenes, is
constructed for thermal SR tasks. Experimental results demonstrate that the
proposed method consistently outperforms state-of-the-art methods across all
scaling factors as well as generates more accurate and detailed high-resolution
images. The code is located at https://github.com/vision4robotics/AnyTSR.

</details>


### [94] [Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration](https://arxiv.org/abs/2504.13717)
*Gianluca Carloni*

Main category: cs.CV

TL;DR: 本论文通过可解释性、因果性和生物视觉角度，将深度学习与人类推理对齐，提高图像分类的效率、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将深度学习与人类推理能力对齐，以实现更高效、可解释和鲁棒的图像分类，特别是针对医疗图像的应用。

Method: 从三个视角进行：评估神经网络可视化技术并验证可解释方法；回顾XAI和因果性，提出因果模块和CROCODILE框架；探索生物视觉，提出CoCoReco网络。

Result: 关键发现包括：激活最大化缺乏洞察，原型学习有效，XAI与因果ML深度连接，弱因果信号改善性能和可解释性，框架在医疗领域泛化，生物灵感提升人类对齐识别。

Conclusion: 本工作促进人类对齐的深度学习，并为桥接研究与临床应用提供途径，提高信任、诊断准确性和安全部署。

Abstract: This work aligns deep learning (DL) with human reasoning capabilities and
needs to enable more efficient, interpretable, and robust image classification.
We approach this from three perspectives: explainability, causality, and
biological vision. Introduction and background open this work before diving
into operative chapters. First, we assess neural networks' visualization
techniques for medical images and validate an explainable-by-design method for
breast mass classification. A comprehensive review at the intersection of XAI
and causality follows, where we introduce a general scaffold to organize past
and future research, laying the groundwork for our second perspective. In the
causality direction, we propose novel modules that exploit feature
co-occurrence in medical images, leading to more effective and explainable
predictions. We further introduce CROCODILE, a general framework that
integrates causal concepts, contrastive learning, feature disentanglement, and
prior knowledge to enhance generalization. Lastly, we explore biological
vision, examining how humans recognize objects, and propose CoCoReco, a
connectivity-inspired network with context-aware attention mechanisms. Overall,
our key findings include: (i) simple activation maximization lacks insight for
medical imaging DL models; (ii) prototypical-part learning is effective and
radiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak
causal signals can be leveraged without a priori information to improve
performance and interpretability; (v) our framework generalizes across medical
domains and out-of-distribution data; (vi) incorporating biological circuit
motifs improves human-aligned recognition. This work contributes toward
human-aligned DL and highlights pathways to bridge the gap between research and
clinical adoption, with implications for improved trust, diagnostic accuracy,
and safe deployment.

</details>


### [95] [ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis](https://arxiv.org/abs/2504.13745)
*Andrea Rigo,Luca Stornaiuolo,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种通过新数据集和微调框架提升文本到图像生成spatial一致性的方法，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在文本提示中spatial关系渲染不足的问题，以及现有方法的高计算成本和灵活性限制。

Method: 构建了基于LAION-400M的spatial明确提示数据集，开发了ESPLoRA框架（使用低秩适配微调），并提出改进的几何约束评价指标和TORE算法。

Result: 在spatial一致性基准测试中，比CoMPaSS框架提高了13.33%。

Conclusion: 该方法在不增加生成时间和不降低输出质量的情况下显著提升了spatial一致性。

Abstract: Diffusion models have revolutionized text-to-image (T2I) synthesis, producing
high-quality, photorealistic images. However, they still struggle to properly
render the spatial relationships described in text prompts. To address the lack
of spatial information in T2I generations, existing methods typically use
external network conditioning and predefined layouts, resulting in higher
computational costs and reduced flexibility. Our approach builds upon a curated
dataset of spatially explicit prompts, meticulously extracted and synthesized
from LAION-400M to ensure precise alignment between textual descriptions and
spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible
fine-tuning framework based on Low-Rank Adaptation, specifically designed to
enhance spatial consistency in generative models without increasing generation
time or compromising the quality of the outputs. In addition to ESPLoRA, we
propose refined evaluation metrics grounded in geometric constraints, capturing
3D spatial relations such as \textit{in front of} or \textit{behind}. These
metrics also expose spatial biases in T2I models which, even when not fully
mitigated, can be strategically exploited by our TORE algorithm to further
improve the spatial consistency of generated images. Our method outperforms the
current state-of-the-art framework, CoMPaSS, by 13.33% on established spatial
consistency benchmarks.

</details>


### [96] [Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis](https://arxiv.org/abs/2504.13754)
*Zhu Zhu,Shuo Jiang,Jingyuan Zheng,Yawen Li,Yifei Chen,Manli Zhao,Weizhong Gu,Feiwei Qin,Jinhu Wang,Gang Yu*

Main category: cs.CV

TL;DR: 本篇论文提出CMSwinKAN模型，用于从全滑片图像中分类神经母细胞瘤，提高准确性和可解释性，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决病理诊断主观性导致的不一致性，以及自动方法的可解释性差、特征提取能力有限和高计算成本问题。

Method: 提出基于对比学习的CMSwinKAN模型，改进了Swin Transformer，融入核激活网络、多尺度特征融合和启发式软投票机制。

Result: 在PpNTs和BreakHis数据集上验证，表现优于最先进模型。

Conclusion: CMSwinKAN提升了病理图像分类的准确性和可解释性，代码已开源。

Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid
malignancies, characterized by significant clinical heterogeneity. Timely and
accurate pathological diagnosis from hematoxylin and eosin-stained whole slide
images is critical for patient prognosis. However, current diagnostic practices
primarily rely on subjective manual examination by pathologists, leading to
inconsistent accuracy. Existing automated whole slide image classification
methods encounter challenges such as poor interpretability, limited feature
extraction capabilities, and high computational costs, restricting their
practical clinical deployment. To overcome these limitations, we propose
CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model
tailored for pathological image classification, which enhances the Swin
Transformer architecture by integrating a Kernel Activation Network within its
multilayer perceptron and classification head modules, significantly improving
both interpretability and accuracy. By fusing multi-scale features and
leveraging contrastive learning strategies, CMSwinKAN mimics clinicians'
comprehensive approach, effectively capturing global and local tissue
characteristics. Additionally, we introduce a heuristic soft voting mechanism
guided by clinical insights to seamlessly bridge patch-level predictions to
whole slide image-level classifications. We validate CMSwinKAN on the PpNTs
dataset, which was collaboratively established with our partner hospital and
the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN
performs better than existing state-of-the-art pathology-specific models
pre-trained on large datasets. Our source code is available at
https://github.com/JSLiam94/CMSwinKAN.

</details>


### [97] [Decoding Vision Transformers: the Diffusion Steering Lens](https://arxiv.org/abs/2504.13763)
*Ryota Takatsuki,Sonia Joseph,Ippei Fujisawa,Ryota Kanai*

Main category: cs.CV

TL;DR: 本论文提出Diffusion Steering Lens (DSL)，一种无训练方法，用于提升Vision Transformers的机制解释性。


<details>
  <summary>Details</summary>
Motivation: Logit Lens在Vision Transformers上应用有限，Diffusion Lens无法捕获子模块的直接贡献，因此需要改进解释性方法。

Method: 提出DSL，通过引导子模块输出并修补间接贡献，实现训练-free的解释性分析。

Result: 通过干预研究验证，DSL提供直观且可靠的Vision Transformers内部处理解释。

Conclusion: DSL克服了现有方法的局限性，为Vision Transformers的解释性提供了有效工具。

Abstract: Logit Lens is a widely adopted method for mechanistic interpretability of
transformer-based language models, enabling the analysis of how internal
representations evolve across layers by projecting them into the output
vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is
technically straightforward, its direct use faces limitations in capturing the
richness of visual representations. Building on the work of Toker et al.
(2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize
intermediate representations in the text encoders of text-to-image diffusion
models, we demonstrate that while Diffusion Lens can effectively visualize
residual stream representations in image encoders, it fails to capture the
direct contributions of individual submodules. To overcome this limitation, we
propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach
that steers submodule outputs and patches subsequent indirect contributions. We
validate our method through interventional studies, showing that DSL provides
an intuitive and reliable interpretation of the internal processing in ViTs.

</details>


### [98] [Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2504.13645)
*Numan Saeed,Shahad Hardan,Muhammad Ridzuan,Nada Saadi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本论文提出PEMMA框架，用于高效地将基于CT训练的肿瘤分割模型适应到PET扫描，提高癌症检测和预后性能。


<details>
  <summary>Details</summary>
Motivation: 由于PET扫描可用性有限，现有方法依赖同时获取CT和PET数据，亟需一种能以CT训练并灵活适应PET的框架。

Method: 提出PEMMA框架，利用transformer的模块性，通过LoRA和DoRA对注意力权重进行低秩适应，实现参数高效的跨模态细调，并扩展到预后任务，测试于UNETR和Swin UNETR。

Result: 方法与早融合相比，仅使用8%的可训练参数，PET扫描Dice分数提高28%；预后任务中，协调指数分别提高10%（适应PET）和23%（适应PET及EHR）。

Conclusion: PEMMA框架实现了参数高效适应，减少跨模态纠缠，避免灾难性遗忘，并展示了显著性能提升。

Abstract: Cancer detection and prognosis relies heavily on medical imaging,
particularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise
in tumor segmentation by fusing information from these modalities. However, a
critical bottleneck exists: the dependency on CT-PET data concurrently for
training and inference, posing a challenge due to the limited availability of
PET scans. Hence, there is a clear need for a flexible and efficient framework
that can be trained with the widely available CT scans and can be still adapted
for PET scans when they become available. In this work, we propose a
parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight
upgrading of a transformer-based segmentation model trained only on CT scans
such that it can be efficiently adapted for use with PET scans when they become
available. This framework is further extended to perform prognosis task
maintaining the same efficient cross-modal fine-tuning approach. The proposed
approach is tested with two well-known segementation backbones, namely UNETR
and Swin UNETR. Our approach offers two main advantages. Firstly, we leverage
the inherent modularity of the transformer architecture and perform low-rank
adaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the
attention weights to achieve parameter-efficient adaptation. Secondly, by
minimizing cross-modal entanglement, PEMMA allows updates using only one
modality without causing catastrophic forgetting in the other. Our method
achieves comparable performance to early fusion, but with only 8% of the
trainable parameters, and demonstrates a significant +28% Dice score
improvement on PET scans when trained with a single modality. Furthermore, in
prognosis, our method improves the concordance index by +10% when adapting a
CT-pretrained model to include PET scans, and by +23% when adapting for both
PET and EHR data.

</details>


### [99] [Fragile Watermarking for Image Certification Using Deep Steganographic Embedding](https://arxiv.org/abs/2504.13759)
*Davide Ghiani,Jefferson David Rodriguez Chivata,Stefano Lilliu,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本研究提出使用脆弱水印和深度隐写术认证ICAO合规面部图像的真实性，实验显示高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 面部图像可能在发行后被无意或恶意修改，欺骗面部识别系统，因此需要一种敏感的完整性认证机制。

Method: 采用深度隐写术嵌入隐藏图像作为完整性标记，并开发分类框架检测和分类图像修改类型。

Result: 实验结果显示高检测准确率，包括跨方法场景。

Conclusion: 脆弱水印通过隐写嵌入是验证生物计量文档完整性的有效工具。

Abstract: Modern identity verification systems increasingly rely on facial images
embedded in biometric documents such as electronic passports. To ensure global
interoperability and security, these images must comply with strict standards
defined by the International Civil Aviation Organization (ICAO), which specify
acquisition, quality, and format requirements. However, once issued, these
images may undergo unintentional degradations (e.g., compression, resizing) or
malicious manipulations (e.g., morphing) and deceive facial recognition
systems. In this study, we explore fragile watermarking, based on deep
steganographic embedding as a proactive mechanism to certify the authenticity
of ICAO-compliant facial images. By embedding a hidden image within the
official photo at the time of issuance, we establish an integrity marker that
becomes sensitive to any post-issuance modification. We assess how a range of
image manipulations affects the recovered hidden image and show that
degradation artifacts can serve as robust forensic cues. Furthermore, we
propose a classification framework that analyzes the revealed content to detect
and categorize the type of manipulation applied. Our experiments demonstrate
high detection accuracy, including cross-method scenarios with multiple deep
steganography-based models. These findings support the viability of fragile
watermarking via steganographic embedding as a valuable tool for biometric
document integrity verification.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [100] [Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion](https://arxiv.org/abs/2504.13791)
*Sandipan Dhar,Md. Tousin Akhter,Nanda Dulal Jana,Swagatam Das*

Main category: cs.SD

TL;DR: 这篇论文引入了CLOT-GAN，一个基于GAN的语音转换模型，使用多个判别器和最优传输来改善语音自然性，并优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决真实语音和GAN生成语音在自然性上的差距，并利用多判别器学习方案更好地优化数据分布。

Method: 引入CLOT-GAN模型，包含DCNN、ViT和conformer等多种判别器，并加入最优传输损失来桥接源数据和目标数据分布。

Result: 在VCC 2018、VCTK和CMU-Arctic数据集上的客观和主观评估中，CLOT-GAN-VC模型优于现有语音转换模型。

Conclusion: CLOT-GAN-VC模型在语音转换任务中表现出色和优越性。

Abstract: After demonstrating significant success in image synthesis, Generative
Adversarial Network (GAN) models have likewise made significant progress in the
field of speech synthesis, leveraging their capacity to adapt the precise
distribution of target data through adversarial learning processes. Notably, in
the realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,
there exists a substantial disparity in naturalness between real and
GAN-generated speech samples. Furthermore, while many GAN models currently
operate on a single generator discriminator learning approach, optimizing
target data distribution is more effectively achievable through a single
generator multi-discriminator learning scheme. Hence, this study introduces a
novel GAN model named Collective Learning Mechanism-based Optimal Transport GAN
(CLOT-GAN) model, incorporating multiple discriminators, including the Deep
Convolutional Neural Network (DCNN) model, Vision Transformer (ViT), and
conformer. The objective of integrating various discriminators lies in their
ability to comprehend the formant distribution of mel-spectrograms, facilitated
by a collective learning mechanism. Simultaneously, the inclusion of Optimal
Transport (OT) loss aims to precisely bridge the gap between the source and
target data distribution, employing the principles of OT theory. The
experimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms
that the CLOT-GAN-VC model outperforms existing VC models in objective and
subjective assessments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [101] [Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems](https://arxiv.org/abs/2504.13320)
*Robert Gruhlke,Matei Hanu,Claudia Schillings,Philipp Wacker*

Main category: stat.ML

TL;DR: 本论文提出了一种无梯度框架，用于贝叶斯最优实验设计（BOED），结合EKI和ALDI方法，并通过近似技术处理高维和PDE问题。


<details>
  <summary>Details</summary>
Motivation: 针对梯度信息不可用的复杂系统中的顺序BOED问题。

Method: 结合Ensemble Kalman Inversion (EKI)用于设计优化和Affine-Invariant Langevin Dynamics (ALDI)用于后验采样，并提出变分高斯和参数化Laplace近似来估计Expected Information Gain (EIG)。

Result: 数值实验在线性高斯模型和PDE约束逆问题中展示了方法的鲁棒性、准确性和效率。

Conclusion: 框架在信息驱动实验设计中表现出色，提供可扩展的解决方案。

Abstract: We introduce a gradient-free framework for Bayesian Optimal Experimental
Design (BOED) in sequential settings, aimed at complex systems where gradient
information is unavailable. Our method combines Ensemble Kalman Inversion (EKI)
for design optimization with the Affine-Invariant Langevin Dynamics (ALDI)
sampler for efficient posterior sampling-both of which are derivative-free and
ensemble-based. To address the computational challenges posed by nested
expectations in BOED, we propose variational Gaussian and parametrized Laplace
approximations that provide tractable upper and lower bounds on the Expected
Information Gain (EIG). These approximations enable scalable utility estimation
in high-dimensional spaces and PDE-constrained inverse problems. We demonstrate
the performance of our framework through numerical experiments ranging from
linear Gaussian models to PDE-based inference tasks, highlighting the method's
robustness, accuracy, and efficiency in information-driven experimental design.

</details>


### [102] [Predicting Forced Responses of Probability Distributions via the Fluctuation-Dissipation Theorem and Generative Modeling](https://arxiv.org/abs/2504.13333)
*Ludovico T. Giorgini,Fabrizio Falasca,Andre N. Souza*

Main category: stat.ML

TL;DR: 本文提出一种新框架，结合广义波动-耗散定理和基于分数的生成模型，估计非线性随机系统高阶矩响应，并在气候模型中验证，优于高斯近似。


<details>
  <summary>Details</summary>
Motivation: 经典广义波动-耗散定理依赖高斯近似，导致高阶矩（如方差、偏度、峰度）估计偏差，需要更准确的方法。

Method: 将广义波动-耗散定理与基于分数的生成模型相结合，直接从数据估计分数函数，而不需完整密度重建。

Result: 在三个气候动力学随机模型上验证，捕捉非线性和非高斯特征，优于传统高斯近似。

Conclusion: 该方法有效，证明了在处理复杂系统响应时的优越性。

Abstract: We present a novel data-driven framework for estimating the response of
higher-order moments of nonlinear stochastic systems to small external
perturbations. The classical Generalized Fluctuation-Dissipation Theorem (GFDT)
links the unperturbed steady-state distribution to the system's linear
response. Standard implementations rely on Gaussian approximations, which can
often accurately predict the mean response but usually introduce significant
biases in higher-order moments, such as variance, skewness, and kurtosis. To
address this limitation, we combine GFDT with recent advances in score-based
generative modeling, which enable direct estimation of the score function from
data without requiring full density reconstruction. Our method is validated on
three reduced-order stochastic models relevant to climate dynamics: a scalar
stochastic model for low-frequency climate variability, a slow-fast triad model
mimicking key features of the El Nino-Southern Oscillation (ENSO), and a
six-dimensional stochastic barotropic model capturing atmospheric regime
transitions. In all cases, the approach captures strongly nonlinear and
non-Gaussian features of the system's response, outperforming traditional
Gaussian approximations.

</details>


### [103] [On the minimax optimality of Flow Matching through the connection to kernel density estimation](https://arxiv.org/abs/2504.13336)
*Lea Kunkel,Mathias Trabs*

Main category: stat.ML

TL;DR: Flow Matching 通过连接到核密度估计，实现了最优收敛率，并在高维设置中表现出色。


<details>
  <summary>Details</summary>
Motivation: 提供 Flow Matching 的理论保证，作为扩散模型的简单灵活替代方案。

Method: 将 Flow Matching 连接到核密度估计，验证收敛率，证明大网络下最优率，并在低维子空间上显示改进。

Result: 改进了高斯核的收敛边界，Flow Matching 实现了最优 Wasserstein 距离收敛率（至多对数因子），并在高维低维子空间设置中率提升。

Conclusion: 为 Flow Matching 的经验成功提供了理论基础，尤其在高维场景中。

Abstract: Flow Matching has recently gained attention in generative modeling as a
simple and flexible alternative to diffusion models, the current state of the
art. While existing statistical guarantees adapt tools from the analysis of
diffusion models, we take a different perspective by connecting Flow Matching
to kernel density estimation. We first verify that the kernel density estimator
matches the optimal rate of convergence in Wasserstein distance up to
logarithmic factors, improving existing bounds for the Gaussian kernel. Based
on this result, we prove that for sufficiently large networks, Flow Matching
also achieves the optimal rate up to logarithmic factors, providing a
theoretical foundation for the empirical success of this method. Finally, we
provide a first justification of Flow Matching's effectiveness in
high-dimensional settings by showing that rates improve when the target
distribution lies on a lower-dimensional linear subspace.

</details>


### [104] [On the Convergence of Irregular Sampling in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2504.13623)
*Armin Iske*

Main category: stat.ML

TL;DR: 这篇论文分析了再现核希尔伯特空间（RKHS）中采样算法的收敛性，并讨论了核回归的逼近性质，在最小假设条件下，给出了误差估计和收敛率。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨核回归的逼近性能，以最小化的假设条件，改进机器学习中的理论基础。

Method: 方法包括证明核回归在RKHS范数下的误差估计，讨论紧致域上的均匀收敛，并为Lipschitz连续和Hölder连续核导出了收敛率。

Result: 结果包括新的误差估计、均匀收敛结果，以及特定核类型的收敛率。

Conclusion: 结论是这项工作提升了对核回归收敛性的理解，可能导致更好的算法设计。

Abstract: We analyse the convergence of sampling algorithms for functions in
reproducing kernel Hilbert spaces (RKHS). To this end, we discuss approximation
properties of kernel regression under minimalistic assumptions on both the
kernel and the input data. We first prove error estimates in the kernel's RKHS
norm. This leads us to new results concerning uniform convergence of kernel
regression on compact domains. For Lipschitz continuous and H\"older continuous
kernels, we prove convergence rates.

</details>


### [105] [Near-optimal algorithms for private estimation and sequential testing of collision probability](https://arxiv.org/abs/2504.13804)
*Robert Busa-Fekete,Umar Syed*

Main category: stat.ML

TL;DR: 本篇论文提出新的算法来估计和测试离散分布的碰撞概率，提高了样本效率，并满足局部差分隐私要求。


<details>
  <summary>Details</summary>
Motivation: 碰撞概率是衡量离散分布扩散的重要指标，在多个科学领域有广泛应用，但现有方法在样本效率和隐私保护方面有改进空间。

Method: 论文描述了一个满足（α, β）-局部差分隐私的碰撞概率估计算法，以及一个能处理未知ε的顺序测试算法。

Result: 估计算法的样本复杂度为 Õ( log(1/β) / (α² ε²) )，比之前工作提高了 1/α² 倍；测试算法使用 Õ(1/ε²) 样本；实验显示样本需求显著降低。

Conclusion: 算法达到了近似最优的样本复杂度，并在实际实验中表现出色。

Abstract: We present new algorithms for estimating and testing \emph{collision
probability}, a fundamental measure of the spread of a discrete distribution
that is widely used in many scientific fields. We describe an algorithm that
satisfies $(\alpha, \beta)$-local differential privacy and estimates collision
probability with error at most $\epsilon$ using
$\tilde{O}\left(\frac{\log(1/\beta)}{\alpha^2 \epsilon^2}\right)$ samples for
$\alpha \le 1$, which improves over previous work by a factor of
$\frac{1}{\alpha^2}$. We also present a sequential testing algorithm for
collision probability, which can distinguish between collision probability
values that are separated by $\epsilon$ using $\tilde{O}(\frac{1}{\epsilon^2})$
samples, even when $\epsilon$ is unknown. Our algorithms have nearly the
optimal sample complexity, and in experiments we show that they require
significantly fewer samples than previous methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [106] [KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding](https://arxiv.org/abs/2504.13216)
*Bokwang Hwang,Seonkyu Lim,Taewoong Kim,Yongjae Geun,Sunghyun Bang,Sohyun Park,Jihyun Park,Myeonggyu Lee,Jinwoo Lee,Yerin Kim,Jinsun Yoo,Jingyeong Hong,Jina Park,Yongchan Kim,Suhyun Kim,Younggyun Hahm,Yiseul Lee,Yejee Kang,Chanhyuk Yoon,Chansu Lee,Heeyewon Jeong,Jiyeon Lee,Seonhye Gu,Hyebin Kang,Yousang Cho,Hangyeol Yoo,KyungTae Lim*

Main category: cs.CL

TL;DR: 简而言之：引入KFinEval-Pilot基准，针对韩国金融领域评估LLM，涵盖知识、推理和毒性，揭示模型性能差异和挑战。


<details>
  <summary>Details</summary>
Motivation: 动机：解决现有英语基准的局限性，针对韩国金融领域开发基准，以评估LLM在知识、法律推理和毒性方面的表现。

Method: 方法：采用半自动化管道，使用GPT-4生成提示并由专家验证，确保相关性和准确性，然后评估多种LLM模型。

Result: 结果：不同LLM模型性能存在差异，存在准确性和安全性权衡，突显在金融应用中的推理和安全挑战。

Conclusion: 结论：KFinEval-Pilot作为诊断工具，帮助开发更安全可靠的金融AI系统，基于韩国真实情境。

Abstract: We introduce KFinEval-Pilot, a benchmark suite specifically designed to
evaluate large language models (LLMs) in the Korean financial domain.
Addressing the limitations of existing English-centric benchmarks,
KFinEval-Pilot comprises over 1,000 curated questions across three critical
areas: financial knowledge, legal reasoning, and financial toxicity. The
benchmark is constructed through a semi-automated pipeline that combines
GPT-4-generated prompts with expert validation to ensure domain relevance and
factual accuracy. We evaluate a range of representative LLMs and observe
notable performance differences across models, with trade-offs between task
accuracy and output safety across different model families. These results
highlight persistent challenges in applying LLMs to high-stakes financial
applications, particularly in reasoning and safety. Grounded in real-world
financial use cases and aligned with the Korean regulatory and linguistic
context, KFinEval-Pilot serves as an early diagnostic tool for developing safer
and more reliable financial AI systems.

</details>


### [107] [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
*Jennifer Haase,Finn Klessascheck,Jan Mendling,Sebastian Pokutta*

Main category: cs.CL

TL;DR: 本研究评估11个LLM在10个日常职业任务中的性能，发现较小模型如Gemma-3和Phi-4在成本效率和隐私方面可行。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLM在组织工作流中的能源消耗、金融成本和数据主权问题，探讨较小本地部署模型何时足够好。

Method: 方法：使用双LLM评估框架，自动化执行任务并标准化评估输出质量、事实准确性和道德责任等10个标准。

Result: 结果：GPT-4o性能最佳但成本高；较小模型在多数任务中表现强劲；聚类分析揭示模型间的权衡，任务类型影响有效性。

Conclusion: 结论：主张从性能最大化基准转向任务和上下文相关的充分性评估，提供可持续性视角下的AI模型评估方法和部署指导。

Abstract: Large language models (LLMs) have become increasingly embedded in
organizational workflows. This has raised concerns over their energy
consumption, financial costs, and data sovereignty. While performance
benchmarks often celebrate cutting-edge models, real-world deployment decisions
require a broader perspective: when is a smaller, locally deployable model
"good enough"? This study offers an empirical answer by evaluating eleven
proprietary and open-weight LLMs across ten everyday occupational tasks,
including summarizing texts, generating schedules, and drafting emails and
proposals. Using a dual-LLM-based evaluation framework, we automated task
execution and standardized evaluation across ten criteria related to output
quality, factual accuracy, and ethical responsibility. Results show that GPT-4o
delivers consistently superior performance but at a significantly higher cost
and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4
achieved strong and reliable results on most tasks, suggesting their viability
in contexts requiring cost-efficiency, local deployment, or privacy. A cluster
analysis revealed three model groups -- premium all-rounders, competent
generalists, and limited but safe performers -- highlighting trade-offs between
quality, control, and sustainability. Significantly, task type influenced model
effectiveness: conceptual tasks challenged most models, while aggregation and
transformation tasks yielded better performances. We argue for a shift from
performance-maximizing benchmarks to task- and context-aware sufficiency
assessments that better reflect organizational priorities. Our approach
contributes a scalable method to evaluate AI models through a sustainability
lens and offers actionable guidance for responsible LLM deployment in practice.

</details>


### [108] [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
*Weijie Shi,Jipeng Zhang,Yaguang Wu,Jingzhi Fang,Ruiyuan Zhang,Jiajie Xu,Jia Zhu,Hao Chen,Yao Zhao,Sirui Han,Xiaofang Zhou*

Main category: cs.CL

TL;DR: 本文提出DIDS方法，优化大型语言模型的多领域数据采样策略，以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有采样策略难以维持领域内一致性和准确测量领域影响，因此需要改进。

Method: DIDS使用梯度聚类算法确保一致性，FIM引导指标测量影响，并结合损失轨迹优化采样比例。

Result: 实验显示DIDS提高3.4%的平均性能，同时保持训练效率相当。

Conclusion: DIDS有效改善数据采样，增强模型在下游任务上的表现。

Abstract: Large language models (LLMs) are commonly trained on multi-domain datasets,
where domain sampling strategies significantly impact model performance due to
varying domain importance across downstream tasks. Existing approaches for
optimizing domain-level sampling strategies struggle with maintaining
intra-domain consistency and accurately measuring domain impact. In this paper,
we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain
consistency, a gradient clustering algorithm is proposed to group training data
based on their learning effects, where a proxy language model and
dimensionality reduction are employed to reduce computational overhead. To
accurately measure domain impact, we develop a Fisher Information Matrix (FIM)
guided metric that quantifies how domain-specific parameter updates affect the
model's output distributions on downstream tasks, with theoretical guarantees.
Furthermore, to determine optimal sampling ratios, DIDS combines both the
FIM-guided domain impact assessment and loss learning trajectories that
indicate domain-specific potential, while accounting for diminishing marginal
returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher
average performance while maintaining comparable training efficiency.

</details>


### [109] [CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models](https://arxiv.org/abs/2504.13261)
*Dong Wang*

Main category: cs.CL

TL;DR: 这篇论文引入CPG-EVAL基准，评估大语言模型在外国语言教学中的教学语法能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型如ChatGPT迅速兴起，对外语教育影响重大，但其教学语法能力尚未得到充分评估。

Method: 开发了一个包含五项任务的基准，用于评估语法识别、细粒度区分、类别区分以及对语言干扰的抵抗力。

Result: 小型模型在单实例任务中表现较好，但多实例和干扰任务中较差；大型模型对干扰抵抗更强，但准确性仍有提升空间。

Conclusion: 需要更好的教学对齐和更严格的基准，以指导LLM在教育中的部署。

Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT
has significantly impacted foreign language education, yet their pedagogical
grammar competence remains under-assessed. This paper introduces CPG-EVAL, the
first dedicated benchmark specifically designed to evaluate LLMs' knowledge of
pedagogical grammar within the context of foreign language instruction.
Methodology: The benchmark comprises five tasks designed to assess grammar
recognition, fine-grained grammatical distinction, categorical discrimination,
and resistance to linguistic interference. Findings: Smaller-scale models can
succeed in single language instance tasks, but struggle with multiple instance
tasks and interference from confusing instances. Larger-scale models show
better resistance to interference but still have significant room for accuracy
improvement. The evaluation indicates the need for better instructional
alignment and more rigorous benchmarks, to effectively guide the deployment of
LLMs in educational contexts. Value: This study offers the first specialized,
theory-driven, multi-tiered benchmark framework for systematically evaluating
LLMs' pedagogical grammar competence in Chinese language teaching contexts.
CPG-EVAL not only provides empirical insights for educators, policymakers, and
model developers to better gauge AI's current abilities in educational
settings, but also lays the groundwork for future research on improving model
alignment, enhancing educational suitability, and ensuring informed
decision-making concerning LLM integration in foreign language instruction.

</details>


### [110] [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
*Feiyang Li,Peng Fang,Zhan Shi,Arijit Khan,Fang Wang,Dan Feng,Weihao Wang,Xin Zhang,Yongjian Cui*

Main category: cs.CL

TL;DR: 这篇论文提出CoT-RAG框架，以提升大型语言模型的链式思考推理性能，解决可靠性低和干扰问题。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLMs生成推理链的可靠性低和自然语言对推理逻辑的干扰问题。

Method: 方法包括知识图驱动的CoT生成、可学习的知识案例感知RAG和伪程序提示执行。

Result: 在九个公共数据集上准确率提升4.0%至23.0%，在四个领域特定数据集上显示显著准确性和高效执行。

Conclusion: CoT-RAG展示了强大的实际适用性和可扩展性。

Abstract: While chain-of-thought (CoT) reasoning improves the performance of large
language models (LLMs) in complex tasks, it still has two main challenges: the
low reliability of relying solely on LLMs to generate reasoning chains and the
interference of natural language reasoning chains on the inference logic of
LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework
with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring
knowledge graphs to modulate reasoning chain generation of LLMs, thereby
enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which
incorporates retrieval-augmented generation (RAG) into knowledge graphs to
retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable
information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to
execute reasoning tasks in pseudo-programs with greater logical rigor. We
conduct a comprehensive evaluation on nine public datasets, covering three
reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG
exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.
Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable
accuracy and efficient execution, highlighting its strong practical
applicability and scalability.

</details>


### [111] [Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.13545)
*Azmarah Rizvi,Navojith Thamindu,A. M. N. H. Adhikari,W. P. U. Senevirathna,Dharshana Kasthurirathna,Lakmini Abeywardhana*

Main category: cs.CL

TL;DR: 本研究开发了一个混合的面向方面的情感分析框架，提高了多语言能力和可解释性，用于银行领域的客户反馈分析。


<details>
  <summary>Details</summary>
Motivation: 情感分析对银行品牌声誉管理至关重要，但现有模型在低资源语言如僧伽罗语上表现不佳，且缺乏可解释性。

Method: 微调XLM-RoBERTa处理僧伽罗语和代码混合文本，整合领域特定词汇修正，使用BERT-base-uncased处理英语，并通过SHAP和LIME提供实时情感解释。

Result: 优于传统模型，在英语中达到92.3%准确率和0.89 F1分数，在僧伽罗语和代码混合内容中达到88.4%准确率，可解释性分析揭示关键情感驱动因素。

Conclusion: 为金融应用提供了稳健、透明的情感分析方法，填补了多语言低资源NLP和可解释性方面的空白。

Abstract: Sentiment analysis is crucial for brand reputation management in the banking
sector, where customer feedback spans English, Sinhala, Singlish, and
code-mixed text. Existing models struggle with low-resource languages like
Sinhala and lack interpretability for practical use. This research develops a
hybrid aspect-based sentiment analysis framework that enhances multilingual
capabilities with explainable outputs. Using cleaned banking customer reviews,
we fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate
domain-specific lexicon correction, and employ BERT-base-uncased for English.
The system classifies sentiment (positive, neutral, negative) with confidence
scores, while SHAP and LIME improve interpretability by providing real-time
sentiment explanations. Experimental results show that our approaches
outperform traditional transformer-based classifiers, achieving 92.3 percent
accuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and
code-mixed content. An explainability analysis reveals key sentiment drivers,
improving trust and transparency. A user-friendly interface delivers
aspect-wise sentiment insights, ensuring accessibility for businesses. This
research contributes to robust, transparent sentiment analysis for financial
applications by bridging gaps in multilingual, low-resource NLP and
explainability.

</details>


### [112] [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
*Yule Liu,Jingyi Zheng,Zhen Sun,Zifan Peng,Wenhan Dong,Zeyang Sha,Shiwen Cui,Weiqiang Wang,Xinlei He*

Main category: cs.CL

TL;DR: 这篇论文提出ThoughtMani方法，通过外部CoT减少大型推理模型的过度思考，提高效率和安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在过度思考问题，生成冗余步骤，现有方法依赖微调，存在数据需求、安全风险和泛化差等问题。

Method: 提出ThoughtMani管道，在思考标记之间插入由较小模型生成的外部CoT，操纵模型减少思考步骤。

Result: 实验显示，应用到QwQ-32B模型时，输出令牌减少约30%，性能不变，安全对齐改善平均10%。

Conclusion: ThoughtMani为构建更高效、可访问的LRM提供有效方式，适用于实际应用。

Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the
effectiveness of scaling test-time computation to enhance reasoning
capabilities in multiple tasks. However, LRMs typically suffer from
"overthinking" problems, where models generate significantly redundant
reasoning steps while bringing limited performance gains. Existing work relies
on fine-tuning to mitigate overthinking, which requires additional data,
unconventional training setups, risky safety misalignment, and poor
generalization.
  Through empirical analysis, we reveal an important characteristic of LRM
behaviors that placing external CoTs generated by smaller models between the
thinking token ($\texttt{<think>}$ and $\texttt{</think>)}$ can effectively
manipulate the model to generate fewer thoughts. Building on these insights, we
propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass
unnecessary intermediate steps and reduce computational costs significantly. We
conduct extensive experiments to validate the utility and efficiency of
ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code
dataset, ThoughtMani keeps the original performance and reduces output token
counts by approximately 30%, with little overhead from the CoT generator.
Furthermore, we find that ThoughtMani enhances safety alignment by an average
of 10%. Since model vendors typically serve models of different sizes
simultaneously, ThoughtMani provides an effective way to construct more
efficient and accessible LRMs for real-world applications.

</details>


### [113] [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
*Cong William Lin,Wu Zhu*

Main category: cs.CL

TL;DR: 本研究探讨AI辅助修订对学术写作的影响，包括采用模式差异和写作风格收敛。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型如ChatGPT重塑内容创作和学术写作，因此调查AI辅助生成修订的影响。

Method: 利用超过627,000篇arXiv论文数据集，微调提示和学科特定LLM开发分类框架，并进行差异-差异分析。

Result: 发现LLM采用在学科、性别、母语状态和职业阶段间存在差异；提升了清晰度、简洁性和正式规范；早采用者等群体显示显著风格转变。

Conclusion: LLM推动学术写作收敛，早采用者、男性、非母语者和初级学者风格更接近资深研究者。

Abstract: Large Language Models (LLMs), such as ChatGPT, are reshaping content creation
and academic writing. This study investigates the impact of AI-assisted
generative revisions on research manuscripts, focusing on heterogeneous
adoption patterns and their influence on writing convergence. Leveraging a
dataset of over 627,000 academic papers from arXiv, we develop a novel
classification framework by fine-tuning prompt- and discipline-specific large
language models to detect the style of ChatGPT-revised texts. Our findings
reveal substantial disparities in LLM adoption across academic disciplines,
gender, native language status, and career stage, alongside a rapid evolution
in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,
and adherence to formal writing conventions, with improvements varying by
revision type. Finally, a difference-in-differences analysis shows that while
LLMs drive convergence in academic writing, early adopters, male researchers,
non-native speakers, and junior scholars exhibit the most pronounced stylistic
shifts, aligning their writing more closely with that of established
researchers.

</details>


### [114] [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
*Jie Zou,Cheng Lin,Weikang Guo,Zheng Wang,Jiwei Wei,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为MCCRS的多类型上下文感知对话推荐系统，通过混合专家模型融合结构化和非结构化上下文信息，提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统中的对话通常包含有限的上下文信息，因此需要整合外部来源，但如何结合不同类型的上下文信息仍是一个挑战。

Method: MCCRS使用多个专家模型，每个专家专注于特定上下文信息（如知识图谱、对话历史、物品评论），并由ChairBot协调生成最终结果。

Result: 实验结果表明，MCCRS比现有基线方法取得了显著更高的性能。

Conclusion: 该方法通过利用多种上下文信息和专家的专业化，打破单一上下文信息的限制，提升了对话推荐系统的整体效果。

Abstract: Conversational recommender systems enable natural language conversations and
thus lead to a more engaging and effective recommendation scenario. As the
conversations for recommender systems usually contain limited contextual
information, many existing conversational recommender systems incorporate
external sources to enrich the contextual information. However, how to combine
different types of contextual information is still a challenge. In this paper,
we propose a multi-type context-aware conversational recommender system, called
MCCRS, effectively fusing multi-type contextual information via
mixture-of-experts to improve conversational recommender systems. MCCRS
incorporates both structured information and unstructured information,
including the structured knowledge graph, unstructured conversation history,
and unstructured item reviews. It consists of several experts, with each expert
specialized in a particular domain (i.e., one specific contextual information).
Multiple experts are then coordinated by a ChairBot to generate the final
results. Our proposed MCCRS model takes advantage of different contextual
information and the specialization of different experts followed by a ChairBot
breaks the model bottleneck on a single contextual information. Experimental
results demonstrate that our proposed MCCRS method achieves significantly
higher performance compared to existing baselines.

</details>


### [115] [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
*Andrea Santilli,Adam Golinski,Michael Kirchhof,Federico Danieli,Arno Blaas,Miao Xiong,Luca Zappella,Sinead Williamson*

Main category: cs.CL

TL;DR: 本研究揭示了语言模型不确定性量化评估中，正确性函数的偏差问题，并建议使用LLM-as-a-judge方法减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对提升语言模型的安全性和可靠性至关重要，但现有评估方法因正确性函数的偏差而存在偏见。

Method: 评估了7种正确性函数（包括基于词汇、嵌入和LLM-as-a-judge方法）在4个数据集、4个模型和6种不确定性量化方法中的表现。

Result: 发现正确性函数的长度偏差与不确定性量化方法的长度偏差相互作用，扭曲了评估结果；LLM-as-a-judge方法偏差最小。

Conclusion: LLM-as-a-judge方法是减轻评估偏差的潜在解决方案。

Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for
improving their safety and reliability. Evaluations often use performance
metrics like AUROC to assess how well UQ methods (e.g., negative sequence
probabilities) correlate with task correctness functions (e.g., ROUGE-L). In
this paper, we show that commonly used correctness functions bias UQ
evaluations by inflating the performance of certain UQ methods. We evaluate 7
correctness functions -- from lexical-based and embedding-based metrics to
LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our
analysis reveals that length biases in the errors of these correctness
functions distort UQ assessments by interacting with length biases in UQ
methods. We identify LLM-as-a-judge approaches as among the least length-biased
choices and hence a potential solution to mitigate these biases.

</details>


### [116] [Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence](https://arxiv.org/abs/2504.13730)
*Paul K. Mandal,Cole Leo,Connor Hurley*

Main category: cs.CL

TL;DR: 本论文提出CONTACT框架，使用大语言模型（LLMs）和最小监督预测领土控制。评估了SetFit和BLOOMZ-560m的提示调优方法，结果显示BLOOMZ-based模型性能更好，并减少了标注负担。代码开源。


<details>
  <summary>Details</summary>
Motivation: 开源情报提供非结构化文本数据，用于领土控制评估。动机是开发一种使用最小监督的框架，减少标注负担，并从开源情报流中提取结构化信息。

Method: 使用SetFit（基于嵌入的少样本分类器）和BLOOMZ-560m的提示调优方法。在ISIS在叙利亚和伊拉克活动的少量手工标注新闻数据集上训练，提取军事行动、伤亡和位置参考等信号。

Result: BLOOMZ-based模型优于SetFit基线，提示-based监督在低资源设置中提高了泛化能力。

Conclusion: CONTACT框架证明了使用少样本方法微调LLMs可以减少标注负担，并支持从开放式OSINT流中进行结构化推理。

Abstract: Open-source intelligence provides a stream of unstructured textual data that
can inform assessments of territorial control. We present CONTACT, a framework
for territorial control prediction using large language models (LLMs) and
minimal supervision. We evaluate two approaches: SetFit, an embedding-based
few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a
multilingual generative LLM. Our model is trained on a small hand-labeled
dataset of news articles covering ISIS activity in Syria and Iraq, using
prompt-conditioned extraction of control-relevant signals such as military
operations, casualties, and location references. We show that the BLOOMZ-based
model outperforms the SetFit baseline, and that prompt-based supervision
improves generalization in low-resource settings. CONTACT demonstrates that
LLMs fine-tuned using few-shot methods can reduce annotation burdens and
support structured inference from open-ended OSINT streams. Our code is
available at https://github.com/PaulKMandal/CONTACT/.

</details>


### [117] [Deep literature reviews: an application of fine-tuned language models to migration research](https://arxiv.org/abs/2504.13685)
*Stefano M. Iacus,Haodong Qi,Jiyoung Han*

Main category: cs.CL

TL;DR: 本文提出一个混合框架，使用大型语言模型（LLM）增强传统文献计量方法，应用于人类迁移研究，揭示研究空白。


<details>
  <summary>Details</summary>
Motivation: 为了提高文献综述的可扩展性和深度，通过整合LLM与传统方法，识别迁移研究中的空白。

Method: 通过微调开源LLM，并引入错误聚焦的验证过程（LLM生成初始标签，人为修正），应用于20,000篇科学文章。

Result: LLM可准确选择相关研究、检测趋势、识别空白；发现气候诱发迁移兴趣增长，但文献对环境危害关注不平衡，忽略健康相关问题。

Conclusion: 框架展示了微调LLM在文献综述中的潜力，提高效率和洞察力，加速知识合成和科学发现。

Abstract: This paper presents a hybrid framework for literature reviews that augments
traditional bibliometric methods with large language models (LLMs). By
fine-tuning open-source LLMs, our approach enables scalable extraction of
qualitative insights from large volumes of research content, enhancing both the
breadth and depth of knowledge synthesis. To improve annotation efficiency and
consistency, we introduce an error-focused validation process in which LLMs
generate initial labels and human reviewers correct misclassifications.
Applying this framework to over 20000 scientific articles about human
migration, we demonstrate that a domain-adapted LLM can serve as a "specialist"
model - capable of accurately selecting relevant studies, detecting emerging
trends, and identifying critical research gaps. Notably, the LLM-assisted
review reveals a growing scholarly interest in climate-induced migration.
However, existing literature disproportionately centers on a narrow set of
environmental hazards (e.g., floods, droughts, sea-level rise, and land
degradation), while overlooking others that more directly affect human health
and well-being, such as air and water pollution or infectious diseases. This
imbalance highlights the need for more comprehensive research that goes beyond
physical environmental changes to examine their ecological and societal
consequences, particularly in shaping migration as an adaptive response.
Overall, our proposed framework demonstrates the potential of fine-tuned LLMs
to conduct more efficient, consistent, and insightful literature reviews across
disciplines, ultimately accelerating knowledge synthesis and scientific
discovery.

</details>


### [118] [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
*Shijie Xia,Yiwei Qin,Xuefeng Li,Yan Ma,Run-Ze Fan,Steffi Chern,Haoyang Zou,Fan Zhou,Xiangkun Hu,Jiahe Jin,Yanheng He,Yixin Ye,Yixiu Liu,Pengfei Liu*

Main category: cs.CL

TL;DR: 这篇论文讨论了大语言模型从第一代（Act I）到第二代（Act II）的转变，强调通过测试时缩放技术实现认知工程，并提供资源民主化。


<details>
  <summary>Details</summary>
Motivation: 解决第一代模型的知识延迟、浅层推理和认知局限问题，并抓住当前AI发展关键时刻推进认知工程。

Method: 澄清认知工程概念基础，提供教程、优化实现和GitHub仓库资源。

Result: 民主化认知工程，使从业者能参与AI第二幕，通过测试时缩放技术。

Conclusion: 当前是认知工程发展的关键时刻，需要通过这些方法推动AI从知识检索向思维构建转变。

Abstract: The first generation of Large Language Models - what might be called "Act I"
of generative AI (2020-2023) - achieved remarkable success through massive
parameter and data scaling, yet exhibited fundamental limitations in knowledge
latency, shallow reasoning, and constrained cognitive processes. During this
era, prompt engineering emerged as our primary interface with AI, enabling
dialogue-level communication through natural language. We now witness the
emergence of "Act II" (2024-present), where models are transitioning from
knowledge-retrieval systems (in latent space) to thought-construction engines
through test-time scaling techniques. This new paradigm establishes a
mind-level connection with AI through language-based thoughts. In this paper,
we clarify the conceptual foundations of cognition engineering and explain why
this moment is critical for its development. We systematically break down these
advanced approaches through comprehensive tutorials and optimized
implementations, democratizing access to cognition engineering and enabling
every practitioner to participate in AI's second act. We provide a regularly
updated collection of papers on test-time scaling in the GitHub Repository:
https://github.com/GAIR-NLP/cognition-engineering

</details>


### [119] [Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2504.13825)
*Junjie Yang,Junhao Song,Xudong Han,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Xinyuan Song,Yichao Zhang,Qian Niu,Benji Peng,Keyu Chen,Ming Liu*

Main category: cs.CL

TL;DR: 这篇论文是对知识蒸馏（KD）技术的综述，涵盖其定义、应用、最新创新方法以及在AI中的作用和未来方向。


<details>
  <summary>Details</summary>
Motivation: 动机是合成最新文献，突出关键发现、贡献和未来方向，以帮助研究者和从业者了解知识蒸馏在AI中的演变。

Method: 方法是通过文献综述，重点介绍注意力机制、块级逻辑蒸馏和解耦蒸馏等创新技术。

Result: 结果显示知识蒸馏显著提高了模型效率和准确性，尤其在压缩大语言模型方面。

Conclusion: 结论总结了知识蒸馏的关键发现、贡献，并指出了未来研究方向，如优化知识转移。

Abstract: Knowledge distillation (KD) is a technique for transferring knowledge from
complex teacher models to simpler student models, significantly enhancing model
efficiency and accuracy. It has demonstrated substantial advancements in
various applications including image classification, object detection, language
modeling, text classification, and sentiment analysis. Recent innovations in KD
methods, such as attention-based approaches, block-wise logit distillation, and
decoupling distillation, have notably improved student model performance. These
techniques focus on stimulus complexity, attention mechanisms, and global
information capture to optimize knowledge transfer. In addition, KD has proven
effective in compressing large language models while preserving accuracy,
reducing computational overhead, and improving inference speed. This survey
synthesizes the latest literature, highlighting key findings, contributions,
and future directions in knowledge distillation to provide insights for
researchers and practitioners on its evolving role in artificial intelligence
and machine learning.

</details>


### [120] [MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835)
*Yicheng Chen,Yining Li,Kai Hu,Zerun Ma,Haochen Ye,Kai Chen*

Main category: cs.CL

TL;DR: 本篇论文提出一种统一方法量化指令调整数据集的信息含量，并引入MIG采样方法以最大化语义空间的信息增益，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在选择高质量多样数据集时存在不足，无法全面捕捉语义空间的意图，因此需要更好的量化多样性和信息含量的方法。

Method: 构建标签图建模语义空间，并基于图中信息分布量化多样性；引入MIG（最大化信息增益）采样方法，迭代选择数据样本以提升信息增益。

Result: 实验在各种数据集和基模型上显示MIG优于最先进方法；使用MIG采样的5% Tulu3数据训练的模型在AlpacaEval和Wildbench上分别提高了5.73%和6.89%，与全数据集性能相当。

Conclusion: MIG方法有效地改进了指令调整数据集的采样过程，提高了模型性能和数据利用效率。

Abstract: Data quality and diversity are key to the construction of effective
instruction-tuning datasets. % With the increasing availability of open-source
instruction-tuning datasets, it is advantageous to automatically select
high-quality and diverse subsets from a vast amount of data. % Existing methods
typically prioritize instance quality and use heuristic rules to maintain
diversity. % However, this absence of a comprehensive view of the entire
collection often leads to suboptimal results. % Moreover, heuristic rules
generally focus on distance or clustering within the embedding space, which
fails to accurately capture the intent of complex instructions in the semantic
space. % To bridge this gap, we propose a unified method for quantifying the
information content of datasets. This method models the semantic space by
constructing a label graph and quantifies diversity based on the distribution
of information within the graph. % Based on such a measurement, we further
introduce an efficient sampling method that selects data samples iteratively to
\textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic
space. % Experiments on various datasets and base models demonstrate that MIG
consistently outperforms state-of-the-art methods. % Notably, the model
fine-tuned with 5\% Tulu3 data sampled by MIG achieves comparable performance
to the official SFT model trained on the full dataset, with improvements of
+5.73\% on AlpacaEval and +6.89\% on Wildbench.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [121] [Stability of Polling Systems for a Large Class of Markovian Switching Policies](https://arxiv.org/abs/2504.13315)
*Konstantin Avrachenkov,Kousik Das,Veeraruna Kavitha,Vartika Singh*

Main category: math.OC

TL;DR: 本文提出了一种适用于具有切换时间的双队列轮询系统的二阶段切换策略类，确保系统稳定并覆盖队列性能指标的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 为了识别一个通用的Markovian切换策略类，使系统稳定，并优化每个队列的性能指标，如覆盖帕累托前沿，包括流体状态下的策略和基于阈值的切换。

Method: 通过定义一个具有最多八个参数的二阶段切换策略类，并推导简单的稳定性条件。

Result: 识别了系统稳定的条件，并展示了该策略类可以实现两个队列中等待客户预期的帕累托前沿。

Conclusion: 所提出的策略类是全面的，可以通过调整参数设计鲁棒的轮询系统。

Abstract: We consider a polling system with two queues, where a single server is
attending the queues in a cyclic order and requires non-zero switching times to
switch between the queues. Our aim is to identify a fairly general and
comprehensive class of Markovian switching policies that renders the system
stable. Potentially a class of policies that can cover the Pareto frontier
related to individual-queue-centric performance measures like the stationary
expected number of waiting customers in each queue; for instance, such a class
of policies is identified recently for a polling system near the fluid regime
(with large arrival and departure rates), and we aim to include that class. We
also aim to include a second class that facilitates switching between the
queues at the instance the occupancy in the opposite queue crosses a threshold
and when that in the visiting queue is below a threshold (this inclusion
facilitates design of `robust' polling systems). Towards this, we consider a
class of two-phase switching policies, which includes the above mentioned
classes. In the maximum generality, our policies can be represented by eight
parameters, while two parameters are sufficient to represent the aforementioned
classes. We provide simple conditions to identify the sub-class of switching
policies that ensure system stability. By numerically tuning the parameters of
the proposed class, we illustrate that the proposed class can cover the Pareto
frontier for the stationary expected number of customers in the two queues.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [122] [Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection](https://arxiv.org/abs/2504.13186)
*Yassine Habchi,Hamza Kheddar,Yassine Himeur,Adel Belouchrani,Erchin Serpedin,Fouad Khelifi,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: 这篇论文综述了深度学习在癌症检测中的先进技术，包括迁移学习、强化学习、联邦学习、Transformer和大语言模型，解决了数据稀缺和隐私问题，并提供了趋势洞见。


<details>
  <summary>Details</summary>
Motivation: 现有研究对深度学习在癌症检测的全面分析不足，本文旨在填补这一空白，审视其更广泛的影响。

Method: 通过审阅迁移学习、强化学习、联邦学习、Transformer和大语言模型等技术，分析其在癌症诊断中的效率，并提出解决数据不平衡等挑战的方案。

Result: 这些技术提升了准确性，处理了数据稀缺问题，促进了隐私保护的协作学习，并提高了医疗数据的可解释性。

Conclusion: 本文为研究者和从业者提供了当前趋势的见解，并指导未来在深度学习用于癌症检测的研究方向。

Abstract: The rapid advancement of deep learning (DL) has transformed healthcare,
particularly in cancer detection and diagnosis. DL surpasses traditional
machine learning and human accuracy, making it a critical tool for identifying
diseases. Despite numerous reviews on DL in healthcare, a comprehensive
analysis of its role in cancer detection remains limited. Existing studies
focus on specific aspects, leaving gaps in understanding its broader impact.
This paper addresses these gaps by reviewing advanced DL techniques, including
transfer learning (TL), reinforcement learning (RL), federated learning (FL),
Transformers, and large language models (LLMs). These approaches enhance
accuracy, tackle data scarcity, and enable decentralized learning while
maintaining data privacy. TL adapts pre-trained models to new datasets,
improving performance with limited labeled data. RL optimizes diagnostic
pathways and treatment strategies, while FL fosters collaborative model
development without sharing sensitive data. Transformers and LLMs,
traditionally used in natural language processing, are now applied to medical
data for improved interpretability. Additionally, this review examines these
techniques' efficiency in cancer diagnosis, addresses challenges like data
imbalance, and proposes solutions. It serves as a resource for researchers and
practitioners, providing insights into current trends and guiding future
research in advanced DL for cancer detection.

</details>


### [123] [Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with Attention Gates (DDUNet)](https://arxiv.org/abs/2504.13200)
*Mohammad Mahdi Danesh Pajouh*

Main category: eess.IV

TL;DR: 本文提出了一种高效的U-Net变体，用于脑肿瘤MRI分割，减少计算需求同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤致死率高，早诊断困难；AI辅助分割潜力大，但现有方法计算资源密集，限制实际应用。

Method: 提出新型双解码器U-Net架构，结合注意力门控跳跃连接，用于MRI脑肿瘤分割。

Result: 在BraTS 2020数据集上，Dice分数为全肿瘤85.06%、肿瘤核心80.61%、增强肿瘤71.26%，50个周期内超越其他U-Net变体。

Conclusion: 证明了在有限资源下可实现高质量分割，有助于早期检测和改善患者预后。

Abstract: Cancer remains one of the leading causes of mortality worldwide, and among
its many forms, brain tumors are particularly notorious due to their aggressive
nature and the critical challenges involved in early diagnosis. Recent advances
in artificial intelligence have shown great promise in assisting medical
professionals with precise tumor segmentation, a key step in timely diagnosis
and treatment planning. However, many state-of-the-art segmentation methods
require extensive computational resources and prolonged training times,
limiting their practical application in resource-constrained settings. In this
work, we present a novel dual-decoder U-Net architecture enhanced with
attention-gated skip connections, designed specifically for brain tumor
segmentation from MRI scans. Our approach balances efficiency and accuracy by
achieving competitive segmentation performance while significantly reducing
training demands. Evaluated on the BraTS 2020 dataset, the proposed model
achieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core
(TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several
commonly used U-Net variants. Our model demonstrates that high-quality brain
tumor segmentation is attainable even under limited computational resources,
thereby offering a viable solution for researchers and clinicians operating
with modest hardware. This resource-efficient model has the potential to
improve early detection and diagnosis of brain tumors, ultimately contributing
to better patient outcomes

</details>


### [124] [Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance](https://arxiv.org/abs/2504.13340)
*Oliver Mills,Philip Conaghan,Nishant Ravikumar,Samuel Relton*

Main category: eess.IV

TL;DR: 这项研究比较了Segment Anything Model (SAM)和3D U-Net在膝关节MRI图像中半月板分割的性能，发现端到端微调的SAM与3D U-Net相当，但未优于后者。


<details>
  <summary>Details</summary>
Motivation: 半月板损伤可能导致膝关节骨关节炎，需要准确自动分割技术进行早期检测和治疗，且尚未使用大型视觉Transformer模型。

Method: 将SAM适应用于3D膝关节MRI图像半月板分割，仅微调解码器和端到端微调，并以3D U-Net作为基线。

Result: 仅微调解码器的SAM Dice分数为0.81±0.03，端到端微调为0.87±0.03，与3D U-Net的0.87±0.03相当，但Hausdorff距离显示SAM在形态匹配上劣势。

Conclusion: SAM尽管具有泛化性，但未优于3D U-Net，可能不适合涉及精细解剖结构、低对比度和边界模糊的类似3D医学图像分割任务。

Abstract: Menisci are cartilaginous tissue found within the knee that contribute to
joint lubrication and weight dispersal. Damage to menisci can lead to onset and
progression of knee osteoarthritis (OA), a condition that is a leading cause of
disability, and for which there are few effective therapies. Accurate automated
segmentation of menisci would allow for earlier detection and treatment of
meniscal abnormalities, as well as shedding more light on the role the menisci
play in OA pathogenesis. Focus in this area has mainly used variants of
convolutional networks, but there has been no attempt to utilise recent large
vision transformer segmentation models. The Segment Anything Model (SAM) is a
so-called foundation segmentation model, which has been found useful across a
range of different tasks due to the large volume of data used for training the
model. In this study, SAM was adapted to perform fully-automated segmentation
of menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained
as a baseline. It was found that, when fine-tuning only the decoder, SAM was
unable to compete with 3D U-Net, achieving a Dice score of $0.81\pm0.03$,
compared to $0.87\pm0.03$, on a held-out test set. When fine-tuning SAM
end-to-end, a Dice score of $0.87\pm0.03$ was achieved. The performance of both
the end-to-end trained SAM configuration and the 3D U-Net were comparable to
the winning Dice score ($0.88\pm0.03$) in the IWOAI Knee MRI Segmentation
Challenge 2019. Performance in terms of the Hausdorff Distance showed that both
configurations of SAM were inferior to 3D U-Net in matching the meniscus
morphology. Results demonstrated that, despite its generalisability, SAM was
unable to outperform a basic 3D U-Net in meniscus segmentation, and may not be
suitable for similar 3D medical image segmentation tasks also involving fine
anatomical structures with low contrast and poorly-defined boundaries.

</details>


### [125] [Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning](https://arxiv.org/abs/2504.13391)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 这篇论文提出了一种改进U-Net模型的语义分割方法，提升CMR图像中心脏结构的检测精度。


<details>
  <summary>Details</summary>
Motivation: 自动非侵入性心脏诊断对早期检测心脏疾病和成本有效的临床管理至关重要，需要精确分割和分析心脏图像以诊断各种疾病。

Method: 模型在U-Net下采样时提取边缘属性和上下文信息，并在上采样时融合这些信息，以定位左心室腔、右心室腔和左心肌。

Result: 与之前模型相比，提高Dice相似系数2%-11%，并降低Hausdorff距离1.6至5.7 mm。

Conclusion: 该方法通过改进语义分割，显著提升了心脏疾病诊断的准确性。

Abstract: Automated noninvasive cardiac diagnosis plays a critical role in the early
detection of cardiac disorders and cost-effective clinical management.
Automated diagnosis involves the automated segmentation and analysis of cardiac
images. Precise delineation of cardiac substructures and extraction of their
morphological attributes are essential for evaluating the cardiac function, and
diagnosing cardiovascular disease such as cardiomyopathy, valvular diseases,
abnormalities related to septum perforations, and blood-flow rate. Semantic
segmentation labels the CMR image at the pixel level, and localizes its
subcomponents to facilitate the detection of abnormalities, including
abnormalities in cardiac wall motion in an aging heart with muscle
abnormalities, vascular abnormalities, and valvular abnormalities. In this
paper, we describe a model to improve semantic segmentation of CMR images. The
model extracts edge-attributes and context information during down-sampling of
the U-Net and infuses this information during up-sampling to localize three
major cardiac structures: left ventricle cavity (LV); right ventricle cavity
(RV); and LV myocardium (LMyo). We present an algorithm and performance
results. A comparison of our model with previous leading models, using
similarity metrics between actual image and segmented image, shows that our
approach improves Dice similarity coefficient (DSC) by 2%-11% and lowers
Hausdorff distance (HD) by 1.6 to 5.7 mm.

</details>


### [126] [DADU: Dual Attention-based Deep Supervised UNet for Automated Semantic Segmentation of Cardiac Images](https://arxiv.org/abs/2504.13415)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 本论文提出了一种增强的深度学习模型，用于心脏磁共振图像的左心室、右心室和心肌疤痕组织的分割，实现了98%的Dice相似系数和较低的Hausdorff距离。


<details>
  <summary>Details</summary>
Motivation: 为了提高心脏磁共振图像分割的准确性，并解决深度神经网络中的梯度消失问题。

Method: 整合UNet、通道和空间注意力、基于边缘检测的跳跃连接以及深度监督学习。

Result: 达到了98%的Dice相似系数和显著较低的Hausdorff距离，优于其他领先技术。

Conclusion: 该方法在心脏磁共振图像分割中表现出高准确性和优越性能。

Abstract: We propose an enhanced deep learning-based model for image segmentation of
the left and right ventricles and myocardium scar tissue from cardiac magnetic
resonance (CMR) images. The proposed technique integrates UNet, channel and
spatial attention, edge-detection based skip-connection and deep supervised
learning to improve the accuracy of the CMR image-segmentation. Images are
processed using multiple channels to generate multiple feature-maps. We built a
dual attention-based model to integrate channel and spatial attention. The use
of extracted edges in skip connection improves the reconstructed images from
feature-maps. The use of deep supervision reduces vanishing gradient problems
inherent in classification based on deep neural networks. The algorithms for
dual attention-based model, corresponding implementation and performance
results are described. The performance results show that this approach has
attained high accuracy: 98% Dice Similarity Score (DSC) and significantly lower
Hausdorff Distance (HD). The performance results outperform other leading
techniques both in DSC and HD.

</details>


### [127] [FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling Attention](https://arxiv.org/abs/2504.13597)
*Jun Zeng,KC Santosh,Deepak Rajan Nayak,Thomas de Lange,Jonas Varkey,Tyler Berzin,Debesh Jha*

Main category: eess.IV

TL;DR: 本文提出FocusNet，一种改进的多模态息肉分割网络，旨在提升临床应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅在单模态和单中心数据上训练，在实际环境中表现不佳，需要更可靠的方法来预防结直肠癌。

Method: 提出FocusNet，包括交叉语义交互解码模块（CIDM）、细节增强模块（DEM）和焦点注意力模块（FAM），在PolypDB多模态数据集上评估。

Result: 模型在不同模态上获得高Dice系数：BLI 82.47%、FICE 88.46%、LCI 92.04%、NBI 82.09%、WLI 93.42%，优于现有方法。

Conclusion: FocusNet显示出高准确性和鲁棒性，适用于多种临床模态，源码公开。

Abstract: Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular
screenings can effectively prevent benign polyps from progressing to CRC. While
deep learning has made impressive strides in polyp segmentation, most existing
models are trained on single-modality and single-center data, making them less
effective in real-world clinical environments. To overcome these limitations,
we propose FocusNet, a Transformer-enhanced focus attention network designed to
improve polyp segmentation. FocusNet incorporates three essential modules: the
Cross-semantic Interaction Decoder Module (CIDM) for generating coarse
segmentation maps, the Detail Enhancement Module (DEM) for refining shallow
features, and the Focus Attention Module (FAM), to balance local detail and
global context through local and pooling attention mechanisms. We evaluate our
model on PolypDB, a newly introduced dataset with multi-modality and
multi-center data for building more reliable segmentation methods. Extensive
experiments showed that FocusNet consistently outperforms existing
state-of-the-art approaches with a high dice coefficients of 82.47% on the BLI
modality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI
modality, demonstrating its accuracy and robustness across five different
modalities. The source code for FocusNet is available at
https://github.com/JunZengz/FocusNet.

</details>


### [128] [Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering](https://arxiv.org/abs/2504.13519)
*Yipeng Sun,Linda-Sophie Schneider,Mingxuan Gu,Siyuan Mei,Chengze Ye,Fabian Wagner,Siming Bayer,Andreas Maier*

Main category: eess.IV

TL;DR: 这篇论文提出了一种可解释的自监督单图像去噪框架Filter2Noise（F2N），用于低剂量CT图像，提高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 监督方法受限于配对数据集，自监督方法需多张图像且缺乏洞察力，因此需要一种可解释的单图像去噪方法。

Method: 提出Attention-Guided Bilateral Filter，通过轻量级模块预测空间变化参数，并引入下采样shuffle策略和新的自监督损失函数。

Result: 在Mayo Clinic 2016数据集上，F2N比ZS-N2N高4.59 dB PSNR，同时提升透明度、用户控制和参数效率。

Conclusion: 该方法为医疗应用提供精确可解释的噪声减少优势，代码已开源。

Abstract: Effective denoising is crucial in low-dose CT to enhance subtle structures
and low-contrast lesions while preventing diagnostic errors. Supervised methods
struggle with limited paired datasets, and self-supervised approaches often
require multiple noisy images and rely on deep networks like U-Net, offering
little insight into the denoising mechanism. To address these challenges, we
propose an interpretable self-supervised single-image denoising framework --
Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral
Filter that adapted to each noisy input through a lightweight module that
predicts spatially varying filter parameters, which can be visualized and
adjusted post-training for user-controlled denoising in specific regions of
interest. To enable single-image training, we introduce a novel downsampling
shuffle strategy with a new self-supervised loss function that extends the
concept of Noise2Noise to a single image and addresses spatially correlated
noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading
self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving
transparency, user control, and parametric efficiency. These features provide
key advantages for medical applications that require precise and interpretable
noise reduction. Our code is demonstrated at
https://github.com/sypsyp97/Filter2Noise.git .

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [129] [MetaDSE: A Few-shot Meta-learning Framework for Cross-workload CPU Design Space Exploration](https://arxiv.org/abs/2504.13568)
*Runzhen Xue,Hao Wu,Mingyu Yan,Ziheng Xiao,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: 这篇论文引入MetaDSE方法，将跨工作负载CPU设计空间探索重新定义为少样本元学习问题，显著降低了预测错误。


<details>
  <summary>Details</summary>
Motivation: 现有DSE方法存在过拟合、数据模糊和工作负载差异问题，需要改进。

Method: 使用模型无关元学习和工作负载自适应架构掩码算法。

Result: 在SPEC CPU 2017基准测试中，预测错误比最先进方法减少44.3%。

Conclusion: MetaDSE提高了跨工作负载CPU DSE的效率，并开源可用。

Abstract: Cross-workload design space exploration (DSE) is crucial in CPU architecture
design. Existing DSE methods typically employ the transfer learning technique
to leverage knowledge from source workloads, aiming to minimize the requirement
of target workload simulation. However, these methods struggle with
overfitting, data ambiguity, and workload dissimilarity.
  To address these challenges, we reframe the cross-workload CPU DSE task as a
few-shot meta-learning problem and further introduce MetaDSE. By leveraging
model agnostic meta-learning, MetaDSE swiftly adapts to new target workloads,
greatly enhancing the efficiency of cross-workload CPU DSE. Additionally,
MetaDSE introduces a novel knowledge transfer method called the
workload-adaptive architectural mask algorithm, which uncovers the inherent
properties of the architecture. Experiments on SPEC CPU 2017 demonstrate that
MetaDSE significantly reduces prediction error by 44.3\% compared to the
state-of-the-art. MetaDSE is open-sourced and available at this
\href{https://anonymous.4open.science/r/Meta_DSE-02F8}{anonymous GitHub.}

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [130] [Statistical Validation in Cultural Adaptations of Cognitive Tests: A Multi- Regional Systematic Review](https://arxiv.org/abs/2504.13495)
*Miit Daga,Priyasha Mohanty,Ram Krishna,Swarna Priya RM*

Main category: cs.CY

TL;DR: 这个系统综述探讨跨文化适应认知评估工具的方法和统计确认，强调整体模型和社区反馈的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了讨论不同文化背景下认知工具的适应方法，以应对全球多样化健康设置的需求。

Method: 通过系统综述六个关于欧洲、亚洲、非洲和南美洲的文化适应开创性研究。

Result: 教育解释MoCA-H分数26.76%的方差，文化语言因素解释6.89%的方差；巴西原住民适应MMSE和BCSB显示敏感性94.4%、特异性99.2%，评定者间一致性78.5%。

Conclusion: 社区反馈、标准化翻译协议和稳健统计验证方法是必要的，并提供证据-based框架用于未来认知评估适应。

Abstract: This systematic review discusses the methodological approaches and
statistical confirmations of cross-cultural adaptations of cognitive evaluation
tools used with different populations. The review considers six seminal studies
on the methodology of cultural adaptation in Europe, Asia, Africa, and South
America. The results indicate that proper adaptations need holistic models with
demographic changes, and education explained as much as 26.76% of the variance
in MoCA-H scores. Cultural-linguistic factors explained 6.89% of the variance
in European adaptations of MoCA-H; however, another study on adapted MMSE and
BCSB among Brazilian Indigenous populations reported excellent diagnostic
performance, with a sensitivity of 94.4% and specificity of 99.2%. There was
78.5% inter-rater agreement on the evaluation of cultural adaptation using the
Manchester Translation Evaluation Checklist. A paramount message of the paper
is that community feedback is necessary for culturally appropriate preparation,
standardized translation protocols also must be included, along with robust
statistical validation methodologies for developing cognitive assessment
instruments. This review supplies evidence-based frameworks for the further
adaptation of cognitive assessments in increasingly diverse global health
settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [131] [Ascribe New Dimensions to Scientific Data Visualization with VR](https://arxiv.org/abs/2504.13448)
*Daniela Ushizima,Guilherme Melo dos Santos,Zineb Sordo,Ronald Pandolfi,Jeffrey Donatelli*

Main category: cs.GR

TL;DR: ASCRIBE-VR 是一个整合 AI 和 VR 的平台，用于提升复杂科学图像的交互式分析和可视化。


<details>
  <summary>Details</summary>
Motivation: 传统鼠标和 2D 方法限制了复杂多尺度科学图像的探索，VR 提供沉浸式交互以提高数据理解。

Method: 开发 ASCRIBE-VR 平台，结合 AI 驱动算法和 VR 技术，支持多模态分析、结构评估和沉浸式可视化，兼容 Meta Quest，并使用 AI 分割和迭代反馈。

Result: 增强科学发现，桥接计算分析与人类直觉，支持 X 射线 CT、磁共振等数据集的探索。

Conclusion: ASCRIBE-VR 通过融合 AI 和 VR，连接人类在循环与数字孪生，提高材料研究中的发现。

Abstract: For over half a century, the computer mouse has been the primary tool for
interacting with digital data, yet it remains a limiting factor in exploring
complex, multi-scale scientific images. Traditional 2D visualization methods
hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR)
offers a transformative alternative, providing immersive, interactive
environments that enhance data comprehension. This article introduces
ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research
with Immersive Browsing \& Exploration, which integrates AI-driven algorithms
with scientific images. ASCRIBE-VR enables multimodal analysis, structural
assessments, and immersive visualization, supporting scientific visualization
of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D
imaging. Our VR tools, compatible with Meta Quest, can consume the output of
our AI-based segmentation and iterative feedback processes to enable seamless
exploration of large-scale 3D images. By merging AI-generated results with VR
visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap
between computational analysis and human intuition in materials research,
connecting human-in-the-loop with digital twins.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [132] [Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes](https://arxiv.org/abs/2504.13276)
*Xinyi Wei,Shuo Han,Ahmed H. Hemida,Charles A. Kamhoua,Jie Fu*

Main category: eess.SY

TL;DR: 本论文研究了在马尔可夫决策过程（MDP）中后门攻击的规划，开发优化策略的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨后门攻击在随机控制系统中的机制，即策略在测试中表现良好，但激活触发时优化攻击者目标。

Method: 将攻击规划问题表述为增强状态空间的MDP约束优化问题，并使用基于梯度的优化方法求解控制和触发策略。

Result: 案例研究实验证明了方法的有效性，实现隐蔽的后门攻击。

Conclusion: 方法有效，验证了在MDP中设计隐蔽后门攻击的可行性。

Abstract: This paper investigates backdoor attack planning in stochastic control
systems modeled as Markov Decision Processes (MDPs). In a backdoor attack, the
adversary provides a control policy that behaves well in the original MDP to
pass the testing phase. However, when such a policy is deployed with a trigger
policy, which perturbs the system dynamics at runtime, it optimizes the
attacker's objective instead. To solve jointly the control policy and its
trigger, we formulate the attack planning problem as a constrained optimal
planning problem in an MDP with augmented state space, with the objective to
maximize the attacker's total rewards in the system with an activated trigger,
subject to the constraint that the control policy is near optimal in the
original MDP. We then introduce a gradient-based optimization method to solve
the optimal backdoor attack policy as a pair of coordinated control and trigger
policies. Experimental results from a case study validate the effectiveness of
our approach in achieving stealthy backdoor attacks.

</details>


### [133] [A Model Predictive Control Approach for Quadrotor Cruise Control](https://arxiv.org/abs/2504.13286)
*Zekai Chen,Leon Kehler*

Main category: eess.SY

TL;DR: 这篇论文设计并验证了基于模型预测控制（MPC）的四旋翼无人机巡航控制系统，能够实现悬停稳定、参考跟踪和干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是应用MPC处理四旋翼无人机的巡航控制，包括悬停点稳定、参考跟踪，并模拟现实干扰如风。

Method: 方法包括设计全状态反馈MPC和输出反馈无偏移MPC来稳定系统和拒绝干扰，通过稳定性分析和数值模拟（使用Python和CVXPY）进行验证。

Result: 结果显示控制器成功实现了参考跟踪和干扰拒绝。

Conclusion: 结论是设计的控制器达到了巡航控制的所有预期目标。

Abstract: This paper investigates the application of a Model Predictive Controller
(MPC) for the cruise control system of a quadrotor, focusing on hovering point
stabilization and reference tracking. Initially, a full-state-feedback MPC is
designed for the ideal scenario. To account for real-world conditions, a
constant disturbance is introduced to the quadrotor, simulating a gust of wind
in a specific direction. In response, an output-feedback offset-free MPC is
developed to stabilize the quadrotor while rejecting the disturbance. We
validate the design of the controller by conducting stability analysis, as well
as numerical simulations under different circumstances. It is shown that the
designed controller can achieve all the expected goals for the cruise control,
including reference tracking and disturbance rejection. This project was
implemented using Python and the CVXPY library for convex optimization.

</details>


### [134] [Integrated Control and Active Perception in POMDPs for Temporal Logic Tasks and Information Acquisition](https://arxiv.org/abs/2504.13288)
*Chongyang Shi,Michael R. Dorothy,Jie Fu*

Main category: eess.SY

TL;DR: 这篇论文研究了在部分可观测马尔可夫决策过程（POMDP）中合成联合控制和主动感知策略，以满足时间逻辑规范并最大化关于特定事件的信息增益。


<details>
  <summary>Details</summary>
Motivation: 动机是除了完成任务外，还需最大化关于秘密事件的信息获取，适用于安全应用如UAV监控。

Method: 方法包括引入最小化Shannon条件熵作为目标，使用HMM和POMDP的可观测算子，计算策略梯度。

Result: 结果建立了条件熵梯度的重要属性，便于高效策略梯度计算，并通过图-based例子验证。

Conclusion: 结论是这种方法能有效实现主动信息获取，提高POMDP在不确定环境下的性能。

Abstract: This paper studies the synthesis of a joint control and active perception
policy for a stochastic system modeled as a partially observable Markov
decision process (POMDP), subject to temporal logic specifications. The POMDP
actions influence both system dynamics (control) and the emission function
(perception). Beyond task completion, the planner seeks to maximize information
gain about certain temporal events (the secret) through coordinated perception
and control. To enable active information acquisition, we introduce minimizing
the Shannon conditional entropy of the secret as a planning objective,
alongside maximizing the probability of satisfying the temporal logic formula
within a finite horizon. Using a variant of observable operators in hidden
Markov models (HMMs) and POMDPs, we establish key properties of the conditional
entropy gradient with respect to policy parameters. These properties facilitate
efficient policy gradient computation. We validate our approach through
graph-based examples, inspired by common security applications with UAV
surveillance.

</details>


### [135] [Implementation of Field Programmable Gate Arrays (FPGAs) in Extremely Cold Environments for Space and Cryogenic Computing Applications](https://arxiv.org/abs/2504.13305)
*Christopher Lewis,Drew Sellers,Michael Hamilton*

Main category: eess.SY

TL;DR: 本文演示了CMOS FPGA在4K低温下的操作，并展示了硬件设计技巧和性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了扩展FPGA在极端低温环境下的应用和可靠性。

Method: 通过HDL设计和外围电路改进，如电压调节器优化。

Result: 拓宽了操作温度范围，提高了抖动性能，减少了LUT延迟，并提升了收发器性能。

Conclusion: FPGA在低温下表现出更好的性能，证明了设计技巧的有效性。

Abstract: The operation of CMOS Field Programmable Gate Arrays (FPGAs) at extremely
cold environments as low as 4 K is demonstrated. Various FPGA and periphery
hardware design techniques spanning from HDL design to improvements of
peripheral circuitry such as discrete voltage regulators are displayed, and
their respective performances are reported. While general operating conditions
for voltage regulators are widened, FPGAs see a broader temperature range with
improved jitter performance, reduced LUT delays, and enhanced transceiver
performance at extremely low temperatures.

</details>


### [136] [Robust Estimation of Battery State of Health Using Reference Voltage Trajectory](https://arxiv.org/abs/2504.13324)
*Rui Huang,Jackson Fogelquist,Xinfan Lin*

Main category: eess.SY

TL;DR: 本文提出了一种基于参考电压的电池SOH估计方法，使用BOL电压响应和优化电流激励，提高估计准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前模型-based SOH估计方法依赖低C-rate测试，提取健康参数，但不实用且易受建模不确定性影响。

Method: 提出参考电压-based方法，使用BOL电压响应作为参考，通过相同电流激励与退化状态比较估计参数，并使用粒子群优化算法优化电流激励。

Result: 模拟结果显示，估计准确性显著提高，能够在10分钟内估计四个关键SOH参数，适用于实际电池健康诊断。

Conclusion: 该方法提高了SOH估计的准确性和实用性，支持快速测试和电池再利用。

Abstract: Accurate estimation of state of health (SOH) is critical for battery
applications. Current model-based SOH estimation methods typically rely on low
C-rate constant current tests to extract health parameters like solid phase
volume fraction and lithium-ion stoichiometry, which are often impractical in
real-world scenarios due to time and operational constraints. Additionally,
these methods are susceptible to modeling uncertainties that can significantly
degrade the estimation accuracy, especially when jointly estimating multiple
parameters. In this paper, we present a novel reference voltage-based method
for robust battery SOH estimation. This method utilizes the voltage response of
a battery under a predefined current excitation at the beginning of life (BOL)
as a reference to compensate for modeling uncertainty. As the battery degrades,
the same excitation is applied to generate the voltage response, which is
compared with the BOL trajectory to estimate the key health parameters
accurately. The current excitation is optimally designed using the Particle
Swarm Optimization algorithm to maximize the information content of the target
parameters. Simulation results demonstrate that our proposed method
significantly improves parameter estimation accuracy under different
degradation levels, compared to conventional methods relying only on direct
voltage measurements. Furthermore, our method jointly estimates four key SOH
parameters in only 10 minutes, making it practical for real-world battery
health diagnostics, e.g., fast testing to enable battery repurposing.

</details>


### [137] [Integration of a Graph-Based Path Planner and Mixed-Integer MPC for Robot Navigation in Cluttered Environments](https://arxiv.org/abs/2504.13372)
*Joshua A. Robbins,Stephen J. Harnett,Andrew F. Thompson,Sean Brennan,Herschel C. Pangborn*

Main category: eess.SY

TL;DR: 这篇论文提出了一种多层规划和控制框架的路径重新规划策略，用于自主移动机器人在部分已知环境中的导航。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人需要在不确定环境中更新路径计划，以应对部分已知的障碍。

Method: 使用基于中轴图的全局路径规划和混合整数模型预测控制（MPC）检测约束可行性，并通过删除图边触发重新规划。

Result: 通过实验演示了该重新规划策略的有效性。

Conclusion: 该策略提高了机器人在不确定环境下的导航鲁棒性。

Abstract: The ability to update a path plan is a required capability for autonomous
mobile robots navigating through uncertain environments. This paper proposes a
re-planning strategy using a multilayer planning and control framework for
cases where the robot's environment is partially known. A medial axis
graph-based planner defines a global path plan based on known obstacles where
each edge in the graph corresponds to a unique corridor. A mixed-integer model
predictive control (MPC) method detects if a terminal constraint derived from
the global plan is infeasible, subject to a non-convex description of the local
environment. Infeasibility detection is used to trigger efficient global
re-planning via medial axis graph edge deletion. The proposed re-planning
strategy is demonstrated experimentally.

</details>


### [138] [Documentation on Encrypted Dynamic Control Simulation Code using Ring-LWE based Cryptosystems](https://arxiv.org/abs/2504.13403)
*Yeongjun Jang,Joowon Lee,Junsoo Kim*

Main category: eess.SY

TL;DR: 这篇论文提供使用Lattigo库的指南，实现高效Ring-LWE基于加密控制器。


<details>
  <summary>Details</summary>
Motivation: 加密控制器计算负载高，需要可访问的实现指南。

Method: 使用开源Lattigo库和示例代码解释Ring-LWE加密控制器。

Result: 开发了高效加密控制器指南和GitHub代码。

Conclusion: 指南有助于减少计算负载，提升加密控制器的实用性。

Abstract: Encrypted controllers offer secure computation by employing modern
cryptosystems to execute control operations directly over encrypted data
without decryption. However, incorporating cryptosystems into dynamic
controllers significantly increases the computational load. This paper aims to
provide an accessible guideline for running encrypted controllers using an
open-source library Lattigo, which supports an efficient implementation of
Ring-Learing With Errors (LWE) based encrypted controllers, and our
explanations are assisted with example codes that are fully available at
https://github.com/CDSL-EncryptedControl/CDSL.

</details>


### [139] [Inverse Inference on Cooperative Control of Networked Dynamical Systems](https://arxiv.org/abs/2504.13701)
*Yushan Li,Jianping He,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 这篇论文提出从离散观测推断网络动态系统连续时间控制关键组件的双层推理框架。


<details>
  <summary>Details</summary>
Motivation: 桥接离散观测与连续时间模型的差距，并有效解耦相关组件。

Method: 采用基于因果关系的离散时间估计器、矩阵对数恢复方法、最小二乘解耦和逆最优控制技术。

Result: 实现了渐进无偏估计、错误界限、组件解耦及必要条件，数值模拟验证了有效性。

Conclusion: 展示了从离散数据推断连续时间控制组件的可行性，并为网络动态系统控制提供新方法。

Abstract: Recent years have witnessed the rapid advancement of understanding the
control mechanism of networked dynamical systems (NDSs), which are governed by
components such as nodal dynamics and topology. This paper reveals that the
critical components in continuous-time state feedback cooperative control of
NDSs can be inferred merely from discrete observations. In particular, we
advocate a bi-level inference framework to estimate the global closed-loop
system and extract the components, respectively. The novelty lies in bridging
the gap from discrete observations to the continuous-time model and effectively
decoupling the concerned components. Specifically, in the first level, we
design a causality-based estimator for the discrete-time closed-loop system
matrix, which can achieve asymptotically unbiased performance when the NDS is
stable. In the second level, we introduce a matrix logarithm based method to
recover the continuous-time counterpart matrix, providing new sampling period
guarantees and establishing the recovery error bound. By utilizing graph
properties of the NDS, we develop least square based procedures to decouple the
concerned components with up to a scalar ambiguity. Furthermore, we employ
inverse optimal control techniques to reconstruct the objective function
driving the control process, deriving necessary conditions for the solutions.
Numerical simulations demonstrate the effectiveness of the proposed method.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [140] [HEAT:History-Enhanced Dual-phase Actor-Critic Algorithm with A Shared Transformer](https://arxiv.org/abs/2504.13193)
*Hong Yang*

Main category: cs.NI

TL;DR: 这篇论文提出了一种名为 HEAT 的算法，用于改善单网关 LoRaWAN 网络的性能，通过整合历史数据和实时交互提升模型。


<details>
  <summary>Details</summary>
Motivation: 为了提高单网关 LoRaWAN 网络的性能，考虑上行和下行参数，并有效整合离线和在线强化学习。

Method: 提出 HEAT 算法，这是一种基于历史增强的两阶段 actor-critic 算法，结合共享 transformer；同时开发了开源模拟器 LoRaWANSim，支持多通道、多解调器和双向通信。

Result: 模拟实验显示，HEAT 相比其他算法提高了 15% 的数据包成功率和 95% 的能量效率。

Conclusion: HEAT 算法有效地改善了网络性能，证明了其在提高数据包成功率和能量效率方面的优势。

Abstract: For a single-gateway LoRaWAN network, this study proposed a history-enhanced
two-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to
improve network performance. HEAT considers uplink parameters and often
neglected downlink parameters, and effectively integrates offline and online
reinforcement learning, using historical data and real-time interaction to
improve model performance. In addition, this study developed an open source
LoRaWAN network simulator LoRaWANSim. The simulator considers the demodulator
lock effect and supports multi-channel, multi-demodulator and bidirectional
communication. Simulation experiments show that compared with the best results
of all compared algorithms, HEAT improves the packet success rate and energy
efficiency by 15% and 95%, respectively.

</details>


### [141] [Optimizing Multi-Gateway LoRaWAN via Cloud-Edge Collaboration and Knowledge Distillation](https://arxiv.org/abs/2504.13194)
*Hong Yang*

Main category: cs.NI

TL;DR: 本文提出HEAT-LDL方法，通过云边协作和边缘智能优化LoRaWAN网络资源分配，提高数据包成功率和能量效率。


<details>
  <summary>Details</summary>
Motivation: 针对大规模多网关LoRaWAN网络的资源分配和决策挑战，实现网关负载均衡和终端节点自主决策。

Method: 结合Actor-Critic架构、Lyapunov优化和云边知识蒸馏，开发HEAT-LDL方法，实现智能下行控制和自主决策。

Result: 模拟实验显示，与最优算法相比，数据包成功率提高20.5%，能量效率提高88.1%。

Conclusion: HEAT-LDL方法有效提升网络性能，验证了云边协作在LoRaWAN中的优势。

Abstract: For large-scale multi-gateway LoRaWAN networks, this study proposes a
cloud-edge collaborative resource allocation and decision-making method based
on edge intelligence, HEAT-LDL (HEAT-Local Distill Lyapunov), which realizes
collaborative decision-making between gateways and terminal nodes. HEAT-LDL
combines the Actor-Critic architecture and the Lyapunov optimization method to
achieve intelligent downlink control and gateway load balancing. When the
signal quality is good, the network server uses the HEAT algorithm to schedule
the terminal nodes. To improve the efficiency of autonomous decision-making of
terminal nodes, HEAT-LDL performs cloud-edge knowledge distillation on the HEAT
teacher model on the terminal node side. When the downlink decision instruction
is lost, the terminal node uses the student model and the edge decider based on
prior knowledge and local history to make collaborative autonomous decisions.
Simulation experiments show that compared with the optimal results of all
compared algorithms, HEAT-LDL improves the packet success rate and energy
efficiency by 20.5% and 88.1%, respectively.

</details>


### [142] [SFL-LEO: Asynchronous Split-Federated Learning Design for LEO Satellite-Ground Network Framework](https://arxiv.org/abs/2504.13479)
*Jiasheng Wu,Jingjing Zhang,Zheng Lin,Zhe Chen,Xiong Wang,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: 本文提出SFL-LEO框架，结合联邦学习和分割学习，解决LEO卫星网络的高动态性和计算限制问题，提高训练性能。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络快速发展，但计算能力受限且网络动态高，亟需高效数据处理方法。

Method: 提出SFL-LEO框架，融合联邦学习和分割学习，使用异步训练策略，在卫星断连时进行本地更新，并在地站聚合模型。

Result: 实验结果显示，SFL-LEO在Starlink带宽下，与传统分割学习方案精度相当，且能在断连期间进行本地训练。

Conclusion: SFL-LEO通过异步训练提升了LEO卫星网络的训练性能，证明了其在高动态环境中的有效性。

Abstract: Recently, the rapid development of LEO satellite networks spurs another
widespread concern-data processing at satellites. However, achieving efficient
computation at LEO satellites in highly dynamic satellite networks is
challenging and remains an open problem when considering the constrained
computation capability of LEO satellites. For the first time, we propose a
novel distributed learning framework named SFL-LEO by combining Federated
Learning (FL) with Split Learning (SL) to accommodate the high dynamics of LEO
satellite networks and the constrained computation capability of LEO satellites
by leveraging the periodical orbit traveling feature. The proposed scheme
allows training locally by introducing an asynchronous training strategy, i.e.,
achieving local update when LEO satellites disconnect with the ground station,
to provide much more training space and thus increase the training performance.
Meanwhile, it aggregates client-side sub-models at the ground station and then
distributes them to LEO satellites by borrowing the idea from the federated
learning scheme. Experiment results driven by satellite-ground bandwidth
measured in Starlink demonstrate that SFL-LEO provides a similar accuracy
performance with the conventional SL scheme because it can perform local
training even within the disconnection duration.

</details>


### [143] [Towards End-to-End Network Intent Management with Large Language Models](https://arxiv.org/abs/2504.13589)
*Lam Dinh,Sihem Cherrared,Xiaofeng Huang,Fabrice Guillemin*

Main category: cs.NI

TL;DR: 这篇论文探讨了使用大语言模型（LLMs）在基于意图的网络（IBN）中生成5G/6G网络配置，并比较了开源和闭源模型的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在解释人类语言和代码生成方面的强大能力，使得它们适合将高层意图转化为低级网络配置；引入FEACI指标以评估生成的答案的格式、解释性、准确性、成本和推理时间，因为现有指标无法捕捉这些特征。

Method: 利用闭源模型（如Google Gemini 1.5 pro、ChatGPT-4）和开源模型（如LLama、Mistral）生成无线接入网络（RAN）和核心网络配置；引入FEACI指标进行定量评估。

Result: 开源模型在翻译性能上可与闭源模型媲美甚至优于后者，且无需昂贵的硬件设置。

Conclusion: 开源LLMs是IBN应用的可靠替代方案。

Abstract: Large Language Models (LLMs) are likely to play a key role in Intent-Based
Networking (IBN) as they show remarkable performance in interpreting human
language as well as code generation, enabling the translation of high-level
intents expressed by humans into low-level network configurations. In this
paper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,
ChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their
capacity to generate E2E network configurations for radio access networks
(RANs) and core networks in 5G/6G mobile networks. We introduce a novel
performance metrics, known as FEACI, to quantitatively assess the format (F),
explainability (E), accuracy (A), cost (C), and inference time (I) of the
generated answer; existing general metrics are unable to capture these
features. The results of our study demonstrate that open-source models can
achieve comparable or even superior translation performance compared with the
closed-source models requiring costly hardware setup and not accessible to all
users.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [144] [Adaptive Long-term Embedding with Denoising and Augmentation for Recommendation](https://arxiv.org/abs/2504.13614)
*Zahra Akhlaghi,Mostafa Haghir Chehreghani*

Main category: cs.IR

TL;DR: 本论文提出ALDA4Rec方法，通过去噪和自适应嵌入改进图神经网络推荐系统，在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 互联网快速发展使个性化推荐系统不可或缺，基于图的系统存在噪声和静态表示问题。

Method: 构建物品-物品图，通过社区检测过滤噪声，丰富用户-物品交互；使用GCN学习短期表示，平均、GRU和注意力机制建模长期嵌入；MLP-based自适应加权优化偏好。

Result: 在四个真实数据集上，ALDA4Rec在准确性和鲁棒性方面优于最先进基线。

Conclusion: 该方法有效提高了推荐系统的性能，并提供了源代码。

Abstract: The rapid growth of the internet has made personalized recommendation systems
indispensable. Graph-based sequential recommendation systems, powered by Graph
Neural Networks (GNNs), effectively capture complex user-item interactions but
often face challenges such as noise and static representations. In this paper,
we introduce the Adaptive Long-term Embedding with Denoising and Augmentation
for Recommendation (ALDA4Rec) method, a novel model that constructs an
item-item graph, filters noise through community detection, and enriches
user-item interactions. Graph Convolutional Networks (GCNs) are then employed
to learn short-term representations, while averaging, GRUs, and attention
mechanisms are utilized to model long-term embeddings. An MLP-based adaptive
weighting strategy is further incorporated to dynamically optimize long-term
user preferences. Experiments conducted on four real-world datasets demonstrate
that ALDA4Rec outperforms state-of-the-art baselines, delivering notable
improvements in both accuracy and robustness. The source code is available at
https://github.com/zahraakhlaghi/ALDA4Rec.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [145] [Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review](https://arxiv.org/abs/2504.13183)
*Rawan AlMakinah*

Main category: cs.HC

TL;DR: 这篇论文通过系统文献综述探讨心理健康从业者对AI对话代理的态度，使用TAM3框架，旨在总结其态度、机会、担忧和影响，以指导AI在心理健康领域的应用。


<details>
  <summary>Details</summary>
Motivation: 动机是评估AI在心理健康中的可行性，特别是从专业人士视角，考虑到机会、担忧和影响，以构建开发和部署框架。

Method: 方法是进行系统文献综述，并以TAM3框架为分析工具。

Result: 结果将包括对文献的合成，展示专业人士的态度和影响因素。

Conclusion: 结论旨在提供一个指导框架，帮助在心理健康领域安全有效地采用AI对话代理。

Abstract: Artificial intelligent (AI) conversational agents hold a promising future in
the field of mental health, especially in helping marginalized communities that
lack access to mental health support services. It is tempting to have a 24/7
mental health companion that can be accessed anywhere using mobile phones to
provide therapist-like advice. Yet, caution should be taken, and studies around
their feasibility need to be surveyed. Before adopting such a rapidly changing
technology, studies on its feasibility should be explored, summarized, and
synthesized to gain a solid understanding of the status quo and to enable us to
build a framework that can guide us throughout the development and deployment
processes. Different perspectives must be considered when investigating the
feasibility of AI conversational agents, including the mental healthcare
professional perspective. The literature can provide insights into their
perspectives in terms of opportunities, concerns, and implications. Mental
health professionals, the subject-matter experts in this field, have their
points of view that should be understood and considered. This systematic
literature review will explore mental health practitioners' attitudes toward AI
conversational agents and the factors that affect their adoption and
recommendation of the technology to augment their services and treatments. The
TAM3 Framework will be the lens through which this systematic literature review
will be conducted.

</details>


### [146] [Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces](https://arxiv.org/abs/2504.13277)
*Soorya Ram Shimgekar,Violeta J. Rodriguez,Paul A. Bloom,Dong Whi Yoo,Koustuv Saha*

Main category: cs.HC

TL;DR: 这项研究使用自杀人际理论分析Reddit自杀帖子，考察支持性回应，并评估AI聊天机器人在心理健康支持中的局限性。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球公共卫生问题，数百万人在线表达自杀意念，但先前研究缺乏理论框架。

Method: 采用IPT理论分析59,607个Reddit r/SuicideWatch帖子，分类SI维度和风险因素，分析支持性回应的语言，并评估AI聊天机器人。

Result: 高风险SI帖子表达计划、尝试、方法和痛苦；支持性回应因SI阶段不同而异；AI提高结构连贯性，但缺乏共情。

Conclusion: 强调在开发AI干预措施时需要谨慎反思和更深入的理解，以提供有效心理健康支持。

Abstract: Suicide is a critical global public health issue, with millions experiencing
suicidal ideation (SI) each year. Online spaces enable individuals to express
SI and seek peer support. While prior research has revealed the potential of
detecting SI using machine learning and natural language analysis, a key
limitation is the lack of a theoretical framework to understand the underlying
factors affecting high-risk suicidal intent. To bridge this gap, we adopted the
Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607
posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions
(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk
factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired
Capability of Suicide). We found that high-risk SI posts express planning and
attempts, methods and tools, and weaknesses and pain. In addition, we also
examined the language of supportive responses through psycholinguistic and
content analyses to find that individuals respond differently to different
stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI
chatbots in providing effective supportive responses to suicidal ideation
posts. We found that although AI improved structural coherence, expert
evaluations highlight persistent shortcomings in providing dynamic,
personalized, and deeply empathetic support. These findings underscore the need
for careful reflection and deeper understanding in both the development and
consideration of AI-driven interventions for effective mental health support.

</details>


### [147] [Creating 'Full-Stack' Hybrid Reasoning Systems that Prioritize and Enhance Human Intelligence](https://arxiv.org/abs/2504.13477)
*Sean Koon*

Main category: cs.HC

TL;DR: 这篇论文提出使用生成式AI工具增强人类推理和问题探索，并描述了一个以人类为中心的人工智能整合模型。


<details>
  <summary>Details</summary>
Motivation: 强调在混合智能中提升人类智慧的紧迫性，因为人类推理可能有缺陷，而现有努力多集中在AI优化上。

Method: 提出开发基于生成式AI的工具来提升人类的反思和技术探索能力，并提供一个高层次模型来整合AI和人类能力。

Result: 论文的结果是提出了一种新模型和工具概念，以centralize人类参与。

Conclusion: 结论是需要开发AI工具来增强人类能力，以应对未来挑战。

Abstract: The idea of augmented or hybrid intelligence offers a compelling vision for
combining human and AI capabilities, especially in tasks where human wisdom,
expertise, or common sense are essential. Unfortunately, human reasoning can be
flawed and shortsighted, resulting in adverse individual impacts or even
long-term societal consequences. While strong efforts are being made to develop
and optimize the AI aspect of hybrid reasoning, the real urgency lies in
fostering wiser and more intelligent human participation. Tools that enhance
critical thinking, ingenuity, expertise, and even wisdom could be essential in
addressing the challenges of our emerging future. This paper proposes the
development of generative AI-based tools that enhance both the human ability to
reflect upon a problem as well as the ability to explore the technical aspects
of it. A high-level model is also described for integrating AI and human
capabilities in a way that centralizes human participation and control.

</details>


### [148] [RAG Without the Lag: Interactive Debugging for Retrieval-Augmented Generation Pipelines](https://arxiv.org/abs/2504.13587)
*Quentin Romero Lauro,Shreya Shankar,Sepanta Zeighami,Aditya Parameswaran*

Main category: cs.HC

TL;DR: RAGGY 是一个用于调试 RAG 管道的工具，提供 Python 库和交互式界面，基于对工程师的研究以提高开发效率。


<details>
  <summary>Details</summary>
Motivation: RAG 管道开发面临错误识别困难和反馈周期缓慢的挑战，需要更好的调试工具。

Method: 设计并实现 RAGGY，包括可组合 RAG 基元和交互式调试界面，并通过对 12 名工程师的定性研究分析调试模式。

Result: 获得专家调试模式洞察和未来 RAG 工具设计启示。

Conclusion: RAGGY 提升了 RAG 管道的调试效率，并更好地适应开发者的工作流程。

Abstract: Retrieval-augmented generation (RAG) pipelines have become the de-facto
approach for building AI assistants with access to external, domain-specific
knowledge. Given a user query, RAG pipelines typically first retrieve (R)
relevant information from external sources, before invoking a Large Language
Model (LLM), augmented (A) with this information, to generate (G) responses.
Modern RAG pipelines frequently chain multiple retrieval and generation
components, in any order. However, developing effective RAG pipelines is
challenging because retrieval and generation components are intertwined, making
it hard to identify which component(s) cause errors in the eventual output. The
parameters with the greatest impact on output quality often require hours of
pre-processing after each change, creating prohibitively slow feedback cycles.
To address these challenges, we present RAGGY, a developer tool that combines a
Python library of composable RAG primitives with an interactive interface for
real-time debugging. We contribute the design and implementation of RAGGY,
insights into expert debugging patterns through a qualitative study with 12
engineers, and design implications for future RAG tools that better align with
developers' natural workflows.

</details>


### [149] [Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm](https://arxiv.org/abs/2504.13667)
*Russell Beale*

Main category: cs.HC

TL;DR: 这篇论文从乐观角度讨论大型语言模型对儿童学习和科技互动的潜在重大影响，强调未来变化将远超当前效果，并通过一个场景和自民族志研究展示，并定义五个重要考虑因素。


<details>
  <summary>Details</summary>
Motivation: 动机是审视LLM对教育的当前影响，并论证这些影响相对于即将到来的变化是次要的，以突出未来潜在的戏剧性影响。

Method: 方法包括回顾LLM对教育的影响、呈现一个小场景和自民族志研究、并定义五个重要考虑因素。

Result: 结果是通过研究展示这些变化的影响，并定义了交互系统设计者必须适应的五个考虑因素。

Conclusion: 结论是交互系统设计者需要在未来适应由LLM带来的变化。

Abstract: This paper presents a hopeful perspective on the potentially dramatic impacts
of Large Language Models on how we children learn and how they will expect to
interact with technology. We review the effects of LLMs on education so far,
and make the case that these effects are minor compared to the upcoming changes
that are occurring. We present a small scenario and self-ethnographic study
demonstrating the effects of these changes, and define five significant
considerations that interactive systems designers will have to accommodate in
the future.

</details>


### [150] [Exploring Multimodal Prompt for Visualization Authoring with Large Language Models](https://arxiv.org/abs/2504.13700)
*Zhen Wen,Luoxuan Weng,Yinghao Tang,Runjin Zhang,Yuxin Liu,Bo Pan,Minfeng Zhu,Wei Chen*

Main category: cs.HC

TL;DR: 这篇论文介绍了VisPilot系统，使用多模态提示（如文本、草图和直接操作）来改善LLM在可视化创作中的表现，解决了文本提示的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决使用自然语言指令LLM在可视化创作中的精度和表现力不足问题，导致误解和耗时迭代。

Method: 进行了实证研究理解LLM对模糊文本提示的解释，引入视觉提示，设计VisPilot系统，并通过案例研究和用户研究评估。

Result: VisPilot提供更直观的创建可视化方式，不影响效率，多模态提示提高了可用性。

Conclusion: 强调多模态提示的重要性，为未来可视化系统设计提供启示，并探讨增强人类-AI协作。

Abstract: Recent advances in large language models (LLMs) have shown great potential in
automating the process of visualization authoring through simple natural
language utterances. However, instructing LLMs using natural language is
limited in precision and expressiveness for conveying visualization intent,
leading to misinterpretation and time-consuming iterations. To address these
limitations, we conduct an empirical study to understand how LLMs interpret
ambiguous or incomplete text prompts in the context of visualization authoring,
and the conditions making LLMs misinterpret user intent. Informed by the
findings, we introduce visual prompts as a complementary input modality to text
prompts, which help clarify user intent and improve LLMs' interpretation
abilities. To explore the potential of multimodal prompting in visualization
authoring, we design VisPilot, which enables users to easily create
visualizations using multimodal prompts, including text, sketches, and direct
manipulations on existing visualizations. Through two case studies and a
controlled user study, we demonstrate that VisPilot provides a more intuitive
way to create visualizations without affecting the overall task efficiency
compared to text-only prompting approaches. Furthermore, we analyze the impact
of text and visual prompts in different visualization tasks. Our findings
highlight the importance of multimodal prompting in improving the usability of
LLMs for visualization authoring. We discuss design implications for future
visualization systems and provide insights into how multimodal prompts can
enhance human-AI collaboration in creative visualization tasks. All materials
are available at https://OSF.IO/2QRAK.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [151] [Pricing AI Model Accuracy](https://arxiv.org/abs/2504.13375)
*Nikhil Kumar*

Main category: econ.TH

TL;DR: 本篇论文研究AI模型市场的竞争，分析公司提高准确性的激励，发现提高准确性不一定增加利润，公司应投资于优势错误维度。


<details>
  <summary>Details</summary>
Motivation: 分析竞争如何影响公司在AI模型市场中提高模型准确性的激励，特别是消费者偏好异质性的影响。

Method: 开发了一个消费者-公司双头垄断模型来分析竞争对公司准确性改进激励的影响。

Result: 竞争市场中，提高整体准确性不一定提高利润；公司应投资于其优势错误维度（如假阳性或假阴性率）；投资对消费者不利但增加整体福利。

Conclusion: 公司通过投资优势维度获益，但这会损害消费者利益，同时提升整体社会福利。

Abstract: This paper examines the market for AI models in which firms compete to
provide accurate model predictions and consumers exhibit heterogeneous
preferences for model accuracy. We develop a consumer-firm duopoly model to
analyze how competition affects firms' incentives to improve model accuracy.
Each firm aims to minimize its model's error, but this choice can often be
suboptimal. Counterintuitively, we find that in a competitive market, firms
that improve overall accuracy do not necessarily improve their profits. Rather,
each firm's optimal decision is to invest further on the error dimension where
it has a competitive advantage. By decomposing model errors into false positive
and false negative rates, firms can reduce errors in each dimension through
investments. Firms are strictly better off investing on their superior
dimension and strictly worse off with investments on their inferior dimension.
Profitable investments adversely affect consumers but increase overall welfare.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [152] [An Addendum to NeBula: Towards Extending TEAM CoSTAR's Solution to Larger Scale Environments](https://arxiv.org/abs/2504.13461)
*Ali Agha,Kyohei Otsu,Benjamin Morrell,David D. Fan,Sung-Kyun Kim,Muhammad Fadhil Ginting,Xianmei Lei,Jeffrey Edlund,Seyed Fakoorian,Amanda Bouman,Fernando Chavez,Taeyeon Kim,Gustavo J. Correa,Maira Saboia,Angel Santamaria-Navarro,Brett Lopez,Boseong Kim,Chanyoung Jung,Mamoru Sobue,Oriana Claudia Peltzer,Joshua Ott,Robert Trybula,Thomas Touma,Marcel Kaufmann,Tiago Stegun Vaquero,Torkom Pailevanian,Matteo Palieri,Yun Chang,Andrzej Reinke,Matthew Anderson,Frederik E. T. Schöller,Patrick Spieler,Lillian M. Clark,Avak Archanian,Kenny Chen,Hovhannes Melikyan,Anushri Dixit,Harrison Delecki,Daniel Pastor,Barry Ridge,Nicolas Marchal,Jose Uribe,Sharmita Dey,Kamak Ebadi,Kyle Coble,Alexander Nikitas Dimopoulos,Vivek Thangavelu,Vivek S. Varadharajan,Nicholas Palomo,Antoni Rosinol,Arghya Chatterjee,Christoforos Kanellakis,Bjorn Lindqvist,Micah Corah,Kyle Strickland,Ryan Stonebraker,Michael Milano,Christopher E. Denniston,Sami Sahnoune,Thomas Claudet,Seungwook Lee,Gautam Salhotra,Edward Terry,Rithvik Musuku,Robin Schmid,Tony Tran,Ara Kourchians,Justin Schachter,Hector Azpurua,Levi Resende,Arash Kalantari,Jeremy Nash,Josh Lee,Christopher Patterson,Jennifer G. Blank,Kartik Patath,Yuki Kubo,Ryan Alimo,Yasin Almalioglu,Aaron Curtis,Jacqueline Sly,Tesla Wells,Nhut T. Ho,Mykel Kochenderfer,Giovanni Beltrame,George Nikolakopoulos,David Shim,Luca Carlone,Joel Burdick*

Main category: cs.RO

TL;DR: 本文介绍了NeBula自治解决方案的扩展，以提升地下探索的范围和规模。


<details>
  <summary>Details</summary>
Motivation: 为了在DARPA地下挑战中增加机器人探索环境的范围和规模。

Method: 扩展包括大规模几何和语义映射、自适应定位系统、可穿越性分析、基于POMDP的全局运动规划、网络和去中心化推理、通信感知任务规划以及多模态地面-空中探索。

Result: 在石灰石矿和DARPA地下挑战等大规模地下环境中部署并验证了系统的应用。

Conclusion: 这些扩展提高了NeBula系统的性能，适用于更大规模的地下探索场景。

Abstract: This paper presents an appendix to the original NeBula autonomy solution
developed by the TEAM CoSTAR (Collaborative SubTerranean Autonomous Robots),
participating in the DARPA Subterranean Challenge. Specifically, this paper
presents extensions to NeBula's hardware, software, and algorithmic components
that focus on increasing the range and scale of the exploration environment.
From the algorithmic perspective, we discuss the following extensions to the
original NeBula framework: (i) large-scale geometric and semantic environment
mapping; (ii) an adaptive positioning system; (iii) probabilistic
traversability analysis and local planning; (iv) large-scale POMDP-based global
motion planning and exploration behavior; (v) large-scale networking and
decentralized reasoning; (vi) communication-aware mission planning; and (vii)
multi-modal ground-aerial exploration solutions. We demonstrate the application
and deployment of the presented systems and solutions in various large-scale
underground environments, including limestone mine exploration scenarios as
well as deployment in the DARPA Subterranean challenge.

</details>


### [153] [Robot Navigation in Dynamic Environments using Acceleration Obstacles](https://arxiv.org/abs/2504.13637)
*Asher Stern,Zvi Shiller*

Main category: cs.RO

TL;DR: 本论文扩展速度障碍概念到加速度障碍，改进了动态环境中的运动规划。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中运动规划的问题，通过精确推导加速度障碍边界，减少加速度调整率。

Method: 逐步分析推导：从速度障碍扩展到基本加速度障碍、加速度障碍和非线性加速度障碍，考虑初始速度和任意轨迹。

Result: 演示表明，非线性加速度障碍减少了加速度调整率，并实现了高效导航。

Conclusion: 该方法适用于自主车辆在复杂动态环境中的应用。

Abstract: This paper addresses the issue of motion planning in dynamic environments by
extending the concept of Velocity Obstacle and Nonlinear Velocity Obstacle to
Acceleration Obstacle AO and Nonlinear Acceleration Obstacle NAO. Similarly to
VO and NLVO, the AO and NAO represent the set of colliding constant
accelerations of the maneuvering robot with obstacles moving along linear and
nonlinear trajectories, respectively. Contrary to prior works, we derive
analytically the exact boundaries of AO and NAO. To enhance an intuitive
understanding of these representations, we first derive the AO in several
steps: first extending the VO to the Basic Acceleration Obstacle BAO that
consists of the set of constant accelerations of the robot that would collide
with an obstacle moving at constant accelerations, while assuming zero initial
velocities of the robot and obstacle. This is then extended to the AO while
assuming arbitrary initial velocities of the robot and obstacle. And finally,
we derive the NAO that in addition to the prior assumptions, accounts for
obstacles moving along arbitrary trajectories. The introduction of NAO allows
the generation of safe avoidance maneuvers that directly account for the
robot's second-order dynamics, with acceleration as its control input. The AO
and NAO are demonstrated in several examples of selecting avoidance maneuvers
in challenging road traffic. It is shown that the use of NAO drastically
reduces the adjustment rate of the maneuvering robot's acceleration while
moving in complex road traffic scenarios. The presented approach enables
reactive and efficient navigation for multiple robots, with potential
application for autonomous vehicles operating in complex dynamic environments.

</details>


### [154] [Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models](https://arxiv.org/abs/2504.13351)
*Chen Wang,Fei Xia,Wenhao Yu,Tingnan Zhang,Ruohan Zhang,C. Karen Liu,Li Fei-Fei,Jie Tan,Jacky Liang*

Main category: cs.RO

TL;DR: This paper introduces Chain-of-Modality (CoM) to enable robots to learn manipulation tasks from multimodal human demonstrations, improving accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Manipulation tasks require capturing control parameters like force, which visual data alone cannot provide, necessitating additional sensing modalities.

Method: Proposes Chain-of-Modality (CoM), a prompting strategy for Vision Language Models to integrate video, muscle activity, and audio data progressively.

Result: Achieves a threefold improvement in accuracy for task plan extraction and control parameters, with strong generalization in real-world experiments.

Conclusion: Enables robots to perform tasks from a single multimodal human video prompt, with resources available online.

Abstract: Learning to perform manipulation tasks from human videos is a promising
approach for teaching robots. However, many manipulation tasks require changing
control parameters during task execution, such as force, which visual data
alone cannot capture. In this work, we leverage sensing devices such as
armbands that measure human muscle activities and microphones that record
sound, to capture the details in the human manipulation process, and enable
robots to extract task plans and control parameters to perform the same task.
To achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy
that enables Vision Language Models to reason about multimodal human
demonstration data -- videos coupled with muscle or audio signals. By
progressively integrating information from each modality, CoM refines a task
plan and generates detailed control parameters, enabling robots to perform
manipulation tasks based on a single multimodal human video prompt. Our
experiments show that CoM delivers a threefold improvement in accuracy for
extracting task plans and control parameters compared to baselines, with strong
generalization to new task setups and objects in real-world robot experiments.
Videos and code are available at https://chain-of-modality.github.io

</details>


### [155] [LangCoop: Collaborative Driving with Language](https://arxiv.org/abs/2504.13406)
*Xiangbo Gao,Yuheng Wu,Rujia Wang,Chenxi Liu,Yang Zhou,Zhengzhong Tu*

Main category: cs.RO

TL;DR: 这篇论文提出LangCoop，使用自然语言作为多代理自治驾驶通信介质，减少96%的带宽，同时保持驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多代理通信的高带宽需求、代理异质性和信息丢失问题。

Method: 引入M³CoT用于结构化零样本视觉语言推理和LangPack用于信息打包，并在CARLA模拟中进行实验。

Result: 实现了96%的通信带宽减少（每条消息小于2KB），并在闭环评估中保持竞争性驾驶性能。

Conclusion: LangCoop通过高效的语言通信提升了自治驾驶系统的安全性和可靠性。

Abstract: Multi-agent collaboration holds great promise for enhancing the safety,
reliability, and mobility of autonomous driving systems by enabling information
sharing among multiple connected agents. However, existing multi-agent
communication approaches are hindered by limitations of existing communication
media, including high bandwidth demands, agent heterogeneity, and information
loss. To address these challenges, we introduce LangCoop, a new paradigm for
collaborative autonomous driving that leverages natural language as a compact
yet expressive medium for inter-agent communication. LangCoop features two key
innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured
zero-shot vision-language reasoning and Natural Language Information Packaging
(LangPack) for efficiently packaging information into concise, language-based
messages. Through extensive experiments conducted in the CARLA simulations, we
demonstrate that LangCoop achieves a remarkable 96\% reduction in communication
bandwidth (< 2KB per message) compared to image-based communication, while
maintaining competitive driving performance in the closed-loop evaluation.

</details>


### [156] [Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction](https://arxiv.org/abs/2504.13647)
*Yushen He,Lei Zhao,Tianchen Deng,Zipeng Fang,Weidong Chen*

Main category: cs.RO

TL;DR: 本论文提出一个轻量级多模态框架，用于服务移动机器人的3D物体检测和轨迹预测，结合LiDAR和相机输入，实现实时高精度感知，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 服务移动机器人需避开动态物体，但计算资源有限，因此需要高效的感知方法。

Method: 框架整合LiDAR和相机数据，引入Cross-Modal Deformable Transformer (CMDT)用于物体检测，以及Reference Trajectory-based Multi-Class Transformer (RTMCT)用于多类物体轨迹预测。

Result: 在CODa基准测试中，检测mAP提升2.03%，行人轨迹预测minADE5降低0.408m；在入门级NVIDIA 3060 GPU上实现13.2 fps实时推理。

Conclusion: 系统部署性强，并发布代码以支持再现和实际应用。

Abstract: Service mobile robots are often required to avoid dynamic objects while
performing their tasks, but they usually have only limited computational
resources. So we present a lightweight multi-modal framework for 3D object
detection and trajectory prediction. Our system synergistically integrates
LiDAR and camera inputs to achieve real-time perception of pedestrians,
vehicles, and riders in 3D space. The framework proposes two novel modules: 1)
a Cross-Modal Deformable Transformer (CMDT) for object detection with high
accuracy and acceptable amount of computation, and 2) a Reference
Trajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse
trajectory prediction of mult-class objects with flexible trajectory lengths.
Evaluations on the CODa benchmark demonstrate superior performance over
existing methods across detection (+2.03% in mAP) and trajectory prediction
(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits
exceptional deployability - when implemented on a wheelchair robot with an
entry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To
facilitate reproducibility and practical deployment, we release the related
code of the method at https://github.com/TossherO/3D_Perception and its ROS
inference version at https://github.com/TossherO/ros_packages.

</details>


### [157] [Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots](https://arxiv.org/abs/2504.13582)
*Zongyuan Chen,Yan Xia,Jiayuan Liu,Jijia Liu,Wenhao Tang,Jiayu Chen,Feng Gao,Longfei Ma,Hongen Liao,Yu Wang,Chao Yu,Boyu Zhang,Fei Xing*

Main category: cs.RO

TL;DR: 本研究提出一种滞后感知神经网络模型，用于软机器人手术应用，通过强化学习训练控制策略，实验显示显著提高精度。


<details>
  <summary>Details</summary>
Motivation: 软机器人适合人机交互手术，但其非线性滞后行为导致建模和控制挑战。

Method: 提出滞后感知全身体神经网络模型，构建并行模拟环境，使用on-policy强化学习训练控制策略，并通过幻影实验验证。

Result: 滞后模型使MSE减少84.95%；轨迹跟踪误差为0.126-0.250 mm；手术实验性能出色。

Conclusion: 方法在手术应用中表现出强性能，并有潜力扩展到临床场景。

Abstract: Soft robots exhibit inherent compliance and safety, which makes them
particularly suitable for applications requiring direct physical interaction
with humans, such as surgical procedures. However, their nonlinear and
hysteretic behavior, resulting from the properties of soft materials, presents
substantial challenges for accurate modeling and control. In this study, we
present a soft robotic system designed for surgical applications and propose a
hysteresis-aware whole-body neural network model that accurately captures and
predicts the soft robot's whole-body motion, including its hysteretic behavior.
Building upon the high-precision dynamic model, we construct a highly parallel
simulation environment for soft robot control and apply an on-policy
reinforcement learning algorithm to efficiently train whole-body motion control
strategies. Based on the trained control policy, we developed a soft robotic
system for surgical applications and validated it through phantom-based laser
ablation experiments in a physical environment. The results demonstrate that
the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95
percent compared to traditional modeling methods. The deployed control
algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm
on the real soft robot, highlighting its precision in real-world conditions.
The proposed method showed strong performance in phantom-based surgical
experiments and demonstrates its potential for complex scenarios, including
future real-world clinical applications.

</details>


### [158] [Learning Through Retrospection: Improving Trajectory Prediction for Automated Driving with Error Feedback](https://arxiv.org/abs/2504.13785)
*Steffen Hagedorn,Aron Distelzweig,Marcel Hallgarten,Alexandru P. Condurache*

Main category: cs.RO

TL;DR: 本论文提出一种新回顾技术，用于自动驾驶中轨迹预测，通过纠正错误提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法在推理中纠正错误，导致错误重复，因此需要利用时间数据改进预测。

Method: 提出回顾技术，通过闭环回滚训练模型，使用聚合反馈反思并纠正先前的预测错误。

Result: 在nuScenes和Argoverse数据集上，最小平均位移错误减少高达31.9%，并更好地处理分布外场景。

Conclusion: 该技术显著提升了轨迹预测的准确性和鲁棒性。

Abstract: In automated driving, predicting trajectories of surrounding vehicles
supports reasoning about scene dynamics and enables safe planning for the ego
vehicle. However, existing models handle predictions as an instantaneous task
of forecasting future trajectories based on observed information. As time
proceeds, the next prediction is made independently of the previous one, which
means that the model cannot correct its errors during inference and will repeat
them. To alleviate this problem and better leverage temporal data, we propose a
novel retrospection technique. Through training on closed-loop rollouts the
model learns to use aggregated feedback. Given new observations it reflects on
previous predictions and analyzes its errors to improve the quality of
subsequent predictions. Thus, the model can learn to correct systematic errors
during inference. Comprehensive experiments on nuScenes and Argoverse
demonstrate a considerable decrease in minimum Average Displacement Error of up
to 31.9% compared to the state-of-the-art baseline without retrospection. We
further showcase the robustness of our technique by demonstrating a better
handling of out-of-distribution scenarios with undetected road-users.

</details>


### [159] [Imitation Learning with Precisely Labeled Human Demonstrations](https://arxiv.org/abs/2504.13803)
*Yilong Song*

Main category: cs.RO

TL;DR: 本研究通过使用彩色抓手和RANSAC、ICP方法，使人类演示在机器人模仿学习中高效，模拟中达到机器人演示88.1%的性能。


<details>
  <summary>Details</summary>
Motivation: 训练通用机器人需要大规模数据集，人类演示易收集但面临精确动作推断、机器人形态差距和与现有训练管道整合的挑战。

Method: 利用手持抓手赋予独特颜色，便于分割，并应用RANSAC和ICP注册方法精确估计末端执行器位姿。

Result: 模拟结果显示，精确标记的人类演示单独使用可达机器人演示性能的88.1%，并在结合使用时提升性能，尽管存在形态差距。

Conclusion: 证明了使用精确标记人类演示的可行性和有效性，可提升机器人模仿学习性能。

Abstract: Within the imitation learning paradigm, training generalist robots requires
large-scale datasets obtainable only through diverse curation. Due to the
relative ease to collect, human demonstrations constitute a valuable addition
when incorporated appropriately. However, existing methods utilizing human
demonstrations face challenges in inferring precise actions, ameliorating
embodiment gaps, and fusing with frontier generalist robot training pipelines.
In this work, building on prior studies that demonstrate the viability of using
hand-held grippers for efficient data collection, we leverage the user's
control over the gripper's appearance--specifically by assigning it a unique,
easily segmentable color--to enable simple and reliable application of the
RANSAC and ICP registration method for precise end-effector pose estimation. We
show in simulation that precisely labeled human demonstrations on their own
allow policies to reach on average 88.1% of the performance of using robot
demonstrations, and boost policy performance when combined with robot
demonstrations, despite the inherent embodiment gap.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [160] [Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting](https://arxiv.org/abs/2504.13267)
*Isaac Adom,Mohammmad Iqbal Hossain,Hassan Mahmoud,Ahmad Alsharif,Mahmoud Nabil Mahmoud,Yang Xiao*

Main category: cs.CR

TL;DR: 本文提出一种隐私保护的交通预测系统，使用功能加密和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决交通拥堵导致的旅行时间延长、污染增加和碰撞风险，以及驾驶员位置信息隐私问题。

Method: 采用k-anonymity方案与功能加密聚合位置数据，并使用Conv-LSTM和Bi-LSTM模型捕获空间和时间特征进行预测。

Result: 在真实数据集上，60分钟预测误差小于10%，同时确保隐私保护。

Conclusion: 该系统在保护隐私的同时实现了高准确性的交通预测。

Abstract: Over the past few years, traffic congestion has continuously plagued the
nation's transportation system creating several negative impacts including
longer travel times, increased pollution rates, and higher collision risks. To
overcome these challenges, Intelligent Transportation Systems (ITS) aim to
improve mobility and vehicular systems, ensuring higher levels of safety by
utilizing cutting-edge technologies, sophisticated sensing capabilities, and
innovative algorithms. Drivers' participatory sensing, current/future location
reporting, and machine learning algorithms have considerably improved real-time
congestion monitoring and future traffic management. However, each driver's
sensitive spatiotemporal location information can create serious privacy
concerns. To address these challenges, we propose in this paper a secure,
privacy-preserving location reporting and traffic forecasting system that
guarantees privacy protection of driver data while maintaining high traffic
forecasting accuracy. Our novel k-anonymity scheme utilizes functional
encryption to aggregate encrypted location information submitted by drivers
while ensuring the privacy of driver location data. Additionally, using the
aggregated encrypted location information as input, this research proposes a
deep learning model that incorporates a Convolutional-Long Short-Term Memory
(Conv-LSTM) module to capture spatial and short-term temporal features and a
Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term
periodic patterns for traffic forecasting. With extensive evaluation on real
datasets, we demonstrate the effectiveness of the proposed scheme with less
than 10% mean absolute error for a 60-minute forecasting horizon, all while
protecting driver privacy.

</details>


### [161] [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
*Liang-bo Ning,Shijie Wang,Wenqi Fan,Qing Li,Xin Xu,Hao Chen,Feiran Huang*

Main category: cs.CR

TL;DR: 本论文提出CheatAgent框架，利用LLM攻击LLM增强的推荐系统，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: LLM增强推荐系统的安全漏洞研究不足，传统方法无效，LLM可作为高效攻击代理。

Method: 开发LLM-based代理，识别最佳插入位置、生成对抗扰动，并通过提示调优和反馈迭代优化攻击策略。

Result: 在三个真实数据集上实验，证明了攻击方法的有效性。

Conclusion: LLM可有效用于攻击黑箱推荐系统，强调了安全性的重要性。

Abstract: Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)
have brought significant advances in personalized user experience and have
attracted considerable attention. Despite the impressive progress, the research
question regarding the safety vulnerability of LLM-empowered RecSys still
remains largely under-investigated. Given the security and privacy concerns, it
is more practical to focus on attacking the black-box RecSys, where attackers
can only observe the system's inputs and outputs. However, traditional attack
approaches employing reinforcement learning (RL) agents are not effective for
attacking LLM-empowered RecSys due to the limited capabilities in processing
complex textual inputs, planning, and reasoning. On the other hand, LLMs
provide unprecedented opportunities to serve as attack agents to attack RecSys
because of their impressive capability in simulating human-like decision-making
processes. Therefore, in this paper, we propose a novel attack framework called
CheatAgent by harnessing the human-like capabilities of LLMs, where an
LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our
method first identifies the insertion position for maximum impact with minimal
input modification. After that, the LLM agent is designed to generate
adversarial perturbations to insert at target positions. To further improve the
quality of generated perturbations, we utilize the prompt tuning technique to
improve attacking strategies via feedback from the victim RecSys iteratively.
Extensive experiments across three real-world datasets demonstrate the
effectiveness of our proposed attacking method.

</details>


### [162] [Investigating cybersecurity incidents using large language models in latest-generation wireless networks](https://arxiv.org/abs/2504.13196)
*Leonid Legashev,Arthur Zhigalov*

Main category: cs.CR

TL;DR: 本研究使用大型语言模型检测无线网络安全攻击，Gemma-7b模型表现最佳（Precision、Recall和F1-Score均0.89），并提供决策解释。


<details>
  <summary>Details</summary>
Motivation: 检测网络安全事件、分析决策支持和评估对抗信息安全威胁措施的有效性，使用现代生成模型。

Method: 包括MIMO系统信号传播数据仿真、合成对抗样本、执行对抗攻击、微调大型语言模型检测攻击、基于提示技术的决策可解释性。

Result: 微调大型语言模型，比较六种模型，Gemma-7b在Precision=0.89、Recall=0.89和F1-Score=0.89上最佳，并通过提示分析数据不一致性和提供缓解建议。

Conclusion: 大型语言模型与二元分类器整合，在网络安全事件调查、决策支持和评估对抗措施有效性方面具有重大实际应用潜力。

Abstract: The purpose of research: Detection of cybersecurity incidents and analysis of
decision support and assessment of the effectiveness of measures to counter
information security threats based on modern generative models. The methods of
research: Emulation of signal propagation data in MIMO systems, synthesis of
adversarial examples, execution of adversarial attacks on machine learning
models, fine tuning of large language models for detecting adversarial attacks,
explainability of decisions on detecting cybersecurity incidents based on the
prompts technique. Scientific novelty: A binary classification of data
poisoning attacks was performed using large language models, and the
possibility of using large language models for investigating cybersecurity
incidents in the latest generation wireless networks was investigated. The
result of research: Fine-tuning of large language models was performed on the
prepared data of the emulated wireless network segment. Six large language
models were compared for detecting adversarial attacks, and the capabilities of
explaining decisions made by a large language model were investigated. The
Gemma-7b model showed the best results according to the metrics Precision =
0.89, Recall = 0.89 and F1-Score = 0.89. Based on various explainability
prompts, the Gemma-7b model notes inconsistencies in the compromised data under
study, performs feature importance analysis and provides various
recommendations for mitigating the consequences of adversarial attacks. Large
language models integrated with binary classifiers of network threats have
significant potential for practical application in the field of cybersecurity
incident investigation, decision support and assessing the effectiveness of
measures to counter information security threats.

</details>


### [163] [Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks](https://arxiv.org/abs/2504.13199)
*Mohammad Saleha,Azadeh Tabatabaeib*

Main category: cs.CR

TL;DR: 本综述探讨多模态AI系统的可信度，聚焦视觉语言任务的公平性、透明度和伦理问题，并比较VQA、图像描述和视觉对话。


<details>
  <summary>Details</summary>
Motivation: 多模态AI整合视觉和文本数据虽提升能力，但公平性、透明度和伦理问题成为关键关切。

Method: 通过2017-2024年文献的综述和比较分析，聚焦视觉语言任务的可信度，合成趋势、挑战和解决方案。

Result: 发现包括：透明度使用注意力图和梯度方法提升解释性；公平性减少VQA和视觉对话偏差；伦理影响强调多语言模型偏差处理和数据伦理。

Conclusion: 强调在视觉语言模型开发中整合公平性、透明度和伦理考虑的重要性。

Abstract: Objective: This review explores the trustworthiness of multimodal artificial
intelligence (AI) systems, specifically focusing on vision-language tasks. It
addresses critical challenges related to fairness, transparency, and ethical
implications in these systems, providing a comparative analysis of key tasks
such as Visual Question Answering (VQA), image captioning, and visual dialogue.
Background: Multimodal models, particularly vision-language models, enhance
artificial intelligence (AI) capabilities by integrating visual and textual
data, mimicking human learning processes. Despite significant advancements, the
trustworthiness of these models remains a crucial concern, particularly as AI
systems increasingly confront issues regarding fairness, transparency, and
ethics. Methods: This review examines research conducted from 2017 to 2024
focusing on forenamed core vision-language tasks. It employs a comparative
approach to analyze these tasks through the lens of trustworthiness,
underlining fairness, explainability, and ethics. This study synthesizes
findings from recent literature to identify trends, challenges, and
state-of-the-art solutions. Results: Several key findings were highlighted.
Transparency: Explainability of vision language tasks is important for user
trust. Techniques, such as attention maps and gradient-based methods, have
successfully addressed this issue. Fairness: Bias mitigation in VQA and visual
dialogue systems is essential for ensuring unbiased outcomes across diverse
demographic groups. Ethical Implications: Addressing biases in multilingual
models and ensuring ethical data handling is critical for the responsible
deployment of vision-language systems. Conclusion: This study underscores the
importance of integrating fairness, transparency, and ethical considerations in
developing vision-language models within a unified framework.

</details>


### [164] [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
*Salman Rahman,Liwei Jiang,James Shiffer,Genglin Liu,Sheriff Issaka,Md Rizwan Parvez,Hamid Palangi,Kai-Wei Chang,Yejin Choi,Saadia Gabriel*

Main category: cs.CR

TL;DR: 本文提出X-Teaming框架和XGuard-Train数据集，通过模拟多轮交互攻击提升语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型多轮交互中安全风险的不足，现有多轮红队测试适应性和多样性挑战。

Method: 使用X-Teaming框架的协作代理进行规划、攻击优化和验证，并构建XGuard-Train数据集。

Result: 多轮越狱攻击成功率高达98.1%，对Claude 3.7 Sonnet模型达96.2%；数据集规模是现有最佳资源的20倍。

Conclusion: 提供工具缓解对话攻击，推动语言模型多轮安全提升。

Abstract: Multi-turn interactions with language models (LMs) pose critical safety
risks, as harmful intent can be strategically spread across exchanges. Yet, the
vast majority of prior work has focused on single-turn safety, while
adaptability and diversity remain among the key challenges of multi-turn
red-teaming. To address these challenges, we present X-Teaming, a scalable
framework that systematically explores how seemingly harmless interactions
escalate into harmful outcomes and generates corresponding attack scenarios.
X-Teaming employs collaborative agents for planning, attack optimization, and
verification, achieving state-of-the-art multi-turn jailbreak effectiveness and
diversity with success rates up to 98.1% across representative leading
open-weight and closed-source models. In particular, X-Teaming achieves a 96.2%
attack success rate against the latest Claude 3.7 Sonnet model, which has been
considered nearly immune to single-turn attacks. Building on X-Teaming, we
introduce XGuard-Train, an open-source multi-turn safety training dataset that
is 20x larger than the previous best resource, comprising 30K interactive
jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our
work offers essential tools and insights for mitigating sophisticated
conversational attacks, advancing the multi-turn safety of LMs.

</details>


### [165] [On-Device Watermarking: A Socio-Technical Imperative For Authenticity In The Age of Generative AI](https://arxiv.org/abs/2504.13205)
*Houssam Kherraz*

Main category: cs.CR

TL;DR: 这篇立场论文主张转向硬件层加密签名水印真实内容，而不是AI生成内容，并借鉴HTTPS和Blu-Ray协议。


<details>
  <summary>Details</summary>
Motivation: AI模型输出日益逼真，现有水印技术有限，因此需要关注硬件层水印可信内容以更好地检测AI生成输出。

Method: 提出社会技术框架，在硬件传感器层进行水印，并与HTTPS证书和Blu-Ray验证协议类比。

Result: 硬件认证更易实现，尤其在政策层面；建议AI水印研究重点转向文本和LLM领域。

Conclusion: 生成模型接近感知上不可区分时，应警惕AI水印过度乐观，并调整研究方向。

Abstract: As generative AI models produce increasingly realistic output, both academia
and industry are focusing on the ability to detect whether an output was
generated by an AI model or not. Many of the research efforts and policy
discourse are centered around robust watermarking of AI outputs. While plenty
of progress has been made, all watermarking and AI detection techniques face
severe limitations. In this position paper, we argue that we are adopting the
wrong approach, and should instead focus on watermarking via cryptographic
signatures trustworthy content rather than AI generated ones. For audio-visual
content, in particular, all real content is grounded in the physical world and
captured via hardware sensors. This presents a unique opportunity to watermark
at the hardware layer, and we lay out a socio-technical framework and draw
parallels with HTTPS certification and Blu-Ray verification protocols. While
acknowledging implementation challenges, we contend that hardware-based
authentication offers a more tractable path forward, particularly from a policy
perspective. As generative models approach perceptual indistinguishability, the
research community should be wary of being overly optimistic with AI
watermarking, and we argue that AI watermarking research efforts are better
spent in the text and LLM space, which are ultimately not traceable to a
physical sensor.

</details>


### [166] [On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks](https://arxiv.org/abs/2504.13209)
*Ting Bi,Chenghang Ye,Zheyu Yang,Ziyi Zhou,Cui Tang,Jun Zhang,Zui Tao,Kailong Wang,Liting Zhou,Yang Yang,Tianlong Yu*

Main category: cs.CR

TL;DR: 本论文首次提出SEAR框架，研究AR和多模态LLM在社会工程攻击中的应用，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AR和多模态LLM快速发展，但整合带来新社会工程攻击风险，需要系统调查。

Method: 提出SEAR框架的三阶段：AR-based社会上下文合成、多模态RAG和ReInteract代理；进行IRB批准研究，60名参与者三种配置。

Result: SEAR框架有效，93.3%易受钓鱼攻击，85%接受电话；存在真实性问题。

Conclusion: 证明AR-LLM驱动攻击可行，并为防御提供洞见。

Abstract: Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are
rapidly evolving, providing unprecedented capabilities for human-computer
interaction. However, their integration introduces a new attack surface for
social engineering. In this paper, we systematically investigate the
feasibility of orchestrating AR-driven Social Engineering attacks using
Multimodal LLM for the first time, via our proposed SEAR framework, which
operates through three key phases: (1) AR-based social context synthesis, which
fuses Multimodal inputs (visual, auditory and environmental cues); (2)
role-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically
retrieves and integrates contextual data while preserving character
differentiation; and (3) ReInteract social engineering agents, which execute
adaptive multiphase attack strategies through inference interaction loops. To
verify SEAR, we conducted an IRB-approved study with 60 participants in three
experimental configurations (unassisted, AR+LLM, and full SEAR pipeline)
compiling a new dataset of 180 annotated conversations in simulated social
scenarios. Our results show that SEAR is highly effective at eliciting
high-risk behaviors (e.g., 93.3% of participants susceptible to email
phishing). The framework was particularly effective in building trust, with 85%
of targets willing to accept an attacker's call after an interaction. Also, we
identified notable limitations such as ``occasionally artificial'' due to
perceived authenticity gaps. This work provides proof-of-concept for AR-LLM
driven social engineering attacks and insights for developing defensive
countermeasures against next-generation augmented reality threats.

</details>


### [167] [The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict](https://arxiv.org/abs/2504.13371)
*Andrew J. Lohn*

Main category: cs.CR

TL;DR: AI 将以多种方式影响网络冲突的攻防平衡，没有单一答案。


<details>
  <summary>Details</summary>
Motivation: 网络领域可能最早受到 AI 影响，因此需要理解 AI 进步如何改变网络冲突。

Method: 通过文献综述，收集攻防优势论点，并分析 AI 进步的影响。

Result: 发现 AI 将提升某些方面，削弱其他方面，并列出 44 种影响方式。

Conclusion: 网络领域多方面，AI 对攻防的影响复杂，没有统一结论。

Abstract: Unlike other domains of conflict, and unlike other fields with high
anticipated risk from AI, the cyber domain is intrinsically digital with a
tight feedback loop between AI training and cyber application. Cyber may have
some of the largest and earliest impacts from AI, so it is important to
understand how the cyber domain may change as AI continues to advance. Our
approach reviewed the literature, collecting nine arguments that have been
proposed for offensive advantage in cyber conflict and nine proposed arguments
for defensive advantage. We include an additional forty-eight arguments that
have been proposed to give cyber conflict and competition its character as
collected separately by Healey, Jervis, and Nandrajog. We then consider how
each of those arguments and propositions might change with varying degrees of
AI advancement. We find that the cyber domain is too multifaceted for a single
answer to whether AI will enhance offense or defense broadly. AI will improve
some aspects, hinder others, and leave some aspects unchanged. We collect and
present forty-four ways that we expect AI to impact the cyber offense-defense
balance and the character of cyber conflict and competition.

</details>


### [168] [Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI](https://arxiv.org/abs/2504.13201)
*Jirui Yang,Zheyu Lin,Shuhan Yang,Zhihui Lu,Xin Du*

Main category: cs.CR

TL;DR: This paper proposes Concept Enhancement Engineering (CEE) to defend embodied large language models against jailbreak attacks, using representation engineering for better safety and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address security risks in embodied intelligence systems with LLMs, such as jailbreak attacks causing harmful outputs or unsafe actions, and the limitations of traditional defenses that increase computational overhead or disrupt performance.

Method: CEE extracts multilingual safety patterns from model activations, constructs control directions using safety-aligned concept subspaces, and applies subspace concept rotation to steer activations for safer inference.

Result: Experiments show CEE effectively mitigates jailbreak attacks while maintaining task performance, outperforming existing methods in robustness and efficiency.

Conclusion: This work offers a scalable and interpretable safety mechanism for embodied AI, bridging theoretical and practical aspects, and highlights the potential of latent-space interventions against adversarial threats.

Abstract: Embodied Intelligence (EI) systems integrated with large language models
(LLMs) face significant security risks, particularly from jailbreak attacks
that manipulate models into generating harmful outputs or executing unsafe
physical actions. Traditional defense strategies, such as input filtering and
output monitoring, often introduce high computational overhead or interfere
with task performance in real-time embodied scenarios. To address these
challenges, we propose Concept Enhancement Engineering (CEE), a novel defense
framework that leverages representation engineering to enhance the safety of
embodied LLMs by dynamically steering their internal activations. CEE operates
by (1) extracting multilingual safety patterns from model activations, (2)
constructing control directions based on safety-aligned concept subspaces, and
(3) applying subspace concept rotation to reinforce safe behavior during
inference. Our experiments demonstrate that CEE effectively mitigates jailbreak
attacks while maintaining task performance, outperforming existing defense
methods in both robustness and efficiency. This work contributes a scalable and
interpretable safety mechanism for embodied AI, bridging the gap between
theoretical representation engineering and practical security applications. Our
findings highlight the potential of latent-space interventions as a viable
defense paradigm against emerging adversarial threats in physically grounded AI
systems.

</details>


### [169] [Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation](https://arxiv.org/abs/2504.13551)
*CheolWon Na,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CR

TL;DR: 本论文提出Q-faker方法，实现查询-free硬黑盒攻击，生成高质量对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击需大量查询或访问目标模型，在真实硬黑盒场景中不可行。

Method: 使用代理模型和控制生成技术，生成针对无关目标的对抗句子。

Result: 在八个数据集上评估，显示高转移性和高品质对抗样本。

Conclusion: 证明方法在硬黑盒设置中有效且实用。

Abstract: Many adversarial attack approaches are proposed to verify the vulnerability
of language models. However, they require numerous queries and the information
on the target model. Even black-box attack methods also require the target
model's output information. They are not applicable in real-world scenarios, as
in hard black-box settings where the target model is closed and inaccessible.
Even the recently proposed hard black-box attacks still require many queries
and demand extremely high costs for training adversarial generators. To address
these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a
novel and efficient method that generates adversarial examples without
accessing the target model. To avoid accessing the target model, we use a
surrogate model instead. The surrogate model generates adversarial sentences
for a target-agnostic attack. During this process, we leverage controlled
generation techniques. We evaluate our proposed method on eight datasets.
Experimental results demonstrate our method's effectiveness including high
transferability and the high quality of the generated adversarial examples, and
prove its practical in hard black-box settings.

</details>


### [170] [OpCode-Based Malware Classification Using Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2504.13408)
*Varij Saini,Rudraksh Gupta,Neel Soni*

Main category: cs.CR

TL;DR: 本报告比较了使用OpCode序列的传统机器学习和深度学习方法在恶意软件分类中的性能，CNN展示了竞争性表现并具有自动特征提取优势。


<details>
  <summary>Details</summary>
Motivation: 评估传统机器学习和深度学习方法在恶意软件分类中的有效性，基于现有工作突出自动特征提取的好处。

Method: 使用两种方法：传统机器学习基于n-gram特征和SVM、KNN、决策树分类器；深度学习使用CNN在原始OpCode数据上自动提取特征。

Result: SVM在传统方法中表现最佳，但CNN显示出竞争性性能，并通过准确率、精确率、召回率和F1分数进行评估。

Conclusion: 虽然传统方法有效，但CNN等深度学习方法在特征提取方面更具优势，表明了改进恶意软件检测的潜力。

Abstract: This technical report presents a comprehensive analysis of malware
classification using OpCode sequences. Two distinct approaches are evaluated:
traditional machine learning using n-gram analysis with Support Vector Machine
(SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers; and a deep
learning approach employing a Convolutional Neural Network (CNN). The
traditional machine learning approach establishes a baseline using handcrafted
1-gram and 2-gram features from disassembled malware samples. The deep learning
methodology builds upon the work proposed in "Deep Android Malware Detection"
by McLaughlin et al. and evaluates the performance of a CNN model trained to
automatically extract features from raw OpCode data. Empirical results are
compared using standard performance metrics (accuracy, precision, recall, and
F1-score). While the SVM classifier outperforms other traditional techniques,
the CNN model demonstrates competitive performance with the added benefit of
automated feature extraction.

</details>


### [171] [Trace Gadgets: Minimizing Code Context for Machine Learning-Based Vulnerability Prediction](https://arxiv.org/abs/2504.13676)
*Felix Mächtle,Nils Loose,Tim Schulz,Florian Sieck,Jan-Niclas Serr,Ralf Möller,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 本论文引入Trace Gadgets代码表示方法，减少代码上下文，提高ML模型在漏洞检测中的性能，并优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 漏洞数量增加，手动识别费时，静态扫描器误报多，ML方法需合适代码上下文以提升检测准确性。

Method: 提出Trace Gadgets，通过移除无关代码捕获漏洞路径语句，并收集大规模真实数据集作为ML模型输入。

Result: 实验显示，使用Trace Gadgets的ML模型检测性能比CodeQL高出至少4%，并在实际应用中发现新漏洞。

Conclusion: Trace Gadgets提升了ML-based漏洞检测的效能，证明其在减少误报和提高准确性方面的优势。

Abstract: As the number of web applications and API endpoints exposed to the Internet
continues to grow, so does the number of exploitable vulnerabilities. Manually
identifying such vulnerabilities is tedious. Meanwhile, static security
scanners tend to produce many false positives. While machine learning-based
approaches are promising, they typically perform well only in scenarios where
training and test data are closely related. A key challenge for ML-based
vulnerability detection is providing suitable and concise code context, as
excessively long contexts negatively affect the code comprehension capabilities
of machine learning models, particularly smaller ones.
  This work introduces Trace Gadgets, a novel code representation that
minimizes code context by removing non-related code. Trace Gadgets precisely
capture the statements that cover the path to the vulnerability. As input for
ML models, Trace Gadgets provide a minimal but complete context, thereby
improving the detection performance. Moreover, we collect a large-scale dataset
generated from real-world applications with manually curated labels to further
improve the performance of ML-based vulnerability detectors. Our results show
that state-of-the-art machine learning models perform best when using Trace
Gadgets compared to previous code representations, surpassing the detection
capabilities of industry-standard static scanners such as GitHub's CodeQL by at
least 4% on a fully unseen dataset. By applying our framework to real-world
applications, we identify and report previously unknown vulnerabilities in
widely deployed software.

</details>


### [172] [Designing a reliable lateral movement detector using a graph foundation model](https://arxiv.org/abs/2504.13527)
*Corentin Larroche*

Main category: cs.CR

TL;DR: 本文探讨了图基础模型在网络安全中的应用，特别是用于横向移动检测，能在无需特定领域训练的情况下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽能加速创新，但处理网络流量或二进制数据效率低下，图基础模型可通过图结构更好地适应这些数据。

Method: 使用预训练的图基础模型构建检测器，直接应用于横向移动检测，无需再训练。

Result: 检测器在不使用领域特定数据训练的情况下，达到了最先进性能。

Conclusion: 这证明了图基础模型在网络安全领域的潜力，有望推动相关应用创新。

Abstract: Foundation models have recently emerged as a new paradigm in machine learning
(ML). These models are pre-trained on large and diverse datasets and can
subsequently be applied to various downstream tasks with little or no
retraining. This allows people without advanced ML expertise to build ML
applications, accelerating innovation across many fields. However, the adoption
of foundation models in cybersecurity is hindered by their inability to
efficiently process data such as network traffic captures or binary
executables. The recent introduction of graph foundation models (GFMs) could
make a significant difference, as graphs are well-suited to representing these
types of data. We study the usability of GFMs in cybersecurity through the lens
of one specific use case, namely lateral movement detection. Using a
pre-trained GFM, we build a detector that reaches state-of-the-art performance
without requiring any training on domain-specific data. This case study thus
provides compelling evidence of the potential of GFMs for cybersecurity.

</details>


### [173] [Can LLMs handle WebShell detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework](https://arxiv.org/abs/2504.13811)
*Feijiang Han,Jiaming Zhang,Chuyi Deng,Jianheng Tang,Yunhuai Liu*

Main category: cs.CR

TL;DR: 本研究评估了七个LLM在WebShell检测中的性能，并引入BFAD框架，显著提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法的数据需求大、遗忘问题和泛化能力差，LLM在代码任务中潜力未被充分利用。

Method: 通过基准测试七个LLM与传统方法的比较，并开发BFAD框架，包括关键函数过滤、上下文代码提取和加权行为剖析。

Result: 大模型高精度低召回，小模型反之；BFAD框架使LLM性能提升，平均F1分数增加13.82%，大模型超越现有最佳方法。

Conclusion: 首次探讨LLM在WebShell检测的可行性与挑战，并提供改进方案。

Abstract: WebShell attacks, in which malicious scripts are injected into web servers,
are a major cybersecurity threat. Traditional machine learning and deep
learning methods are hampered by issues such as the need for extensive training
data, catastrophic forgetting, and poor generalization. Recently, Large
Language Models (LLMs) have gained attention for code-related tasks, but their
potential in WebShell detection remains underexplored. In this paper, we make
two major contributions: (1) a comprehensive evaluation of seven LLMs,
including GPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against
traditional sequence- and graph-based methods using a dataset of 26.59K PHP
scripts, and (2) the Behavioral Function-Aware Detection (BFAD) framework,
designed to address the specific challenges of applying LLMs to this domain.
Our framework integrates three components: a Critical Function Filter that
isolates malicious PHP function calls, a Context-Aware Code Extraction strategy
that captures the most behaviorally indicative code segments, and Weighted
Behavioral Function Profiling (WBFP) that enhances in-context learning by
prioritizing the most relevant demonstrations based on discriminative
function-level profiles. Our results show that larger LLMs achieve near-perfect
precision but lower recall, while smaller models exhibit the opposite
trade-off. However, all models lag behind previous State-Of-The-Art (SOTA)
methods. With BFAD, the performance of all LLMs improved, with an average F1
score increase of 13.82%. Larger models such as GPT-4, LLaMA 3.1 70B, and Qwen
2.5 14B outperform SOTA methods, while smaller models such as Qwen 2.5 3B
achieve performance competitive with traditional methods. This work is the
first to explore the feasibility and limitations of LLMs for WebShell
detection, and provides solutions to address the challenges in this task.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [174] [SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents](https://arxiv.org/abs/2504.13541)
*Avaneesh Devkota,Rachmad Vidya Wicaksana Putra,Muhammad Shafique*

Main category: cs.NE

TL;DR: 这篇论文提出SwitchMT，一种自适应任务切换方法，用于强化学习的多任务学习，提高自主代理的性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在多任务环境中因任务干扰而难以泛化，且需要处理数据流，而现有基于SNN的方法依赖固定任务切换间隔，限制了可扩展性。

Method: SwitchMT使用具有活性树突和对决结构的深度脉冲Q网络，以及基于奖励和网络内部动态的自适应任务切换策略。

Result: 实验结果显示SwitchMT在多任务学习中优于现有方法，在Atari游戏中取得竞争性分数（Pong: -8.8, Breakout: 5.6, Enduro: 355.2），证明了更好的泛化能力。

Conclusion: SwitchMT有效减少任务干扰，实现可扩展的多任务学习，为开发高效通用代理奠定基础。

Abstract: The ability to train intelligent autonomous agents (such as mobile robots) on
multiple tasks is crucial for adapting to dynamic real-world environments.
However, state-of-the-art reinforcement learning (RL) methods only excel in
single-task settings, and still struggle to generalize across multiple tasks
due to task interference. Moreover, real-world environments also demand the
agents to have data stream processing capabilities. Toward this, a
state-of-the-art work employs Spiking Neural Networks (SNNs) to improve
multi-task learning by exploiting temporal information in data stream, while
enabling lowpower/energy event-based operations. However, it relies on fixed
context/task-switching intervals during its training, hence limiting the
scalability and effectiveness of multi-task learning. To address these
limitations, we propose SwitchMT, a novel adaptive task-switching methodology
for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT
employs the following key ideas: (1) a Deep Spiking Q-Network with active
dendrites and dueling structure, that utilizes task-specific context signals to
create specialized sub-networks; and (2) an adaptive task-switching policy that
leverages both rewards and internal dynamics of the network parameters.
Experimental results demonstrate that SwitchMT achieves superior performance in
multi-task learning compared to state-of-the-art methods. It achieves
competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6,
and Enduro: 355.2) compared to the state-of-the-art, showing its better
generalized learning capability. These results highlight the effectiveness of
our SwitchMT methodology in addressing task interference while enabling
multi-task learning automation through adaptive task switching, thereby paving
the way for more efficient generalist agents with scalable multi-task learning
capabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [175] [Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance](https://arxiv.org/abs/2504.13376)
*Aitor Gómez-Tejedor,Eneko Osaba,Esther Villar-Rodriguez*

Main category: quant-ph

TL;DR: 本研究分析量子退火处理器上的次要嵌入问题，评估嵌入质量对性能的影响，并比较Minorminer和Clique Embedding算法。


<details>
  <summary>Details</summary>
Motivation: 动机源于量子退火器在处理适合架构的问题与非硬件原生拓扑时的性能差异，旨在分析嵌入质量的影响和评估Minorminer的性能。

Method: 方法包括实验分析嵌入质量对D-Wave量子退火器的性能影响，评估Minorminer的嵌入质量，并与Clique Embedding算法比较。

Result: 结果显示嵌入平均链长与相对错误相关，Minorminer未 consistently 优于Clique Embedding，表明有改进空间。

Conclusion: 结论强调嵌入质量对量子退火性能的关键影响，并指出Minorminer有显著改进潜力。

Abstract: This study addresses the minor-embedding problem, which involves mapping the
variables of an Ising model onto a quantum annealing processor. The primary
motivation stems from the observed performance disparity of quantum annealers
when solving problems suited to the processor's architecture versus those with
non-hardware-native topologies. Our research has two main objectives: i) to
analyze the impact of embedding quality on the performance of D-Wave Systems
quantum annealers, and ii) to evaluate the quality of the embeddings generated
by Minorminer, an algorithm provided by D-Wave and widely recognized as the
standard minor-embedding technique in the literature. Regarding the first
objective, our experiments reveal a clear correlation between the average chain
length of embeddings and the relative errors of the solutions sampled. This
underscores the critical influence of embedding quality on quantum annealing
performance. For the second objective, we focus on the Minorminer technique,
assessing its capacity to embed problems, the quality of the embeddings
produced, and the robustness of the results. We also compare its performance
with Clique Embedding, another algorithm developed by D-Wave, which is
deterministic and designed to embed fully connected Ising models into quantum
annealing processors, serving as a worst-case scenario. The results demonstrate
that there is significant room for improvement for Minorminer, as it has not
consistently outperformed the worst-case scenario.

</details>


### [176] [Adaptive Non-local Observable on Quantum Neural Networks](https://arxiv.org/abs/2504.13414)
*Hsin-Yi Lin,Huan-Hsin Tseng,Samuel Yen-Chi Chen,Shinjae Yoo*

Main category: quant-ph

TL;DR: 本文提出一种受海森堡图启发的自适应非局部测量框架，提升变分量子电路在量子机器学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子电路依赖固定埃尔米特可观测量，模型复杂度有限，本文受海森堡图启发，旨在增加复杂度。

Method: 引入动态埃尔米特可观测量和演化参数，优化旋转对应可观测量空间轨迹，并提出两种非局部测量方案。

Result: 数值模拟显示在分类任务中优于传统变分量子电路，更强大且资源高效。

Conclusion: 结合变分旋转和非局部可观测量，增强量子比特交互，提供灵活电路设计，作为量子神经网络的更好方法。

Abstract: Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning
typically rely on a fixed Hermitian observable, often built from Pauli
operators. Inspired by the Heisenberg picture, we propose an adaptive non-local
measurement framework that substantially increases the model complexity of the
quantum circuits. Our introduction of dynamical Hermitian observables with
evolving parameters shows that optimizing VQC rotations corresponds to tracing
a trajectory in the observable space. This viewpoint reveals that standard VQCs
are merely a special case of the Heisenberg representation.
  Furthermore, we show that properly incorporating variational rotations with
non-local observables enhances qubit interaction and information mixture,
admitting flexible circuit designs. Two non-local measurement schemes are
introduced, and numerical simulations on classification tasks confirm that our
approach outperforms conventional VQCs, yielding a more powerful and
resource-efficient approach as a Quantum Neural Network.

</details>


### [177] [A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices](https://arxiv.org/abs/2504.13232)
*Sayed Pouria Talebi,Clive Cheong Took,Danilo P. Mandic*

Main category: quant-ph

TL;DR: 这篇论文使用四元数代数设计量子学习机器的适应和优化技术，建立了计算模型，并基于HR-calculus开发训练框架。


<details>
  <summary>Details</summary>
Motivation: 解决量子学习机器训练中适应和优化技术设计的问题。

Method: 使用四元数代数推导计算和测量操作模型，制定自适应学习问题，并利用HR-calculus开发训练框架。

Result: 建立了数学上易处理的模型，并确立了性能标准如收敛条件。

Conclusion: 建立了类似于经典神经元的量子信息处理单元。

Abstract: This article considers the problem of designing adaption and optimisation
techniques for training quantum learning machines. To this end, the division
algebra of quaternions is used to derive an effective model for representing
computation and measurement operations on qubits. In turn, the derived model,
serves as the foundation for formulating an adaptive learning problem on
principal quantum learning units, thereby establishing quantum information
processing units akin to that of neurons in classical approaches. Then,
leveraging the modern HR-calculus, a comprehensive training framework for
learning on quantum machines is developed. The quaternion-valued model
accommodates mathematical tractability and establishment of performance
criteria, such as convergence conditions.

</details>


### [178] [Quantum repeaters enhanced by vacuum beam guides](https://arxiv.org/abs/2504.13397)
*Yu Gan,Mohadeseh Azar,Nitish Kumar Chandra,Xin Jin,Jinglei Cheng,Kaushik P. Seshadreesan,Junyu Liu*

Main category: quant-ph

TL;DR: 本论文探讨使用真空光束导管（VBG）来解决量子通信网络中的光子损失和相干性衰减问题，通过延长中继器间距减少节点数量并降低成本。


<details>
  <summary>Details</summary>
Motivation: 量子通信网络因光子损失和相干性衰减而面临传输距离限制和中继站密集需求，本文提出VBG作为传统光纤的替代方案。

Method: 通过将VBG整合到中继器架构中，进行成本函数分析，评估第一、第二和第三代中继器的性能权衡。

Result: 结果显示第一代中继器显著降低成本，第三代中继器受益于传输成功率提升，第二代中继器受逻辑门错误限制，所有代次均从减少光子损失中获益。

Conclusion: VBG是可扩展高性能量子网络的有力工具，特别是与近期量子硬件相结合。

Abstract: The development of large-scale quantum communication networks faces critical
challenges due to photon loss and decoherence in optical fiber channels. These
fundamentally limit transmission distances and demand dense networks of
repeater stations. This work investigates using vacuum beam guides (VBGs)-a
promising ultra-low-loss transmission platform-as an alternative to traditional
fiber links. By incorporating VBGs into repeater-based architectures, we
demonstrate that the inter-repeater spacing can be substantially extended,
resulting in fewer required nodes and significantly reducing hardware and
operational complexity. We perform a cost-function analysis to quantify
performance trade-offs across first, second, and third-generation repeaters.
Our results show that first-generation repeaters reduce costs dramatically by
eliminating entanglement purification. Third-generation repeaters benefit from
improved link transmission success, which is crucial for quantum error
correction. In contrast, second-generation repeaters exhibit a more nuanced
response; although transmission loss is reduced, their performance remains
primarily limited by logical gate errors rather than channel loss. These
findings highlight that while all repeater generations benefit from reduced
photon loss, the magnitude of improvement depends critically on the underlying
error mechanisms. Vacuum beam guides thus emerge as a powerful enabler for
scalable, high-performance quantum networks, particularly in conjunction with
near-term quantum hardware capabilities.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [179] [Adaptive AI decision interface for autonomous electronic material discovery](https://arxiv.org/abs/2504.13344)
*Yahao Dai,Henry Chan,Aikaterini Vriza,Fredrick Kim,Yunfei Wang,Wei Liu,Naisong Shan,Jing Xu,Max Weires,Yukun Wu,Zhiqiang Cao,C. Suzanne Miller,Ralu Divan,Xiaodan Gu,Chenhui Zhu,Sihong Wang,Jie Xu*

Main category: cond-mat.mtrl-sci

TL;DR: AI通过自适应决策接口提升电子材料实验效率，在64次试验中将μC*提高150%，达1275 F cm-1 V-1 s-1。


<details>
  <summary>Details</summary>
Motivation: AI/AE在电子材料发现中受数据稀缺和适应性不足影响，无法像人类科学家一样实时决策。

Method: 开发AI决策接口，包括AI顾问进行实时监控、数据分析和人机协作，应用于MIECPs研究，使用OECT作为测试设备。

Result: 实现了μC*150%提升，达到1275 F cm-1 V-1 s-1；识别了晶体层间距和比表面积等关键因素，并发现新聚合物多晶型。

Conclusion: 该平台显著加速材料发现，证明AI在电子材料研究中的潜力。

Abstract: AI-powered autonomous experimentation (AI/AE) can accelerate materials
discovery but its effectiveness for electronic materials is hindered by data
scarcity from lengthy and complex design-fabricate-test-analyze cycles. Unlike
experienced human scientists, even advanced AI algorithms in AI/AE lack the
adaptability to make informative real-time decisions with limited datasets.
Here, we address this challenge by developing and implementing an AI decision
interface on our AI/AE system. The central element of the interface is an AI
advisor that performs real-time progress monitoring, data analysis, and
interactive human-AI collaboration for actively adapting to experiments in
different stages and types. We applied this platform to an emerging type of
electronic materials-mixed ion-electron conducting polymers (MIECPs) -- to
engineer and study the relationships between multiscale morphology and
properties. Using organic electrochemical transistors (OECT) as the testing-bed
device for evaluating the mixed-conducting figure-of-merit -- the product of
charge-carrier mobility and the volumetric capacitance ({\mu}C*), our adaptive
AI/AE platform achieved a 150% increase in {\mu}C* compared to the commonly
used spin-coating method, reaching 1,275 F cm-1 V-1 s-1 in just 64 autonomous
experimental trials. A study of 10 statistically selected samples identifies
two key structural factors for achieving higher volumetric capacitance: larger
crystalline lamellar spacing and higher specific surface area, while also
uncovering a new polymer polymorph in this material.

</details>
