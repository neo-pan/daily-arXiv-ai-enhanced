<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.CV](#cs.CV) [Total: 26]
- [nlin.CD](#nlin.CD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 47]
- [hep-th](#hep-th) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 14]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.OC](#math.OC) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data](https://arxiv.org/abs/2504.12417)
*Dewang Kumar Agarwal,Dimitris J. Bertsimas*

Main category: cs.AI

TL;DR: 本研究使用AI开发2型糖尿病治疗指南，表现优于医生实践。


<details>
  <summary>Details</summary>
Motivation: 创建精确、结构化的数据支持型2型糖尿病治疗进展指南，以供临床采用，并解决观察数据中的混杂偏差。

Method: 使用机器学习和优化去除偏差，训练树模型，结合成治疗管道，并在BMC和Hartford未见数据上测试。

Result: AI管道使HbA1c中位数降低多0.26%（BMC）和0.13%（Hartford）。

Conclusion: AI方法精确、可解释、efficient，预计优于当前实践，可改善患者结果。

Abstract: Objective: Create precise, structured, data-backed guidelines for type 2
diabetes treatment progression, suitable for clinical adoption.
  Research Design and Methods: Our training cohort was composed of patient
(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to
2014. We divide visits into 4 groups based on the patient's treatment regimen
before the visit, and further divide them into subgroups based on the
recommended treatment during the visit. Since each subgroup has observational
data, which has confounding bias (sicker patients are prescribed more
aggressive treatments), we used machine learning and optimization to remove
some datapoints so that the remaining data resembles a randomized trial. On
each subgroup, we train AI-backed tree-based models to prescribe treatment
changes. Once we train these tree models, we manually combine the models for
every group to create an end-to-end prescription pipeline for all patients in
that group. In this process, we prioritize stepping up to a more aggressive
treatment before considering less aggressive options. We tested this pipeline
on unseen data from BMC, and an external dataset from Hartford healthcare (type
2 diabetes patient visits from January 2020 to May 2024).
  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more
than what the doctors achieved on the unseen BMC patients. For the Hartford
cohort, our pipelines were better by 0.13%.
  Conclusions: This precise, interpretable, and efficient AI-backed approach to
treatment progression in type 2 diabetes is predicted to outperform the current
practice and can be deployed to improve patient outcomes.

</details>


### [2] [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
*George Fatouros,Georgios Makridis,George Kousiouris,John Soldatos,Anargyros Tsadimas,Dimosthenis Kyriazis*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于大语言模型的对话代理系统，用于提升MLOps中的人机协作，使复杂工具更易访问。


<details>
  <summary>Details</summary>
Motivation: 解决复杂MLOps平台如Kubeflow的可访问性问题，降低技术门槛，使高级ML工具更广泛可用。

Method: 引入Swarm Agent架构，采用分层模块设计，包括KFP Agent、MinIO Agent和RAG Agent，通过迭代推理和上下文感知处理自然语言交互。

Result: 用户可通过对话接口发现、执行和监控ML管道，管理数据集和工件，减少复杂性并降低进入障碍。

Conclusion: 这种方法使高级ML工具更易访问，并保持扩展到其他平台的灵活性。

Abstract: This paper presents a Large Language Model (LLM) based conversational agent
system designed to enhance human-machine collaboration in Machine Learning
Operations (MLOps). We introduce the Swarm Agent, an extensible architecture
that integrates specialized agents to create and manage ML workflows through
natural language interactions. The system leverages a hierarchical, modular
design incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline
orchestration, a MinIO Agent for data management, and a Retrieval-Augmented
Generation (RAG) Agent for domain-specific knowledge integration. Through
iterative reasoning loops and context-aware processing, the system enables
users with varying technical backgrounds to discover, execute, and monitor ML
pipelines; manage datasets and artifacts; and access relevant documentation,
all via intuitive conversational interfaces. Our approach addresses the
accessibility gap in complex MLOps platforms like Kubeflow, making advanced ML
tools broadly accessible while maintaining the flexibility to extend to other
platforms. The paper describes the architecture, implementation details, and
demonstrates how this conversational MLOps assistant reduces complexity and
lowers barriers to entry for users across diverse technical skill levels.

</details>


### [3] [Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it](https://arxiv.org/abs/2504.12482)
*Luciano Floridi,Carlotta Buttaboni,Emmie Hine,Jessica Morley,Claudio Novelli,Tyler Schroder*

Main category: cs.AI

TL;DR: 这篇论文介绍了Agentic AI Optimisation (AAIO)，旨在优化网站与自主AI代理之间的互动，类似于SEO，并讨论了其治理、伦理、法律和社会影响。


<details>
  <summary>Details</summary>
Motivation: 由于Agentic Artificial Intelligence (AAI)系统的出现，需要一种新的优化范式来确保代理与平台的无缝互动，类似于SEO对数字内容发现的影响。

Method: 通过考察网站优化与代理AI成功之间的相互依赖性，探讨AAIO创建的良性循环，并探索治理、伦理、法律和社会影响。

Result: 突出了AAIO的良性循环，强调了主动监管框架的必要性，以减轻潜在负面影响。

Conclusion: 肯定了AAIO在自主数字代理时代作为数字基础设施的基本作用，并倡导公平和包容性的访问。

Abstract: The emergence of Agentic Artificial Intelligence (AAI) systems capable of
independently initiating digital interactions necessitates a new optimisation
paradigm designed explicitly for seamless agent-platform interactions. This
article introduces Agentic AI Optimisation (AAIO) as an essential methodology
for ensuring effective integration between websites and agentic AI systems.
Like how Search Engine Optimisation (SEO) has shaped digital content
discoverability, AAIO can define interactions between autonomous AI agents and
online platforms. By examining the mutual interdependency between website
optimisation and agentic AI success, the article highlights the virtuous cycle
that AAIO can create. It further explores the governance, ethical, legal, and
social implications (GELSI) of AAIO, emphasising the necessity of proactive
regulatory frameworks to mitigate potential negative impacts. The article
concludes by affirming AAIO's essential role as part of a fundamental digital
infrastructure in the era of autonomous digital agents, advocating for
equitable and inclusive access to its benefits.

</details>


### [4] [Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope](https://arxiv.org/abs/2504.12497)
*Robert E. Wray,Steven J. Jones,John E. Laird*

Main category: cs.AI

TL;DR: 这篇论文探讨了在开放世界中，AI 代理如何应对超出设计范围的未知情况，并提出了一种结合人类认知评估和元推理的新方法。


<details>
  <summary>Details</summary>
Motivation: 代理在开放世界中会遇到无法预料的情况，缺乏相关知识或时间做出决策，因此需要快速识别和适应这些情况。

Method: 提出了一种结合领域通用元知识（受人类认知启发的评估）和元推理的方法。

Result: 该方法有潜力提供快速、适应的响应，满足开放世界代理的性能需求。

Conclusion: 这种方法能够更好地处理未知情况，提高代理的适应性和鲁棒性。

Abstract: Regardless of past learning, an agent in an open world will face unfamiliar
situations and events outside of prior experience, existing models, or
policies. Further, the agent will sometimes lack relevant knowledge and/or
sufficient time to assess the situation, generate and evaluate options, and
pursue a robustly considered course of action. How can an agent respond
reasonably to situations that are outside of its original design scope? How can
it recognize such situations sufficiently quickly and reliably to determine
reasonable, adaptive courses of action? We identify key characteristics needed
for solutions, evaluate the state-of-the-art by these requirements, and outline
a proposed, novel approach that combines domain-general meta-knowledge (in the
form of appraisals inspired by human cognition) and metareasoning. It has the
potential to provide fast, adaptive responses to unfamiliar situations, more
fully meeting the performance characteristics required for open-world, general
agents.

</details>


### [5] [Is Trust Correlated With Explainability in AI? A Meta-Analysis](https://arxiv.org/abs/2504.12529)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 本研究通过元分析发现AI可解释性与用户信任有中等正相关，但不是唯一因素，并强调其在责任和信任方面的社会技术含义。


<details>
  <summary>Details</summary>
Motivation: 研究动机是 critically examine 普遍认为的假设，即AI系统的可解释性会 inherently 提升用户信任。

Method: 采用元分析方法，对90项现有文献进行全面审查。

Result: 发现AI可解释性与用户信任之间存在统计上显著但中度的正相关，表明可解释性有助于信任但非主导因素。

Conclusion: 强调解释性AI的学术贡献及社会技术含义，呼吁在医疗和司法等领域促进责任、解决算法偏差，以实现公平可持续的AI采用和持久信任。

Abstract: This study critically examines the commonly held assumption that
explicability in artificial intelligence (AI) systems inherently boosts user
trust. Utilizing a meta-analytical approach, we conducted a comprehensive
examination of the existing literature to explore the relationship between AI
explainability and trust. Our analysis, incorporating data from 90 studies,
reveals a statistically significant but moderate positive correlation between
the explainability of AI systems and the trust they engender among users. This
indicates that while explainability contributes to building trust, it is not
the sole or predominant factor in this equation. In addition to academic
contributions to the field of Explainable AI (XAI), this research highlights
its broader socio-technical implications, particularly in promoting
accountability and fostering user trust in critical domains such as healthcare
and justice. By addressing challenges like algorithmic bias and ethical
transparency, the study underscores the need for equitable and sustainable AI
adoption. Rather than focusing solely on immediate trust, we emphasize the
normative importance of fostering authentic and enduring trustworthiness in AI
systems.

</details>


### [6] [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
*Haidar Khan,Hisham A. Alyahya,Yazeed Alnumay,M Saiful Bari,Bülent Yener*

Main category: cs.AI

TL;DR: ZeroSumEval 是一种新型基于零和游戏的LLM评估协议，使用动态基准评估AI能力，揭示模型局限性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法存在过拟合、高成本和偏差问题，ZeroSumEval 通过竞争性游戏提供更可靠的动态评估。

Method: 采用多种零和游戏（如PyJail、国际象棋、扑克等）构建标准化框架，进行超过7000次模拟实验评估13个模型。

Result: 前沿模型（如GPT和Claude系列）在常见游戏中表现良好，但挣扎于创造新问题和创意任务，无法可靠越狱。

Conclusion: ZeroSumEval 是一种有效、可扩展的评估框架，突显LLM弱点，并开源代码以促进进一步研究。

Abstract: Evaluating the capabilities of Large Language Models (LLMs) has traditionally
relied on static benchmark datasets, human assessments, or model-based
evaluations - methods that often suffer from overfitting, high costs, and
biases. ZeroSumEval is a novel competition-based evaluation protocol that
leverages zero-sum games to assess LLMs with dynamic benchmarks that resist
saturation. ZeroSumEval encompasses a diverse suite of games, including
security challenges (PyJail), classic games (Chess, Liar's Dice, Poker),
knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These
games are designed to evaluate a range of AI capabilities such as strategic
reasoning, planning, knowledge application, and creativity. Building upon
recent studies that highlight the effectiveness of game-based evaluations for
LLMs, ZeroSumEval enhances these approaches by providing a standardized and
extensible framework. To demonstrate this, we conduct extensive experiments
with >7000 simulations across 7 games and 13 models. Our results show that
while frontier models from the GPT and Claude families can play common games
and answer questions, they struggle to play games that require creating novel
and challenging questions. We also observe that models cannot reliably
jailbreak each other and fail generally at tasks requiring creativity. We
release our code at https://github.com/facebookresearch/ZeroSumEval.

</details>


### [7] [The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance](https://arxiv.org/abs/2504.12612)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.AI

TL;DR: 这篇论文提出一种不依赖内部记忆或外部元信息的系统，用于从内容中后验追踪多代理生成链的来源历史。


<details>
  <summary>Details</summary>
Motivation: 在多代理AI协作中，内容不断修改，难以追踪贡献，因此需要一种追踪生成历史的方法。

Method: 提出基于符号编年史的系统，通过反馈循环在生成过程中更新和同步编年史。

Result: 开发了一个可用于后验归因的系统，提高AI协作的可问责性。

Conclusion: 旨在在演化的网络生态中发展可问责的协作人工智能。

Abstract: Provenance is the chronology of things, resonating with the fundamental
pursuit to uncover origins, trace connections, and situate entities within the
flow of space and time. As artificial intelligence advances towards autonomous
agents capable of interactive collaboration on complex tasks, the provenance of
generated content becomes entangled in the interplay of collective creation,
where contributions are continuously revised, extended or overwritten. In a
multi-agent generative chain, content undergoes successive transformations,
often leaving little, if any, trace of prior contributions. In this study, we
investigates the problem of tracking multi-agent provenance across the temporal
dimension of generation. We propose a chronological system for post hoc
attribution of generative history from content alone, without reliance on
internal memory states or external meta-information. At its core lies the
notion of symbolic chronicles, representing signed and time-stamped records, in
a form analogous to the chain of custody in forensic science. The system
operates through a feedback loop, whereby each generative timestep updates the
chronicle of prior interactions and synchronises it with the synthetic content
in the very act of generation. This research seeks to develop an accountable
form of collaborative artificial intelligence within evolving cyber ecosystems.

</details>


### [8] [Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning](https://arxiv.org/abs/2504.12680)
*Baining Zhao,Ziyou Wang,Jianjie Fang,Chen Gao,Fanhang Man,Jinqiang Cui,Xin Wang,Xinlei Chen,Yong Li,Wenwu Zhu*

Main category: cs.AI

TL;DR: 本论文提出Embodied-R框架，结合大型视觉语言模型和小型语言模型，使用强化学习在有限资源下实现空间推理能力，训练后达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练模型如何获得空间推理能力，并开发高效的推理框架。

Method: 使用Embodied-R框架，结合VLMs和LMs，通过RL和新型奖励系统训练，仅需5k样本。

Result: 模型在分布内和分布外任务上匹配SOTA模型，并表现出系统分析和上下文整合等新兴思考模式。

Conclusion: 框架在有限计算资源下实现高效推理，并探讨了响应长度、奖励设计和训练策略等研究问题。

Abstract: Humans can perceive and reason about spatial relationships from sequential
visual observations, such as egocentric video streams. However, how pretrained
models acquire such abilities, especially high-level reasoning, remains
unclear. This paper introduces Embodied-R, a collaborative framework combining
large-scale Vision-Language Models (VLMs) for perception and small-scale
Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a
novel reward system considering think-answer logical consistency, the model
achieves slow-thinking capabilities with limited computational resources. After
training on only 5k embodied video samples, Embodied-R with a 3B LM matches
state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on
both in-distribution and out-of-distribution embodied spatial reasoning tasks.
Embodied-R also exhibits emergent thinking patterns such as systematic analysis
and contextual integration. We further explore research questions including
response length, training on VLM, strategies for reward design, and differences
in model generalization after SFT (Supervised Fine-Tuning) and RL training.

</details>


### [9] [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
*Arth Bohra,Manvel Saroyan,Danil Melkozerov,Vahe Karufanyan,Gabriel Maher,Pascal Weinberger,Artem Harutyunyan,Giovanni Campagna*

Main category: cs.AI

TL;DR: 本论文引入WebLists基准测试评估网页数据提取任务，并提出BardeenAgent框架，提高了提取性能。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理研究偏重导航和交易，忽略大规模结构化数据提取。

Method: 提出BardeenAgent框架，将执行转换为可重复程序，利用HTML结构构建CSS选择器进行数据提取。

Result: BardeenAgent在WebLists基准上实现66%召回率，比现有最先进代理提高一倍，并降低成本。

Conclusion: BardeenAgent显著提升数据提取能力，证明利用HTML结构的有效性。

Abstract: Most recent web agent research has focused on navigation and transaction
tasks, with little emphasis on extracting structured data at scale. We present
WebLists, a benchmark of 200 data-extraction tasks across four common business
and enterprise use-cases. Each task requires an agent to navigate to a webpage,
configure it appropriately, and extract complete datasets with well-defined
schemas. We show that both LLMs with search capabilities and SOTA web agents
struggle with these tasks, with a recall of 3% and 31%, respectively, despite
higher performance on question-answering tasks.
  To address this challenge, we propose BardeenAgent, a novel framework that
enables web agents to convert their execution into repeatable programs, and
replay them at scale across pages with similar structure. BardeenAgent is also
the first LLM agent to take advantage of the regular structure of HTML. In
particular BardeenAgent constructs a generalizable CSS selector to capture all
relevant items on the page, then fits the operations to extract the data.
  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more
than doubling the performance of SOTA web agents, and reducing cost per output
row by 3x.

</details>


### [10] [InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning](https://arxiv.org/abs/2504.13032)
*Zheng Wang,Shu Xian Teo,Jun Jie Chew,Wei Shi*

Main category: cs.AI

TL;DR: 本文提出InstructRAG方法，使用RAG和多代理元强化学习提升LLM任务规划性能，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有TAO方法受LLM知识限制，RAG虽有潜力，但面临可扩展性和可转移性挑战。

Method: 提出InstructRAG框架，包括图结构、RL-Agent提升覆盖和ML-Agent提升泛化，端到端训练。

Result: 实验在四个数据集上显示性能提升最高19.2%，优于现有方法。

Conclusion: InstructRAG显著改善任务规划性能，并高效适应新任务。

Abstract: Recent advancements in large language models (LLMs) have enabled their use as
agents for planning complex tasks. Existing methods typically rely on a
thought-action-observation (TAO) process to enhance LLM performance, but these
approaches are often constrained by the LLMs' limited knowledge of complex
tasks. Retrieval-augmented generation (RAG) offers new opportunities by
leveraging external databases to ground generation in retrieved information. In
this paper, we identify two key challenges (enlargability and transferability)
in applying RAG to task planning. We propose InstructRAG, a novel solution
within a multi-agent meta-reinforcement learning framework, to address these
challenges. InstructRAG includes a graph to organize past instruction paths
(sequences of correct actions), an RL-Agent with Reinforcement Learning to
expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to
improve task generalization for transferability. The two agents are trained
end-to-end to optimize overall planning performance. Our experiments on four
widely used task planning datasets demonstrate that InstructRAG significantly
enhances performance and adapts efficiently to new tasks, achieving up to a
19.2% improvement over the best existing approach.

</details>


### [11] [Exploring Expert Failures Improves LLM Agent Tuning](https://arxiv.org/abs/2504.13145)
*Li-Cheng Lan,Andrew Bai,Minhao Cheng,Ruochen Wang,Cho-Jui Hsieh,Tianyi Zhou*

Main category: cs.AI

TL;DR: 这篇论文提出EEF方法，通过利用专家失败轨迹中的有益行动改进LLM代理训练，在WebShop和SciWorld基准上取得新纪录。


<details>
  <summary>Details</summary>
Motivation: 现有RFT方法偏好简单场景，复杂子任务未解决；作者发现失败专家轨迹可提供宝贵指导。

Method: 提出EEF，识别并整合失败专家轨迹中的有益行动，同时排除有害行动。

Result: EEF在WebShop中获62%胜率，超过RFT（53.6%）和GPT-4（35.6%），并在WebShop和SciWorld中设置新状态。

Conclusion: EEF成功解决之前无法解决的子任务，提升了代理性能。

Abstract: Large Language Models (LLMs) have shown tremendous potential as agents,
excelling at tasks that require multiple rounds of reasoning and interactions.
Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for
finetuning LLMs as agents: it first imitates expert-generated successful
trajectories and further improves agentic skills through iterative fine-tuning
on successful, self-generated trajectories. However, since the expert (e.g.,
GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler
scenarios, many complex subtasks remain unsolved and persistently
out-of-distribution (OOD). Upon investigating these challenging subtasks, we
discovered that previously failed expert trajectories can often provide
valuable guidance, e.g., plans and key actions, that can significantly improve
agent exploration efficiency and acquisition of critical skills. Motivated by
these observations, we propose Exploring Expert Failures (EEF), which
identifies beneficial actions from failed expert trajectories and integrates
them into the training dataset. Potentially harmful actions are meticulously
excluded to prevent contamination of the model learning process. By leveraging
the beneficial actions in expert failures, EEF successfully solves some
previously unsolvable subtasks and improves agent tuning performance.
Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT
(53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new
state-of-the-art as the first method to surpass a score of 0.81 in WebShop and
exceed 81 in SciWorld.

</details>


### [12] [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
*Yash Savani,Asher Trockman,Zhili Feng,Avi Schwarzschild,Alexander Robey,Marc Finzi,J. Zico Kolter*

Main category: cs.AI

TL;DR: 这篇论文介绍了反蒸馏采样方法，通过修改模型的下一个标记概率分布来毒害推理痕迹，减少模型蒸馏的有效性，同时保持模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决前沿模型生成推理痕迹可能被用于未经授权模型蒸馏的漏洞。

Method: 方法是通过战略性地修改模型的下一个标记概率分布来毒害推理痕迹。

Result: 结果是推理痕迹对模型蒸馏的效力显著降低，而模型的实际性能得以保留。

Conclusion: 结论是反蒸馏采样提供了一种在不影响模型性能的情况下限制模型蒸馏有效性的能力。

Abstract: Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
\emph{Antidistillation sampling} provides exactly this capability. By
strategically modifying a model's next-token probability distribution,
antidistillation sampling poisons reasoning traces, rendering them
significantly less effective for distillation while preserving the model's
practical utility. For further details, see https://antidistillation.com.

</details>


### [13] [Readable Twins of Unreadable Models](https://arxiv.org/abs/2504.13150)
*Krzysztof Pancerz,Piotr Kulicki,Michał Kalisz,Andrzej Burda,Maciej Stanisławski,Jaromir Sarzyński*

Main category: cs.AI

TL;DR: 本文提出通过创建不精确信息流模型来使深度学习模型可解释的方法，并以MNIST手写数字识别为例。


<details>
  <summary>Details</summary>
Motivation: 创建负责任的AI系统需要可解释性，特别是针对深度学习模型的解释性需求。

Method: 引入可读双生模型（不精确信息流模型），并呈现从深度学习模型到该模型的完整转换过程。

Result: 以MNIST数据集的图像分类模型为例进行了演示。

Conclusion: 这种方法提升了深度学习模型的可解释性，支持负责任AI的发展。

Abstract: Creating responsible artificial intelligence (AI) systems is an important
issue in contemporary research and development of works on AI. One of the
characteristics of responsible AI systems is their explainability. In the
paper, we are interested in explainable deep learning (XDL) systems. On the
basis of the creation of digital twins of physical objects, we introduce the
idea of creating readable twins (in the form of imprecise information flow
models) for unreadable deep learning models. The complete procedure for
switching from the deep learning model (DLM) to the imprecise information flow
model (IIFM) is presented. The proposed approach is illustrated with an example
of a deep learning classification model for image recognition of handwritten
digits from the MNIST data set.

</details>


### [14] [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
*Kevin Lin,Charlie Snell,Yu Wang,Charles Packer,Sarah Wooders,Ion Stoica,Joseph E. Gonzalez*

Main category: cs.AI

TL;DR: 提出睡眠时间计算方法，减少LLM测试时计算需求，提高效率。


<details>
  <summary>Details</summary>
Motivation: 应对LLM测试时间计算扩展的延迟和成本问题。

Method: 通过离线预计算潜在查询，修改推理任务如Stateful GSM-Symbolic和Stateful AIME，使用多查询分摊计算。

Result: 测试时间计算减少5倍，准确率提升至多13%和18%，多查询场景下成本减少2.5倍，与查询可预测性相关。

Conclusion: 睡眠时间计算对可预测查询有效，可用于实际任务。

Abstract: Scaling test-time compute has emerged as a key ingredient for enabling large
language models (LLMs) to solve difficult problems, but comes with high latency
and inference cost. We introduce sleep-time compute, which allows models to
"think" offline about contexts before queries are presented: by anticipating
what queries users might ask and pre-computing useful quantities, we can
significantly reduce the compute requirements at test-time. To demonstrate the
efficacy of our method, we create modified versions of two reasoning tasks -
Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can
reduce the amount of test-time compute needed to achieve the same accuracy by ~
5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time
compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic
and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,
which extends GSM-Symbolic by including multiple related queries per context.
By amortizing sleep-time compute across related queries about the same context
using Multi-Query GSM-Symbolic, we can decrease the average cost per query by
2.5x. We then conduct additional analysis to understand when sleep-time compute
is most effective, finding the predictability of the user query to be well
correlated with the efficacy of sleep-time compute. Finally, we conduct a
case-study of applying sleep-time compute to a realistic agentic SWE task.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2504.12359)
*Yuanbo Tang,Yan Tang,Naifan Zhang,Meixuan Chen,Yang Li*

Main category: cs.LG

TL;DR: 这篇论文研究MoE LLMs中专家协作模式，提出HSDL方法识别模式和CAEP算法修剪专家，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: MoE LLMs在多任务适应性上表现出色，但专家协作机制不清晰，限制了可解释性和优化。

Method: 提出分层稀疏字典学习(HSDL)揭示专家协作模式，以及贡献感知专家修剪(CAEP)算法修剪低贡献专家。

Result: 实验显示协作模式与输入类型相关，具有语义意义；修剪方法平均提升2.5%性能，优于现有方法。

Conclusion: 为提升MoE LLMs效率和可解释性提供洞见，改善专家互动理解和模型优化。

Abstract: Mixture-of-Experts based large language models (MoE LLMs) have shown
significant promise in multitask adaptability by dynamically routing inputs to
specialized experts. Despite their success, the collaborative mechanisms among
experts are still not well understood, limiting both the interpretability and
optimization of these models. In this paper, we focus on two critical issues:
(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs
through expert pruning. To address the first issue, we propose a hierarchical
sparse dictionary learning (HSDL) method that uncovers the collaboration
patterns among experts. For the second issue, we introduce the
Contribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes
low-contribution experts. Our extensive experiments demonstrate that expert
collaboration patterns are closely linked to specific input types and exhibit
semantic significance across various tasks. Moreover, pruning experiments show
that our approach improves overall performance by 2.5\% on average,
outperforming existing methods. These findings offer valuable insights into
enhancing the efficiency and interpretability of MoE LLMs, offering a clearer
understanding of expert interactions and improving model optimization.

</details>


### [16] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/abs/2504.12397)
*Kristjan Greenewald,Luis Lastras,Thomas Parnell,Vraj Shah,Lucian Popa,Giulio Zizzo,Chulaka Gunasekara,Ambrish Rawat,David Cox*

Main category: cs.LG

TL;DR: aLoRA 通过仅在激活后适应权重，避免KV缓存重新计算，提高LoRA在多轮对话中的切换效率。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在多轮设置中切换时需重新计算KV缓存导致的低效率问题。

Method: 修改LoRA框架，仅在aLoRA被调用后适应权重，使用基础模型的KV缓存。

Result: aLoRA在准确性上与标准LoRA相当，同时显著提高了推理效率。

Conclusion: aLoRA 启用构建专业化模型（intrinsics），提供高效的模型切换和性能提升。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is highly inefficient, as the key-value (KV) cache of the
entire turn history must be recomputed with the LoRA weights before generation
can begin. To address this problem, we propose Activated LoRA (aLoRA), which
modifies the LoRA framework to only adapt weights for the tokens in the
sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA
to accept the base model's KV cache of the input string, meaning that aLoRA can
be instantly activated whenever needed in a chain without recomputing the
cache. This enables building what we call \emph{intrinsics}, i.e. highly
specialized models invoked to perform well-defined operations on portions of an
input chain or conversation that otherwise uses the base model by default. We
use aLoRA to train a set of intrinsics models, demonstrating competitive
accuracy with standard LoRA while achieving significant inference benefits.

</details>


### [17] [Standardization of Multi-Objective QUBOs](https://arxiv.org/abs/2504.12419)
*Loong Kuan Lee,Thore Thassilo Gerlach,Nico Piatkowski*

Main category: cs.LG

TL;DR: 这篇论文提出了一种通过计算QUBO目标方差的缩放方法，以平衡多目标优化，并通过实证验证其优势。


<details>
  <summary>Details</summary>
Motivation: 多目标优化中QUBO问题面临目标规模不一和权重选择困难，需要有效平衡多个目标。

Method: 提出一种精确计算每个QUBO目标方差的缩放技术，使目标具有单位方差，便于等权重标量化。

Result: 通过各种多目标优化问题的实证评估，展示了方法的优势，减少了手动权重选择的复杂性。

Conclusion: 该方法提供可靠、高效的解决方案，改善了多目标QUBO优化的性能。

Abstract: Multi-objective optimization involving Quadratic Unconstrained Binary
Optimization (QUBO) problems arises in various domains. A fundamental challenge
in this context is the effective balancing of multiple objectives, each
potentially operating on very different scales. This imbalance introduces
complications such as the selection of appropriate weights when scalarizing
multiple objectives into a single objective function. In this paper, we propose
a novel technique for scaling QUBO objectives that uses an exact computation of
the variance of each individual QUBO objective. By scaling each objective to
have unit variance, we align all objectives onto a common scale, thereby
allowing for more balanced solutions to be found when scalarizing the
objectives with equal weights, as well as potentially assisting in the search
or choice of weights during scalarization. Finally, we demonstrate its
advantages through empirical evaluations on various multi-objective
optimization problems. Our results are noteworthy since manually selecting
scalarization weights is cumbersome, and reliable, efficient solutions are
scarce.

</details>


### [18] [Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks](https://arxiv.org/abs/2504.12446)
*Sebastian Seidel,Uwe M. Borghoff*

Main category: cs.LG

TL;DR: 本论文提出从前馈神经网络中提取决策树的方法，以提高AI的可解释性和信任。


<details>
  <summary>Details</summary>
Motivation: AI系统的不透明性导致信任和接受挑战，因此探索连接主义和符号主义方法的交集。

Method: 通过步步为营的方法，利用分布式表示识别符号组件，追踪神经元激活值和输入配置，映射到决策树结构。

Result: 开发原型，使用Keras和TensorFlow，验证了从神经网络提取符号表示的可行性。

Conclusion: 提取符号模型增强AI信任，促进责任和可解释性。

Abstract: Artificial intelligence (AI) has emerged as a transformative force across
industries, driven by advances in deep learning and natural language
processing, and fueled by large-scale data and computing resources. Despite its
rapid adoption, the opacity of AI systems poses significant challenges to trust
and acceptance.
  This work explores the intersection of connectionist and symbolic approaches
to artificial intelligence, focusing on the derivation of interpretable
symbolic models, such as decision trees, from feedforward neural networks
(FNNs). Decision trees provide a transparent framework for elucidating the
operations of neural networks while preserving their functionality. The
derivation is presented in a step-by-step approach and illustrated with several
examples. A systematic methodology is proposed to bridge neural and symbolic
paradigms by exploiting distributed representations in FNNs to identify
symbolic components, including fillers, roles, and their interrelationships.
The process traces neuron activation values and input configurations across
network layers, mapping activations and their underlying inputs to decision
tree edges. The resulting symbolic structures effectively capture FNN decision
processes and enable scalability to deeper networks through iterative
refinement of subpaths for each hidden layer.
  To validate the theoretical framework, a prototype was developed using Keras
.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This
prototype demonstrates the feasibility of extracting symbolic representations
from neural networks, enhancing trust in AI systems, and promoting
accountability.

</details>


### [19] [Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation](https://arxiv.org/abs/2504.12450)
*Ziqi Li,Zhan Peng*

Main category: cs.LG

TL;DR: 本篇论文测试了在机器学习模型中使用Moran特征向量处理空间效应的有效性，发现仅使用位置坐标的模型在正空间自相关情况下准确性更高。


<details>
  <summary>Details</summary>
Motivation: 探讨Moran特征向量空间过滤方法是否可扩展到机器学习模型中，以处理空间效应。

Method: 生成合成数据集模拟空间效应，使用不同空间权重矩阵计算Moran特征向量，有无先验选择，测试Random Forests、LightGBM、XGBoost和TabNet模型，并与仅用坐标模型比较交叉验证R2值，使用GeoShapley提取系数。

Result: 结果显示，仅使用位置坐标的机器学习模型在各种实验和数据集中的准确性优于基于特征向量的方法。

Conclusion: 这些发现适用于正空间自相关过程，但不一定适用于负空间自相关或网络自相关，在这些情况下Moran特征向量可能仍有价值。

Abstract: Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in
accounting for spatial effects in statistical models. Can this extend to
machine learning? This paper examines the effectiveness of using Moran
Eigenvectors as additional spatial features in machine learning models. We
generate synthetic datasets with known processes involving spatially varying
and nonlinear effects across two different geometries. Moran Eigenvectors
calculated from different spatial weights matrices, with and without a priori
eigenvector selection, are tested. We assess the performance of popular machine
learning models, including Random Forests, LightGBM, XGBoost, and TabNet, and
benchmark their accuracies in terms of cross-validated R2 values against models
that use only coordinates as features. We also extract coefficients and
functions from the models using GeoShapley and compare them with the true
processes. Results show that machine learning models using only location
coordinates achieve better accuracies than eigenvector-based approaches across
various experiments and datasets. Furthermore, we discuss that while these
findings are relevant for spatial processes that exhibit positive spatial
autocorrelation, they do not necessarily apply when modeling network
autocorrelation and cases with negative spatial autocorrelation, where Moran
Eigenvectors would still be useful.

</details>


### [20] [M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness](https://arxiv.org/abs/2504.12458)
*Jansen S. B. Pereira,Giovani Valdrighi,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合min-max公平性约束的梯度提升算法，以确保机器学习模型对边缘化群体的预测公平。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性是关键问题，避免基于性别、种族等保护属性的歧视；本文应用子群正义概念到梯度提升机器中。

Method: 扩展梯度提升框架，结合传统损失和min-max公平性项，使用原始-对偶优化在每个提升轮次解决问题。

Result: 算法在温和条件下理论收敛，并实证显示在二元和子群公平性方面的强大灵活性。

Conclusion: 该框架通用，可适应多种公平性概念，是一个有效的方法。

Abstract: In recent years, fairness in machine learning has emerged as a critical
concern to ensure that developed and deployed predictive models do not have
disadvantageous predictions for marginalized groups. It is essential to
mitigate discrimination against individuals based on protected attributes such
as gender and race. In this work, we consider applying subgroup justice
concepts to gradient-boosting machines designed for supervised learning
problems. Our approach expanded gradient-boosting methodologies to explore a
broader range of objective functions, which combines conventional losses such
as the ones from classification and regression and a min-max fairness term. We
study relevant theoretical properties of the solution of the min-max
optimization problem. The optimization process explored the primal-dual
problems at each boosting round. This generic framework can be adapted to
diverse fairness concepts. The proposed min-max primal-dual gradient boosting
algorithm was theoretically shown to converge under mild conditions and
empirically shown to be a powerful and flexible approach to address binary and
subgroup fairness.

</details>


### [21] [Dense Backpropagation Improves Training for Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.12463)
*Ashwinee Panda,Vatsal Baherwani,Zain Sarwar,Benjamin Therien,Supriyo Chakraborty,Tom Goldstein*

Main category: cs.LG

TL;DR: 本文提出Default MoE方法，通过密集梯度更新改善Mixture of Experts (MoE)模型的训练稳定性，同时保持稀疏激活。


<details>
  <summary>Details</summary>
Motivation: MoE预训练比密集Transformer更可扩展，但稀疏反向更新导致训练不稳定和性能不佳。

Method: 使用Default MoE，通过指数移动平均的专家输出作为默认值，替换缺失激活，让路由器为每个token接收所有专家信号。

Result: Default MoE在多种设置中优于标准TopK路由，且不增加显著计算开销。

Conclusion: 该方法显著提升训练性能，并提供开源代码实现。

Abstract: Mixture of Experts (MoE) pretraining is more scalable than dense Transformer
pretraining, because MoEs learn to route inputs to a sparse set of their
feedforward parameters. However, this means that MoEs only receive a sparse
backward update, leading to training instability and suboptimal performance. We
present a lightweight approximation method that gives the MoE router a dense
gradient update while continuing to sparsely activate its parameters. Our
method, which we refer to as Default MoE, substitutes missing expert
activations with default outputs consisting of an exponential moving average of
expert outputs previously seen over the course of training. This allows the
router to receive signals from every expert for each token, leading to
significant improvements in training performance. Our Default MoE outperforms
standard TopK routing in a variety of settings without requiring significant
computational overhead. Code: https://github.com/vatsal0/default-moe.

</details>


### [22] [Geometric Generality of Transformer-Based Gröbner Basis Computation](https://arxiv.org/abs/2504.12465)
*Yuta Kambe,Yota Maeda,Tristan Vaccon*

Main category: cs.LG

TL;DR: 这篇论文证明了之前数据集生成算法的泛化性，并提出改进算法，以提升Transformer在Gröbner基计算中的训练效果。


<details>
  <summary>Details</summary>
Motivation: 动机是解决机器学习模型在数学问题求解中对高质量数据集的依赖，特别是针对Gröbner基计算的数据集缺乏理论保证。

Method: 方法包括证明之前算法生成的数据集具有足够的泛化性，并提出一个扩展的算法来系统构建理想生成元的数据集。

Result: 结果是提供了严格的几何基础，确保Transformer能够学习到多样化的Gröbner基，从而响应Lample和Charton的训练理念。

Conclusion: 结论是这为Transformer处理数学问题提供了坚实的理论基础，推进了深度学习与符号数学的交叉。

Abstract: The intersection of deep learning and symbolic mathematics has seen rapid
progress in recent years, exemplified by the work of Lample and Charton. They
demonstrated that effective training of machine learning models for solving
mathematical problems critically depends on high-quality, domain-specific
datasets. In this paper, we address the computation of Gr\"obner basis using
Transformers. While a dataset generation method tailored to Transformer-based
Gr\"obner basis computation has previously been proposed, it lacked theoretical
guarantees regarding the generality or quality of the generated datasets. In
this work, we prove that datasets generated by the previously proposed
algorithm are sufficiently general, enabling one to ensure that Transformers
can learn a sufficiently diverse range of Gr\"obner bases. Moreover, we propose
an extended and generalized algorithm to systematically construct datasets of
ideal generators, further enhancing the training effectiveness of Transformer.
Our results provide a rigorous geometric foundation for Transformers to address
a mathematical problem, which is an answer to Lample and Charton's idea of
training on diverse or representative inputs.

</details>


### [23] [You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models](https://arxiv.org/abs/2504.12471)
*Shiwei Ding,Lan Zhang,Zhenlin Wang,Giuseppe Ateniese,Xiaoyong Yuan*

Main category: cs.LG

TL;DR: D2FT框架通过选择性使用注意力模块减少大模型微调的计算和通信成本，同时保持较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大模型微调在内存受限设备上的挑战，包括高计算成本和工作负载不平衡。

Method: 引入D2FT框架，使用三种选择策略和背包优化优化注意力模块的选择，并扩展到LoRA技术。

Result: 在CIFAR-10、CIFAR-100和Stanford Cars数据集上，减少40%的计算成本和50%的通信成本，准确率仅下降1%到2%；D2FT LoRA在Stanford Cars上准确率下降4%到6%。

Conclusion: D2FT是一种高效的微调框架，能够显著降低成本，同时保持可接受的性能。

Abstract: Fine-tuning plays a crucial role in adapting models to downstream tasks with
minimal training efforts. However, the rapidly increasing size of foundation
models poses a daunting challenge for accommodating foundation model
fine-tuning in most commercial devices, which often have limited memory
bandwidth. Techniques like model sharding and tensor parallelism address this
issue by distributing computation across multiple devices to meet memory
requirements. Nevertheless, these methods do not fully leverage their
foundation nature in facilitating the fine-tuning process, resulting in high
computational costs and imbalanced workloads. We introduce a novel Distributed
Dynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations
across attention modules based on our observation that not all attention
modules are necessary for forward and backward propagation in fine-tuning
foundation models. Through three innovative selection strategies, D2FT
significantly reduces the computational workload required for fine-tuning
foundation models. Furthermore, D2FT addresses workload imbalances in
distributed computing environments by optimizing these selection strategies via
multiple knapsack optimization. Our experimental results demonstrate that the
proposed D2FT framework reduces the training computational costs by 40% and
training communication costs by 50% with only 1% to 2% accuracy drops on the
CIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show
that D2FT can be effectively extended to recent LoRA, a state-of-the-art
parameter-efficient fine-tuning technique. By reducing 40% computational cost
or 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on
Stanford Cars dataset.

</details>


### [24] [Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2504.12501)
*Nathan Lambert*

Main category: cs.LG

TL;DR: 这本书温和地介绍了强化学习从人类反馈（RLHF）的核心方法，涵盖起源、定义、问题制定以及优化阶段。


<details>
  <summary>Details</summary>
Motivation: 为具有量化背景的人提供RLHF的温和介绍，源于经济学、哲学和最优控制等领域的汇合。

Method: 详细描述RLHF的优化阶段，包括指令调整、奖励模型训练、拒绝采样、强化学习和直接对齐算法。

Result: 提供了RLHF的全面学习框架和高级话题讨论。

Conclusion: 讨论了合成数据、评估中的未研究问题以及领域的开放问题。

Abstract: Reinforcement learning from human feedback (RLHF) has become an important
technical and storytelling tool to deploy the latest machine learning systems.
In this book, we hope to give a gentle introduction to the core methods for
people with some level of quantitative background. The book starts with the
origins of RLHF -- both in recent literature and in a convergence of disparate
fields of science in economics, philosophy, and optimal control. We then set
the stage with definitions, problem formulation, data collection, and other
common math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction tuning to
training a reward model and finally all of rejection sampling, reinforcement
learning, and direct alignment algorithms. The book concludes with advanced
topics -- understudied research questions in synthetic data and evaluation --
and open questions for the field.

</details>


### [25] [Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study](https://arxiv.org/abs/2504.12503)
*Kaira M. Samuel,Faez Ahmed*

Main category: cs.LG

TL;DR: 本论文引入持续学习（CL）到工程设计中，通过基准测试CL方法在回归任务上的表现，展示了其在减少遗忘和提高泛化方面的潜力。Replay策略在性能上接近重训练，但训练时间减少近一半。


<details>
  <summary>Details</summary>
Motivation: 工程问题中机器学习涉及高计算成本和有限数据集，随着数据演变，重新训练模型不可行。CL可缓解灾难性遗忘。

Method: 基准测试多种CL方法于五个工程数据集，构建九个新工程CL基准，评估遗忘和泛化能力。

Result: 初步结果显示CL方法优于朴素基线；Replay策略在多个基准上性能与重训练相当，同时减少近一半训练时间。

Conclusion: 证明CL在工程工作流中的潜力，并提供代码和数据集以供进一步研究。

Abstract: Engineering problems that apply machine learning often involve
computationally intensive methods but rely on limited datasets. As engineering
data evolves with new designs and constraints, models must incorporate new
knowledge over time. However, high computational costs make retraining models
from scratch infeasible. Continual learning (CL) offers a promising solution by
enabling models to learn from sequential data while mitigating catastrophic
forgetting, where a model forgets previously learned mappings. This work
introduces CL to engineering design by benchmarking several CL methods on
representative regression tasks. We apply these strategies to five engineering
datasets and construct nine new engineering CL benchmarks to evaluate their
ability to address forgetting and improve generalization. Preliminary results
show that applying existing CL methods to these tasks improves performance over
naive baselines. In particular, the Replay strategy achieved performance
comparable to retraining in several benchmarks while reducing training time by
nearly half, demonstrating its potential for real-world engineering workflows.
The code and datasets used in this work will be available at:
https://github.com/kmsamuel/cl-for-engineering-release.

</details>


### [26] [MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models](https://arxiv.org/abs/2504.12526)
*Junyang Zhang,Tianyi Zhu,Cheng Luo,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出MOM方法，通过分区关键层和KV缓存卸载，显著减少GPU内存使用，扩展上下文长度至455k标记，而不影响准确性和输出一致性。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型性能出色，但推理过程中高GPU内存需求导致部署困难。

Method: 提出内存高效卸载小序列推理方法（MOM），将关键层分区为更小的'小序列'，并与KV缓存卸载无缝集成。

Result: 实验显示，MOM平均减少峰值内存使用超过50%，将Meta-Llama-3.2-8B的最大上下文长度从155k扩展到455k标记，保持输出一致和准确性，与传统方法相比上下文长度扩展提高35%，大幅减少预填充内存消耗。

Conclusion: 这一突破性方法改变了研究重点，将未来优化从预填充阶段转向改进解码阶段的残差KV缓存效率。

Abstract: Long-context language models exhibit impressive performance but remain
challenging to deploy due to high GPU memory demands during inference. We
propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that
partitions critical layers into smaller "mini-sequences" and integrates
seamlessly with KV cache offloading. Experiments on various Llama, Qwen, and
Mistral models demonstrate that MOM reduces peak memory usage by over 50\% on
average. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k
to 455k tokens on a single A100 80GB GPU, while keeping outputs identical and
not compromising accuracy. MOM also maintains highly competitive throughput due
to minimal computational overhead and efficient last-layer processing. Compared
to traditional chunked prefill methods, MOM achieves a 35\% greater context
length extension. More importantly, our method drastically reduces prefill
memory consumption, eliminating it as the longstanding dominant memory
bottleneck during inference. This breakthrough fundamentally changes research
priorities, redirecting future efforts from prefill-stage optimizations to
improving decode-stage residual KV cache efficiency.

</details>


### [27] [Generalization through variance: how noise shapes inductive biases in diffusion models](https://arxiv.org/abs/2504.12532)
*John J. Vastola*

Main category: cs.LG

TL;DR: 扩散模型如何泛化以及通过方差实现泛化的理论解释。


<details>
  <summary>Details</summary>
Motivation: 解释扩散模型为什么能泛化，尽管训练目标是训练分布的分数函数，且模型足够 expressive。

Method: 开发数学理论，使用物理启发的路径积分方法，分析欠参数化和过参数化模型。

Result: 发现扩散模型学到的分布类似于训练分布但填补了空白，这是由于训练中噪声目标的协方差结构。

Conclusion: 表征了这种归纳偏差如何与特征相关的归纳偏差互动。

Abstract: How diffusion models generalize beyond their training set is not known, and
is somewhat mysterious given two facts: the optimum of the denoising score
matching (DSM) objective usually used to train diffusion models is the score
function of the training distribution; and the networks usually used to learn
the score function are expressive enough to learn this score to high accuracy.
We claim that a certain feature of the DSM objective -- the fact that its
target is not the training distribution's score, but a noisy quantity only
equal to it in expectation -- strongly impacts whether and to what extent
diffusion models generalize. In this paper, we develop a mathematical theory
that partly explains this 'generalization through variance' phenomenon. Our
theoretical analysis exploits a physics-inspired path integral approach to
compute the distributions typically learned by a few paradigmatic under- and
overparameterized diffusion models. We find that the distributions diffusion
models effectively learn to sample from resemble their training distributions,
but with 'gaps' filled in, and that this inductive bias is due to the
covariance structure of the noisy target used during training. We also
characterize how this inductive bias interacts with feature-related inductive
biases.

</details>


### [28] [TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback](https://arxiv.org/abs/2504.12557)
*Siow Meng Low,Akshat Kumar*

Main category: cs.LG

TL;DR: 这篇论文提出了一种从稀疏标记数据中学习安全约束的安全强化学习方法，并通过实验证明了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 安全约束未知或难以指定，需要从稀疏标记的数据中学习安全定义。

Method: 设计安全模型进行信用分配，估计每个决策步骤的安全影响；展示了模型架构；改进了安全RL问题并导出了优化算法。

Result: 实证结果显示，该方法在未知安全定义和各种连续控制任务中有效且可扩展。

Conclusion: 该方法有效地满足了未知的安全约束。

Abstract: In safe reinforcement learning (RL), auxiliary safety costs are used to align
the agent to safe decision making. In practice, safety constraints, including
cost functions and budgets, are unknown or hard to specify, as it requires
anticipation of all possible unsafe behaviors. We therefore address a general
setting where the true safety definition is unknown, and has to be learned from
sparsely labeled data. Our key contributions are: first, we design a safety
model that performs credit assignment to estimate each decision step's impact
on the overall safety using a dataset of diverse trajectories and their
corresponding binary safety labels (i.e., whether the corresponding trajectory
is safe/unsafe). Second, we illustrate the architecture of our safety model to
demonstrate its ability to learn a separate safety score for each timestep.
Third, we reformulate the safe RL problem using the proposed safety model and
derive an effective algorithm to optimize a safe yet rewarding policy. Finally,
our empirical results corroborate our findings and show that this approach is
effective in satisfying unknown safety definition, and scalable to various
continuous control tasks.

</details>


### [29] [Fine Flood Forecasts: Incorporating local data into global models through fine-tuning](https://arxiv.org/abs/2504.12559)
*Emil Ryd,Grey Nearing*

Main category: cs.LG

TL;DR: 这篇论文提出通过在全球数据集上预训练模型，然后在本地流域数据上微调的方法，以提高洪水预测准确性并便于本地部署。


<details>
  <summary>Details</summary>
Motivation: 洪水是常见自然灾害，机器学习模型需全球数据训练，但可能导致本地预测者失去所有权；传统水文学强调本地数据价值。

Method: 预训练模型于大型全球数据集，然后在单个流域数据上微调。

Result: 性能提升，尤其在全球训练中表现较差的流域，验证本地数据提供额外信息。

Conclusion: 提供路线图，让国家预测者使用本地数据适应全球模型，促进ML-based水文预测系统的操作部署。

Abstract: Floods are the most common form of natural disaster and accurate flood
forecasting is essential for early warning systems. Previous work has shown
that machine learning (ML) models are a promising way to improve flood
predictions when trained on large, geographically-diverse datasets. This
requirement of global training can result in a loss of ownership for national
forecasters who cannot easily adapt the models to improve performance in their
region, preventing ML models from being operationally deployed. Furthermore,
traditional hydrology research with physics-based models suggests that local
data -- which in many cases is only accessible to local agencies -- is valuable
for improving model performance. To address these concerns, we demonstrate a
methodology of pre-training a model on a large, global dataset and then
fine-tuning that model on data from individual basins. This results in
performance increases, validating our hypothesis that there is extra
information to be captured in local data. In particular, we show that
performance increases are most significant in watersheds that underperform
during global training. We provide a roadmap for national forecasters who wish
to take ownership of global models using their own data, aiming to lower the
barrier to operational deployment of ML-based hydrological forecast systems.

</details>


### [30] [Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks](https://arxiv.org/abs/2504.12561)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 本论文提出Kernel Ridge Regression (KRR)作为Kernel Logistic Regression (KLR)的更快替代方案，用于关联记忆，实现了相似的存储容量和噪声鲁棒性，但显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: Hebbian学习限制Hopfield网络容量，而KLR通过迭代学习提升性能，但存在训练速度问题，因此需要更高效的方法。

Method: 提出KRR，通过闭式解非迭代学习双变量。

Result: KRR达到存储容量比率1.5和80%噪声鲁棒性，与KLR相当，但大幅降低训练时间。

Conclusion: 确立KRR作为高效构建高性能关联记忆的方法。

Abstract: Hebbian learning limits Hopfield network capacity. While kernel methods like
Kernel Logistic Regression (KLR) improve performance via iterative learning, we
propose Kernel Ridge Regression (KRR) as an alternative. KRR learns dual
variables non-iteratively via a closed-form solution, offering significant
learning speed advantages. We show KRR achieves comparably high storage
capacity (reaching ratio 1.5 shown) and noise robustness (recalling from around
80% corrupted patterns) as KLR, while drastically reducing training time,
establishing KRR as an efficient method for building high-performance
associative memories.

</details>


### [31] [Evolutionary Policy Optimization](https://arxiv.org/abs/2504.12568)
*Zelal Su "Lain" Mustafaoglu,Keshav Pingali,Risto Miikkulainen*

Main category: cs.LG

TL;DR: 这篇论文提出EPO算法，将进化计算与策略梯度方法结合，改善强化学习中的探索-利用权衡，在Atari基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中探索-利用权衡的挑战，策略梯度方法擅长利用但探索不足，进化计算方法擅长探索但缺乏利用机制。

Method: 提出EPO算法，通过整合神经进化与策略梯度方法进行策略优化。

Result: 在Atari Pong和Breakout基准测试中，EPO提高了策略质量和样本效率，比标准方法表现更好。

Conclusion: EPO适用于需要探索和局部优化的任务，提供高效解决方案。

Abstract: A key challenge in reinforcement learning (RL) is managing the
exploration-exploitation trade-off without sacrificing sample efficiency.
Policy gradient (PG) methods excel in exploitation through fine-grained,
gradient-based optimization but often struggle with exploration due to their
focus on local search. In contrast, evolutionary computation (EC) methods excel
in global exploration, but lack mechanisms for exploitation. To address these
limitations, this paper proposes Evolutionary Policy Optimization (EPO), a
hybrid algorithm that integrates neuroevolution with policy gradient methods
for policy optimization. EPO leverages the exploration capabilities of EC and
the exploitation strengths of PG, offering an efficient solution to the
exploration-exploitation dilemma in RL. EPO is evaluated on the Atari Pong and
Breakout benchmarks. Experimental results show that EPO improves both policy
quality and sample efficiency compared to standard PG and EC methods, making it
effective for tasks that require both exploration and local optimization.

</details>


### [32] [The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2504.12569)
*You Rim Choi,Subeom Park,Seojun Heo,Eunchung Noh,Hyung-Sin Kim*

Main category: cs.LG

TL;DR: MagMatch 是一种新的 OSSL 框架，通过原型对比学习隔离 OOD 样本，提高分类和 OOD 检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决 OSSL 中从可能包含 ID 和 OOD 类的未标记数据学习的问题，现有方法在特征空间构建上 suboptimal。

Method: 提出 MagMatch 框架，使用原型-based 对比学习、ID-Selective Magnetic (ISM) 模块和 Selective Magnetic Alignment (SMA) 损失函数，选择性对齐 ID 样本并隔离 OOD 样本。

Result: 实验显示 MagMatch 在多种数据集上，分类准确性和 OOD 检测 AUROC 显著优于现有方法，尤其在泛化到未见 OOD 数据上。

Conclusion: MagMatch 通过有效隔离 OOD 样本和动态对齐，提升了 OSSL 的整体性能。

Abstract: Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of
learning from unlabeled data that may include both in-distribution (ID) and
unknown out-of-distribution (OOD) classes. However, existing OSSL methods form
suboptimal feature spaces by either excluding OOD samples, interfering with
them, or overtrusting their information during training. In this work, we
introduce MagMatch, a novel framework that naturally isolates OOD samples
through a prototype-based contrastive learning paradigm. Unlike conventional
methods, MagMatch does not assign any prototypes to OOD samples; instead, it
selectively aligns ID samples with class prototypes using an ID-Selective
Magnetic (ISM) module, while allowing OOD samples - the "others" - to remain
unaligned in the feature space. To support this process, we propose Selective
Magnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts
alignment based on sample confidence. Extensive experiments on diverse datasets
demonstrate that MagMatch significantly outperforms existing methods in both
closed-set classification accuracy and OOD detection AUROC, especially in
generalizing to unseen OOD data.

</details>


### [33] [Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients](https://arxiv.org/abs/2504.12577)
*Leming Wu,Yaochu Jin,Kuangrong Hao,Han Yu*

Main category: cs.LG

TL;DR: FedDua 通过基于梯度的安全加权平均方法改善联邦学习，减少数据量报告不准确的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中常用基于数据量的加权平均易受客户端虚假报告影响，导致模型偏差。

Method: 提出FedDua方法，通过分析本地模型梯度准确预测客户端数据量，并可无缝集成到各种联邦学习算法中。

Result: 在三个基准数据集上实验，FedDua 比四种流行方法平均提高全局模型性能3.17%。

Conclusion: FedDua 有效地解决了联邦学习中数据量报告不准确的问题，提升了模型性能。

Abstract: Federated learning (FL) enables collaborative training of deep learning
models without requiring data to leave local clients, thereby preserving client
privacy. The aggregation process on the server plays a critical role in the
performance of the resulting FL model. The most commonly used aggregation
method is weighted averaging based on the amount of data from each client,
which is thought to reflect each client's contribution. However, this method is
prone to model bias, as dishonest clients might report inaccurate training data
volumes to the server, which is hard to verify. To address this issue, we
propose a novel secure \underline{Fed}erated \underline{D}ata
q\underline{u}antity-\underline{a}ware weighted averaging method (FedDua). It
enables FL servers to accurately predict the amount of training data from each
client based on their local model gradients uploaded. Furthermore, it can be
seamlessly integrated into any FL algorithms that involve server-side model
aggregation. Extensive experiments on three benchmarking datasets demonstrate
that FedDua improves the global model performance by an average of 3.17%
compared to four popular FL aggregation methods in the presence of inaccurate
client data volume declarations.

</details>


### [34] [ChemKANs for Combustion Chemistry Modeling and Acceleration](https://arxiv.org/abs/2504.12580)
*Benjamin C. Koenig,Suyong Kim,Sili Deng*

Main category: cs.LG

TL;DR: 本研究开发了ChemKANs框架，使用KAN-ODE结合物理知识，提高化学动力学模型的效率和准确性，尤其在噪声数据和燃烧模拟加速方面。


<details>
  <summary>Details</summary>
Motivation: 化学动力学模型推断面临大型ODE系统、时间尺度差异、非线性和噪声数据等挑战，机器学习技术应用困难。

Method: 通过在KAN-ODE基础上添加物理知识、热力学定律和元素守恒损失函数，开发ChemKANs框架，提升训练效率和预测准确性。

Result: ChemKANs在稀疏噪声数据中无过拟合，优于DeepONet；使用344个参数准确模拟氢燃烧，提供2倍加速，并可扩展到更大问题。

Conclusion: ChemKANs在燃烧物理和化学动力学中有潜力，展示了KAN-ODE的可扩展性。

Abstract: Efficient chemical kinetic model inference and application for combustion
problems is challenging due to large ODE systems and wideley separated time
scales. Machine learning techniques have been proposed to streamline these
models, though strong nonlinearity and numerical stiffness combined with noisy
data sources makes their application challenging. The recently developed
Kolmogorov-Arnold Networks (KANs) and KAN ordinary differential equations
(KAN-ODEs) have been demonstrated as powerful tools for scientific applications
thanks to their rapid neural scaling, improved interpretability, and smooth
activation functions. Here, we develop ChemKANs by augmenting the KAN-ODE
framework with physical knowledge of the flow of information through the
relevant kinetic and thermodynamic laws, as well as an elemental conservation
loss term. This novel framework encodes strong inductive bias that enables
streamlined training and higher accuracy predictions, while facilitating
parameter sparsity through full sharing of information across all inputs and
outputs. In a model inference investigation, we find that ChemKANs exhibit no
overfitting or model degradation when tasked with extracting predictive models
from data that is both sparse and noisy, a task that a standard DeepONet
struggles to accomplish. Next, we find that a remarkably parameter-lean ChemKAN
(only 344 parameters) can accurately represent hydrogen combustion chemistry,
providing a 2x acceleration over the detailed chemistry in a solver that is
generalizable to larger-scale turbulent flow simulations. These demonstrations
indicate potential for ChemKANs in combustion physics and chemical kinetics,
and demonstrate the scalability of generic KAN-ODEs in significantly larger and
more numerically challenging problems than previously studied.

</details>


### [35] [Software Engineering Principles for Fairer Systems: Experiments with GroupCART](https://arxiv.org/abs/2504.12587)
*Kewen Peng,Hao Zhuo,Yicheng Yang,Tim Menzies*

Main category: cs.LG

TL;DR: 这篇论文提出GroupCART，一种公平性感知的决策树方法，旨在减少对保护群体的歧视，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统决策树仅优化目标属性的信息增益，可能导致对性别、种族等保护群体的不公平歧视。

Method: GroupCART通过同时优化目标属性的熵减少和保护属性的熵增加，并在模型构建中支持自定义权重，以平衡预测性能和公平性。

Result: 实验结果显示，GroupCART无需数据转换即可实现更公平的模型，且性能下降最小。

Conclusion: 研究证明，通过多任务公平性感知学习可以有效缓解决策树模型中的算法偏见，并提供了代码和数据集。

Abstract: Discrimination-aware classification aims to make accurate predictions while
satisfying fairness constraints. Traditional decision tree learners typically
optimize for information gain in the target attribute alone, which can result
in models that unfairly discriminate against protected social groups (e.g.,
gender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a
tree-based ensemble optimizer that avoids bias during model construction by
optimizing not only for decreased entropy in the target attribute but also for
increased entropy in protected attributes. Our experiments show that GroupCART
achieves fairer models without data transformation and with minimal performance
degradation. Furthermore, the method supports customizable weighting, offering
a smooth and flexible trade-off between predictive performance and fairness
based on user requirements. These results demonstrate that algorithmic bias in
decision tree models can be mitigated through multi-task, fairness-aware
learning. All code and datasets used in this study are available at:
https://github.com/anonymous12138/groupCART.

</details>


### [36] [Simplifying Graph Transformers](https://arxiv.org/abs/2504.12588)
*Liheng Ma,Soumyasundar Pal,Yingxue Zhang,Philip H. S. Torr,Mark Coates*

Main category: cs.LG

TL;DR: 本文提出对Transformer的简单修改，使其适用于图学习，并获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Graph Transformer架构复杂，难以整合Transformer的训练进展，因此需要简单有效的修改。

Method: 提出三个修改：(1) 简化的L2注意力机制，(2) 自适应的根均方归一化，(3) 带有共享编码器的相对位置编码偏差。

Result: 在多种图数据集上获得显著性能提升，并在图同构测试中表现出色。

Conclusion: 这些修改有效，证明了其在图学习中的适用性和表现力。

Abstract: Transformers have attained outstanding performance across various modalities,
employing scaled-dot-product (SDP) attention mechanisms. Researchers have
attempted to migrate Transformers to graph learning, but most advanced Graph
Transformers are designed with major architectural differences, either
integrating message-passing or incorporating sophisticated attention
mechanisms. These complexities prevent the easy adoption of Transformer
training advances. We propose three simple modifications to the plain
Transformer to render it applicable to graphs without introducing major
architectural distortions. Specifically, we advocate for the use of (1)
simplified $L_2$ attention to measure the magnitude closeness of tokens; (2)
adaptive root-mean-square normalization to preserve token magnitude
information; and (3) a relative positional encoding bias with a shared encoder.
Significant performance gains across a variety of graph datasets justify the
effectiveness of our proposed modifications. Furthermore, empirical evaluation
on the expressiveness benchmark reveals noteworthy realized expressiveness in
the graph isomorphism.

</details>


### [37] [Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer](https://arxiv.org/abs/2504.12589)
*Huaizhi Qu,Inyoung Choi,Zhen Tan,Song Wang,Sukwon Yun,Qi Long,Faizan Siddiqui,Kwonjoon Lee,Tianlong Chen*

Main category: cs.LG

TL;DR: 本论文提出BetaConform框架，用于高效精确估计LLM集合判断性能。


<details>
  <summary>Details</summary>
Motivation: LLM集合广泛用于判断，但对其准确性的高效估计方法未知。

Method: 提出Beta-二项分布混合模型、符合预测驱动的自适应停止方法、先验转移机制，并整合成BetaConform框架。

Result: 实验验证显示，使用TruthfulQA数据集的10个样本，错误幅度可小至3.37%。

Conclusion: BetaConform框架提供理论保证的分布估计，使用最小标记样本。

Abstract: LLM ensembles are widely used for LLM judges. However, how to estimate their
accuracy, especially in an efficient way, is unknown. In this paper, we present
a principled maximum a posteriori (MAP) framework for an economical and precise
estimation of the performance of LLM ensemble judgment. We first propose a
mixture of Beta-Binomial distributions to model the judgment distribution,
revising from the vanilla Binomial distribution. Next, we introduce a conformal
prediction-driven approach that enables adaptive stopping during iterative
sampling to balance accuracy with efficiency. Furthermore, we design a prior
transfer mechanism that utilizes learned distributions on open-source datasets
to improve estimation on a target dataset when only scarce annotations are
available. Finally, we present BetaConform, a framework that integrates our
distribution assumption, adaptive stopping, and the prior transfer mechanism to
deliver a theoretically guaranteed distribution estimation of LLM ensemble
judgment with minimum labeled samples. BetaConform is also validated
empirically. For instance, with only 10 samples from the TruthfulQA dataset,
for a Llama ensembled judge, BetaConform gauges its performance with error
margin as small as 3.37%.

</details>


### [38] [Meta-Dependence in Conditional Independence Testing](https://arxiv.org/abs/2504.12594)
*Bijan Mazaheri,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: 本论文研究了约束-based因果发现算法中条件独立性属性的元依赖，使用几何直觉和信息投影提供简单度量，并通过数据验证。


<details>
  <summary>Details</summary>
Motivation: 有限数据可能导致因果结构与条件独立性对应关系破坏，需要理解这些属性间的依赖。

Method: 将条件独立性属性建模为约束联合分布的流形，使用信息投影计算元依赖，并进行合成和真实数据实证。

Result: 开发了简单计算的元依赖度量，并通过实验证实其有效性。

Conclusion: 改进了因果发现算法的鲁棒性，有助于处理有限数据场景。

Abstract: Constraint-based causal discovery algorithms utilize many statistical tests
for conditional independence to uncover networks of causal dependencies. These
approaches to causal discovery rely on an assumed correspondence between the
graphical properties of a causal structure and the conditional independence
properties of observed variables, known as the causal Markov condition and
faithfulness. Finite data yields an empirical distribution that is "close" to
the actual distribution. Across these many possible empirical distributions,
the correspondence to the graphical properties can break down for different
conditional independencies, and multiple violations can occur at the same time.
We study this "meta-dependence" between conditional independence properties
using the following geometric intuition: each conditional independence property
constrains the space of possible joint distributions to a manifold. The
"meta-dependence" between conditional independences is informed by the position
of these manifolds relative to the true probability distribution. We provide a
simple-to-compute measure of this meta-dependence using information projections
and consolidate our findings empirically using both synthetic and real-world
data.

</details>


### [39] [Stochastic Gradient Descent in Non-Convex Problems: Asymptotic Convergence with Relaxed Step-Size via Stopping Time Methods](https://arxiv.org/abs/2504.12601)
*Ruinan Jin,Difei Cheng,Hong Qiao,Xin Shi,Shaodong Liu,Bo Zhang*

Main category: cs.LG

TL;DR: 本文引入基于停止时间方法的分析框架，在更宽松步长条件和弱假设下，证明SGD在非凸优化中的几乎处处收敛和L2收敛。


<details>
  <summary>Details</summary>
Motivation: 现有SGD收敛分析需Robbins-Monro条件，但实践中步长方案多样，本文旨在桥接这一差距，提供更一般的结果。

Method: 引入新型基于停止时间方法的分析框架，用于SGD渐近收敛分析。

Result: 在非凸设置下，证明步长满足∑ε_t=∞和∑ε_t^p<∞ (p>2)时的几乎处处收敛，消除全局Lipschitz连续性假设，放宽随机梯度高阶矩有界性要求，并建立L2收敛。

Conclusion: 放宽假设使理论结果更通用，提升实际应用性。

Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning
research. Previous convergence analyses of SGD under the vanishing step-size
setting typically require Robbins-Monro conditions. However, in practice, a
wider variety of step-size schemes are frequently employed, yet existing
convergence results remain limited and often rely on strong assumptions. This
paper bridges this gap by introducing a novel analytical framework based on a
stopping-time method, enabling asymptotic convergence analysis of SGD under
more relaxed step-size conditions and weaker assumptions. In the non-convex
setting, we prove the almost sure convergence of SGD iterates for step-sizes $
\{ \epsilon_t \}_{t \geq 1} $ satisfying $\sum_{t=1}^{+\infty} \epsilon_t =
+\infty$ and $\sum_{t=1}^{+\infty} \epsilon_t^p < +\infty$ for some $p > 2$.
Compared with previous studies, our analysis eliminates the global Lipschitz
continuity assumption on the loss function and relaxes the boundedness
requirements for higher-order moments of stochastic gradients. Building upon
the almost sure convergence results, we further establish $L_2$ convergence.
These significantly relaxed assumptions make our theoretical results more
general, thereby enhancing their applicability in practical scenarios.

</details>


### [40] [Machine Learning Methods for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.12610)
*Akshata Hegde,Tom Nguyen,Jianlin Cheng*

Main category: cs.LG

TL;DR: 本篇论文审阅了基于机器学习的基因调控网络（GRN）推断方法，突出了人工智能进步和未来方向。


<details>
  <summary>Details</summary>
Motivation: 动机是支持GRN推断在基因调控研究中的应用和发展新型机器学习方法，计算生物学和高速测序技术的进步提高了推断准确性。

Method: 方法包括审阅机器学习技术（如监督、非监督、半监督和对比学习），常用数据集和评估指标，并强调深度学习在提升性能中的作用。

Result: 结果展示了机器学习方法在GRN推断中的有效性，并讨论了潜在未来改进方向。

Conclusion: 结论是机器学习尤其是深度学习在GRN推断中具有重要潜力，并指出了未来研究方向。

Abstract: Gene Regulatory Networks (GRNs) are intricate biological systems that control
gene expression and regulation in response to environmental and developmental
cues. Advances in computational biology, coupled with high throughput
sequencing technologies, have significantly improved the accuracy of GRN
inference and modeling. Modern approaches increasingly leverage artificial
intelligence (AI), particularly machine learning techniques including
supervised, unsupervised, semi-supervised, and contrastive learning to analyze
large scale omics data and uncover regulatory gene interactions. To support
both the application of GRN inference in studying gene regulation and the
development of novel machine learning methods, we present a comprehensive
review of machine learning based GRN inference methodologies, along with the
datasets and evaluation metrics commonly used. Special emphasis is placed on
the emerging role of cutting edge deep learning techniques in enhancing
inference performance. The potential future directions for improving GRN
inference are also discussed.

</details>


### [41] [Uncertainty Quantification in Graph Neural Networks with Shallow Ensembles](https://arxiv.org/abs/2504.12627)
*Tirtha Vinchurkar,Kareem Abdelmaqsoud,John R. Kitchin*

Main category: cs.LG

TL;DR: 本研究使用DPOSE方法增强SchNet模型的不确定性量化能力，以更好地处理Graph Neural Networks在域外数据上的问题，并在QM9、OC20和Gold Molecular Dynamics数据集上验证。


<details>
  <summary>Details</summary>
Motivation: GNNs在遇到域外数据时预测不可靠且难以识别，因此需要引入不确定性量化技术来提高可靠性。

Method: 将Direct Propagation of Shallow Ensembles (DPOSE)整合到SchNet模型中，并通过QM9、OC20和Gold Molecular Dynamics等数据集进行评估。

Result: DPOSE成功区分域内和域外样本，对未观察到的分子和材料类别显示更高不确定性。

Conclusion: 突显轻量级不确定性量化方法可改善GNN的鲁棒性，并为未来与主动学习策略整合奠定基础。

Abstract: Machine-learned potentials (MLPs) have revolutionized materials discovery by
providing accurate and efficient predictions of molecular and material
properties. Graph Neural Networks (GNNs) have emerged as a state-of-the-art
approach due to their ability to capture complex atomic interactions. However,
GNNs often produce unreliable predictions when encountering out-of-domain data
and it is difficult to identify when that happens. To address this challenge,
we explore Uncertainty Quantification (UQ) techniques, focusing on Direct
Propagation of Shallow Ensembles (DPOSE) as a computationally efficient
alternative to deep ensembles. By integrating DPOSE into the SchNet model, we
assess its ability to provide reliable uncertainty estimates across diverse
Density Functional Theory datasets, including QM9, OC20, and Gold Molecular
Dynamics. Our findings often demonstrate that DPOSE successfully distinguishes
between in-domain and out-of-domain samples, exhibiting higher uncertainty for
unobserved molecule and material classes. This work highlights the potential of
lightweight UQ methods in improving the robustness of GNN-based materials
modeling and lays the foundation for future integration with active learning
strategies.

</details>


### [42] [Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification](https://arxiv.org/abs/2504.12644)
*Reek Majumder,Mashrur Chowdhury,Sakib Mahmud Khan,Zadid Khan,Fahim Ahmad,Frank Ngeni,Gurcan Comert,Judith Mwakalonge,Dimitra Michalaka*

Main category: cs.LG

TL;DR: 这篇论文比较了混合经典-量子深度学习模型与经典模型在对抗攻击下的鲁棒性，用于自动驾驶车辆的交通标志分类，结果显示混合模型更准确。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型在自动驾驶车辆感知模块中的错误分类可能导致严重后果，且对抗攻击可能导致输出不准确，因此需要开发更鲁棒的模型。

Method: 使用AlexNet和VGG-16作为特征提取器，通过转移学习构建混合经典-量子深度学习模型，测试超过1000个量子电路，对抗PGD、FGSA和GA攻击。

Result: 混合模型在无攻击场景下准确率超过95%；在GA和FGSA攻击下超过91%；在PGD攻击下，AlexNet-based模型准确率85%，而经典模型低于21%。

Conclusion: 混合经典-量子深度学习模型在对抗设置下为交通标志分类提供了比经典模型更高的准确率。

Abstract: Deep learning (DL)-based image classification models are essential for
autonomous vehicle (AV) perception modules since incorrect categorization might
have severe repercussions. Adversarial attacks are widely studied cyberattacks
that can lead DL models to predict inaccurate output, such as incorrectly
classified traffic signs by the perception module of an autonomous vehicle. In
this study, we create and compare hybrid classical-quantum deep learning
(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate
robustness against adversarial attacks for perception modules. Before feeding
them into the quantum system, we used transfer learning models, alexnet and
vgg-16, as feature extractors. We tested over 1000 quantum circuits in our
HCQ-DL models for projected gradient descent (PGD), fast gradient sign attack
(FGSA), and gradient attack (GA), which are three well-known untargeted
adversarial approaches. We evaluated the performance of all models during
adversarial attacks and no-attack scenarios. Our HCQ-DL models maintain
accuracy above 95\% during a no-attack scenario and above 91\% for GA and FGSA
attacks, which is higher than C-DL models. During the PGD attack, our
alexnet-based HCQ-DL model maintained an accuracy of 85\% compared to C-DL
models that achieved accuracies below 21\%. Our results highlight that the
HCQ-DL models provide improved accuracy for traffic sign classification under
adversarial settings compared to their classical counterparts.

</details>


### [43] [Feature selection based on cluster assumption in PU learning](https://arxiv.org/abs/2504.12651)
*Motonobu Uchikoshi,Youhei Akimoto*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于聚类假设的正负不平衡学习特征选择方法FSCPU。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法在正负不平衡学习场景中表现不佳，尤其当正样本聚类时，可能无法捕获正样本的统计特性，导致性能 suboptimal。

Method: 提出FSCPU，将特征选择问题表述为二元优化任务，并明确融入聚类假设。

Result: 在合成数据集上显示有效性，在三个公开数据集上与10种传统算法比较，FSCPU在下游分类任务中表现出竞争力，即使聚类假设不严格。

Conclusion: FSCPU是一种鲁棒的特征选择方法，适用于正负不平衡学习任务。

Abstract: Feature selection is essential for efficient data mining and sometimes
encounters the positive-unlabeled (PU) learning scenario, where only a few
positive labels are available, while most data remains unlabeled. In certain
real-world PU learning tasks, data subjected to adequate feature selection
often form clusters with concentrated positive labels. Conventional feature
selection methods that treat unlabeled data as negative may fail to capture the
statistical characteristics of positive data in such scenarios, leading to
suboptimal performance. To address this, we propose a novel feature selection
method based on the cluster assumption in PU learning, called FSCPU. FSCPU
formulates the feature selection problem as a binary optimization task, with an
objective function explicitly designed to incorporate the cluster assumption in
the PU learning setting. Experiments on synthetic datasets demonstrate the
effectiveness of FSCPU across various data conditions. Moreover, comparisons
with 10 conventional algorithms on three open datasets show that FSCPU achieves
competitive performance in downstream classification tasks, even when the
cluster assumption does not strictly hold.

</details>


### [44] [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)
*Menglan Chen,Xianghe Pang,Jingjing Dong,WenHao Wang,Yaxin Du,Siheng Chen*

Main category: cs.LG

TL;DR: 这篇论文提出VLMGuard-R1框架，通过多模态推理驱动的提示重写提升视觉语言模型的安全性，并在实验中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决视觉语言模型多模态复杂性带来的安全风险，通过跨模态推理防范潜在威胁。

Method: 方法包括设计三阶段推理管道合成数据集，训练重写器动态优化用户输入以提升安全性，而不改变模型核心参数。

Result: 实验结果显示，VLMGuard-R1在三个基准测试中优于四个基线，在SIUO基准上五种模型平均安全性提升43.59%。

Conclusion: 结论是，该框架有效提高了VLM的安全性，提供了一种主动的防护策略。

Abstract: Aligning Vision-Language Models (VLMs) with safety standards is essential to
mitigate risks arising from their multimodal complexity, where integrating
vision and language unveils subtle threats beyond the reach of conventional
safeguards. Inspired by the insight that reasoning across modalities is key to
preempting intricate vulnerabilities, we propose a novel direction for VLM
safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce
VLMGuard-R1, a proactive framework that refines user inputs through a
reasoning-guided rewriter, dynamically interpreting text-image interactions to
deliver refined prompts that bolster safety across diverse VLM architectures
without altering their core parameters. To achieve this, we devise a
three-stage reasoning pipeline to synthesize a dataset that trains the rewriter
to infer subtle threats, enabling tailored, actionable responses over generic
refusals. Extensive experiments across three benchmarks with five VLMs reveal
that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1
achieves a remarkable 43.59\% increase in average safety across five models on
the SIUO benchmark.

</details>


### [45] [Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy](https://arxiv.org/abs/2504.12665)
*Siwei Huang,Chenhao Yang,Chuan Hu*

Main category: cs.LG

TL;DR: 本研究提出DSPR模型，通过实验和CNN-Bi-LSTM-TPA网络预测驾驶员主观风险，准确率达87.91%，以提升自动驾驶系统安全和信任。


<details>
  <summary>Details</summary>
Motivation: 驾驶员风险感知影响自动驾驶系统的接受和信任，但现有方法难以评估这种主观风险。

Method: 提出DSPR模型，招募20名参与者进行驾驶实验，使用CNN-Bi-LSTM-TPA网络结合半监督学习预测主观风险评分。

Result: DSPR模型预测准确率最高87.91%，半监督策略提高20.12%，CNN-Bi-LSTM-TPA在LSTM结构中表现最佳。

Conclusion: 提供有效方法评估驾驶员风险感知，支持自动驾驶系统安全提升和信任改善。

Abstract: Drivers' perception of risk determines their acceptance, trust, and use of
the Automated Driving Systems (ADSs). However, perceived risk is subjective and
difficult to evaluate using existing methods. To address this issue, a driver's
subjective perceived risk (DSPR) model is proposed, regarding perceived risk as
a dynamically triggered mechanism with anisotropy and attenuation. 20
participants are recruited for a driver-in-the-loop experiment to report their
real-time subjective risk ratings (SRRs) when experiencing various automatic
driving scenarios. A convolutional neural network and bidirectional long
short-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is
embedded into a semi-supervised learning strategy to predict SRRs, aiming to
reduce data noise caused by subjective randomness of participants. The results
illustrate that DSPR achieves the highest prediction accuracy of 87.91% in
predicting SRRs, compared to three state-of-the-art risk models. The
semi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA
network presents the highest accuracy among four different LSTM structures.
This study offers an effective method for assessing driver's perceived risk,
providing support for the safety enhancement of ADS and driver's trust
improvement.

</details>


### [46] [Physics Informed Constrained Learning of Dynamics from Static Data](https://arxiv.org/abs/2504.12675)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: 本研究开发了一种新的PINN学习范式Constrained Learning和MPOCtrL优化方法，能使用部分观察数据，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 克服现有PINN对完整时间序列数据的依赖，解决数据稀缺和高维度挑战。

Method: 开发Constrained Learning框架和MPOCtrL优化方法，数学公式化，平衡物理模型和数据拟合。

Result: 实验显示MPOCtrL有效检测非线性依赖，在代谢通量分析中优于现有数据驱动方法。

Conclusion: MPOCtrL是一种有效的工具，提高了PINN在部分数据情况下的性能，代码开源。

Abstract: A physics-informed neural network (PINN) models the dynamics of a system by
integrating the governing physical laws into the architecture of a neural
network. By enforcing physical laws as constraints, PINN overcomes challenges
with data scarsity and potentially high dimensionality. Existing PINN
frameworks rely on fully observed time-course data, the acquisition of which
could be prohibitive for many systems. In this study, we developed a new PINN
learning paradigm, namely Constrained Learning, that enables the approximation
of first-order derivatives or motions using non-time course or partially
observed data. Computational principles and a general mathematical formulation
of Constrained Learning were developed. We further introduced MPOCtrL (Message
Passing Optimization-based Constrained Learning) an optimization approach
tailored for the Constrained Learning framework that strives to balance the
fitting of physical models and observed data. Its code is available at github
link: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and
real-world data demonstrated that MPOCtrL can effectively detect the nonlinear
dependency between observed data and the underlying physical properties of the
system. In particular, on the task of metabolic flux analysis, MPOCtrL
outperforms all existing data-driven flux estimators.

</details>


### [47] [Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification](https://arxiv.org/abs/2504.12712)
*Hyunji Jung,Hanseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: 本论文研究了在连续学习中，使用梯度下降训练多个线性分类任务时的收敛性，当任务联合线性可分时，模型收敛到联合最大边际解，并分析了遗忘现象。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度下降在单个任务上偏向个体最大边际解，但论文旨在探索在任务连续呈现时，模型如何收敛到联合解，以及遗忘和知识转移的机制。

Method: 通过顺序运行梯度下降，每个任务固定迭代次数，分析任务在循环或随机顺序下的表现。

Result: 结果显示：当任务可分时，收敛到联合最大边际解；循环顺序下，遗忘量趋于零；不可分时，收敛到联合损失最小值。

Conclusion: 结论是，在特定条件下，连续学习的梯度下降可以实现有效的收敛和最小化遗忘。

Abstract: We study continual learning on multiple linear classification tasks by
sequentially running gradient descent (GD) for a fixed budget of iterations per
task. When all tasks are jointly linearly separable and are presented in a
cyclic/random order, we show the directional convergence of the trained linear
classifier to the joint (offline) max-margin solution. This is surprising
because GD training on a single task is implicitly biased towards the
individual max-margin solution for the task, and the direction of the joint
max-margin solution can be largely different from these individual solutions.
Additionally, when tasks are given in a cyclic order, we present a
non-asymptotic analysis on cycle-averaged forgetting, revealing that (1)
alignment between tasks is indeed closely tied to catastrophic forgetting and
backward knowledge transfer and (2) the amount of forgetting vanishes to zero
as the cycle repeats. Lastly, we analyze the case where the tasks are no longer
jointly separable and show that the model trained in a cyclic order converges
to the unique minimum of the joint loss function.

</details>


### [48] [Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection](https://arxiv.org/abs/2504.12715)
*Long Zeng,Jianxiang Yu,Jiapeng Zhu,Qingsong Zhong,Xiang Li*

Main category: cs.LG

TL;DR: 这篇论文通过在图数据上应用VQ-VAE，解决了现有自监督学习方法的扰动问题，并提出新策略提升性能，在多个任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习依赖扰动可能破坏信息，VQ-VAE在图数据上的应用未被充分探索。

Method: 经验分析向量量化，提出退火-based编码策略和分层两层代码本，以解决代码本利用不足和空间稀疏问题。

Result: 在自监督链路预测和节点分类任务中，模型在多个数据集上优于16个基线方法。

Conclusion: 向量量化显著提升了模型捕获图拓扑的能力，并通过提出的策略有效应对挑战。

Abstract: Graph self-supervised learning has gained significant attention recently.
However, many existing approaches heavily depend on perturbations, and
inappropriate perturbations may corrupt the graph's inherent information. The
Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder
extensively used in fields such as computer vision; however, its application to
graph data remains underexplored. In this paper, we provide an empirical
analysis of vector quantization in the context of graph autoencoders,
demonstrating its significant enhancement of the model's capacity to capture
graph topology. Furthermore, we identify two key challenges associated with
vector quantization when applying in graph data: codebook underutilization and
codebook space sparsity. For the first challenge, we propose an annealing-based
encoding strategy that promotes broad code utilization in the early stages of
training, gradually shifting focus toward the most effective codes as training
progresses. For the second challenge, we introduce a hierarchical two-layer
codebook that captures relationships between embeddings through clustering. The
second layer codebook links similar codes, encouraging the model to learn
closer embeddings for nodes with similar features and structural topology in
the graph. Our proposed model outperforms 16 representative baseline methods in
self-supervised link prediction and node classification tasks across multiple
datasets.

</details>


### [49] [TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations](https://arxiv.org/abs/2504.12721)
*Yihang Lu,Yangyang Xu,Qitao Qing,Xianwei Meng*

Main category: cs.LG

TL;DR: 本论文引入TimeCapsule模型，通过高维信息压缩简化长期时间序列预测（LTSF），实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管复杂深度学习模型被强调，但简单模型如线性模型或MLP往往表现更好，本文旨在重新审视并简化核心技术如冗余减少和多尺度建模。

Method: 提出TimeCapsule模型，将时间序列建模为3D张量（时间、变量、级别维度），使用模式乘法捕获多模式依赖并实现维度压缩，结合JEPA支持预测表示学习。

Result: 在多个挑战性基准测试中，TimeCapsule达到了最先进性能。

Conclusion: TimeCapsule提供了一个通用且简化的框架，提高了LTSF模型的效率和性能。

Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF)
often emphasize complex, handcrafted designs, while simpler architectures like
linear models or MLPs have often outperformed these intricate solutions. In
this paper, we revisit and organize the core ideas behind several key
techniques, such as redundancy reduction and multi-scale modeling, which are
frequently employed in advanced LTSF models. Our goal is to streamline these
ideas for more efficient deep learning utilization. To this end, we introduce
TimeCapsule, a model built around the principle of high-dimensional information
compression that unifies these techniques in a generalized yet simplified
framework. Specifically, we model time series as a 3D tensor, incorporating
temporal, variate, and level dimensions, and leverage mode production to
capture multi-mode dependencies while achieving dimensionality compression. We
propose an internal forecast within the compressed representation domain,
supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the
learning of predictive representations. Extensive experiments on challenging
benchmarks demonstrate the versatility of our method, showing that TimeCapsule
can achieve state-of-the-art performance.

</details>


### [50] [GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection](https://arxiv.org/abs/2504.12740)
*Yifan Cao,Zhilong Mi,Ziqiao Yin,Binghui Guo,Jin Dong*

Main category: cs.LG

TL;DR: 这篇论文提出GPMFS方法，结合全局和个性化特征选择，提高高维多标签学习性能。


<details>
  <summary>Details</summary>
Motivation: 高维多标签学习受维度诅咒影响，现有方法忽略标签个性化特征，导致性能受限。

Method: GPMFS先利用标签相关性识别全局特征，然后通过阈值控制策略为每个标签添加个性化特征子集。

Result: 实验在多个真实数据集上显示GPMFS性能优越、可解释性和鲁棒性强，并提供标签特定强度洞察。

Conclusion: 证明个性化特征选择方法必要性及潜在应用价值。

Abstract: As artificial intelligence methods are increasingly applied to complex task
scenarios, high dimensional multi-label learning has emerged as a prominent
research focus. At present, the curse of dimensionality remains one of the
major bottlenecks in high-dimensional multi-label learning, which can be
effectively addressed through multi-label feature selection methods. However,
existing multi-label feature selection methods mostly focus on identifying
global features shared across all labels, which overlooks personalized
characteristics and specific requirements of individual labels. This
global-only perspective may limit the ability to capture label-specific
discriminative information, thereby affecting overall performance. In this
paper, we propose a novel method called GPMFS (Global Foundation and
Personalized Optimization for Multi-Label Feature Selection). GPMFS firstly
identifies global features by exploiting label correlations, then adaptively
supplements each label with a personalized subset of discriminative features
using a threshold-controlled strategy. Experiments on multiple real-world
datasets demonstrate that GPMFS achieves superior performance while maintaining
strong interpretability and robustness. Furthermore, GPMFS provides insights
into the label-specific strength across different multi-label datasets, thereby
demonstrating the necessity and potential applicability of personalized feature
selection approaches.

</details>


### [51] [Decentralized Nonconvex Composite Federated Learning with Gradient Tracking and Momentum](https://arxiv.org/abs/2504.12742)
*Yuan Zhou,Xinli Shi,Xuelong Li,Jiachen Zhong,Guanghui Wen,Jinde Cao*

Main category: cs.LG

TL;DR: 本文提出DEPOSITUM算法，用于非凸复合优化中的去中心化联邦学习，提高效率并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习日益受关注，但非凸复合优化问题尚未充分探索，需要新的方法解决数据异质性和通信成本。

Method: DEPOSITUM基于近端随机梯度跟踪，使用动量减少方差，支持本地更新，缓解数据异质性并降低通信开销。

Result: 理论上迭代复杂度为O(1/ε²)，实现线性加速；实验上在神经网络训练中表现优于其他算法。

Conclusion: DEPOSITUM通过理论分析和实证验证，证明了其在去中心化联邦学习中的有效性和鲁棒性。

Abstract: Decentralized Federated Learning (DFL) eliminates the reliance on the
server-client architecture inherent in traditional federated learning,
attracting significant research interest in recent years. Simultaneously, the
objective functions in machine learning tasks are often nonconvex and
frequently incorporate additional, potentially nonsmooth regularization terms
to satisfy practical requirements, thereby forming nonconvex composite
optimization problems. Employing DFL methods to solve such general optimization
problems leads to the formulation of Decentralized Nonconvex Composite
Federated Learning (DNCFL), a topic that remains largely underexplored. In this
paper, we propose a novel DNCFL algorithm, termed \bf{DEPOSITUM}. Built upon
proximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data
heterogeneity by enabling clients to approximate the global gradient. The
introduction of momentums in the proximal gradient descent step, replacing
tracking variables, reduces the variance introduced by stochastic gradients.
Additionally, DEPOSITUM supports local updates of client variables,
significantly reducing communication costs. Theoretical analysis demonstrates
that DEPOSITUM achieves an expected $\epsilon$-stationary point with an
iteration complexity of $\mathcal{O}(1/\epsilon^2)$. The proximal gradient,
consensus errors, and gradient estimation errors decrease at a sublinear rate
of $\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm
achieves network-independent linear speedup without requiring mega-batch
sampling. Finally, we apply DEPOSITUM to the training of neural networks on
real-world datasets, systematically examining the influence of various
hyperparameters on its performance. Comparisons with other federated composite
optimization algorithms validate the effectiveness of the proposed method.

</details>


### [52] [GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks](https://arxiv.org/abs/2504.12764)
*Hao Xu,Xiangru Jian,Xinjian Zhao,Wei Pang,Chao Zhang,Suyuchen Wang,Qixin Zhang,Joao Monteiro,Qiuzhuang Sun,Tianshu Yu*

Main category: cs.LG

TL;DR: 本文提出GraphOmni基准框架，评估LLM的图推理能力，并通过强化学习动态优化策略，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 基于分析发现没有单一序列化或提示策略优于其他，旨在改进LLM图推理性能。

Method: 分析图类型、序列化格式和提示方案，并提出强化学习-based动态选择序列化-提示配对的方法。

Result: 实证结果显示无策略一致优越，提出的方法显著提升准确性。

Conclusion: GraphOmni的设计为未来通用图推理模型研究提供坚实基础。

Abstract: In this paper, we presented GraphOmni, a comprehensive benchmark framework
for systematically evaluating the graph reasoning capabilities of LLMs. By
analyzing critical dimensions, including graph types, serialization formats,
and prompt schemes, we provided extensive insights into the strengths and
limitations of current LLMs. Our empirical findings emphasize that no single
serialization or prompting strategy consistently outperforms others. Motivated
by these insights, we propose a reinforcement learning-based approach that
dynamically selects the best serialization-prompt pairings, resulting in
significant accuracy improvements. GraphOmni's modular and extensible design
establishes a robust foundation for future research, facilitating advancements
toward general-purpose graph reasoning models.

</details>


### [53] [Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch](https://arxiv.org/abs/2504.12801)
*Advait Gadhikar,Tom Jacobs,Chao Zhou,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 本文提出Sign-In方法，通过动态重参数化诱导符号翻转，以改善从零开始训练稀疏神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: 存在从零开始训练稀疏神经网络（PaI）和稠密到稀疏训练之间的性能差距，这是高效深度学习的主要障碍；根据彩票票假设，PaI依赖于特定问题的参数初始化。

Method: 提出Sign-In方法，使用动态重参数化来诱导符号翻转，这种翻转与稠密到稀疏训练互补。

Result: 实验和理论表明PaI性能有所改善，但也指出了关闭PaI和稠密到稀疏训练差距的主要开放挑战。

Conclusion: Sign-In方法作为正交方法改善了PaI性能，但关闭性能差距的主要挑战仍未解决。

Abstract: The performance gap between training sparse neural networks from scratch
(PaI) and dense-to-sparse training presents a major roadblock for efficient
deep learning. According to the Lottery Ticket Hypothesis, PaI hinges on
finding a problem specific parameter initialization. As we show, to this end,
determining correct parameter signs is sufficient. Yet, they remain elusive to
PaI. To address this issue, we propose Sign-In, which employs a dynamic
reparameterization that provably induces sign flips. Such sign flips are
complementary to the ones that dense-to-sparse training can accomplish,
rendering Sign-In as an orthogonal method. While our experiments and theory
suggest performance improvements of PaI, they also carve out the main open
challenge to close the gap between PaI and dense-to-sparse training.

</details>


### [54] [Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies](https://arxiv.org/abs/2504.12803)
*Nitin Gupta,Indu Bala,Bapi Dutta,Luis Martínez,Anupam Yadav*

Main category: cs.LG

TL;DR: 本研究分析粒子群优化（PSO）中不同通信拓扑（环形、星形、冯·诺依曼）对收敛性和搜索行为的影响，使用IOHxplainer工具，提供选择拓扑的指南，以提升优化算法的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 群智能算法在工程和医疗等领域优化复杂系统，但由于配置和超参数不清楚，导致可靠性低，本研究旨在通过分析PSO拓扑来解决此问题。

Method: 使用适应后的IOHxplainer工具，调查拓扑对信息流、多样性和收敛速度的影响，并通过可视化和统计分析进行评估。

Result: 增强了PSO决策的可解释性，并提供了针对特定优化任务选择合适拓扑的实用指南。

Conclusion: 有助于使基于群的优化更透明、稳健和可信，提升整体可靠性。

Abstract: Swarm intelligence effectively optimizes complex systems across fields like
engineering and healthcare, yet algorithm solutions often suffer from low
reliability due to unclear configurations and hyperparameters. This study
analyzes Particle Swarm Optimization (PSO), focusing on how different
communication topologies Ring, Star, and Von Neumann affect convergence and
search behaviors. Using an adapted IOHxplainer , an explainable benchmarking
tool, we investigate how these topologies influence information flow,
diversity, and convergence speed, clarifying the balance between exploration
and exploitation. Through visualization and statistical analysis, the research
enhances interpretability of PSO's decisions and provides practical guidelines
for choosing suitable topologies for specific optimization tasks. Ultimately,
this contributes to making swarm based optimization more transparent, robust,
and trustworthy.

</details>


### [55] [A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks](https://arxiv.org/abs/2504.12806)
*Georgios Papadopoulos,Shaltiel Eloul,Yash Satsangi,Jamie Heredge,Niraj Kumar,Chun-Fu Chen,Marco Pistoia*

Main category: cs.LG

TL;DR: 本论文提出了一种从变分量子神经网络(VQNN)梯度中重建输入数据的数值方案，结合梯度估计、有限差分方法和Kalman滤波，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: VQNN的损失景观有指数增长的局部最小值，导致从梯度中恢复信息比经典神经网络更困难。

Method: 基于梯度反演，结合梯度估计、有限差分方法、自适应低通滤波，并用Kalman滤波优化以实现高效收敛。

Result: 实验显示，算法能反演批量训练数据，前提是VQNN模型足够过参数化。

Conclusion: 方案成功重建输入训练和实际数据，证明其在实际应用中的实用性。

Abstract: The loss landscape of Variational Quantum Neural Networks (VQNNs) is
characterized by local minima that grow exponentially with increasing qubits.
Because of this, it is more challenging to recover information from model
gradients during training compared to classical Neural Networks (NNs). In this
paper we present a numerical scheme that successfully reconstructs input
training, real-world, practical data from trainable VQNNs' gradients. Our
scheme is based on gradient inversion that works by combining gradients
estimation with the finite difference method and adaptive low-pass filtering.
The scheme is further optimized with Kalman filter to obtain efficient
convergence. Our experiments show that our algorithm can invert even
batch-trained data, given the VQNN model is sufficiently over-parameterized.

</details>


### [56] [Predicting Stock Prices using Permutation Decision Trees and Strategic Trailing](https://arxiv.org/abs/2504.12828)
*Vishrut Ramraj,Nithin Nagaraj,Harikrishnan N B*

Main category: cs.LG

TL;DR: 本文使用Permutation Decision Trees (PDT) 和战略追踪预测印度股票市场，基于NIFTY 50指数5分钟K线数据，策略避免卖空，PDT机器人表现优于LSTM、RNN和持有策略。


<details>
  <summary>Details</summary>
Motivation: 由于印度市场监管限制（如禁止卖空），本文旨在开发高频交易策略，利用短期波动获利并超越市场平均回报。

Method: 使用Yahoo Finance数据，48天训练、12天测试，结合技术指标、尾随止损超参数，比较PDT、LSTM和RNN模型。

Result: PDT机器人12天测试期盈利1.3468%，LSTM为0.1238%，RNN为0.3096%，均优于买入持有策略的-2.2508%，超出市场和无风险利率。

Conclusion: 提出的交易机器人有潜力提供高于市场平均和10年期印度政府债券无风险利率的回报。

Abstract: In this paper, we explore the application of Permutation Decision Trees (PDT)
and strategic trailing for predicting stock market movements and executing
profitable trades in the Indian stock market. We focus on high-frequency data
using 5-minute candlesticks for the top 50 stocks listed in the NIFTY 50 index.
We implement a trading strategy that aims to buy stocks at lower prices and
sell them at higher prices, capitalizing on short-term market fluctuations. Due
to regulatory constraints in India, short selling is not considered in our
strategy. The model incorporates various technical indicators and employs
hyperparameters such as the trailing stop-loss value and support thresholds to
manage risk effectively. Our results indicate that the proposed trading bot has
the potential to outperform the market average and yield returns higher than
the risk-free rate offered by 10-year Indian government bonds. We trained and
tested data on a 60 day dataset provided by Yahoo Finance. Specifically, 12
days for testing and 48 days for training. Our bot based on permutation
decision tree achieved a profit of 1.3468 % over a 12-day testing period, where
as a bot based on LSTM gave a return of 0.1238 % over a 12-day testing period
and a bot based on RNN gave a return of 0.3096 % over a 12-day testing period.
All of the bots outperform the buy-and-hold strategy, which resulted in a loss
of 2.2508 %.

</details>


### [57] [ALT: A Python Package for Lightweight Feature Representation in Time Series Classification](https://arxiv.org/abs/2504.12841)
*Balázs P. Halmos,Balázs Hajós,Vince Á. Molnár,Marcell T. Kurbucz,Antal Jakovác*

Main category: cs.LG

TL;DR: ALT是一个开源Python包，用于高效准确的时间序列分类，通过自适应算法改进前身方法。


<details>
  <summary>Details</summary>
Motivation: 为了提升时间序列分类的准确性和效率，捕捉不同时间尺度的模式，并提供可扩展的工具。

Method: 实现自适应法律-based转换算法，使用可变长度的时间窗口将原始数据转换为线性可分特征空间。

Result: 在真实数据集上基准测试，达到最先进性能，计算开销小，适用于物理等领域。

Conclusion: ALT提升了时间序列分类的实用性和性能，是一个有价值的开源工具。

Abstract: We introduce ALT, an open-source Python package created for efficient and
accurate time series classification (TSC). The package implements the adaptive
law-based transformation (ALT) algorithm, which transforms raw time series data
into a linearly separable feature space using variable-length shifted time
windows. This adaptive approach enhances its predecessor, the linear law-based
transformation (LLT), by effectively capturing patterns of varying temporal
scales. The software is implemented for scalability, interpretability, and ease
of use, achieving state-of-the-art performance with minimal computational
overhead. Extensive benchmarking on real-world datasets demonstrates the
utility of ALT for diverse TSC tasks in physics and related domains.

</details>


### [58] [FedX: Adaptive Model Decomposition and Quantization for IoT Federated Learning](https://arxiv.org/abs/2504.12849)
*Phung Lai,Xiaopeng Jiang,Hai Phan,Cristian Borcea,Khang Tran,An Chen,Vijaya Datta Mayyuri,Ruoming Jin*

Main category: cs.LG

TL;DR: FedX 是一种自适应模型分解和量化联邦学习系统，针对资源受限的 IoT 设备，平衡模型效用和计算/通信开销。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决联邦学习在异构、资源受限设备上的挑战，实现隐私保护应用中的高效训练。

Method: FedX 通过服务器端量化，将全局模型分解为自适应子网络，迭代最小化本地和服务器数据损失，并使用正则化项优化。

Result: 实验显示，FedX 改善量化时间高达 8.43 倍，设备计算时间 1.5 倍，总训练时间 1.36 倍，并理论保证全局模型收敛。

Conclusion: FedX 高效结合联邦学习与模型量化，提升训练效率，验证其优化优势。

Abstract: Federated Learning (FL) allows collaborative training among multiple devices
without data sharing, thus enabling privacy-sensitive applications on mobile or
Internet of Things (IoT) devices, such as mobile health and asset tracking.
However, designing an FL system with good model utility that works with low
computation/communication overhead on heterogeneous, resource-constrained
mobile/IoT devices is challenging. To address this problem, this paper proposes
FedX, a novel adaptive model decomposition and quantization FL system for IoT.
To balance utility with resource constraints on IoT devices, FedX decomposes a
global FL model into different sub-networks with adaptive numbers of quantized
bits for different devices. The key idea is that a device with fewer resources
receives a smaller sub-network for lower overhead but utilizes a larger number
of quantized bits for higher model utility, and vice versa. The quantization
operations in FedX are done at the server to reduce the computational load on
devices. FedX iteratively minimizes the losses in the devices' local data and
in the server's public data using quantized sub-networks under a regularization
term, and thus it maximizes the benefits of combining FL with model
quantization through knowledge sharing among the server and devices in a
cost-effective training process. Extensive experiments show that FedX
significantly improves quantization times by up to 8.43X, on-device computation
time by 1.5X, and total end-to-end training time by 1.36X, compared with
baseline FL systems. We guarantee the global model convergence theoretically
and validate local model convergence empirically, highlighting FedX's
optimization efficiency.

</details>


### [59] [iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise to Improve Imbalanced Data Classification](https://arxiv.org/abs/2504.12850)
*Khaled SH. Raslan,Almohammady S. Alsharkawy,K. R. Raslan*

Main category: cs.LG

TL;DR: 本论文提出iHHO-SMOTe方法，通过去除噪声和异常值改进SMOTE，处理类别不平衡数据集，取得高分类性能。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是机器学习中的挑战，SMOTE虽能生成少数类样本，但易受噪声和异常值影响。

Method: 先用随机森林选特征，再用DBSCAN检测并移除少数类异常值，然后应用iHHO-SMOTe进行过采样。

Result: 实验显示AUC>0.99，G-means=0.99，F1-score>0.967，性能卓越。

Conclusion: 该方法在噪声减少和异常处理上表现出色，是处理不平衡数据集的强有力方案。

Abstract: Classifying imbalanced datasets remains a significant challenge in machine
learning, particularly with big data where instances are unevenly distributed
among classes, leading to class imbalance issues that impact classifier
performance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses
this challenge by generating new instances for the under-represented minority
class, it faces obstacles in the form of noise and outliers during the creation
of new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses
the limitations of SMOTE by first cleansing the data from noise points. This
process involves employing feature selection using a random forest to identify
the most valuable features, followed by applying the Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) algorithm to detect outliers
based on the selected features. The identified outliers from the minority
classes are then removed, creating a refined dataset for subsequent
oversampling using the hybrid approach called iHHO-SMOTe. The comprehensive
experiments across diverse datasets demonstrate the exceptional performance of
the proposed model, with an AUC score exceeding 0.99, a high G-means score of
0.99 highlighting its robustness, and an outstanding F1-score consistently
exceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a
formidable contender in addressing imbalanced datasets, focusing on noise
reduction and outlier handling for improved classification models.

</details>


### [60] [A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning](https://arxiv.org/abs/2504.12875)
*Phung Lai,Guanxiong Liu,Hai Phan,Issa Khalil,Abdallah Khreishah,Xintao Wu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为CollaPois的协作后门中毒攻击，针对联邦学习中非IID数据的漏洞，提高了攻击的隐蔽性和有效性。


<details>
  <summary>Details</summary>
Motivation: 揭示联邦学习在非IID数据分布下的新漏洞，这些漏洞增加了模型中毒的风险。

Method: 开发CollaPois攻击，通过分发感染Trojan的预训练模型，让受损客户端协作产生恶意梯度，使模型收敛到受感染区域。

Result: 实验证明CollaPois优于现有攻击，能绕过现有防御，尤其在数据分布多样时，即使少量受损客户端也能有效。

Conclusion: CollaPois突显了非IID数据下的安全风险，数据与受损客户端相似的客户端风险更高，需要开发更robust的防御机制。

Abstract: Federated learning (FL) enables collaborative model training using
decentralized private data from multiple clients. While FL has shown robustness
against poisoning attacks with basic defenses, our research reveals new
vulnerabilities stemming from non-independent and identically distributed
(non-IID) data among clients. These vulnerabilities pose a substantial risk of
model poisoning in real-world FL scenarios.
  To demonstrate such vulnerabilities, we develop a novel collaborative
backdoor poisoning attack called CollaPois. In this attack, we distribute a
single pre-trained model infected with a Trojan to a group of compromised
clients. These clients then work together to produce malicious gradients,
causing the FL model to consistently converge towards a low-loss region
centered around the Trojan-infected model. Consequently, the impact of the
Trojan is amplified, especially when the benign clients have diverse local data
distributions and scattered local gradients. CollaPois stands out by achieving
its goals while involving only a limited number of compromised clients, setting
it apart from existing attacks. Also, CollaPois effectively avoids noticeable
shifts or degradation in the FL model's performance on legitimate data samples,
allowing it to operate stealthily and evade detection by advanced robust FL
algorithms.
  Thorough theoretical analysis and experiments conducted on various benchmark
datasets demonstrate the superiority of CollaPois compared to state-of-the-art
backdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,
especially in scenarios where clients possess diverse data distributions.
Moreover, the results show that CollaPois remains effective even when involving
a small number of compromised clients. Notably, clients whose local data is
closely aligned with compromised clients experience higher risks of backdoor
infections.

</details>


### [61] [Can Masked Autoencoders Also Listen to Birds?](https://arxiv.org/abs/2504.12880)
*Lukas Rauch,Ilyass Moummad,René Heinrich,Alexis Joly,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 本论文引入Bird-MAE模型，通过在BirdSet数据集上预训练，针对鸟类声音分类优化MAE模型，并提出prototypical probing方法，提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAE模型在AudioSet上预训练无法捕获生物声学领域的细粒度特征，鸟类声音分类对环境健康评估至关重要，需要领域专用模型。

Method: 提出Bird-MAE模型，在BirdSet上预训练；探索预训练、微调和冻结表示的调整；引入prototypical probing方法，利用冻结表示进行参数高效的探测。

Result: Bird-MAE在BirdSet所有下游任务上达到最先进性能，多标签分类大幅提升；prototypical probing比线性探测提高37% MAP，并将与微调的差距缩小到约3%。

Conclusion: 领域专用预训练和prototypical probing方法显著提升鸟类声音分类性能，证明了针对特定领域的模型优化有效性。

Abstract: Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the
fine-grained acoustic characteristics of specialized domains such as
bioacoustic monitoring. Bird sound classification is critical for assessing
environmental health, yet general-purpose models inadequately address its
unique acoustic challenges. To address this, we introduce Bird-MAE, a
domain-specialized MAE pretrained on the large-scale BirdSet dataset. We
explore adjustments to pretraining, fine-tuning and utilizing frozen
representations. Bird-MAE achieves state-of-the-art results across all BirdSet
downstream tasks, substantially improving multi-label classification
performance compared to the general-purpose Audio-MAE baseline. Additionally,
we propose prototypical probing, a parameter-efficient method for leveraging
MAEs' frozen representations. Bird-MAE's prototypical probes outperform linear
probing by up to 37\% in MAP and narrow the gap to fine-tuning to approximately
3\% on average on BirdSet.

</details>


### [62] [Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?](https://arxiv.org/abs/2504.12883)
*Tom Jacobs,Chao Zhou,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 这篇论文探讨显式正则化如何影响隐式偏差，并提出动态权重衰减策略以提升模型泛化。


<details>
  <summary>Details</summary>
Motivation: 隐式偏差解释过参数化模型的良好泛化，显式正则化用于防止过拟合，但二者互动需深入理解以控制偏差。

Method: 将显式正则化融入镜像流框架，分析其对训练动态几何的影响，包括位置偏差、类型偏差和范围收缩，适用于稀疏编码、矩阵感知等场景。

Result: 提出在训练中关闭权重衰减以改善泛化，并在实验中验证其有效性。

Conclusion: 强调动态权重衰减调度的潜在益处，可优化模型性能。

Abstract: Implicit bias plays an important role in explaining how overparameterized
models generalize well. Explicit regularization like weight decay is often
employed in addition to prevent overfitting. While both concepts have been
studied separately, in practice, they often act in tandem. Understanding their
interplay is key to controlling the shape and strength of implicit bias, as it
can be modified by explicit regularization. To this end, we incorporate
explicit regularization into the mirror flow framework and analyze its lasting
effects on the geometry of the training dynamics, covering three distinct
effects: positional bias, type of bias, and range shrinking. Our analytical
approach encompasses a broad class of problems, including sparse coding, matrix
sensing, single-layer attention, and LoRA, for which we demonstrate the utility
of our insights. To exploit the lasting effect of regularization and highlight
the potential benefit of dynamic weight decay schedules, we propose to switch
off weight decay during training, which can improve generalization, as we
demonstrate in experiments.

</details>


### [63] [Exact Learning Dynamics of In-Context Learning in Linear Transformers and Its Application to Non-Linear Transformers](https://arxiv.org/abs/2504.12916)
*Nischal Mainali,Lucas Teixeira*

Main category: cs.LG

TL;DR: 这篇论文通过简化线性Transformer模型的SGD动态分析，揭示了上下文学习（ICL）的机制，并提供工具解释其在复杂模型中的表现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在上下文学习中表现出色，但其底层机制仍不清楚，需要精确分析以理解其工作原理。

Method: 推导简化线性Transformer的封闭形式SGD动态，分析时间尺度分离和ICL发展，并引入谱秩动态和子空间稳定性等宏观测量验证非线性模型。

Result: 发现时间尺度分离、ICL的固定点和非线性学习行为，并解释了注意力网络中ICL的突然出现和模块算术模型的延迟泛化。

Conclusion: 提供了一个精确的ICL动态模型和理论工具，假设其现象可扩展到非线性Transformer模型，用于分析复杂训练过程。

Abstract: Transformer models exhibit remarkable in-context learning (ICL), adapting to
novel tasks from examples within their context, yet the underlying mechanisms
remain largely mysterious. Here, we provide an exact analytical
characterization of ICL emergence by deriving the closed-form stochastic
gradient descent (SGD) dynamics for a simplified linear transformer performing
regression tasks. Our analysis reveals key properties: (1) a natural separation
of timescales directly governed by the input data's covariance structure,
leading to staged learning; (2) an exact description of how ICL develops,
including fixed points corresponding to learned algorithms and conservation
laws constraining the dynamics; and (3) surprisingly nonlinear learning
behavior despite the model's linearity. We hypothesize this phenomenology
extends to non-linear models. To test this, we introduce theory-inspired
macroscopic measures (spectral rank dynamics, subspace stability) and use them
to provide mechanistic explanations for (1) the sudden emergence of ICL in
attention-only networks and (2) delayed generalization (grokking) in modular
arithmetic models. Our work offers an exact dynamical model for ICL and
theoretically grounded tools for analyzing complex transformer training.

</details>


### [64] [Sliced-Wasserstein Distance-based Data Selection](https://arxiv.org/abs/2504.12918)
*Julien Pallage,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 本文提出了一种基于切片Wasserstein距离的无监督异常检测方法，用于机器学习训练数据选择，适用于关键领域如电力系统，并提供可伸缩近似和新数据集。


<details>
  <summary>Details</summary>
Motivation: 针对关键部门决策管道的需求，提供保守数据选择和最优传输解释，以提升机器学习模型在电力系统等领域的可靠性。

Method: 提出基于切片Wasserstein距离的异常检测方法，并开发两个高效近似：减少基数表示和欧几里得距离近似；同时开放一个新数据集，并应用于合成和实际数据。

Result: 在合成数据集上展示过滤模式，数值基准测试数据选择性能，并将其用于新数据集的预测基准测试。

Conclusion: 方法在数据选择和预测任务中表现出有效性，具有在关键领域应用的潜力。

Abstract: We propose a new unsupervised anomaly detection method based on the
sliced-Wasserstein distance for training data selection in machine learning
approaches. Our filtering technique is interesting for decision-making
pipelines deploying machine learning models in critical sectors, e.g., power
systems, as it offers a conservative data selection and an optimal transport
interpretation. To ensure the scalability of our method, we provide two
efficient approximations. The first approximation processes reduced-cardinality
representations of the datasets concurrently. The second makes use of a
computationally light Euclidian distance approximation. Additionally, we open
the first dataset showcasing localized critical peak rebate demand response in
a northern climate. We present the filtering patterns of our method on
synthetic datasets and numerically benchmark our method for training data
selection. Finally, we employ our method as part of a first forecasting
benchmark for our open-source dataset.

</details>


### [65] [IdentiARAT: Toward Automated Identification of Individual ARAT Items from Wearable Sensors](https://arxiv.org/abs/2504.12921)
*Daniel Homm,Patrick Carqueville,Christian Eichhorn,Thomas Weikert,Thomas Menard,David A. Plecher,Chris Awai Easthope*

Main category: cs.LG

TL;DR: 本研究使用腕戴式IMU传感器和MiniROCKET自动标记ARAT项目，以解决其主观性和耗时问题。


<details>
  <summary>Details</summary>
Motivation: 克服ARAT评估的手动主观性和耗时问题，通过自动化标记过程。

Method: 使用IMU传感器收集数据，采用MiniROCKET进行时间序列分类，测试各种预处理策略，并使用45名参与者的数据。

Result: MiniROCKET为ARAT领域提供快速可靠的分类，但难以区分相似的项目。

Conclusion: 该方法有前景，但需要通过高级模型和数据增强进行改进。

Abstract: This study explores the potential of using wrist-worn inertial sensors to
automate the labeling of ARAT (Action Research Arm Test) items. While the ARAT
is commonly used to assess upper limb motor function, its limitations include
subjectivity and time consumption of clinical staff. By using IMU (Inertial
Measurement Unit) sensors and MiniROCKET as a time series classification
technique, this investigation aims to classify ARAT items based on sensor
recordings. We test common preprocessing strategies to efficiently leverage
included information in the data. Afterward, we use the best preprocessing to
improve the classification. The dataset includes recordings of 45 participants
performing various ARAT items. Results show that MiniROCKET offers a fast and
reliable approach for classifying ARAT domains, although challenges remain in
distinguishing between individual resembling items. Future work may involve
improving classification through more advanced machine-learning models and data
enhancements.

</details>


### [66] [RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs](https://arxiv.org/abs/2504.12949)
*Zhenao Song*

Main category: cs.LG

TL;DR: 提出RL-PINNs框架，使用强化学习优化PDE求解中的采样策略，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有PINNs采样方法依赖多轮采样和重训练，计算效率低，本文旨在解决冗余点和梯度计算开销问题。

Method: 将采样建模为Markov决策过程，RL代理动态选择点，使用函数变异作为奖励信号，并采用延迟奖励机制。

Result: 在各种PDE基准测试中，RL-PINNs在准确性上优于现有方法，采样开销微小，可扩展到高维和高阶问题。

Conclusion: RL-PINNs显著提升了PDE求解的效率和准确性，证明了其有效性。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs). However, their performance
heavily relies on the strategy used to select training points. Conventional
adaptive sampling methods, such as residual-based refinement, often require
multi-round sampling and repeated retraining of PINNs, leading to computational
inefficiency due to redundant points and costly gradient
computations-particularly in high-dimensional or high-order derivative
scenarios. To address these limitations, we propose RL-PINNs, a reinforcement
learning(RL)-driven adaptive sampling framework that enables efficient training
with only a single round of sampling. Our approach formulates adaptive sampling
as a Markov decision process, where an RL agent dynamically selects optimal
training points by maximizing a long-term utility metric. Critically, we
replace gradient-dependent residual metrics with a computationally efficient
function variation as the reward signal, eliminating the overhead of derivative
calculations. Furthermore, we employ a delayed reward mechanism to prioritize
long-term training stability over short-term gains. Extensive experiments
across diverse PDE benchmarks, including low-regular, nonlinear,
high-dimensional, and high-order problems, demonstrate that RL-PINNs
significantly outperforms existing residual-driven adaptive methods in
accuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,
making them scalable to high-dimensional and high-order problems.

</details>


### [67] [Transferrable Surrogates in Expressive Neural Architecture Search Spaces](https://arxiv.org/abs/2504.12971)
*Shiwen Qin,Gabriela Kadlecová,Martin Pilát,Shay B. Cohen,Roman Neruda,Elliot J. Crowley,Jovita Lukasik,Linus Ericsson*

Main category: cs.LG

TL;DR: 这篇论文探讨了使用代理模型来提高神经架构搜索（NAS）的效率和性能，平衡探索宽广搜索空间与架构评估。


<details>
  <summary>Details</summary>
Motivation: NAS 在探索表达性强的宽广搜索空间以实现架构创新与高效评估架构之间存在挑战。

Method: 通过训练代理模型，使用零成本代理指标和神经图特征（GRAF），或微调现成的大型语言模型来预测架构性能。

Result: 代理模型在不同数据集上具有高预测能力，能过滤不良架构加速搜索、提升性能，并可直接用作搜索目标实现巨大加速。

Conclusion: 代理模型能显著提高NAS的搜索效率和最终性能。

Abstract: Neural architecture search (NAS) faces a challenge in balancing the
exploration of expressive, broad search spaces that enable architectural
innovation with the need for efficient evaluation of architectures to
effectively search such spaces. We investigate surrogate model training for
improving search in highly expressive NAS search spaces based on context-free
grammars. We show that i) surrogate models trained either using zero-cost-proxy
metrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM
have high predictive power for the performance of architectures both within and
across datasets, ii) these surrogates can be used to filter out bad
architectures when searching on novel datasets, thereby significantly speeding
up search and achieving better final performances, and iii) the surrogates can
be further used directly as the search objective for huge speed-ups.

</details>


### [68] [A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](https://arxiv.org/abs/2504.12984)
*Yaoyao Ding,Bohan Hou,Xiao Zhang,Allan Lin,Tianqi Chen,Cody Yu Hao,Yida Wang,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: 本文提出一种支持任意位宽低精度计算的虚拟机（VM），用于提升LLM服务效率，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM服务需求高计算资源，现有的低精度方法受限于位宽为2的幂且优化不足。

Method: 引入GPGPU计算的VM，支持线程块级编程、层次内存、代数布局和多种低精度数据类型，并自动编译为高效GPU程序。

Result: 实验显示VM在低精度类型上优于Triton、Ladder、QuantLLM和Marlin，性能提升分别为1.75x、2.61x、1.29x和1.03x。

Conclusion: 该VM有效支持多样低精度数据类型，实现高效计算并显著改善性能。

Abstract: Serving Large Language Models (LLMs) is critical for AI-powered applications
but demands substantial computational resources, particularly in memory
bandwidth and computational throughput. Low-precision computation has emerged
as a key technique to improve efficiency while reducing resource consumption.
Existing approaches for generating low-precision kernels are limited to weight
bit widths that are powers of two and suffer from suboptimal performance due to
high-level GPU programming abstractions. These abstractions restrict critical
optimizations, such as fine-grained register management and optimized memory
access patterns, which are essential for efficient low-precision computations.
In this paper, we introduce a virtual machine (VM) designed for General-Purpose
GPU (GPGPU) computing, enabling support for low-precision data types with
arbitrary bit widths while maintaining GPU programmability. The proposed VM
features a thread-block-level programming model, a hierarchical memory space, a
novel algebraic layout system, and extensive support for diverse low-precision
data types. VM programs are compiled into highly efficient GPU programs with
automatic vectorization and instruction selection. Extensive experiments
demonstrate that our VM efficiently supports a full spectrum of low-precision
data types, and outperforms state-of-the-art low-precision kernels on their
supported types. Compared to existing compilers like Triton and Ladder, as well
as hand-optimized kernels such as QuantLLM and Marlin, our VM achieves
performance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.

</details>


### [69] [Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts](https://arxiv.org/abs/2504.12988)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 这篇论文推广了学习延迟框架，将查询分配给 k 个最自信的代理，并引入自适应 Top-k(x) 版本，证明其理论一致性和实验有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在高风险决策场景中提升可靠性，通过利用集体专业知识来解决现有单代理延迟方法的局限性。

Method: 提出 Top-k 和 Top-k(x) 学习延迟框架，推导新代理损失函数，并证明其贝叶斯一致性和 (R, G) 一致性。

Result: 在分类和回归任务的多种基准上实验显示框架有效，且模型级联是其特例。

Conclusion: 该框架提高了决策系统的灵活性和成本效率，确保了最优分配的收敛。

Abstract: Learning-to-Defer (L2D) enables decision-making systems to improve
reliability by selectively deferring uncertain predictions to more competent
agents. However, most existing approaches focus exclusively on single-agent
deferral, which is often inadequate in high-stakes scenarios that require
collective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of
the classical two-stage L2D framework that allocates each query to the $k$ most
confident agents instead of a single one. To further enhance flexibility and
cost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive
extension that learns the optimal number of agents to consult for each query,
based on input complexity, agent competency distributions, and consultation
costs. For both settings, we derive a novel surrogate loss and prove that it is
Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent, ensuring
convergence to the Bayes-optimal allocation. Notably, we show that the
well-established model cascades paradigm arises as a restricted instance of our
Top-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse
benchmarks demonstrate the effectiveness of our framework on both
classification and regression tasks.

</details>


### [70] [Chain-of-Thought Prompting for Out-of-Distribution Samples: A Latent-Variable Study](https://arxiv.org/abs/2504.12991)
*Yu Wang,Fu-Chieh Chang,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 这项工作研究了Chain-of-Thought (CoT)提示在分布偏移下的泛化能力，通过扩展潜在变量框架和实验，发现CoT在OOD样本与训练数据相似时表现良好，但相似度降低时性能下降。


<details>
  <summary>Details</summary>
Motivation: CoT提示作为提升大语言模型上下文学习的技术，其在分布偏移下的泛化能力尚未被充分理解，需要深入探究。

Method: 扩展了CoT提示的潜在变量框架，并研究了两种OOD场景：(i) 潜在变量被打乱成新组合，(ii) 潜在变量被统一缩放。

Result: CoT推理在OOD样本的潜在变量与训练时相似的条件下能有效泛化，但当相似度降低时性能下降。

Conclusion: 为CoT提示在OOD条件下的优势和局限性提供了基础洞见，并建议开发更具弹性的推理策略。

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful technique to
improve in-context learning (ICL) in large language models (LLMs) by breaking
complex reasoning into intermediate steps. However, the ability of CoT to
generalize under distribution shift remains poorly understood. In this work, we
extend a latent-variable framework for CoT prompting and study its behavior on
two prototypical out-of-distribution (OOD) scenarios: (i) the latent variables
for CoT steps are permuted into novel combinations, and (ii) the latent
variables uniformly scaled by a factor. Our experiments demonstrate that CoT
inference generalizes effectively to OOD samples whose latent variables closely
resemble those seen during training, but its performance degrades as this
similarity decreases. These findings provide foundational insights into the
strengths and limitations of CoT prompting under OOD conditions and suggest
directions for developing more resilient reasoning strategies in future LLMs.

</details>


### [71] [Inference-friendly Graph Compression for Graph Neural Networks](https://arxiv.org/abs/2504.13034)
*Yangxin Fan,Haolai Che,Yinghui Wu*

Main category: cs.LG

TL;DR: 这篇论文提出IFGC图压缩方案，旨在加速GNNs推理过程，通过压缩图保留推理结果，并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: GNNs在图分析中表现优异，但推理过程成本高，限制了其在大规模图上的应用。

Method: 提出IFGC框架，包括inference equivalence relation、SPGC、(α, r)-compression和anchored compression，并设计算法保证效率和推理质量。

Result: 在多种大规模图上进行实验，证实了方法的有效性和效率。

Conclusion: IFGC为GNNs推理提供高效压缩方案，提升了实际应用潜力。

Abstract: Graph Neural Networks (GNNs) have demonstrated promising performance in graph
analysis. Nevertheless, the inference process of GNNs remains costly, hindering
their applications for large graphs. This paper proposes inference-friendly
graph compression (IFGC), a graph compression scheme to accelerate GNNs
inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed
graph $G_c$, to best preserve the inference results of $M$ over $G$, such that
the result can be directly inferred by accessing $G_c$ with no or little
decompression cost. (1) We characterize IFGC with a class of inference
equivalence relation. The relation captures the node pairs in $G$ that are not
distinguishable for GNN inference. (2) We introduce three practical
specifications of IFGC for representative GNNs: structural preserving
compression (SPGC), which computes $G_c$ that can be directly processed by GNN
inference without decompression; ($\alpha$, $r$)-compression, that allows for a
configurable trade-off between compression ratio and inference quality, and
anchored compression that preserves inference results for specific nodes of
interest. For each scheme, we introduce compression and inference algorithms
with guarantees of efficiency and quality of the inferred results. We conduct
extensive experiments on diverse sets of large-scale graphs, which verifies the
effectiveness and efficiency of our graph compression approaches.

</details>


### [72] [An All-Atom Generative Model for Designing Protein Complexes](https://arxiv.org/abs/2504.13075)
*Ruizhe Chen,Dongyu Xue,Xiangxin Zhou,Zaixiang Zheng,Xiangxiang Zeng,Quanquan Gu*

Main category: cs.LG

TL;DR: 本论文引入APM模型，用于多链蛋白质建模，实现了从头设计蛋白质复合物并在多项任务中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管单链蛋白质建模已取得进展，但多链蛋白质的研究仍未充分，本文旨在填补这一空白以更好地理解生物功能。

Method: 引入APM模型，整合原子级信息并利用多链蛋白质数据，精确建模链间相互作用，并支持折叠和逆折叠任务。

Result: APM在监督微调下提升性能，支持零样本采样，实现了多链蛋白质的建模和设计，并在某些任务中达到最先进结果。

Conclusion: APM展示了在下游应用中的多功能性，通过结合数据和模型创新，推进了蛋白质复合物研究，代码将在GitHub上发布。

Abstract: Proteins typically exist in complexes, interacting with other proteins or
biomolecules to perform their specific biological roles. Research on
single-chain protein modeling has been extensively and deeply explored, with
advancements seen in models like the series of ESM and AlphaFold. Despite these
developments, the study and modeling of multi-chain proteins remain largely
uncharted, though they are vital for understanding biological functions.
Recognizing the importance of these interactions, we introduce APM (All-Atom
Protein Generative Model), a model specifically designed for modeling
multi-chain proteins. By integrating atom-level information and leveraging data
on multi-chain proteins, APM is capable of precisely modeling inter-chain
interactions and designing protein complexes with binding capabilities from
scratch. It also performs folding and inverse-folding tasks for multi-chain
proteins. Moreover, APM demonstrates versatility in downstream applications: it
achieves enhanced performance through supervised fine-tuning (SFT) while also
supporting zero-shot sampling in certain tasks, achieving state-of-the-art
results. Code will be released at https://github.com/bytedance/apm.

</details>


### [73] [An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research](https://arxiv.org/abs/2504.13101)
*Patrik Reizinger,Randall Balestriero,David Klindt,Wieland Brendel*

Main category: cs.LG

TL;DR: 这篇论文探讨自监督学习中的柏拉图表示假设，提出奇异可识别性理论解释其现象，并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解释自监督学习表示收敛的理论基础，并桥接理论与实践的差距。

Method: 通过综合可识别性理论的证据，提出扩展为奇异可识别性理论。

Result: 奇异可识别性理论提供更广泛框架，深入理解数据假设，并推进可解释和泛化表示的学习。

Conclusion: 强调未来研究三个关键方向：训练动态和收敛特性、有限样本和数据多样性影响、归纳偏差的作用。

Abstract: Self-Supervised Learning (SSL) powers many current AI systems. As research
interest and investment grow, the SSL design space continues to expand. The
Platonic view of SSL, following the Platonic Representation Hypothesis (PRH),
suggests that despite different methods and engineering approaches, all
representations converge to the same Platonic ideal. However, this phenomenon
lacks precise theoretical explanation. By synthesizing evidence from
Identifiability Theory (IT), we show that the PRH can emerge in SSL. However,
current IT cannot explain SSL's empirical success. To bridge the gap between
theory and practice, we propose expanding IT into what we term Singular
Identifiability Theory (SITh), a broader theoretical framework encompassing the
entire SSL pipeline. SITh would allow deeper insights into the implicit data
assumptions in SSL and advance the field towards learning more interpretable
and generalizable representations. We highlight three critical directions for
future research: 1) training dynamics and convergence properties of SSL; 2) the
impact of finite samples, batch size, and data diversity; and 3) the role of
inductive biases in architecture, augmentations, initialization schemes, and
optimizers.

</details>


### [74] [Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification](https://arxiv.org/abs/2504.13111)
*Kumar Manas,Christian Schlauch,Adrian Paschke,Christian Wirth,Nadja Klein*

Main category: cs.LG

TL;DR: 本论文提出SHIFT框架，通过结合不确定性建模和从驾驶规则中提取的信息先验，改善深度学习轨迹预测的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习轨迹预测模型在分布外泛化中的挑战，主要由于数据不平衡和缺乏多样性，导致鲁棒性和校准性不足。

Method: 提出SHIFT框架，将轨迹预测重构为分类任务，使用异方差谱归一化高斯过程分离认识不确定性和随机不确定性，并通过大语言模型的检索增强生成框架从驾驶规则（如停止规则和可驾驶性约束）中自动学习信息先验。

Result: 在nuScenes数据集上，包括低数据和跨位置场景的评估中，SHIFT超越最先进方法，在不确定性校准和位移指标上获得显著提升，尤其在复杂场景如交叉路口。

Conclusion: SHIFT框架展示了在轨迹预测中更好的不确定性校准和鲁棒性，特别适用于高不确定性的挑战性环境。

Abstract: Deep learning-based trajectory prediction models have demonstrated promising
capabilities in capturing complex interactions. However, their
out-of-distribution generalization remains a significant challenge,
particularly due to unbalanced data and a lack of enough data and diversity to
ensure robustness and calibration. To address this, we propose SHIFT (Spectral
Heteroscedastic Informed Forecasting for Trajectories), a novel framework that
uniquely combines well-calibrated uncertainty modeling with informative priors
derived through automated rule extraction. SHIFT reformulates trajectory
prediction as a classification task and employs heteroscedastic
spectral-normalized Gaussian processes to effectively disentangle epistemic and
aleatoric uncertainties. We learn informative priors from training labels,
which are automatically generated from natural language driving rules, such as
stop rules and drivability constraints, using a retrieval-augmented generation
framework powered by a large language model. Extensive evaluations over the
nuScenes dataset, including challenging low-data and cross-location scenarios,
demonstrate that SHIFT outperforms state-of-the-art methods, achieving
substantial gains in uncertainty calibration and displacement metrics. In
particular, our model excels in complex scenarios, such as intersections, where
uncertainty is inherently higher. Project page:
https://kumarmanas.github.io/SHIFT/.

</details>


### [75] [Hadamard product in deep learning: Introduction, Advances and Challenges](https://arxiv.org/abs/2504.13112)
*Grigorios G Chrysos,Yongtao Wu,Razvan Pascanu,Philip Torr,Volkan Cevher*

Main category: cs.LG

TL;DR: 这篇调查综述分析了Hadamard乘积在深度学习中的应用，建立了其分类，并强调了其在计算效率和表示能力方面的优势。


<details>
  <summary>Details</summary>
Motivation: 卷积和自注意力机制主导了深度学习架构，但Hadamard乘积作为基础原语尚未系统分析，尽管它在各种应用中广泛使用。

Method: 通过呈现Hadamard乘积在深度学习中的全面分类，识别了四个主要领域：高阶相关、多模态数据融合、动态表示调制和高效成对操作。

Result: 展示了Hadamard乘积在多模态融合任务（如视觉问答）和表示掩码应用（如图像修复和剪枝）中的适用性和有效性。

Conclusion: 巩固了现有知识，为未来架构创新奠定基础，并将Hadamard乘积定位为深度学习工具包中的关键组成部分，提供计算效率和表示能力的权衡。

Abstract: While convolution and self-attention mechanisms have dominated architectural
design in deep learning, this survey examines a fundamental yet understudied
primitive: the Hadamard product. Despite its widespread implementation across
various applications, the Hadamard product has not been systematically analyzed
as a core architectural primitive. We present the first comprehensive taxonomy
of its applications in deep learning, identifying four principal domains:
higher-order correlation, multimodal data fusion, dynamic representation
modulation, and efficient pairwise operations. The Hadamard product's ability
to model nonlinear interactions with linear computational complexity makes it
particularly valuable for resource-constrained deployments and edge computing
scenarios. We demonstrate its natural applicability in multimodal fusion tasks,
such as visual question answering, and its effectiveness in representation
masking for applications including image inpainting and pruning. This
systematic review not only consolidates existing knowledge about the Hadamard
product's role in deep learning architectures but also establishes a foundation
for future architectural innovations. Our analysis reveals the Hadamard product
as a versatile primitive that offers compelling trade-offs between
computational efficiency and representational power, positioning it as a
crucial component in the deep learning toolkit.

</details>


### [76] [Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum Autoencoders](https://arxiv.org/abs/2504.13113)
*Jason Zev Ludmir,Sophia Rebello,Jacob Ruiz,Tirthak Patel*

Main category: cs.LG

TL;DR: 本篇论文提出Quorum，一个无需训练的无监督量子异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 解决金融、医疗和能源等行业中检测关键异常的挑战，以及量子机器学习中梯度计算和无监督学习的困难。

Method: 提出Quorum框架，这是首个无需训练的量子无监督异常检测方法。

Result: 摘要未指定具体结果，但暗示框架可提升异常检测效能。

Conclusion: Quorum为量子计算在异常检测中的应用提供了新途径。

Abstract: Detecting mission-critical anomalous events and data is a crucial challenge
across various industries, including finance, healthcare, and energy. Quantum
computing has recently emerged as a powerful tool for tackling several machine
learning tasks, but training quantum machine learning models remains
challenging, particularly due to the difficulty of gradient calculation. The
challenge is even greater for anomaly detection, where unsupervised learning
methods are essential to ensure practical applicability. To address these
issues, we propose Quorum, the first quantum anomaly detection framework
designed for unsupervised learning that operates without requiring any
training.

</details>


### [77] [Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced Herd-Level Data Using Machine Learning Algorithms](https://arxiv.org/abs/2504.13116)
*Niamh Mimnagh,Andrew Parnell,Conor McAloon,Jaden Carlson,Maria Guelbenzu,Jonas Brock,Damien Barrett,Guy McGrath,Jamie Tratalos,Rafael Moral*

Main category: cs.LG

TL;DR: 本研究使用机器学习预测爱尔兰BVD阳性牛群，随机森林模型表现最佳，帮助减少监测成本。


<details>
  <summary>Details</summary>
Motivation: 随着BVD根除计划进展，预测模型可降低疾病再现风险。

Method: 评估机器学习算法如随机森林和XGBoost，通过模拟研究和实际数据，处理数据不平衡并使用敏感性、AUC等指标。

Result: 随机森林模型敏感性和AUC最高，实际预测正确识别219/250阳性牛群，并减少测试牛群数量。

Conclusion: 机器学习可有效支持BVD的针对性监测策略。

Abstract: Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication
programme in Ireland, with the herd-level prevalence declining from 11.3% in
2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the
development of predictive models for targeted surveillance becomes increasingly
important to mitigate the risk of disease re-emergence. In this study, we
evaluate the performance of a range of machine learning algorithms, including
binary classification and anomaly detection techniques, for predicting
BVD-positive herds using highly imbalanced herd-level data. We conduct an
extensive simulation study to assess model performance across varying sample
sizes and class imbalance ratios, incorporating resampling, class weighting,
and appropriate evaluation metrics (sensitivity, positive predictive value,
F1-score and AUC values). Random forests and XGBoost models consistently
outperformed other methods, with the random forest model achieving the highest
sensitivity and AUC across scenarios, including real-world prediction of 2023
herd status, correctly identifying 219 of 250 positive herds while halving the
number of herds that require compared to a blanket-testing strategy.

</details>


### [78] [Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction](https://arxiv.org/abs/2504.13142)
*Kristen Goebel,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: 本论文引入TAL转移学习框架，利用表型数据预测果树耐寒性，即使无直接耐寒性数据也能提高准确性。


<details>
  <summary>Details</summary>
Motivation: 耐寒性数据稀缺且需专业设备，而表型数据易得，农民需更好预测以减少霜冻损害。

Method: 提出TAL框架，包括基于模型选择和平均的新方法，使用深度多任务模型。

Result: 在真实葡萄品种数据上，TAL利用表型数据改善了耐寒性预测。

Conclusion: TAL框架能有效利用辅助标签数据，提升无耐寒性数据的预测性能。

Abstract: Cold temperatures can cause significant frost damage to fruit crops depending
on their resilience, or cold hardiness, which changes throughout the dormancy
season. This has led to the development of predictive cold-hardiness models,
which help farmers decide when to deploy expensive frost-mitigation measures.
Unfortunately, cold-hardiness data for model training is only available for
some fruit cultivars due to the need for specialized equipment and expertise.
Rather, farmers often do have years of phenological data (e.g. date of
budbreak) that they regularly collect for their crops. In this work, we
introduce a new transfer-learning framework, Transfer via Auxiliary Labels
(TAL), that allows farmers to leverage the phenological data to produce more
accurate cold-hardiness predictions, even when no cold-hardiness data is
available for their specific crop. The framework assumes a set of source tasks
(cultivars) where each has associated primary labels (cold hardiness) and
auxiliary labels (phenology). However, the target task (new cultivar) is
assumed to only have the auxiliary labels. The goal of TAL is to predict
primary labels for the target task via transfer from the source tasks.
Surprisingly, despite the vast literature on transfer learning, to our
knowledge, the TAL formulation has not been previously addressed. Thus, we
propose several new TAL approaches based on model selection and averaging that
can leverage recent deep multi-task models for cold-hardiness prediction. Our
results on real-world cold-hardiness and phenological data for multiple grape
cultivars demonstrate that TAL can leverage the phenological data to improve
cold-hardiness predictions in the absence of cold-hardiness data.

</details>


### [79] [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
*Aaron Mueller,Atticus Geiger,Sarah Wiegreffe,Dana Arad,Iván Arcuschin,Adam Belfki,Yik Siu Chan,Jaden Fiotto-Kaufman,Tal Haklay,Michael Hanna,Jing Huang,Rohan Gupta,Yaniv Nikankin,Hadas Orgad,Nikhil Prakash,Anja Reusch,Aruna Sankaranarayanan,Shun Shao,Alessandro Stolfo,Martin Tutek,Amir Zur,David Bau,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 提出MIB基准用于评估机械解释性方法在神经语言模型中的性能。


<details>
  <summary>Details</summary>
Motivation: 为了评估新机械解释性方法是否真正改进，提出有意义的评价标准。

Method: 提出MIB基准，包括电路定位和因果变量定位两个轨道，使用方法如归因修补、稀疏自动编码器等进行比较。

Result: 电路定位中，归因和掩码优化方法表现最佳；因果变量定位中，监督DAS方法最佳，而SAE特征不优于神经元。

Conclusion: MIB基准有助于有意义的比较方法，并增强对领域进步的信心。

Abstract: How can we know whether new mechanistic interpretability methods achieve real
improvements? In pursuit of meaningful and lasting evaluation standards, we
propose MIB, a benchmark with two tracks spanning four tasks and five models.
MIB favors methods that precisely and concisely recover relevant causal
pathways or specific causal variables in neural language models. The circuit
localization track compares methods that locate the model components - and
connections between them - most important for performing a task (e.g.,
attribution patching or information flow routes). The causal variable
localization track compares methods that featurize a hidden vector, e.g.,
sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate
model features for a causal variable relevant to the task. Using MIB, we find
that attribution and mask optimization methods perform best on circuit
localization. For causal variable localization, we find that the supervised DAS
method performs best, while SAE features are not better than neurons, i.e.,
standard dimensions of hidden vectors. These findings illustrate that MIB
enables meaningful comparisons of methods, and increases our confidence that
there has been real progress in the field.

</details>


### [80] [It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization](https://arxiv.org/abs/2504.13173)
*Ali Behrouz,Meisam Razaviyayn,Peilin Zhong,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本论文受注意力偏差启发，提出Miras框架设计新序列模型Moneta、Yaad和Memora，性能优于Transformer和线性RNN。


<details>
  <summary>Details</summary>
Motivation: 受人类认知注意力偏差现象启发，旨在提升基础模型的架构设计和能力。

Method: 重新概念化神经架构为关联记忆模块，引入替代注意力偏差配置、保留门和Miras框架，包括四种设计选择：关联记忆架构、注意力偏差目标、保留门和记忆学习算法。

Result: 实验显示Miras的不同设计选择产生模型优势，在语言建模、常识推理和回忆密集任务上优于Transformer和现代线性RNN。

Conclusion: Miras框架提供灵活方法，设计高性能序列模型，扩展了现有架构的潜力。

Abstract: Designing efficient and effective architectural backbones has been in the
core of research efforts to enhance the capability of foundation models.
Inspired by the human cognitive phenomenon of attentional bias-the natural
tendency to prioritize certain events or stimuli-we reconceptualize neural
architectures, including Transformers, Titans, and modern linear recurrent
neural networks as associative memory modules that learn a mapping of keys and
values using an internal objective, referred to as attentional bias.
Surprisingly, we observed that most existing sequence models leverage either
(1) dot-product similarity, or (2) L2 regression objectives as their
attentional bias. Going beyond these objectives, we present a set of
alternative attentional bias configurations along with their effective
approximations to stabilize their training procedure. We then reinterpret
forgetting mechanisms in modern deep learning architectures as a form of
retention regularization, providing a novel set of forget gates for sequence
models. Building upon these insights, we present Miras, a general framework to
design deep learning architectures based on four choices of: (i) associative
memory architecture, (ii) attentional bias objective, (iii) retention gate, and
(iv) memory learning algorithm. We present three novel sequence models-Moneta,
Yaad, and Memora-that go beyond the power of existing linear RNNs while
maintaining a fast parallelizable training process. Our experiments show
different design choices in Miras yield models with varying strengths. For
example, certain instances of Miras achieve exceptional performance in special
tasks such as language modeling, commonsense reasoning, and recall intensive
tasks, even outperforming Transformers and other modern linear recurrent
models.

</details>


### [81] [Aligning Constraint Generation with Design Intent in Parametric CAD](https://arxiv.org/abs/2504.13178)
*Evan Casey,Tianyu Zhang,Shu Ishida,John Roger Thompson,Amir Khasahmadi,Joseph George Lambourne,Pradeep Kumar Jayaraman,Karl D. D. Willis*

Main category: cs.LG

TL;DR: 本论文将LLM对齐技术应用于CAD草图约束生成，提高约束完整性。


<details>
  <summary>Details</summary>
Motivation: 解决CAD设计中'设计对齐'问题，确保约束完全约束几何而不过度或扭曲。

Method: 使用对齐技术训练约束生成模型，通过约束求解器反馈改进。

Result: 实现93%的草图完全约束率，优于SFT基线的34%和无对齐的8.9%。

Conclusion: 方法可应用于任何模型，并为语言与设计领域对齐研究奠基。

Abstract: We adapt alignment techniques from reasoning LLMs to the task of generating
engineering sketch constraints found in computer-aided design (CAD) models.
Engineering sketches consist of geometric primitives (e.g. points, lines)
connected by constraints (e.g. perpendicular, tangent) that define the
relationships between them. For a design to be easily editable, the constraints
must effectively capture design intent, ensuring the geometry updates
predictably when parameters change. Although current approaches can generate
CAD designs, an open challenge remains to align model outputs with design
intent, we label this problem `design alignment'. A critical first step towards
aligning generative CAD models is to generate constraints which fully-constrain
all geometric primitives, without over-constraining or distorting sketch
geometry. Using alignment techniques to train an existing constraint generation
model with feedback from a constraint solver, we are able to fully-constrain
93% of sketches compared to 34% when using a na\"ive supervised fine-tuning
(SFT) baseline and only 8.9% without alignment. Our approach can be applied to
any existing constraint generation model and sets the stage for further
research bridging alignment strategies between the language and design domains.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [82] [A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition](https://arxiv.org/abs/2504.13102)
*Wei Huang,Shumeng Sun,Junpeng Lu,Zhenpeng Xu,Zhengyang Xiu,Hao Zhang*

Main category: cs.SD

TL;DR: 本文提出MT-BCA-CNN模型，通过多任务学习和通道注意力机制，提高了少样本水下声学目标识别的性能，准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 水下声学目标识别对海洋保护和国防安全重要，但深度学习面临样本稀缺和环境干扰挑战。

Method: 提出MT-BCA-CNN，结合通道注意力机制和多任务学习，优化特征提取和分类任务。

Result: 实验显示97%准确率和95%F1分数，优于传统和最新方法，消融研究证实机制有效。

Conclusion: 为少样本水下声学识别提供高效解决方案，推动海洋生物声学和声呐处理研究。

Abstract: Underwater acoustic target recognition (UATR) is of great significance for
the protection of marine diversity and national defense security. The
development of deep learning provides new opportunities for UATR, but faces
challenges brought by the scarcity of reference samples and complex
environmental interference. To address these issues, we proposes a multi-task
balanced channel attention convolutional neural network (MT-BCA-CNN). The
method integrates a channel attention mechanism with a multi-task learning
strategy, constructing a shared feature extractor and multi-task classifiers to
jointly optimize target classification and feature reconstruction tasks. The
channel attention mechanism dynamically enhances discriminative acoustic
features such as harmonic structures while suppressing noise. Experiments on
the Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\%
classification accuracy and 95\% $F1$-score in 27-class few-shot scenarios,
significantly outperforming traditional CNN and ACNN models, as well as popular
state-of-the-art UATR methods. Ablation studies confirm the synergistic
benefits of multi-task learning and attention mechanisms, while a dynamic
weighting adjustment strategy effectively balances task contributions. This
work provides an efficient solution for few-shot underwater acoustic
recognition, advancing research in marine bioacoustics and sonar signal
processing.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [83] [RIS-Assisted Beamfocusing in Near-Field IoT Communication Systems: A Transformer-Based Approach](https://arxiv.org/abs/2504.12889)
*Quan Zhou,Jingjing Zhao,Kaiquan Cai,Yanbo Zhu*

Main category: eess.SP

TL;DR: 本论文提出了一种基于RIS的波束聚焦机制，使用Transformer-based的两阶段算法，针对ELAA系统中的近场传播问题，提高波束选择精度。


<details>
  <summary>Details</summary>
Motivation: ELAA系统中天线数量巨大导致信号传播转向近场球波，设计包含角度和距离域的二维波束码本具有挑战性。

Method: 引入Transformer-based的两阶段波束训练算法：第一阶段使用简单码本粗略估计设备位置，第二阶段使用精确码本进行细粒度搜索。

Result: 在SNR=20dB时，波束选择准确率达97%，比基线方法提高10%至50%。

Conclusion: 实验结果显示，该方法大大提升了RIS辅助波束聚焦的精度。

Abstract: The massive number of antennas in extremely large aperture array (ELAA)
systems shifts the propagation regime of signals in internet of things (IoT)
communication systems towards near-field spherical wave propagation. We propose
a reconfigurable intelligent surfaces (RIS)-assisted beamfocusing mechanism,
where the design of the two-dimensional beam codebook that contains both the
angular and distance domains is challenging. To address this issue, we
introduce a novel Transformer-based two-stage beam training algorithm, which
includes the coarse and fine search phases. The proposed mechanism provides a
fine-grained codebook with enhanced spatial resolution, enabling precise
beamfocusing. Specifically, in the first stage, the beam training is performed
to estimate the approximate location of the device by using a simple codebook,
determining whether it is within the beamfocusing range (BFR) or the
none-beamfocusing range (NBFR). In the second stage, by using a more precise
codebook, a fine-grained beam search strategy is conducted. Experimental
results unveil that the precision of the RIS-assisted beamfocusing is greatly
improved. The proposed method achieves beam selection accuracy up to 97% at
signal-to-noise ratio (SNR) of 20 dB, and improves 10% to 50% over the baseline
method at different SNRs.

</details>


### [84] [Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining](https://arxiv.org/abs/2504.12758)
*Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文展示XL MIMO系统可作为神经网络，实现低复杂度OTA边缘推理。


<details>
  <summary>Details</summary>
Motivation: 动机是探索XL MIMO系统类似于神经网络的特性，以实现无需数字处理的OTA边缘推理，并为超低功耗设备提供高效解决方案。

Method: 方法是将XL MIMO通道系数视为隐藏层随机节点，接收端模拟组合器视为输出层，框架化为ELM模型。

Result: 结果显示XL-MIMO-ELM框架实现了近实时训练和高效分类，与深度学习方法相比复杂度降低几个数量级。

Conclusion: 结论是XL MIMO系统标志着MIMO范式转变，适用于超低功耗无线设备，具有显著优势。

Abstract: In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input
Multiple-Output (MIMO) wireless system with appropriate analog combining
components exhibits the properties of a universal function approximator,
similar to a feedforward neural network. By treating the XL MIMO channel
coefficients as the random nodes of a hidden layer, and the receiver's analog
combiner as a trainable output layer, we cast the end-to-end system to the
Extreme Learning Machine (ELM) framework, leading to a novel formulation for
Over-The-Air (OTA) edge inference without requiring traditional digital
processing nor pre-processing at the transmitter. Through theoretical analysis
and numerical evaluation, we showcase that XL-MIMO-ELM enables
near-instantaneous training and efficient classification, suggesting the
paradigm shift of beyond massive MIMO systems as neural networks alongside
their profound communications role. Compared to deep learning approaches and
conventional ELMs, the proposed framework achieves on par performance with
orders of magnitude lower complexity, making it highly attractive for ultra low
power wireless devices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [85] [Themisto: Jupyter-Based Runtime Benchmark](https://arxiv.org/abs/2504.12365)
*Konstantin Grotov,Sergey Titov*

Main category: cs.SE

TL;DR: 这篇论文引入了一个基准测试，使用Jupyter笔记本开发轨迹，评估大语言模型如何利用运行时信息预测代码输出和生成代码，结果显示当前模型表现不佳，并强调需更多研究运行时上下文。


<details>
  <summary>Details</summary>
Motivation: 动机是发现当前大语言模型在利用运行时信息方面的不足，并指出这是一个被低估的研究领域。

Method: 方法是构建一个由Jupyter笔记本开发轨迹组成的基准测试。

Result: 结果是当前一代大语言模型在这些任务上表现较差。

Conclusion: 结论是存在一个未被充分研究的领域，即在代码模型中整合运行时上下文。

Abstract: In this work, we present a benchmark that consists of Jupyter notebooks
development trajectories and allows measuring how large language models (LLMs)
can leverage runtime information for predicting code output and code
generation. We demonstrate that the current generation of LLMs performs poorly
on these tasks and argue that there exists a significantly understudied domain
in the development of code-based models, which involves incorporating the
runtime context.

</details>


### [86] [Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation](https://arxiv.org/abs/2504.12608)
*Mingwei Liu,Juntao Li,Ying Wang,Xueying Du,Zuoyu Ou,Qiuyuan Chen,Bingxu An,Zhao Wei,Yong Xu,Fangming Zou,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 这篇论文研究了大型语言模型在代码生成中的重复问题，并提出DeRep方法来减轻重复，提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代码生成中代码重复导致的低效和可读性差问题。

Method: 进行实证研究和提出基于规则的DeRep技术来检测和减轻重复。

Result: 发现重复问题普遍，DeRep显著改善重复指标（rep-3、rep-line、sim-line平均改善91.3%、93.5%、79.9%），Pass@1增加208.3%。

Conclusion: DeRep有效提升代码质量，并能改善现有方法的性能。

Abstract: Despite recent advances in Large Language Models (LLMs) for code generation,
the quality of LLM-generated code still faces significant challenges. One
significant issue is code repetition, which refers to the model's tendency to
generate structurally redundant code, resulting in inefficiencies and reduced
readability. To address this, we conduct the first empirical study to
investigate the prevalence and nature of repetition across 19 state-of-the-art
code LLMs using three widely-used benchmarks. Our study includes both
quantitative and qualitative analyses, revealing that repetition is pervasive
and manifests at various granularities and extents, including character,
statement, and block levels. We further summarize a taxonomy of 20 repetition
patterns. Building on our findings, we propose DeRep, a rule-based technique
designed to detect and mitigate repetition in generated code. We evaluate DeRep
using both open-source benchmarks and in an industrial setting. Our results
demonstrate that DeRep significantly outperforms baselines in reducing
repetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,
rep-line, and sim-line metrics) and enhancing code quality (with a Pass@1
increase of 208.3% over greedy search). Furthermore, integrating DeRep improves
the performance of existing repetition mitigation methods, with Pass@1
improvements ranging from 53.7% to 215.7%.

</details>


### [87] [A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology](https://arxiv.org/abs/2504.12977)
*Maksim Vishnevskiy*

Main category: cs.SE

TL;DR: 这篇论文提出了一种基于海德格尔根本本体论的创新IT系统，用于更深入分析用户查询。


<details>
  <summary>Details</summary>
Motivation: 动机是克服现有系统仅限于范畴分析的局限性，通过海德格尔的现象学方法揭示查询中的深层本体模式，尤其在IT语境的复杂互动中。

Method: 方法使用两种语言：范畴语言处理用户输入，存在语言进行内部分析，通过现象学还原模块桥接，分析查询、识别递归结构并提供洞见。

Result: 结果展示了系统的架构、操作原则、技术实现、用例（包括真实IT对话）、与现有工具的比较、优势和局限性。

Conclusion: 结论是这为通用查询分析工具铺平道路，通过形式化存在语言减少计算问题。

Abstract: This paper presents a novel research analytical IT system grounded in Martin
Heidegger's Fundamental Ontology, distinguishing between beings (das Seiende)
and Being (das Sein). The system employs two modally distinct, descriptively
complete languages: a categorical language of beings for processing user inputs
and an existential language of Being for internal analysis. These languages are
bridged via a phenomenological reduction module, enabling the system to analyze
user queries (including questions, answers, and dialogues among IT
specialists), identify recursive and self-referential structures, and provide
actionable insights in categorical terms. Unlike contemporary systems limited
to categorical analysis, this approach leverages Heidegger's phenomenological
existential analysis to uncover deeper ontological patterns in query
processing, aiding in resolving logical traps in complex interactions, such as
metaphor usage in IT contexts. The path to full realization involves
formalizing the language of Being by a research team based on Heidegger's
Fundamental Ontology; given the existing completeness of the language of
beings, this reduces the system's computability to completeness, paving the way
for a universal query analysis tool. The paper presents the system's
architecture, operational principles, technical implementation, use
cases--including a case based on real IT specialist dialogues--comparative
evaluation with existing tools, and its advantages and limitations.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [88] [Design Topological Materials by Reinforcement Fine-Tuned Generative Model](https://arxiv.org/abs/2504.13048)
*Haosheng Xu,Dongheng Qian,Zhixuan Liu,Yadong Jiang,Jing Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 这篇论文使用强化微调的生成模型生成新的拓扑绝缘体和拓扑晶体绝缘体，成功发现了如Ge₂Bi₂O₆这样的材料。


<details>
  <summary>Details</summary>
Motivation: 拓扑绝缘体和拓扑晶体绝缘体具有重要应用价值，但这类材料稀缺，传统方法受限，因此需要生成新材料。

Method: 通过对预训练生成模型应用强化微调（ReFT），以使模型目标与材料设计目标一致。

Result: 生成了大量新拓扑材料，示例Ge₂Bi₂O₆具有0.26 eV的全带隙，是已知最大的之一。

Conclusion: ReFT方法有效提升了生成拓扑材料的性能，并证明了其在发现新材料方面的潜力。

Abstract: Topological insulators (TIs) and topological crystalline insulators (TCIs)
are materials with unconventional electronic properties, making their discovery
highly valuable for practical applications. However, such materials,
particularly those with a full band gap, remain scarce. Given the limitations
of traditional approaches that scan known materials for candidates, we focus on
the generation of new topological materials through a generative model.
Specifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained
generative model, thereby aligning the model's objectives with our material
design goals. We demonstrate that ReFT is effective in enhancing the model's
ability to generate TIs and TCIs, with minimal compromise on the stability of
the generated materials. Using the fine-tuned model, we successfully identify a
large number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a
representative example--a TI with a full band gap of 0.26 eV, ranking among the
largest known in this category.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [89] [Predictive control of blast furnace temperature in steelmaking with hybrid depth-infused quantum neural networks](https://arxiv.org/abs/2504.12389)
*Nayoung Lee,Minsoo Shin,Asel Sagingalieva,Ayush Joshi Tripathi,Karan Pinto,Alexey Melnikov*

Main category: quant-ph

TL;DR: 本文提出结合混合量子机器学习与粉煤灰注入控制的方法，提高高炉温度预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理高炉温度波动的复杂非线性性，亟需改进以优化钢铁生产效率和生产力。

Method: 采用量子增强特征空间探索与经典回归模型相结合的预测优化方法，预测温度变化并优化粉煤灰注入。

Result: 预测准确性提升超过25%，温度稳定性从±50度改善到±7.6度。

Conclusion: 突显混合量子机器学习模型在工业钢铁生产中的应用潜力。

Abstract: Accurate prediction and stabilization of blast furnace temperatures are
crucial for optimizing the efficiency and productivity of steel production.
Traditional methods often struggle with the complex and non-linear nature of
the temperature fluctuations within blast furnaces. This paper proposes a novel
approach that combines hybrid quantum machine learning with pulverized coal
injection control to address these challenges. By integrating classical machine
learning techniques with quantum computing algorithms, we aim to enhance
predictive accuracy and achieve more stable temperature control. For this we
utilized a unique prediction-based optimization method. Our method leverages
quantum-enhanced feature space exploration and the robustness of classical
regression models to forecast temperature variations and optimize pulverized
coal injection values. Our results demonstrate a significant improvement in
prediction accuracy over 25 percent and our solution improved temperature
stability to +-7.6 degrees of target range from the earlier variance of +-50
degrees, highlighting the potential of hybrid quantum machine learning models
in industrial steel production applications.

</details>


### [90] [Featuremetric benchmarking: Quantum computer benchmarks based on circuit features](https://arxiv.org/abs/2504.12575)
*Timothy Proctor,Anh Tran,Xingxin Liu,Aditya Dhumuntarao,Stefan Seritan,Alaina Green,Norbert M Linke*

Main category: quant-ph

TL;DR: 本论文提出了一种基于量子电路特征的基准测试框架，以更好地评估多量子比特量子计算机的性能。


<details>
  <summary>Details</summary>
Motivation: 基准测试对于简洁总结量子计算机性能并衡量其向实用量子计算进步的程度至关重要。

Method: 提出featuremetric基准测试框架，量化性能随电路特征变化；泛化体积分基准测试；使用IBM Q和IonQ系统演示，并采用高斯过程回归分析数据。

Result: 展示了更丰富和忠实的性能模型；通过少量电路数据创建直观的二维性能区域。

Conclusion: 这种框架改进了量子计算机基准测试，提供更准确的性能总结。

Abstract: Benchmarks that concisely summarize the performance of many-qubit quantum
computers are essential for measuring progress towards the goal of useful
quantum computation. In this work, we present a benchmarking framework that is
based on quantifying how a quantum computer's performance on quantum circuits
varies as a function of features of those circuits, such as circuit depth,
width, two-qubit gate density, problem input size, or algorithmic depth. Our
featuremetric benchmarking framework generalizes volumetric benchmarking -- a
widely-used methodology that quantifies performance versus circuit width and
depth -- and we show that it enables richer and more faithful models of quantum
computer performance. We demonstrate featuremetric benchmarking with example
benchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to
produce performance summaries from the data using Gaussian process regression.
Our data analysis methods are also of interest in the special case of
volumetric benchmarking, as they enable the creation of intuitive
two-dimensional capability regions using data from few circuits.

</details>


### [91] [Query Complexity of Classical and Quantum Channel Discrimination](https://arxiv.org/abs/2504.12989)
*Theshani Nuradha,Mark M. Wilde*

Main category: quant-ph

TL;DR: 本论文研究量子通道鉴别的查询复杂度，焦点在于最小通道使用次数以达到所需错误概率。结果显示查询复杂度与错误概率和通道保真度的对数相关，并为二元和多通道情况提供了界限。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨查询复杂度，而不是仅关注错误概率衰减率，以更好地理解通道鉴别的资源需求。

Method: 方法包括证明查询复杂度与逆错误概率的对数成正比，以及与通道保真度的负对数成反比，并给出不同通道鉴别的界限。

Result: 结果精确表征了二元经典通道的查询复杂度，并提供了二元不对称和多通道鉴别的上下界，依赖于散度和保真度。

Conclusion: 结论是，查询复杂度依赖于通道保真度和散度，为通道鉴别提供了新的分析框架。

Abstract: Quantum channel discrimination has been studied from an information-theoretic
perspective, wherein one is interested in the optimal decay rate of error
probabilities as a function of the number of unknown channel accesses. In this
paper, we study the query complexity of quantum channel discrimination, wherein
the goal is to determine the minimum number of channel uses needed to reach a
desired error probability. To this end, we show that the query complexity of
binary channel discrimination depends logarithmically on the inverse error
probability and inversely on the negative logarithm of the (geometric and
Holevo) channel fidelity. As a special case of these findings, we precisely
characterize the query complexity of discriminating between two classical
channels. We also provide lower and upper bounds on the query complexity of
binary asymmetric channel discrimination and multiple quantum channel
discrimination. For the former, the query complexity depends on the geometric
R\'enyi and Petz R\'enyi channel divergences, while for the latter, it depends
on the negative logarithm of (geometric and Uhlmann) channel fidelity. For
multiple channel discrimination, the upper bound scales as the logarithm of the
number of channels.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [92] [Enhanced Battery Capacity Estimation in Data-Limited Scenarios through Swarm Learning](https://arxiv.org/abs/2504.12444)
*Jiawei Zhang,Yu Zhang,Wei Xu,Yifei Zhang,Weiran Jiang,Qi Jiao,Yao Ren,Ziyou Song*

Main category: eess.SY

TL;DR: 本论文提出一种群智能电池管理系统，使用去中心化群学习框架和可信度权重模型合并机制，在数据有限场景下提升电池容量估计准确性，同时确保数据隐私和安全性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在数据有限场景下性能较差，且缺乏同时确保数据隐私和容错的电池管理框架。

Method: 提出群电池管理系统，结合去中心化群学习框架和基于可信度权重的模型合并机制。

Result: 在包含66个商用LiNiCoAlO2电池的数据集上验证，在数据平衡、容量偏差、特征偏差和质量偏差等场景下，群学习提高了估计准确性，与中心学习相当。

Conclusion: 群学习框架在数据有限情况下提升了电池容量估计准确性，并实现了与中心学习的类似性能水平。

Abstract: Data-driven methods have shown potential in electric-vehicle battery
management tasks such as capacity estimation, but their deployment is
bottlenecked by poor performance in data-limited scenarios. Sharing battery
data among algorithm developers can enable accurate and generalizable
data-driven models. However, an effective battery management framework that
simultaneously ensures data privacy and fault tolerance is still lacking. This
paper proposes a swarm battery management system that unites a decentralized
swarm learning (SL) framework and credibility weight-based model merging
mechanism to enhance battery capacity estimation in data-limited scenarios
while ensuring data privacy and security. The effectiveness of the SL framework
is validated on a dataset comprising 66 commercial LiNiCoAlO2 cells cycled
under various operating conditions. Specifically, the capacity estimation
performance is validated in four cases, including data-balanced, volume-biased,
feature-biased, and quality-biased scenarios. Our results show that SL can
enhance the estimation accuracy in all data-limited cases and achieve a similar
level of accuracy with central learning where large amounts of data are
available.

</details>


### [93] [Robust Visual Servoing under Human Supervision for Assembly Tasks](https://arxiv.org/abs/2504.12506)
*Victor Nan Fernandez-Ayala,Jorge Silva,Meng Guo,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 我们提出一个框架，使移动机械手可靠地完成拾取和放置任务，用于组装结构块，通过视觉伺服、控制屏障函数和实验验证。


<details>
  <summary>Details</summary>
Motivation: 为了提高移动机械手在组装任务中的可靠性和鲁棒性，特别是在处理相机位姿错误和确保结构稳定性。

Method: 使用手眼视觉伺服控制和控制屏障函数跟踪物体，确保标记可见；结合眼到手设置进行精确放置，并整合人为干预和适应屏障函数。

Result: 实验在6自由度移动臂上验证了框架的有效性，并分析了其对相机位姿错误的鲁棒性。

Conclusion: 框架通过结合多种技术提高了任务可靠性，并证明了在实际应用中的可行性。

Abstract: We propose a framework enabling mobile manipulators to reliably complete
pick-and-place tasks for assembling structures from construction blocks. The
picking uses an eye-in-hand visual servoing controller for object tracking with
Control Barrier Functions (CBFs) to ensure fiducial markers in the blocks
remain visible. An additional robot with an eye-to-hand setup ensures precise
placement, critical for structural stability. We integrate human-in-the-loop
capabilities for flexibility and fault correction and analyze robustness to
camera pose errors, proposing adapted barrier functions to handle them. Lastly,
experiments validate the framework on 6-DoF mobile arms.

</details>


### [94] [Optimizing Utility-Scale Solar Siting for Local Economic Benefits and Regional Decarbonization](https://arxiv.org/abs/2504.12508)
*Papa Yaw Owusu-Obeng,Steven R. Miller,Sarah Banas Mills,Michael T. Craig*

Main category: eess.SY

TL;DR: 本研究整合本地经济影响到太阳能选址决策中，发现优先高回报区域可提高经济效益，同时系统成本增加很小。


<details>
  <summary>Details</summary>
Motivation: 中西部太阳能发展迅速，但传统规划未整合本地经济影响，且忽略农田转换的机会成本，本研究旨在填补这一空白。

Method: 将本地经济指标纳入电力系统规划模型，构建供应和收益曲线，使用多目标优化分析大湖区县域。

Result: 经济较大县收益更高，每兆瓦可达34,500美元；考虑机会成本减收益16%；优先高回报县可增益11%，投资转移，成本增0.5%。

Conclusion: 强调在太阳能规划中整合经济考虑，以协调脱碳与经济开发目标。

Abstract: The Midwest, with its vast agricultural lands, is rapidly emerging as a key
region for utility-scale solar expansion. However, traditional power planning
has yet to integrate local economic impact directly into capacity expansion to
guide optimal siting decisions. Moreover, existing economic assessments tend to
emphasize local benefits while overlooking the opportunity costs of converting
productive farmland for solar development. This study addresses these gaps by
endogenously incorporating local economic metrics into a power system planning
model to evaluate how economic impacts influence solar siting, accounting for
the cost of lost agricultural output. We analyze all counties within the Great
Lakes region, constructing localized supply and marginal benefit curves that
are embedded within a multi-objective optimization framework aimed at
minimizing system costs and maximizing community economic benefits. Our
findings show that counties with larger economies and lower farmland
productivity deliver the highest local economic benefit per megawatt (MW) of
installed solar capacity. In Ohio, for example, large counties generate up to
$34,500 per MW, driven in part by high property tax revenues, while smaller
counties yield 31% less. Accounting for the opportunity cost of displaced
agricultural output reduces local benefits by up to 16%, depending on farmland
quality. A scenario prioritizing solar investment in counties with higher
economic returns increases total economic benefits by $1 billion (or 11%) by
2040, with solar investment shifting away from Michigan and Wisconsin (down by
39%) toward Ohio and Indiana (up by 75%), with only a marginal increase of 0.5%
in system-wide costs. These findings underscore the importance of integrating
economic considerations into utility-scale solar planning to better align
decarbonization goals with regional and local economic development.

</details>


### [95] [Spike-Kal: A Spiking Neuron Network Assisted Kalman Filter](https://arxiv.org/abs/2504.12703)
*Xun Xiao,Junbo Tie,Jinyue Zhao,Ziqi Wang,Yuan Li,Qiang Dou,Lei Wang*

Main category: eess.SY

TL;DR: 本文使用SNN优化Kalman滤波，减少噪声先验依赖，提高效率和准确性，错误降低18%-65%。


<details>
  <summary>Details</summary>
Motivation: Kalman滤波依赖系统建模和噪声统计准确性，这些在实际中难获取；SNN的非线性建模和自动特征提取提供改进机会。

Method: 提出SNN与Kalman滤波整合策略，训练SNN直接逼近最优增益矩阵，减轻计算负担。

Result: 平均错误比其它方法减少18%-65%。

Conclusion: 该方法提升Kalman滤波的适应性和效率，同时保持估计准确性和鲁棒性。

Abstract: Kalman filtering can provide an optimal estimation of the system state from
noisy observation data. This algorithm's performance depends on the accuracy of
system modeling and noise statistical characteristics, which are usually
challenging to obtain in practical applications. The powerful nonlinear
modeling capabilities of deep learning, combined with its ability to extract
features from large amounts of data automatically, offer new opportunities for
improving the Kalman filter. This paper proposes a novel method that leverages
the Spiking Neural Network to optimize the Kalman filter. Our approach aims to
reduce the reliance on prior knowledge of system and observation noises,
allowing for adaptation to varying statistical characteristics of time-varying
noise. Furthermore, we investigate the potential of SNNs in improving the
computational efficiency of the Kalman filter. In our method, we design an
integration strategy between the SNN and the Kalman filter. The SNN is trained
to directly approximate the optimal gain matrix from observation data, thereby
alleviating the computational burden of complex matrix operations inherent in
traditional Kalman filtering while maintaining the accuracy and robustness of
state estimation. Its average error has been reduced by 18\%-65\% compared with
other methods.

</details>


### [96] [Incorporating a Deep Neural Network into Moving Horizon Estimation for Embedded Thermal Torque Derating of an Electric Machine](https://arxiv.org/abs/2504.12736)
*Alexander Winkler,Pranav Shah,Katrin Baumgärtner,Vasu Sharma,David Gordon,Jakob Andert*

Main category: eess.SY

TL;DR: 本研究提出一种将DNN整合到MHE的新框架，用于电动车PMSM温度估计，展示了数据驱动方法的实时可行性。


<details>
  <summary>Details</summary>
Motivation: 从传统物理模型转向数据驱动技术，以评估DNN-based MHE在实时安全应用中的可行性。

Method: 训练LSTM-DNN模型于合成数据，整合到MHE的离散时间公式中，使用acados框架进行MiL模拟。

Result: 在噪声或故障条件下实现准确温度估计，并达到三倍实时能力，证明了实际部署的可行性。

Conclusion: 突显DNN-based MHE结合模型驱动和数据驱动优势的潜力，适用于实时安全关键应用。

Abstract: This study introduces a novel state estimation framework that incorporates
Deep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from
traditional physics-based models to rapidly developed data-driven techniques. A
DNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data
generated by a high-fidelity thermal model of a Permanent Magnet Synchronous
Machine (PMSM), which undergoes thermal derating as part of the torque control
strategy in a battery electric vehicle. The MHE is constructed by integrating
the trained DNN with a simplified driving dynamics model in a discrete-time
formulation, incorporating the LSTM hidden and cell states in the state vector
to retain system dynamics. The resulting optimal control problem (OCP) is
formulated as a nonlinear program (NLP) and implemented using the acados
framework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature
estimation, even under noisy sensor conditions or failures. Achieving threefold
real-time capability on embedded hardware confirms the feasibility of the
approach for practical deployment. The primary focus of this study is to assess
the feasibility of the MHE framework using a DNN-based plant model instead of
focusing on quantitative comparisons of vehicle performance. Overall, this
research highlights the potential of DNN-based MHE for real-time,
safety-critical applications by combining the strengths of model-based and
data-driven methods.

</details>


### [97] [Market-Driven Flexibility Provision: A Tri-Level Optimization Approach for Carbon Reduction](https://arxiv.org/abs/2504.12877)
*Shijie Pan,Gerrit Rolofs,Luca Pontecorvi,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 这篇论文提出一个激励框架，通过三层优化模型引导用户在低碳强度时使用能源，减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 可再生能源的不确定性和间歇性需要电力系统的灵活性；电价和碳强度不匹配可能导致用户在高碳强度时使用能源。

Method: 整合激励-based关税和三层优化模型，鼓励用户提交灵活性出价并获得奖励；使用修改后的IEEE-33 bus系统进行模拟。

Result: 模拟结果显示框架有效引导用户消费行为向低碳强度方向。

Conclusion: 框架在鼓励用户参与电力市场和减少碳排放方面有效。

Abstract: The integration of renewable energy resources (RES) in the power grid can
reduce carbon intensity, but also presents certain challenges. The uncertainty
and intermittent nature of RES emphasize the need for flexibility in power
systems. Moreover, there are noticeable mismatches between real-time
electricity prices and carbon intensity patterns throughout the day. These
discrepancies may lead customers to schedule energy-intensive tasks during the
early hours of the day, a period characterized by lower electricity prices but
higher carbon intensity. This paper introduces a novel and comprehensive
framework aimed at encouraging customer participation in electricity markets
and aligning their flexibility with carbon intensity trends. The proposed
approach integrates an incentive-based tariff with a tri-level optimization
model, where customers are motivated to submit flexibility bids and, in return,
receive financial rewards based on their contributions. The tri-level model
ensures a dynamic interaction between the market operation platform (MOP) and
end-users. Simulations are performed on a modified IEEE-33 bus system,
supported by two scenarios with different RES generations and customer
behaviors. Results demonstrate the effectiveness of the proposed framework in
guiding the customers' consumption behaviors towards low carbon intensity.

</details>


### [98] [Safe Physics-Informed Machine Learning for Dynamics and Control](https://arxiv.org/abs/2504.12952)
*Jan Drgona,Truong X. Nghiem,Thomas Beckers,Mahyar Fazlyab,Enrique Mallada,Colin Jones,Draguna Vrabie,Steven L. Brunton,Rolf Findeisen*

Main category: eess.SY

TL;DR: 这篇教程论文概述了在动态和控制中整合物理模型与安全保证的物理信息机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习提升复杂动态系统的建模和控制，但安全和稳定性是关键挑战，尤其在自动驾驶、机器人、医疗和能源等安全关键应用中。

Method: 探索安全约束方法，包括结构先验、Lyapunov函数、控制屏障函数、预测控制、投影、鲁棒优化、不确定性量化、可达性分析和神经网络验证。

Result: 通过示例演示了结合数据驱动和物理原则的安全学习框架，实现模型的安全性和稳定性。

Conclusion: 提供路径，实现复杂动态系统的安全控制，融合数据驱动方法与物理原理的严谨性。

Abstract: This tutorial paper focuses on safe physics-informed machine learning in the
context of dynamics and control, providing a comprehensive overview of how to
integrate physical models and safety guarantees. As machine learning techniques
enhance the modeling and control of complex dynamical systems, ensuring safety
and stability remains a critical challenge, especially in safety-critical
applications like autonomous vehicles, robotics, medical decision-making, and
energy systems. We explore various approaches for embedding and ensuring safety
constraints, such as structural priors, Lyapunov functions, Control Barrier
Functions, predictive control, projections, and robust optimization techniques,
ensuring that the learned models respect stability and safety criteria.
Additionally, we delve into methods for uncertainty quantification and safety
verification, including reachability analysis and neural network verification
tools, which help validate that control policies remain within safe operating
bounds even in uncertain environments. The paper includes illustrative examples
demonstrating the implementation aspects of safe learning frameworks that
combine the strengths of data-driven approaches with the rigor of physical
principles, offering a path toward the safe control of complex dynamical
systems.

</details>


### [99] [Adaptive Task Space Non-Singular Terminal Super-Twisting Sliding Mode Control of a 7-DOF Robotic Manipulator](https://arxiv.org/abs/2504.13056)
*L. Wan,S. Smith,Y. -J. Pan,E. Witrant*

Main category: eess.SY

TL;DR: 本文提出了一种带自适应增益的任务空间非奇异终端超扭转滑动模式控制器，用于7-DOF机器人操纵器的鲁棒轨迹跟踪，解决了抖动和干扰问题。


<details>
  <summary>Details</summary>
Motivation: 针对机器人操纵器在灵巧操作中的抖动、未知干扰和旋转运动跟踪挑战，提出改进方法以提升高自由度系统的性能。

Method: 开发NT-STSM控制器，包含边界性证明和增益选择指南，通过自适应技术处理干扰。

Result: 模拟和硬件实验显示，控制器在未知干扰下实现准确跟踪，减少控制努力，并优于传统控制器。

Conclusion: 该控制器减轻抖动和不稳定性，是机器人灵巧操作和工业应用的可靠解决方案。

Abstract: This paper presents a new task-space Non-singular Terminal Super-Twisting
Sliding Mode (NT-STSM) controller with adaptive gains for robust trajectory
tracking of a 7-DOF robotic manipulator. The proposed approach addresses the
challenges of chattering, unknown disturbances, and rotational motion tracking,
making it suited for high-DOF manipulators in dexterous manipulation tasks. A
rigorous boundedness proof is provided, offering gain selection guidelines for
practical implementation. Simulations and hardware experiments with external
disturbances demonstrate the proposed controller's robust, accurate tracking
with reduced control effort under unknown disturbances compared to other
NT-STSM and conventional controllers. The results demonstrated that the
proposed NT-STSM controller mitigates chattering and instability in complex
motions, making it a viable solution for dexterous robotic manipulations and
various industrial applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [100] [Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective](https://arxiv.org/abs/2504.12309)
*Yi-De Lin,Guan-Ze Liao*

Main category: cs.CY

TL;DR: 本研究开发AI驱动的知识图谱系统，分析可持续发展目标（SDG）的相互联系，并基于数据提出新目标。


<details>
  <summary>Details</summary>
Motivation: 2030年临近，SDG进展缓慢，需要创新策略加速全球优先事项。

Method: 使用官方SDG文本、Elsevier关键词数据集和1127个TED演讲稿，应用AI推测设计、大语言模型和检索增强生成，在2023年269个演讲的试点中构建知识图谱。

Result: 发现目标10和16有强关联，目标6覆盖最小；模拟对话揭示新中心节点；提出六个潜在新目标，聚焦公平性、韧性和技术包容。

Conclusion: 该框架为政策制定者提供新见解，并为未来SDG多模态应用奠定基础。

Abstract: From 2000 to 2015, the UN's Millennium Development Goals guided global
priorities. The subsequent Sustainable Development Goals (SDGs) adopted a more
dynamic approach, with annual indicator updates. As 2030 nears and progress
lags, innovative acceleration strategies are critical. This study develops an
AI-powered knowledge graph system to analyze SDG interconnections, discover
potential new goals, and visualize them online. Using official SDG texts,
Elsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot
on 269 talks from 2023 applies AI-speculative design, large language models,
and retrieval-augmented generation. Key findings include: (1) Heatmap analysis
reveals strong associations between Goal 10 and Goal 16, and minimal coverage
of Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new
central nodes, showing how richer data supports divergent thinking and goal
clarity. (3) Six potential new goals are proposed, centered on equity,
resilience, and technology-driven inclusion. This speculative-AI framework
offers fresh insights for policymakers and lays groundwork for future
multimodal and cross-system SDG applications.

</details>


### [101] [Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance](https://arxiv.org/abs/2504.12358)
*Aditi Verma,Elizabeth Williams*

Main category: cs.CY

TL;DR: 这篇论文主张为核领域人工智能建立预见性治理系统和全球人工智能观察站，以应对安全问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型迅速嵌入核能研究和工作，但其对安全、安全保障的影响尚未充分理解，因此需要探讨治理措施。

Method: 通过借鉴科学和技术研究、公共政策和前瞻性研究领域的成果，来探讨核人工智能观察站和预见性治理系统的轮廓。

Result: 论文提出建立预见性治理系统和全球人工智能观察站，以操作化预见性治理。

Conclusion: 需要创建一个针对核领域人工智能的预见性治理系统和全球观察站，以管理其潜在风险。

Abstract: AI models are rapidly becoming embedded in all aspects of nuclear energy
research and work but the safety, security, and safeguards consequences of this
embedding are not well understood. In this paper, we call for the creation of
an anticipatory system of governance for AI in the nuclear sector as well as
the creation of a global AI observatory as a means for operationalizing
anticipatory governance. The paper explores the contours of the nuclear AI
observatory and an anticipatory system of governance by drawing on work in
science and technology studies, public policy, and foresight studies.

</details>


### [102] [What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States](https://arxiv.org/abs/2504.12476)
*Andreas Jungherr,Adrian Rauchfleisch*

Main category: cs.CY

TL;DR: 这项研究通过调查德国和美国的公众偏好，探讨AI对齐期望，发现美国支持度更高，并分析影响因素。


<details>
  <summary>Details</summary>
Motivation: 公众对AI期望知之甚少，且国家间差异大，引发了对AI社会影响的担忧。

Method: 采用两项调查：在德国（n=1800）和美国（n=1756），评估对AI调节的准确性、安全、偏见缓解和理想化想象的支持。

Result: 美国受访者AI使用率和支持度均更高；两国均优先支持准确性和安全，公平等规范性目标在德国更谨慎；AI使用和言论自由态度影响德国偏好更大。

Conclusion: 为AI治理提供经验基础，强调基于公众态度的规范性期望在理论和政策中的重要性。

Abstract: Recent advances in generative Artificial Intelligence have raised public
awareness, shaping expectations and concerns about their societal implications.
Central to these debates is the question of AI alignment -- how well AI systems
meet public expectations regarding safety, fairness, and social values.
However, little is known about what people expect from AI-enabled systems and
how these expectations differ across national contexts. We present evidence
from two surveys of public preferences for key functional features of
AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We
examine support for four types of alignment in AI moderation: accuracy and
reliability, safety, bias mitigation, and the promotion of aspirational
imaginaries. U.S. respondents report significantly higher AI use and
consistently greater support for all alignment features, reflecting broader
technological openness and higher societal involvement with AI. In both
countries, accuracy and safety enjoy the strongest support, while more
normatively charged goals -- like fairness and aspirational imaginaries --
receive more cautious backing, particularly in Germany. We also explore how
individual experience with AI, attitudes toward free speech, political
ideology, partisan affiliation, and gender shape these preferences. AI use and
free speech support explain more variation in Germany. In contrast, U.S.
responses show greater attitudinal uniformity, suggesting that higher exposure
to AI may consolidate public expectations. These findings contribute to debates
on AI governance and cross-national variation in public preferences. More
broadly, our study demonstrates the value of empirically grounding AI alignment
debates in public attitudes and of explicitly developing normatively grounded
expectations into theoretical and policy discussions on the governance of
AI-generated content.

</details>


### [103] [Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice](https://arxiv.org/abs/2504.12545)
*Benign John Ihugba,Afsana Nasrin,Ling Wu,Lin Li,Lijun Qian,Xishuang Dong*

Main category: cs.CY

TL;DR: 这篇论文创建了第一个针对大规模枪击事件的数据集，使用命名实体识别（NER）和大语言模型（LLM）如GPT-4o进行关键信息提取。


<details>
  <summary>Details</summary>
Motivation: 大规模枪击事件产生大量非结构化数据，阻碍调查和政策制定，现有研究缺乏自动化提取方法。

Method: 采用NER技术结合LLM的少样本提示，从新闻、警报告和社交媒体中提取实体，如罪犯、受害者、地点和犯罪工具。

Result: 实验显示GPT-4o在微精度、微召回和微F1分数上最佳，o1-mini竞争力强，增加样本数显著提升性能。

Conclusion: LLM如GPT-4o在少样本NER任务中表现出色，提供高效工具支持法律和调查工作。

Abstract: Mass-shooting events pose a significant challenge to public safety,
generating large volumes of unstructured textual data that hinder effective
investigations and the formulation of public policy. Despite the urgency, few
prior studies have effectively automated the extraction of key information from
these events to support legal and investigative efforts. This paper presented
the first dataset designed for knowledge acquisition on mass-shooting events
through the application of named entity recognition (NER) techniques. It
focuses on identifying key entities such as offenders, victims, locations, and
criminal instruments, that are vital for legal and investigative purposes. The
NER process is powered by Large Language Models (LLMs) using few-shot
prompting, facilitating the efficient extraction and organization of critical
information from diverse sources, including news articles, police reports, and
social media. Experimental results on real-world mass-shooting corpora
demonstrate that GPT-4o is the most effective model for mass-shooting NER,
achieving the highest Micro Precision, Micro Recall, and Micro F1-scores.
Meanwhile, o1-mini delivers competitive performance, making it a
resource-efficient alternative for less complex NER tasks. It is also observed
that increasing the shot count enhances the performance of all models, but the
gains are more substantial for GPT-4o and o1-mini, highlighting their superior
adaptability to few-shot learning scenarios.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [104] [Specialized text classification: an approach to classifying Open Banking transactions](https://arxiv.org/abs/2504.12319)
*Duc Tuyen TA,Wajdi Ben Saad,Ji Young Oh*

Main category: cs.IR

TL;DR: 这篇论文介绍了一个针对法国银行交易的语言-based分类系统，使用NLP技术，针对法语文本，展示了比通用方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 响应PSD2法规，解决银行领域特定文本语料的NLP应用不足，以更好地理解客户行为、防止欺诈和提供定制服务。

Method: 系统包括数据收集、标注、预处理、建模和评估阶段，融入语言特定技术和领域知识，针对法国市场和法语文本。

Result: 与通用方法相比，系统显示出增强的性能和效率。

Conclusion: 定制化方法在处理银行等专业化领域文本时更有效。

Abstract: With the introduction of the PSD2 regulation in the EU which established the
Open Banking framework, a new window of opportunities has opened for banks and
fintechs to explore and enrich Bank transaction descriptions with the aim of
building a better understanding of customer behavior, while using this
understanding to prevent fraud, reduce risks and offer more competitive and
tailored services.
  And although the usage of natural language processing models and techniques
has seen an incredible progress in various applications and domains over the
past few years, custom applications based on domain-specific text corpus remain
unaddressed especially in the banking sector.
  In this paper, we introduce a language-based Open Banking transaction
classification system with a focus on the french market and french language
text. The system encompasses data collection, labeling, preprocessing,
modeling, and evaluation stages. Unlike previous studies that focus on general
classification approaches, this system is specifically tailored to address the
challenges posed by training a language model with a specialized text corpus
(Banking data in the French context). By incorporating language-specific
techniques and domain knowledge, the proposed system demonstrates enhanced
performance and efficiency compared to generic approaches.

</details>


### [105] [SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation](https://arxiv.org/abs/2504.12722)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.IR

TL;DR: 本论文提出SimUSER框架，作为推荐系统的代理，模拟用户行为以桥接离线评估与在线表现。


<details>
  <summary>Details</summary>
Motivation: 推荐系统评估面临离线指标与在线行为脱节的问题，且真实用户数据稀缺，受隐私限制。

Method: 引入SimUSER框架，从历史数据提取个性，构建具有persona、memory、perception和brain模块的代理，与系统交互。

Result: SimUSER与真实人类行为更接近；实验揭示缩略图对点击率、曝光效应和评论对参与度的影响；优化参数提升了真实世界用户参与度。

Conclusion: 通过SimUSER改进推荐系统参数，实现离线测试到在线性能的更好转化。

Abstract: Recommender systems play a central role in numerous real-life applications,
yet evaluating their performance remains a significant challenge due to the gap
between offline metrics and online behaviors. Given the scarcity and limits
(e.g., privacy issues) of real user data, we introduce SimUSER, an agent
framework that serves as believable and cost-effective human proxies. SimUSER
first identifies self-consistent personas from historical data, enriching user
profiles with unique backgrounds and personalities. Then, central to this
evaluation are users equipped with persona, memory, perception, and brain
modules, engaging in interactions with the recommender system. SimUSER exhibits
closer alignment with genuine humans than prior work, both at micro and macro
levels. Additionally, we conduct insightful experiments to explore the effects
of thumbnails on click rates, the exposure effect, and the impact of reviews on
user engagement. Finally, we refine recommender system parameters based on
offline A/B test results, resulting in improved user engagement in the real
world.

</details>


### [106] [Towards Lossless Token Pruning in Late-Interaction Retrieval Models](https://arxiv.org/abs/2504.12778)
*Yuxuan Zong,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本论文提出一种原则性方法修剪ColBERT模型标记，不影响检索分数，并通过实验证明使用仅30%的标记即可保持性能。


<details>
  <summary>Details</summary>
Motivation: ColBERT模型需大量内存存储文档标记上下文表示，现有的修剪方法无法保证不影响检索分数，因此需开发一种可靠的修剪策略。

Method: 引入三种正则化损失和两种修剪策略，以诱导高修剪率并确保不影响查询与文档间的分数计算。

Result: 实验在域内和域外验证，显示使用仅30%的标记即可保留ColBERT的性能。

Conclusion: 证明了通过原则性方法可以有效减少内存使用，同时维持检索效果。

Abstract: Late interaction neural IR models like ColBERT offer a competitive
effectiveness-efficiency trade-off across many benchmarks. However, they
require a huge memory space to store the contextual representation for all the
document tokens. Some works have proposed using either heuristics or
statistical-based techniques to prune tokens from each document. This however
doesn't guarantee that the removed tokens have no impact on the retrieval
score. Our work uses a principled approach to define how to prune tokens
without impacting the score between a document and a query. We introduce three
regularization losses, that induce a solution with high pruning ratios, as well
as two pruning strategies. We study them experimentally (in and out-domain),
showing that we can preserve ColBERT's performance while using only 30\% of the
tokens.

</details>


### [107] [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
*Nandan Thakur,Jimmy Lin,Sam Havens,Michael Carbin,Omar Khattab,Andrew Drozdov*

Main category: cs.IR

TL;DR: 这篇论文介绍了FreshStack，一个用于从社区问题和答案自动构建信息检索（IR）评估基准的框架。


<details>
  <summary>Details</summary>
Motivation: 动机是构建现实、可扩展和未受污染的IR和RAG评估基准，特别是针对快速增长的、最近的和利基主题。

Method: 方法包括：(1) 从代码和技术文档自动收集语料库，(2) 从社区问题和答案生成关键信息，(3) 使用检索技术的融合和混合架构进行文档检索。

Result: 结果显示，现有的检索模型在五个主题上显著低于理想方法，表明有改进空间；重新排序器在两个主题上没有明显提高准确性。

Conclusion: 结论是，希望FreshStack促进未来在构建现实IR和RAG评估基准方面的研究，并提供了数据集链接。

Abstract: We introduce FreshStack, a reusable framework for automatically building
information retrieval (IR) evaluation benchmarks from community-asked questions
and answers. FreshStack conducts the following steps: (1) automatic corpus
collection from code and technical documentation, (2) nugget generation from
community-asked questions and answers, and (3) nugget-level support, retrieving
documents using a fusion of retrieval techniques and hybrid architectures. We
use FreshStack to build five datasets on fast-growing, recent, and niche topics
to ensure the tasks are sufficiently challenging. On FreshStack, existing
retrieval models, when applied out-of-the-box, significantly underperform
oracle approaches on all five topics, denoting plenty of headroom to improve IR
quality. In addition, we identify cases where rerankers do not clearly improve
first-stage retrieval accuracy (two out of five topics). We hope that
FreshStack will facilitate future work toward constructing realistic, scalable,
and uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are
available at: https://fresh-stack.github.io.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [108] [Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination](https://arxiv.org/abs/2504.12714)
*Kunal Jha,Wilka Carvalho,Yancheng Liang,Simon S. Du,Max Kleiman-Weiner,Natasha Jaques*

Main category: cs.MA

TL;DR: 这篇论文探讨了通过在多个环境中强化学习来实现零样本协调（ZSC），使AI能与新伙伴在新任务中合作，而无需特定训练。


<details>
  <summary>Details</summary>
Motivation: 现有AI在单一任务上训练，无法泛化到新任务，因此需要开发通用合作技能以适应新伙伴。

Method: 使用Cross-Environment Cooperation (CEC)范式和Jax-based程序生成器，在环境分布上进行强化学习，创建大量协调挑战。

Result: CEC在与人类合作时定量和定性上优于基线，代理学习了有效的通用规范。

Conclusion: 结果表明，这种方法可设计无需人类数据的通用合作AI。

Abstract: Zero-shot coordination (ZSC), the ability to adapt to a new partner in a
cooperative task, is a critical component of human-compatible AI. While prior
work has focused on training agents to cooperate on a single task, these
specialized models do not generalize to new tasks, even if they are highly
similar. Here, we study how reinforcement learning on a distribution of
environments with a single partner enables learning general cooperative skills
that support ZSC with many new partners on many new problems. We introduce two
Jax-based, procedural generators that create billions of solvable coordination
challenges. We develop a new paradigm called Cross-Environment Cooperation
(CEC), and show that it outperforms competitive baselines quantitatively and
qualitatively when collaborating with real people. Our findings suggest that
learning to collaborate across many unique scenarios encourages agents to
develop general norms, which prove effective for collaboration with different
partners. Together, our results suggest a new route toward designing generalist
cooperative agents capable of interacting with humans without requiring human
data.

</details>


### [109] [The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems](https://arxiv.org/abs/2504.12735)
*Lidong Zhai,Zhijie Qiu,Xizhong Guo,Jiaqi Li*

Main category: cs.MA

TL;DR: 本文提出Academy of Athens多代理七层框架，解决AI艺术创作中多代理系统的协作挑战。


<details>
  <summary>Details</summary>
Motivation: 系统解决多代理系统在AI艺术创作中的挑战，如协作效率、角色分配、环境适应和任务并行。

Method: 提出七层框架，包括多代理协作等层级，并通过艺术创作实验验证。

Result: 框架在任务协作、跨场景适应和模型融合方面展示独特优势。

Conclusion: 框架提供结构化方法，促进AI艺术创新应用，并建议未来使用元学习和联邦学习等技术。

Abstract: This paper proposes the "Academy of Athens" multi-agent seven-layer
framework, aimed at systematically addressing challenges in multi-agent systems
(MAS) within artificial intelligence (AI) art creation, such as collaboration
efficiency, role allocation, environmental adaptation, and task parallelism.
The framework divides MAS into seven layers: multi-agent collaboration,
single-agent multi-role playing, single-agent multi-scene traversal,
single-agent multi-capability incarnation, different single agents using the
same large model to achieve the same target agent, single-agent using different
large models to achieve the same target agent, and multi-agent synthesis of the
same target agent. Through experimental validation in art creation, the
framework demonstrates its unique advantages in task collaboration, cross-scene
adaptation, and model fusion. This paper further discusses current challenges
such as collaboration mechanism optimization, model stability, and system
security, proposing future exploration through technologies like meta-learning
and federated learning. The framework provides a structured methodology for
multi-agent collaboration in AI art creation and promotes innovative
applications in the art field.

</details>


### [110] [Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis](https://arxiv.org/abs/2504.12777)
*James Rudd-Jones,Mirco Musolesi,María Pérez-Ortiz*

Main category: cs.MA

TL;DR: 本论文提出使用多代理强化学习（MARL）增强气候模拟，以克服传统方法在政策合成中的局限性。


<details>
  <summary>Details</summary>
Motivation: 气候政策开发面临不确定性、复杂动态和利益冲突的挑战，传统优化方法难以处理非线性动态和不确定性量化。

Method: 提出一个框架，将MARL应用于气候模拟，解决奖励定义、可伸缩性、不确定性传播和解决方案验证等挑战。

Result: 框架为更先进的政策探索提供基础，但需处理可解释性和实用性问题。

Conclusion: 框架有重要限制，需未来研究改进。

Abstract: Climate policy development faces significant challenges due to deep
uncertainty, complex system dynamics, and competing stakeholder interests.
Climate simulation methods, such as Earth System Models, have become valuable
tools for policy exploration. However, their typical use is for evaluating
potential polices, rather than directly synthesizing them. The problem can be
inverted to optimize for policy pathways, but the traditional optimization
approaches often struggle with non-linear dynamics, heterogeneous agents, and
comprehensive uncertainty quantification. We propose a framework for augmenting
climate simulations with Multi-Agent Reinforcement Learning (MARL) to address
these limitations. We identify key challenges at the interface between climate
simulations and the application of MARL in the context of policy synthesis,
including reward definition, scalability with increasing agents and state
spaces, uncertainty propagation across linked systems, and solution validation.
Additionally, we discuss challenges in making MARL-derived solutions
interpretable and useful for policy-makers. Our framework provides a foundation
for more sophisticated climate policy exploration while acknowledging important
limitations and areas for future research.

</details>


### [111] [QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?](https://arxiv.org/abs/2504.12961)
*Zhouyang Jiang,Bin Zhang,Airong Wei,Zhiwei Xu*

Main category: cs.MA

TL;DR: QLLM 是一种新算法，使用大型语言模型提升多智能体强化学习中的信用分配，表现优于现有基准，并具有良好可扩展性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中信用分配的挑战，包括归因不精确、可解释性有限和高维状态空间的可扩展性差。

Method: 提出 QLLM 算法，通过大型语言模型自动构建信用分配函数，引入 TFCAF 概念和 coder-evaluator 框架来生成、验证和优化代码。

Result: 在标准基准测试中优于现有最先进方法，具有强泛化能力和与多种 MARL 算法的兼容性。

Conclusion: QLLM 被定位为复杂多智能体场景中的有前景且通用的解决方案。

Abstract: Credit assignment has remained a fundamental challenge in multi-agent
reinforcement learning (MARL). Previous studies have primarily addressed this
issue through value decomposition methods under the centralized training with
decentralized execution paradigm, where neural networks are utilized to
approximate the nonlinear relationship between individual Q-values and the
global Q-value. Although these approaches have achieved considerable success in
various benchmark tasks, they still suffer from several limitations, including
imprecise attribution of contributions, limited interpretability, and poor
scalability in high-dimensional state spaces. To address these challenges, we
propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic
construction of credit assignment functions using large language models (LLMs).
Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit
allocation process is represented as a direct and expressive nonlinear
functional formulation. A custom-designed \textit{coder-evaluator} framework is
further employed to guide the generation, verification, and refinement of
executable code by LLMs, significantly mitigating issues such as hallucination
and shallow reasoning during inference. Extensive experiments conducted on
several standard MARL benchmarks demonstrate that the proposed method
consistently outperforms existing state-of-the-art baselines. Moreover, QLLM
exhibits strong generalization capability and maintains compatibility with a
wide range of MARL algorithms that utilize mixing networks, positioning it as a
promising and versatile solution for complex multi-agent scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [112] [Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation](https://arxiv.org/abs/2504.12436)
*Nairouz Mrabah,Nicolas Richet,Ismail Ben Ayed,Éric Granger*

Main category: cs.CV

TL;DR: This paper proposes a Sparse Optimization (SO) framework for adapting Vision-Language Models (VLMs) in few-shot settings, reducing overfitting and memory usage while improving performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in adapting VLMs to new domains with few labeled samples, such as severe overfitting, computational constraints, and limitations of existing methods like low-rank reparameterization.

Method: Introduces a Sparse Optimization framework with two paradigms: local sparsity and global density for minimal parameter updates, and local randomness and global importance for gradient sparsification based on random selection and importance pruning.

Result: Achieves state-of-the-art few-shot adaptation performance on 11 diverse datasets with reduced memory overhead.

Conclusion: The SO method effectively mitigates overfitting, ensures stable adaptation in low-data regimes, and enhances model efficiency and generalization.

Abstract: Adapting Vision-Language Models (VLMs) to new domains with few labeled
samples remains a significant challenge due to severe overfitting and
computational constraints. State-of-the-art solutions, such as low-rank
reparameterization, mitigate these issues but often struggle with
generalization and require extensive hyperparameter tuning. In this paper, a
novel Sparse Optimization (SO) framework is proposed. Unlike low-rank
approaches that typically constrain updates to a fixed subspace, our SO method
leverages high sparsity to dynamically adjust very few parameters. We introduce
two key paradigms. First, we advocate for \textit{local sparsity and global
density}, which updates a minimal subset of parameters per iteration while
maintaining overall model expressiveness. As a second paradigm, we advocate for
\textit{local randomness and global importance}, which sparsifies the gradient
using random selection while pruning the first moment based on importance. This
combination significantly mitigates overfitting and ensures stable adaptation
in low-data regimes. Extensive experiments on 11 diverse datasets show that SO
achieves state-of-the-art few-shot adaptation performance while reducing memory
overhead.

</details>


### [113] [AdaVid: Adaptive Video-Language Pretraining](https://arxiv.org/abs/2504.12513)
*Chaitanya Patel,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出AdaVid框架，实现高效可适应视频编码器，减少计算需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频编码器在计算受限设备部署困难和短视频处理限制问题。

Method: 引入AdaVid框架，使用适应性transformer块动态调整嵌入维度，并提出轻量级分层网络处理长视频。

Result: AdaVid-EgoVLP在半计算量下匹配EgoVLP性能，相同资源下优于EgoVLP；在长视频基准上实现计算效率与准确性平衡。

Conclusion: AdaVid框架提供灵活的视频编码器解决方案，能根据资源动态调整计算开销。

Abstract: Contrastive video-language pretraining has demonstrated great success in
learning rich and robust video representations. However, deploying such video
encoders on compute-constrained edge devices remains challenging due to their
high computational demands. Additionally, existing models are typically trained
to process only short video clips, often limited to 4 to 64 frames. In this
paper, we introduce AdaVid, a flexible architectural framework designed to
learn efficient video encoders that can dynamically adapt their computational
footprint based on available resources. At the heart of AdaVid is an adaptive
transformer block, inspired by Matryoshka Representation Learning, which allows
the model to adjust its hidden embedding dimension at inference time. We show
that AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D
dataset, matches the performance of the standard EgoVLP on short video-language
benchmarks using only half the compute, and even outperforms EgoVLP when given
equal computational resources. We further explore the trade-off between frame
count and compute on the challenging Diving48 classification benchmark, showing
that AdaVid enables the use of more frames without exceeding computational
limits. To handle longer videos, we also propose a lightweight hierarchical
network that aggregates short clip features, achieving a strong balance between
compute efficiency and accuracy across several long video benchmarks.

</details>


### [114] [Decision-based AI Visual Navigation for Cardiac Ultrasounds](https://arxiv.org/abs/2504.12535)
*Andy Dimnaku,Dominic Yurk,Zhiyuan Gao,Arun Padmanabhan,Mandar Aras,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 这篇论文提出了一种AI导航系统，用于帮助新手操作员识别心脏超声中的下腔静脉（IVC），以扩展诊断范围。


<details>
  <summary>Details</summary>
Motivation: 心脏超声检查需要专家和高品质设备，通常仅限于医院，AI被用于辅助新手获取标准视图并使诊断更易获取。

Method: 训练了一个基于心脏超声视频的离线二元分类决策模型，并整合了新颖的定位算法，利用学习特征实时标注IVC位置。

Result: 模型在高质量医院视频上表现出强定位性能，并在低质量Butterfly iQ手持设备视频上实现了零样本出色性能。

Conclusion: 该系统正在临床试验中，并已在Butterfly iQ应用中可用，有助于将超声诊断扩展到医院外。

Abstract: Ultrasound imaging of the heart (echocardiography) is widely used to diagnose
cardiac diseases. However, obtaining an echocardiogram requires an expert
sonographer and a high-quality ultrasound imaging device, which are generally
only available in hospitals. Recently, AI-based navigation models and
algorithms have been used to aid novice sonographers in acquiring the
standardized cardiac views necessary to visualize potential disease
pathologies. These navigation systems typically rely on directional guidance to
predict the necessary rotation of the ultrasound probe. This paper demonstrates
a novel AI navigation system that builds on a decision model for identifying
the inferior vena cava (IVC) of the heart. The decision model is trained
offline using cardiac ultrasound videos and employs binary classification to
determine whether the IVC is present in a given ultrasound video. The
underlying model integrates a novel localization algorithm that leverages the
learned feature representations to annotate the spatial location of the IVC in
real-time. Our model demonstrates strong localization performance on
traditional high-quality hospital ultrasound videos, as well as impressive
zero-shot performance on lower-quality ultrasound videos from a more affordable
Butterfly iQ handheld ultrasound machine. This capability facilitates the
expansion of ultrasound diagnostics beyond hospital settings. Currently, the
guidance system is undergoing clinical trials and is available on the Butterfly
iQ app.

</details>


### [115] [Privacy-Preserving Operating Room Workflow Analysis using Digital Twins](https://arxiv.org/abs/2504.12552)
*Alejandra Perez,Han Zhang,Yu-Chun Ku,Lalithkumar Seenivasan,Roger Soberanis,Jose L. Porras,Richard Day,Jeff Jopling,Peter Najjar,Mathias Unberath*

Main category: cs.CV

TL;DR: This paper proposes a privacy-preserving method using Digital Twins for operating room (OR) event detection, achieving performance comparable to traditional RGB-based approaches.


<details>
  <summary>Details</summary>
Motivation: To optimize OR workflows by addressing privacy concerns that limit the use of computer vision in automated event detection from videos.

Method: A two-stage pipeline: first, use vision foundation models for depth estimation and semantic segmentation to create de-identified Digital Twins from RGB videos; second, apply the SafeOR model to detect events from segmentation masks and depth maps.

Result: The Digital Twin-based approach performs on par or better than raw RGB models in detecting OR events, evaluated on a dataset of 38 simulated surgical trials with five event classes.

Conclusion: Digital Twins enable privacy-preserving OR workflow analysis, facilitate data sharing across institutions, and enhance model generalizability by reducing domain-specific differences.

Abstract: Purpose: The operating room (OR) is a complex environment where optimizing
workflows is critical to reduce costs and improve patient outcomes. The use of
computer vision approaches for the automatic recognition of perioperative
events enables identification of bottlenecks for OR optimization. However,
privacy concerns limit the use of computer vision for automated event detection
from OR videos, which makes privacy-preserving approaches needed for OR
workflow analysis. Methods: We propose a two-stage pipeline for
privacy-preserving OR video analysis and event detection. In the first stage,
we leverage vision foundation models for depth estimation and semantic
segmentation to generate de-identified Digital Twins (DT) of the OR from
conventional RGB videos. In the second stage, we employ the SafeOR model, a
fused two-stream approach that processes segmentation masks and depth maps for
OR event detection. We evaluate this method on an internal dataset of 38
simulated surgical trials with five event classes. Results: Our results
indicate that this DT-based approach to the OR event detection model achieves
performance on par and sometimes even better than raw RGB video-based models on
detecting OR events. Conclusion: DTs enable privacy-preserving OR workflow
analysis, facilitating the sharing of de-identified data across institutions
and they can potentially enhance model generalizability by mitigating
domain-specific appearance differences.

</details>


### [116] [CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework](https://arxiv.org/abs/2504.12576)
*Wentao Wu,Xiao Wang,Chenglong Li,Bo Jiang,Jin Tang,Bin Luo,Qi Liu*

Main category: cs.CV

TL;DR: 提出CM3AE框架，用于RGB-Event感知预训练，支持多模态输入，提升融合能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有事件数据预训练方法与RGB帧连接不足，限制多模态融合适用性的问题。

Method: 设计CM3AE框架，输入包括RGB图像、事件图像和事件体素；采用多模态融合重建模块和对比学习策略。

Result: 构建2,535,759对RGB-Event数据对数据集，在五个下游任务上实验证明框架有效性。

Conclusion: CM3AE框架显著提升多模态感知能力，代码和预训练模型将开源。

Abstract: Event cameras have attracted increasing attention in recent years due to
their advantages in high dynamic range, high temporal resolution, low power
consumption, and low latency. Some researchers have begun exploring
pre-training directly on event data. Nevertheless, these efforts often fail to
establish strong connections with RGB frames, limiting their applicability in
multi-modal fusion scenarios. To address these issues, we propose a novel CM3AE
pre-training framework for the RGB-Event perception. This framework accepts
multi-modalities/views of data as input, including RGB images, event images,
and event voxels, providing robust support for both event-based and RGB-event
fusion based downstream tasks. Specifically, we design a multi-modal fusion
reconstruction module that reconstructs the original image from fused
multi-modal features, explicitly enhancing the model's ability to aggregate
cross-modal complementary information. Additionally, we employ a multi-modal
contrastive learning strategy to align cross-modal feature representations in a
shared latent space, which effectively enhances the model's capability for
multi-modal understanding and capturing global dependencies. We construct a
large-scale dataset containing 2,535,759 RGB-Event data pairs for the
pre-training. Extensive experiments on five downstream tasks fully demonstrated
the effectiveness of CM3AE. Source code and pre-trained models will be released
on https://github.com/Event-AHU/CM3AE.

</details>


### [117] [Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation](https://arxiv.org/abs/2504.12606)
*Changsheng Lv,Mengshi Qi,Zijian Fu,Huadong Ma*

Main category: cs.CV

TL;DR: 本论文提出Robo-SGG方法，利用布局信息提升场景图生成在损坏图像下的鲁棒性，实验显示显著性能改善。


<details>
  <summary>Details</summary>
Motivation: 现有场景图生成方法在损坏图像上性能下降，核心挑战是域移，需要利用域不变的布局信息增强鲁棒性。

Method: 使用Instance Normalization过滤域特定特征，并通过Layout-Oriented Restitution恢复结构特征；引入Layout-Embedded Encoder作为plug-and-play组件增强对象和谓词特征。

Result: 在VG-C数据集上，PredCls、SGCls、SGDet任务的mR@50分别相对提升5.6%、8.0%、6.5%；在VG-C和GQA-C基准上达到新SOTA。

Conclusion: 方法有效，提升了鲁棒场景图生成性能，并计划发布源代码和模型。

Abstract: In this paper, we introduce a novel method named Robo-SGG, i.e.,
Layout-Oriented Normalization and Restitution for Robust Scene Graph
Generation. Compared to the existing SGG setting, the robust scene graph
generation aims to perform inference on a diverse range of corrupted images,
with the core challenge being the domain shift between the clean and corrupted
images. Existing SGG methods suffer from degraded performance due to
compromised visual features e.g., corruption interference or occlusions. To
obtain robust visual features, we exploit the layout information, which is
domain-invariant, to enhance the efficacy of existing SGG methods on corrupted
images. Specifically, we employ Instance Normalization(IN) to filter out the
domain-specific feature and recover the unchangeable structural features, i.e.,
the positional and semantic relationships among objects by the proposed
Layout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder
(LEE) that augments the existing object and predicate encoders within the SGG
framework, enriching the robust positional and semantic features of objects and
predicates. Note that our proposed Robo-SGG module is designed as a
plug-and-play component, which can be easily integrated into any baseline SGG
model. Extensive experiments demonstrate that by integrating the
state-of-the-art method into our proposed Robo-SGG, we achieve relative
improvements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet
tasks on the VG-C dataset, respectively, and achieve new state-of-the-art
performance in corruption scene graph generation benchmark (VG-C and GQA-C). We
will release our source code and model.

</details>


### [118] [Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping](https://arxiv.org/abs/2504.12368)
*Babak Ghassemi,Cassio Fraga-Dantas,Raffaele Gaetano,Dino Ienco,Omid Ghorbanzadeh,Emma Izquierdo-Verdiguier,Francesco Vuolo*

Main category: cs.CV

TL;DR: 论文提出 BRIDGE-LC 框架，整合多尺度地理空间信息以提升土地覆盖分类的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习和深度学习算法在分析地球观测数据时忽略了地理空间元数据，这可能提高跨区域规模的准确性和可扩展性。

Method: 提出 BRIDGE-LC 框架，使用轻量级多层感知器整合细粒度（纬度/经度）和粗粒度（生物地理区域）空间信息，训练时使用两者，推断仅需细粒度信息。

Result: 实验结果显示，整合空间信息改善了土地覆盖映射性能，尤其在联合使用细粒度和粗粒度信息时，表现最佳。

Conclusion: 整合地理空间信息可显著提高土地覆盖映射的性能和效率。

Abstract: Land use and land cover mapping from Earth Observation (EO) data is a
critical tool for sustainable land and resource management. While advanced
machine learning and deep learning algorithms excel at analyzing EO imagery
data, they often overlook crucial geospatial metadata information that could
enhance scalability and accuracy across regional, continental, and global
scales. To address this limitation, we propose BRIDGE-LC (Bi-level
Representation Integration for Disentangled GEospatial Land Cover), a novel
deep learning framework that integrates multi-scale geospatial information into
the land cover classification process. By simultaneously leveraging
fine-grained (latitude/longitude) and coarse-grained (biogeographical region)
spatial information, our lightweight multi-layer perceptron architecture learns
from both during training but only requires fine-grained information for
inference, allowing it to disentangle region-specific from region-agnostic land
cover features while maintaining computational efficiency. To assess the
quality of our framework, we use an open-access in-situ dataset and adopt
several competing classification approaches commonly considered for large-scale
land cover mapping. We evaluated all approaches through two scenarios: an
extrapolation scenario in which training data encompasses samples from all
biogeographical regions, and a leave-one-region-out scenario where one region
is excluded from training. We also explore the spatial representation learned
by our model, highlighting a connection between its internal manifold and the
geographical information used during training. Our results demonstrate that
integrating geospatial information improves land cover mapping performance,
with the most substantial gains achieved by jointly leveraging both fine- and
coarse-grained spatial information.

</details>


### [119] [NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results](https://arxiv.org/abs/2504.12711)
*Xin Li,Yeying Jin,Xin Jin,Zongwei Wu,Bingchen Li,Yufei Wang,Wenhan Yang,Yu Li,Zhibo Chen,Bihan Wen,Robby T. Tan,Radu Timofte,Qiyu Rong,Hongyuan Jing,Mengmeng Zhang,Jinglong Li,Xiangyu Lu,Yi Ren,Yuting Liu,Meng Zhang,Xiang Chen,Qiyuan Guan,Jiangxin Dong,Jinshan Pan,Conglin Gou,Qirui Yang,Fangpu Zhang,Yunlong Lin,Sixiang Chen,Guoxi Huang,Ruirui Lin,Yan Zhang,Jingyu Yang,Huanjing Yue,Jiyuan Chen,Qiaosi Yi,Hongjun Wang,Chenxi Xie,Shuai Li,Yuhui Wu,Kaiyi Ma,Jiakui Hu,Juncheng Li,Liwen Pan,Guangwei Gao,Wenjie Li,Zhenyu Jin,Heng Guo,Zhanyu Ma,Yubo Wang,Jinghua Wang,Wangzhi Xing,Anjusree Karnavar,Diqi Chen,Mohammad Aminul Islam,Hao Yang,Ruikun Zhang,Liyuan Pan,Qianhao Luo,XinCao,Han Zhou,Yan Min,Wei Dong,Jun Chen,Taoyi Wu,Weijia Dou,Yu Wang,Shengjie Zhao,Yongcheng Huang,Xingyu Han,Anyan Huang,Hongtao Wu,Hong Wang,Yefeng Zheng,Abhijeet Kumar,Aman Kumar,Marcos V. Conde,Paula Garrido,Daniel Feijoo,Juan C. Benito,Guanglu Dong,Xin Lin,Siyuan Liu,Tianheng Zheng,Jiayu Zhong,Shouyi Wang,Xiangtai Li,Lanqing Guo,Lu Qi,Chao Ren,Shuaibo Wang,Shilong Zhang,Wanyu Zhou,Yunze Wu,Qinzhong Tan,Jieyuan Pei,Zhuoxuan Li,Jiayu Wang,Haoyu Bian,Haoran Sun,Subhajit Paul,Ni Tang,Junhao Huang,Zihan Cheng,Hongyun Zhu,Yuehan Wu,Kaixin Deng,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhizun Luo,Zeyu Xiao,Zhuoyuan Li,Nguyen Pham Hoang Le,An Dinh Thien,Son T. Luu,Kiet Van Nguyen,Ronghua Xu,Xianmin Tian,Weijian Zhou,Jiacheng Zhang,Yuqian Chen,Yihang Duan,Yujie Wu,Suresh Raikwar,Arsh Garg,Kritika,Jianhua Zheng,Xiaoshan Ma,Ruolin Zhao,Yongyu Yang,Yongsheng Liang,Guiming Huang,Qiang Li,Hongbin Zhang,Xiangyu Zheng,A. N. Rajagopalan*

Main category: cs.CV

TL;DR: 这篇论文回顾了NTIRE 2025挑战赛，聚焦日夜雨滴去除的双焦点图像，使用新数据集Raindrop Clarity建立了基准。


<details>
  <summary>Details</summary>
Motivation: 现有去雨数据集多样性和挑战性不足，需要更全面的基准来评估不同光照和焦点条件下的雨滴去除技术。

Method: 通过收集Raindrop Clarity数据集（包括日夜雨滴/背景焦点退化），组织挑战赛，提供训练（14139张）、验证（240张）和测试（731张）子集，评估参赛方案。

Result: 有361名参与者，32个团队提交有效方案，在Raindrop Clarity数据集上达到了最先进性能。

Conclusion: 该挑战赛建立了新的强大基准，促进了雨滴去除任务在复杂条件下的发展。

Abstract: This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal
for Dual-Focused Images. This challenge received a wide range of impressive
solutions, which are developed and evaluated using our collected real-world
Raindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop
Clarity dataset is more diverse and challenging in degradation types and
contents, which includes day raindrop-focused, day background-focused, night
raindrop-focused, and night background-focused degradations. This dataset is
divided into three subsets for competition: 14,139 images for training, 240
images for validation, and 731 images for testing. The primary objective of
this challenge is to establish a new and powerful benchmark for the task of
removing raindrops under varying lighting and focus conditions. There are a
total of 361 participants in the competition, and 32 teams submitting valid
solutions and fact sheets for the final testing phase. These submissions
achieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.
The project can be found at
https://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.

</details>


### [120] [Post-pre-training for Modality Alignment in Vision-Language Foundation Models](https://arxiv.org/abs/2504.12717)
*Shin'ya Yamaguchi,Dewei Feng,Sekitoshi Kanai,Kazuki Adachi,Daiki Chijiwa*

Main category: cs.CV

TL;DR: 本论文提出CLIP-Refine，一种后预训练方法，用于减少CLIP模型的模态间隙，并在小数据集上训练，提高零样本性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型虽在零样本任务上表现良好，但存在模态间隙，限制下游任务性能；现有方法训练成本高或导致性能下降。

Method: 引入随机特征对齐(RaFA)和混合对比蒸馏(HyCD)技术，通过一轮训练在小数据集上对齐图像和文本特征空间。

Result: 实验在多个分类和检索任务上显示，CLIP-Refine成功缓解模态间隙，并提升零样本性能。

Conclusion: CLIP-Refine提供了一种高效的后预训练方法，能在不牺牲零样本性能的情况下改善特征对齐。

Abstract: Contrastive language image pre-training (CLIP) is an essential component of
building modern vision-language foundation models. While CLIP demonstrates
remarkable zero-shot performance on downstream tasks, the multi-modal feature
spaces still suffer from a modality gap, which is a gap between image and text
feature clusters and limits downstream task performance. Although existing
works attempt to address the modality gap by modifying pre-training or
fine-tuning, they struggle with heavy training costs with large datasets or
degradations of zero-shot performance. This paper presents CLIP-Refine, a
post-pre-training method for CLIP models at a phase between pre-training and
fine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training
on small image-text datasets without zero-shot performance degradations. To
this end, we introduce two techniques: random feature alignment (RaFA) and
hybrid contrastive-distillation (HyCD). RaFA aligns the image and text features
to follow a shared prior distribution by minimizing the distance to random
reference vectors sampled from the prior. HyCD updates the model with hybrid
soft labels generated by combining ground-truth image-text pair labels and
outputs from the pre-trained CLIP model. This contributes to achieving both
maintaining the past knowledge and learning new knowledge to align features.
Our extensive experiments with multiple classification and retrieval tasks show
that CLIP-Refine succeeds in mitigating the modality gap and improving the
zero-shot performance.

</details>


### [121] [Parsimonious Dataset Construction for Laparoscopic Cholecystectomy Structure Segmentation](https://arxiv.org/abs/2504.12573)
*Yuning Zhou,Henry Badgery,Matthew Read,James Bailey,Catherine Davey*

Main category: cs.CV

TL;DR: 使用主动学习减少医疗图像标注成本，构建高效数据集，提高DNN性能。


<details>
  <summary>Details</summary>
Motivation: 医疗标注成本高，阻碍深度学习应用；引入主动学习构建高质量、低成本腹腔镜胆囊切除术数据集。

Method: 采用主动学习选择手术视频帧；DNNs通过现有数据集识别最信息量数据；评估不同数据信息量测量，优先使用深度特征距离。

Result: 主动学习选择一半数据，DNNs mIoU达0.4349，与全数据集0.4374几乎相当。

Conclusion: 证明主动学习有效，减少数据标注需求，同时提升DNN性能和泛化能力。

Abstract: Labeling has always been expensive in the medical context, which has hindered
related deep learning application. Our work introduces active learning in
surgical video frame selection to construct a high-quality, affordable
Laparoscopic Cholecystectomy dataset for semantic segmentation. Active learning
allows the Deep Neural Networks (DNNs) learning pipeline to include the dataset
construction workflow, which means DNNs trained by existing dataset will
identify the most informative data from the newly collected data. At the same
time, DNNs' performance and generalization ability improve over time when the
newly selected and annotated data are included in the training data. We
assessed different data informativeness measurements and found the deep
features distances select the most informative data in this task. Our
experiments show that with half of the data selected by active learning, the
DNNs achieve almost the same performance with 0.4349 mean Intersection over
Union (mIoU) compared to the same DNNs trained on the full dataset (0.4374
mIoU) on the critical anatomies and surgical instruments.

</details>


### [122] [Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts](https://arxiv.org/abs/2504.12782)
*Leyang Li,Shilin Lu,Yan Ren,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 本文提出ANT框架，用于自动引导去噪轨迹避免 unwanted concepts，实现高效且无 artifacts 的概念擦除。


<details>
  <summary>Details</summary>
Motivation: 解决现有概念擦除方法的局限性，如锚点自由方法导致视觉 artifacts，锚点基于方法依赖启发式锚点选择，确保文本到图像模型的道德部署。

Method: ANT框架通过逆转条件方向的分类器自由引导和轨迹感知目标函数修改内容；针对单个概念，使用增强权重显着性地图识别关键参数；针对多概念，提供即插即用方案。

Result: 实验显示ANT在单多概念擦除中达到最先进性能，提供高质量、安全输出而不降低生成保真度。

Conclusion: ANT是一种高效方法，提升文本到图像模型的安全性，适用于实际部署。

Abstract: Ensuring the ethical deployment of text-to-image models requires effective
techniques to prevent the generation of harmful or inappropriate content. While
concept erasure methods offer a promising solution, existing finetuning-based
approaches suffer from notable limitations. Anchor-free methods risk disrupting
sampling trajectories, leading to visual artifacts, while anchor-based methods
rely on the heuristic selection of anchor concepts. To overcome these
shortcomings, we introduce a finetuning framework, dubbed ANT, which
Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is
built on a key insight: reversing the condition direction of classifier-free
guidance during mid-to-late denoising stages enables precise content
modification without sacrificing early-stage structural integrity. This
inspires a trajectory-aware objective that preserves the integrity of the
early-stage score function field, which steers samples toward the natural image
manifold, without relying on heuristic anchor concept selection. For
single-concept erasure, we propose an augmentation-enhanced weight saliency map
to precisely identify the critical parameters that most significantly
contribute to the unwanted concept, enabling more thorough and efficient
erasure. For multi-concept erasure, our objective function offers a versatile
plug-and-play solution that significantly boosts performance. Extensive
experiments demonstrate that ANT achieves state-of-the-art results in both
single and multi-concept erasure, delivering high-quality, safe outputs without
compromising the generative fidelity. Code is available at
https://github.com/lileyang1210/ANT

</details>


### [123] [Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization](https://arxiv.org/abs/2504.12807)
*Ach Khozaimi,Isnani Darti,Syaiful Anam,Wuryansari Muharini Kusumawinahyu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种优化Dense-UNet201的方法，用于Pap涂片图像分割，提高子宫颈癌诊断准确性。


<details>
  <summary>Details</summary>
Motivation: Pap涂片图像分割对子宫颈癌诊断至关重要，但传统模型难以处理复杂的细胞结构和图像变异。

Method: 提出混合Dense-UNet201，使用预训练DenseNet201作为U-Net编码器，并通过修改后的蜘蛛猴优化（SMO）算法进行优化。

Result: 在SIPaKMeD数据集上，SMO Dense-UNet201实现了96.16%的准确率、91.63%的IoU和95.63%的Dice系数，优于U-Net、Res-UNet50和Efficient-UNetB0。

Conclusion: 这些发现突显了图像预处理、预训练模型和元启发式优化在医疗图像分析中的有效性，并为子宫颈细胞分割提供新见解。

Abstract: Pap smear image segmentation is crucial for cervical cancer diagnosis.
However, traditional segmentation models often struggle with complex cellular
structures and variations in pap smear images. This study proposes a hybrid
Dense-UNet201 optimization approach that integrates a pretrained DenseNet201 as
the encoder for the U-Net architecture and optimizes it using the spider monkey
optimization (SMO) algorithm. The Dense-UNet201 model excelled at feature
extraction. The SMO was modified to handle categorical and discrete parameters.
The SIPaKMeD dataset was used in this study and evaluated using key performance
metrics, including loss, accuracy, Intersection over Union (IoU), and Dice
coefficient. The experimental results showed that Dense-UNet201 outperformed
U-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a
segmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score
of 95.63%. These findings underscore the effectiveness of image preprocessing,
pretrained models, and metaheuristic optimization in improving medical image
analysis and provide new insights into cervical cell segmentation methods.

</details>


### [124] [Image-Editing Specialists: An RLAIF Approach for Diffusion Models](https://arxiv.org/abs/2504.12833)
*Elior Benarous,Yilun Du,Heng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过在线强化学习训练图像编辑扩散模型，提高了与用户指令的对齐性和真实性，只需少量数据即可实现精确编辑。


<details>
  <summary>Details</summary>
Motivation: 解决图像编辑中结构保留和语义对齐的挑战，并减少对大规模人类标注数据集的依赖。

Method: 引入在线强化学习框架，利用视觉提示和仅5张参考图像，在10个训练步骤内对扩散模型进行训练。

Result: 模型实现了复杂场景下的精确编辑，保持了无关区域的高保真度，并在机器人应用中提升了模拟环境的真实性。

Conclusion: 该方法简化了用户编辑过程，并证明了其在机器人学等领域的实用价值。

Abstract: We present a novel approach to training specialized instruction-based
image-editing diffusion models, addressing key challenges in structural
preservation with input images and semantic alignment with user prompts. We
introduce an online reinforcement learning framework that aligns the diffusion
model with human preferences without relying on extensive human annotations or
curating a large dataset. Our method significantly improves the realism and
alignment with instructions in two ways. First, the proposed models achieve
precise and structurally coherent modifications in complex scenes while
maintaining high fidelity in instruction-irrelevant areas. Second, they capture
fine nuances in the desired edit by leveraging a visual prompt, enabling
detailed control over visual edits without lengthy textual prompts. This
approach simplifies users' efforts to achieve highly specific edits, requiring
only 5 reference images depicting a certain concept for training. Experimental
results demonstrate that our models can perform intricate edits in complex
scenes, after just 10 training steps. Finally, we showcase the versatility of
our method by applying it to robotics, where enhancing the visual realism of
simulated environments through targeted sim-to-real image edits improves their
utility as proxies for real-world settings.

</details>


### [125] [Disentangling Polysemantic Channels in Convolutional Neural Networks](https://arxiv.org/abs/2504.12939)
*Robin Hesse,Jonas Fischer,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出算法解开CNN多义通道，提高可解释性。


<details>
  <summary>Details</summary>
Motivation: CNN学习多义通道编码不同概念，解释困难。

Method: 算法重构权重，利用前层激活模式，将多义通道拆分为单一概念通道。

Result: 提升CNN可解释性，改进特征可视化等技术。

Conclusion: 最终增强神经网络的解释能力。

Abstract: Mechanistic interpretability is concerned with analyzing individual
components in a (convolutional) neural network (CNN) and how they form larger
circuits representing decision mechanisms. These investigations are challenging
since CNNs frequently learn polysemantic channels that encode distinct
concepts, making them hard to interpret. To address this, we propose an
algorithm to disentangle a specific kind of polysemantic channel into multiple
channels, each responding to a single concept. Our approach restructures
weights in a CNN, utilizing that different concepts within the same channel
exhibit distinct activation patterns in the previous layer. By disentangling
these polysemantic features, we enhance the interpretability of CNNs,
ultimately improving explanatory techniques such as feature visualizations.

</details>


### [126] [Vision and Language Integration for Domain Generalization](https://arxiv.org/abs/2504.12966)
*Yanmei Wang,Xiyao Liu,Fupeng Chu,Zhi Han*

Main category: cs.CV

TL;DR: 本论文提出VLCA方法，使用语言空间作为桥梁改善视觉领域的域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 由于图像域间差距和缺乏合适基元，难以找到可靠特征空间；语言具有语义完整性，因此使用语言桥接。

Method: 提出VLCA，通过语言空间捕获类别间语义表示，视觉空间探索共同模式，并对齐多模态空间。

Result: 实验证明方法有效。

Conclusion: VLCA提升了模型在未知域上的泛化能力。

Abstract: Domain generalization aims at training on source domains to uncover a
domain-invariant feature space, allowing the model to perform robust
generalization ability on unknown target domains. However, due to domain gaps,
it is hard to find reliable common image feature space, and the reason for that
is the lack of suitable basic units for images. Different from image in vision
space, language has comprehensive expression elements that can effectively
convey semantics. Inspired by the semantic completeness of language and
intuitiveness of image, we propose VLCA, which combine language space and
vision space, and connect the multiple image domains by using semantic space as
the bridge domain. Specifically, in language space, by taking advantage of the
completeness of language basic units, we tend to capture the semantic
representation of the relations between categories through word vector
distance. Then, in vision space, by taking advantage of the intuitiveness of
image features, the common pattern of sample features with the same class is
explored through low-rank approximation. In the end, the language
representation is aligned with the vision representation through the multimodal
space of text and image. Experiments demonstrate the effectiveness of the
proposed method.

</details>


### [127] [ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models](https://arxiv.org/abs/2504.13061)
*Linkang Du,Zheng Zhu,Min Chen,Zhou Su,Shouling Ji,Peng Cheng,Jiming Chen,Zhikun Zhang*

Main category: cs.CV

TL;DR: 本文提出ArtistAuditor方法，通过分析风格特征审计文本到图像模型是否使用了特定艺术家的作品，实现了高准确率（AUC > 0.937）。


<details>
  <summary>Details</summary>
Motivation: 解决用户使用扩散模型模仿艺术家画作导致的版权侵权问题，尤其当艺术品或模型已在线发布，无法修改时。

Method: 使用风格提取器获取多粒度风格表示，将艺术品视为风格采样，并通过训练的判别器进行审计决策。

Result: 实验在六个模型-数据集组合上AUC值超过0.937，证明了方法的转移性和实际有效性。

Conclusion: 提供了实际实施的见解，通过在线平台场景验证了有效性，并开源代码。

Abstract: Text-to-image models based on diffusion processes, such as DALL-E, Stable
Diffusion, and Midjourney, are capable of transforming texts into detailed
images and have widespread applications in art and design. As such, amateur
users can easily imitate professional-level paintings by collecting an artist's
work and fine-tuning the model, leading to concerns about artworks' copyright
infringement. To tackle these issues, previous studies either add visually
imperceptible perturbation to the artwork to change its underlying styles
(perturbation-based methods) or embed post-training detectable watermarks in
the artwork (watermark-based methods). However, when the artwork or the model
has been published online, i.e., modification to the original artwork or model
retraining is not feasible, these strategies might not be viable.
  To this end, we propose a novel method for data-use auditing in the
text-to-image generation model. The general idea of ArtistAuditor is to
identify if a suspicious model has been finetuned using the artworks of
specific artists by analyzing the features related to the style. Concretely,
ArtistAuditor employs a style extractor to obtain the multi-granularity style
representations and treats artworks as samplings of an artist's style. Then,
ArtistAuditor queries a trained discriminator to gain the auditing decisions.
The experimental results on six combinations of models and datasets show that
ArtistAuditor can achieve high AUC values (> 0.937). By studying
ArtistAuditor's transferability and core modules, we provide valuable insights
into the practical implementation. Finally, we demonstrate the effectiveness of
ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor
is open-sourced at https://github.com/Jozenn/ArtistAuditor.

</details>


### [128] [Pose and Facial Expression Transfer by using StyleGAN](https://arxiv.org/abs/2504.13021)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TL;DR: 本论文提出一种自监督方法，使用StyleGAN2在人脸图像间转移姿势和表情。


<details>
  <summary>Details</summary>
Motivation: 动机是实现无需手动标注的可控人脸合成，以应用于身份保持下的图像生成。

Method: 方法包括两个编码器和映射网络，将输入投影到StyleGAN2潜在空间，并通过自监督视频训练生成输出。

Result: 结果显示模型可合成随机身份图像，并实现姿势和表情控制，达到接近实时性能。

Conclusion: 结论是该方法高效且有效，提升了人脸转移任务的实用性。

Abstract: We propose a method to transfer pose and expression between face images.
Given a source and target face portrait, the model produces an output image in
which the pose and expression of the source face image are transferred onto the
target identity. The architecture consists of two encoders and a mapping
network that projects the two inputs into the latent space of StyleGAN2, which
finally generates the output. The training is self-supervised from video
sequences of many individuals. Manual labeling is not required. Our model
enables the synthesis of random identities with controllable pose and
expression. Close-to-real-time performance is achieved.

</details>


### [129] [Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval](https://arxiv.org/abs/2504.13035)
*WonJun Moon,Cheol-Ho Cho,Woojin Jun,Minho Shim,Taeoh Kim,Inwoong Lee,Dongyoon Wee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出一种原型框架来平衡部分相关视频检索的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 在检索系统中，同时实现准确性和效率具有挑战性，尤其在部分相关视频检索中，包含更多多样上下文可提高准确性但增加计算和内存成本。

Method: 提出原型框架，将视频多样上下文编码成固定数量的原型；引入文本关联和视频理解策略、交叉和单模态重建任务、视频混合技术。

Result: 在TVR、ActivityNet-Captions和QVHighlights数据集上评估，证明方法有效且不牺牲效率。

Conclusion: 该方法成功解决了准确性和效率之间的权衡问题。

Abstract: In a retrieval system, simultaneously achieving search accuracy and
efficiency is inherently challenging. This challenge is particularly pronounced
in partially relevant video retrieval (PRVR), where incorporating more diverse
context representations at varying temporal scales for each video enhances
accuracy but increases computational and memory costs. To address this
dichotomy, we propose a prototypical PRVR framework that encodes diverse
contexts within a video into a fixed number of prototypes. We then introduce
several strategies to enhance text association and video understanding within
the prototypes, along with an orthogonal objective to ensure that the
prototypes capture a diverse range of content. To keep the prototypes
searchable via text queries while accurately encoding video contexts, we
implement cross- and uni-modal reconstruction tasks. The cross-modal
reconstruction task aligns the prototypes with textual features within a shared
space, while the uni-modal reconstruction task preserves all video contexts
during encoding. Additionally, we employ a video mixing technique to provide
weak guidance to further align prototypes and associated textual
representations. Extensive evaluations on TVR, ActivityNet-Captions, and
QVHighlights validate the effectiveness of our approach without sacrificing
efficiency.

</details>


### [130] [VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models](https://arxiv.org/abs/2504.13122)
*Haojian Huang,Haodong Chen,Shengqiong Wu,Meng Luo,Jinlan Fu,Xinya Du,Hanwang Zhang,Hao Fei*

Main category: cs.CV

TL;DR: 本论文提出 VistaDPO 框架，通过分层空间-时间偏好优化和新数据集，改善 Large Video Models 的视频理解问题。


<details>
  <summary>Details</summary>
Motivation: 解决 LVMs 在视频理解中与人类直觉不一致和视频幻觉的问题。

Method: 引入 VistaDPO 框架，实现实例、时间和感知三个层次的文本-视频偏好对齐，并构建 VistaDPO-7k 数据集。

Result: 实验证明 VistaDPO 显著提升 LVMs 在视频幻觉、QA 和标题生成任务中的性能。

Conclusion: VistaDPO 有效缓解视频-语言不对齐和幻觉问题。

Abstract: Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown
promise in video understanding but often suffer from misalignment with human
intuition and video hallucination issues. To address these challenges, we
introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal
Direct Preference Optimization. VistaDPO enhances text-video preference
alignment across three hierarchical levels: i) Instance Level, aligning overall
video content with responses; ii) Temporal Level, aligning video temporal
semantics with event descriptions; and iii) Perceptive Level, aligning spatial
objects with language tokens. Given the lack of datasets for fine-grained
video-language preference alignment, we construct VistaDPO-7k, a dataset of
7.2K QA pairs annotated with chosen and rejected responses, along with
spatial-temporal grounding information such as timestamps, keyframes, and
bounding boxes. Extensive experiments on benchmarks such as Video
Hallucination, Video QA, and Captioning performance tasks demonstrate that
VistaDPO significantly improves the performance of existing LVMs, effectively
mitigating video-language misalignment and hallucination. The code and data are
available at https://github.com/HaroldChen19/VistaDPO.

</details>


### [131] [Event-Enhanced Blurry Video Super-Resolution](https://arxiv.org/abs/2504.13042)
*Dachun Kai,Yueyi Zhang,Jin Wang,Zeyu Xiao,Zhiwei Xiong,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 本论文针对模糊视频超分辨率问题，引入事件信号，提出Ev-DeblurVSR网络，提高细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前BVSR方法无法有效恢复锐利细节，缺乏运动信息和高频细节，因此引入事件信号来解决这些挑战。

Method: 提出Ev-DeblurVSR网络，包括互惠特征去模糊模块和混合形变对齐模块，利用事件和帧信息融合及运动估计。

Result: 在合成和真实数据集上达到新最先进性能，比FMA-Net高2.59 dB且快7.28倍。

Conclusion: 该方法显著提升了BVSR的准确性和效率，证明事件信号的有效性。

Abstract: In this paper, we tackle the task of blurry video super-resolution (BVSR),
aiming to generate high-resolution (HR) videos from low-resolution (LR) and
blurry inputs. Current BVSR methods often fail to restore sharp details at high
resolutions, resulting in noticeable artifacts and jitter due to insufficient
motion information for deconvolution and the lack of high-frequency details in
LR frames. To address these challenges, we introduce event signals into BVSR
and propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse
information from frames and events for feature deblurring, we introduce a
reciprocal feature deblurring module that leverages motion information from
intra-frame events to deblur frame features while reciprocally using global
scene context from the frames to enhance event features. Furthermore, to
enhance temporal consistency, we propose a hybrid deformable alignment module
that fully exploits the complementary motion information from inter-frame
events and optical flow to improve motion estimation in the deformable
alignment process. Extensive evaluations demonstrate that Ev-DeblurVSR
establishes a new state-of-the-art performance on both synthetic and real-world
datasets. Notably, on real data, our method is +2.59 dB more accurate and
7.28$\times$ faster than the recent best BVSR baseline FMA-Net. Code:
https://github.com/DachunKai/Ev-DeblurVSR.

</details>


### [132] [Science-T2I: Addressing Scientific Illusions in Image Synthesis](https://arxiv.org/abs/2504.13129)
*Jialuo Li,Wenhao Chai,Xingyu Fu,Haiyang Xu,Saining Xie*

Main category: cs.CV

TL;DR: 这篇论文提出了一种将科学知识整合到生成模型的新方法，提升图像合成的真实性和一致性，包括Science-T2I数据集、SciScore奖励模型和两阶段训练框架。


<details>
  <summary>Details</summary>
Motivation: 动机是提升生成模型的科学真实性和一致性，解决现有模型在科学知识方面的不足。

Method: 方法包括构建Science-T2I数据集、开发SciScore奖励模型（基于增强CLIP模型），并提出监督微调和掩码在线微调的两阶段训练框架。

Result: 结果显示，SciScore性能与人类相当，提升5%；应用于FLUX模型，SciScore得分提升超过50%。

Conclusion: 结论是，该框架设定了生成内容科学真实性的新标准，并通过实验证明了其有效性。

Abstract: We present a novel approach to integrating scientific knowledge into
generative models, enhancing their realism and consistency in image synthesis.
First, we introduce Science-T2I, an expert-annotated adversarial dataset
comprising adversarial 20k image pairs with 9k prompts, covering wide distinct
scientific knowledge categories. Leveraging Science-T2I, we present SciScore,
an end-to-end reward model that refines the assessment of generated images
based on scientific knowledge, which is achieved by augmenting both the
scientific comprehension and visual capabilities of pre-trained CLIP model.
Additionally, based on SciScore, we propose a two-stage training framework,
comprising a supervised fine-tuning phase and a masked online fine-tuning
phase, to incorporate scientific knowledge into existing generative models.
Through comprehensive experiments, we demonstrate the effectiveness of our
framework in establishing new standards for evaluating the scientific realism
of generated content. Specifically, SciScore attains performance comparable to
human-level, demonstrating a 5% improvement similar to evaluations conducted by
experienced human evaluators. Furthermore, by applying our proposed fine-tuning
method to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.

</details>


### [133] [PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding](https://arxiv.org/abs/2504.13180)
*Jang Hyun Cho,Andrea Madotto,Effrosyni Mavroudi,Triantafyllos Afouras,Tushar Nagarajan,Muhammad Maaz,Yale Song,Tengyu Ma,Shuming Hu,Suyog Jain,Miguel Martin,Huiyu Wang,Hanoona Rasheed,Peize Sun,Po-Yao Huang,Daniel Bolya,Nikhila Ravi,Shashank Jain,Tammy Stark,Shane Moon,Babak Damavandi,Vivian Lee,Andrew Westbury,Salman Khan,Philipp Krähenbühl,Piotr Dollár,Lorenzo Torresani,Kristen Grauman,Christoph Feichtenhofer*

Main category: cs.CV

TL;DR: 本论文提出一个完全开放和可重现的框架，构建感知语言模型（PLM）用于图像和视频理解，包括发布新数据和基准测试。


<details>
  <summary>Details</summary>
Motivation: 许多高性能视觉语言模型是封闭源代码的， obscuring 了数据、设计和训练细节，使用蒸馏方法虽取得benchmark结果，但科学进步难以衡量，因此需要透明的研究框架。

Method: 分析不使用专有模型蒸馏的标准训练管道，探索大规模合成数据，识别视频理解差距，并发布2.8M人类标注的视频问答对和时空定位的视频字幕；同时引入PLM-VideoBench用于评估视频理解任务。

Result: 发布了数据、训练配方、代码和模型，使工作完全可重现；提供了专注于视频'什么'、'哪里'、'什么时候'和'如何'的理解任务的基准测试套件。

Conclusion: 通过开放资源，促进了图像和视频理解的透明和可重现研究。

Abstract: Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code & models.

</details>


### [134] [Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off](https://arxiv.org/abs/2504.13078)
*Riza Velioglu,Petra Bevandic,Robin Chan,Barbara Hammer*

Main category: cs.CV

TL;DR: 本论文引入TryOffDiff，一种基于扩散的VTOFF模型，用于从穿着者提取标准化服装图像，并提升p2p-VTON性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决虚拟试穿和试脱中的服装提取挑战，特别是减少属性转移问题，如皮肤颜色转移。

Method: 方法基于潜扩散框架，使用SigLIP图像条件和类别特定嵌入，支持多服装VTOFF。

Result: 结果在VITON-HD数据集上达到最先进水平，在DressCode上表现强劲，并首创多服装VTOFF。

Conclusion: 结论是TryOffDiff改进了服装虚拟试脱技术，并通过与VTON模型结合，减少了不必要的属性转移。

Abstract: Computer vision is transforming fashion through Virtual Try-On (VTON) and
Virtual Try-Off (VTOFF). VTON generates images of a person in a specified
garment using a target photo and a standardized garment image, while a more
challenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo
of another person wearing the garment. VTOFF, on the other hand, extracts
standardized garment images from clothed individuals. We introduce TryOffDiff,
a diffusion-based VTOFF model. Built on a latent diffusion framework with
SigLIP image conditioning, it effectively captures garment properties like
texture, shape, and patterns. TryOffDiff achieves state-of-the-art results on
VITON-HD and strong performance on DressCode dataset, covering upper-body,
lower-body, and dresses. Enhanced with class-specific embeddings, it pioneers
multi-garment VTOFF, the first of its kind. When paired with VTON models, it
improves p2p-VTON by minimizing unwanted attribute transfer, such as skin
color. Code is available at: https://rizavelioglu.github.io/tryoffdiff/

</details>


### [135] [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
*Yongqian Peng,Yuxi Ma,Mengmeng Wang,Yuxuan Wang,Yizhou Wang,Chi Zhang,Yixin Zhu,Zilong Zheng*

Main category: cs.CV

TL;DR: 这篇论文通过IEI框架和CreativeMashup数据集评估视觉语言模型的组合创造力，发现VLMs在理解任务中超过平均人类但不如专家，在生成任务中融入框架可提升创造质量。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs是否具有真正组合创造力而非模式匹配，受到认知科学启发。

Method: 提出IEI框架（识别-解释-暗示），构建666个艺术生成的视觉混搭数据集，并进行理解和生成任务实验。

Result: VLMs在理解任务中优于平均人类但不及专家；在生成任务中，应用IEI框架显著改善创造输出。

Conclusion: 为评估人工智能创造力提供理论基础，并给出改进VLMs生成任务的实用指南。

Abstract: The ability to combine existing concepts into novel ideas stands as a
fundamental hallmark of human intelligence. Recent advances in Vision-Language
Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their
outputs reflect combinational creativity--defined by M. A. Boden (1998) as
synthesizing novel ideas through combining existing concepts--or sophisticated
pattern matching of training data. Drawing inspiration from cognitive science,
we investigate the combinational creativity of VLMs from the lens of concept
blending. We propose the Identification-Explanation-Implication (IEI)
framework, which decomposes creative processes into three levels: identifying
input spaces, extracting shared attributes, and deriving novel semantic
implications. To validate this framework, we curate CreativeMashup, a
high-quality dataset of 666 artist-generated visual mashups annotated according
to the IEI framework. Through extensive experiments, we demonstrate that in
comprehension tasks, best VLMs have surpassed average human performance while
falling short of expert-level understanding; in generation tasks, incorporating
our IEI framework into the generation pipeline significantly enhances the
creative quality of VLMs outputs. Our findings establish both a theoretical
foundation for evaluating artificial creativity and practical guidelines for
improving creative generation in VLMs.

</details>


### [136] [Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training](https://arxiv.org/abs/2504.13123)
*Xinsong Zhang,Yarong Zeng,Xinting Huang,Hu Hu,Runquan Xie,Han Hu,Zhanhui Kang*

Main category: cs.CV

TL;DR: 这篇论文提出使用低幻觉合成标题来提升视觉语言模型预训练性能，作为真实数据的替代。


<details>
  <summary>Details</summary>
Motivation: 现有训练范式依赖高质量图像文本对，但此类数据稀缺和饱和，限制了领域进步。

Method: 提出一个新管道，使用连续DPO方法生成高质量、低幻觉、知识丰富的合成标题。

Result: 非幻觉标题率从48.2%提高到77.9%；在35个视觉语言任务上性能提升至少6.2%；FID分数降低17.1和13.3；发布Hunyuan-Recap100M数据集。

Conclusion: 合成标题可作为真实数据的替代，并通过实证验证显著提升模型性能。

Abstract: In recent years, the field of vision-language model pre-training has
experienced rapid advancements, driven primarily by the continuous enhancement
of textual capabilities in large language models. However, existing training
paradigms for multimodal large language models heavily rely on high-quality
image-text pairs. As models and data scales grow exponentially, the
availability of such meticulously curated data has become increasingly scarce
and saturated, thereby severely limiting further advancements in this domain.
This study investigates scalable caption generation techniques for
vision-language model pre-training and demonstrates that large-scale
low-hallucination synthetic captions can serve dual purposes: 1) acting as a
viable alternative to real-world data for pre-training paradigms and 2)
achieving superior performance enhancement when integrated into vision-language
models through empirical validation. This paper presents three key
contributions: 1) a novel pipeline for generating high-quality,
low-hallucination, and knowledge-rich synthetic captions. Our continuous DPO
methodology yields remarkable results in reducing hallucinations. Specifically,
the non-hallucination caption rate on a held-out test set increases from 48.2%
to 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals
that our synthetic captions confer superior pre-training advantages over their
counterparts. Across 35 vision language tasks, the model trained with our data
achieves a significant performance gain of at least 6.2% compared to alt-text
pairs and other previous work. Meanwhile, it also offers considerable support
in the text-to-image domain. With our dataset, the FID score is reduced by 17.1
on a real-world validation benchmark and 13.3 on the MSCOCO validation
benchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and
knowledge-intensive synthetic caption dataset.

</details>


### [137] [$\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark](https://arxiv.org/abs/2504.13143)
*Siwei Yang,Mude Hui,Bingchen Zhao,Yuyin Zhou,Nataniel Ruiz,Cihang Xie*

Main category: cs.CV

TL;DR: 本论文引入Complex-Edit基准，用于系统评估基于指令的图像编辑模型在不同复杂性指令下的性能。使用GPT-4o生成指令，并发现开源模型性能较差等问题。


<details>
  <summary>Details</summary>
Motivation: 动机是填补现有评估不足，通过系统基准评估图像编辑模型在复杂指令下的表现。

Method: 方法包括使用GPT-4o和'Chain-of-Edit'管道生成指令、指标设计及VLM-based自动评估。

Result: 结果显示开源模型落后闭源模型，复杂度增加影响元素保留和美学；逐步执行和合成数据问题显著。

Conclusion: 结论指出指令复杂度挑战模型性能，并强调合成数据训练的潜在风险。

Abstract: We introduce $\texttt{Complex-Edit}$, a comprehensive benchmark designed to
systematically evaluate instruction-based image editing models across
instructions of varying complexity. To develop this benchmark, we harness
GPT-4o to automatically collect a diverse set of editing instructions at scale.
Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first
generate individual atomic editing tasks independently and then integrate them
to form cohesive, complex instructions. Additionally, we introduce a suite of
metrics to assess various aspects of editing performance, along with a
VLM-based auto-evaluation pipeline that supports large-scale assessments. Our
benchmark yields several notable insights: 1) Open-source models significantly
underperform relative to proprietary, closed-source models, with the
performance gap widening as instruction complexity increases; 2) Increased
instructional complexity primarily impairs the models' ability to retain key
elements from the input images and to preserve the overall aesthetic quality;
3) Decomposing a complex instruction into a sequence of atomic steps, executed
in a step-by-step manner, substantially degrades performance across multiple
metrics; 4) A straightforward Best-of-N selection strategy improves results for
both direct editing and the step-by-step sequential approach; and 5) We observe
a ``curse of synthetic data'': when synthetic data is involved in model
training, the edited images from such models tend to appear increasingly
synthetic as the complexity of the editing instructions rises -- a phenomenon
that intriguingly also manifests in the latest GPT-4o outputs.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [138] [Attractor-merging Crises and Intermittency in Reservoir Computing](https://arxiv.org/abs/2504.12695)
*Tempei Kabayama,Motomasa Komuro,Yasuo Kuniyoshi,Kazuyuki Aihara,Kohei Nakajima*

Main category: nlin.CD

TL;DR: 储层计算可以将吸引子嵌入随机神经网络中，形成镜像。通过调整全局参数，会出现吸引子合并危机伴随间歇性，这种现象是RNNs的固有特性，与训练数据无关。


<details>
  <summary>Details</summary>
Motivation: 探讨储层计算中对称约束如何导致吸引子嵌入，以及调整参数引发危机的机制。

Method: 通过调整全局参数并分析相空间结构。

Result: 发现调整参数会导致吸引子合并危机伴随间歇性，并证明这种分叉场景适用于RNNs的一般类别，与训练数据无关。

Conclusion: 这种分叉场景是RNNs的固有特性，不依赖于训练数据。

Abstract: Reservoir computing can embed attractors into random neural networks (RNNs),
generating a ``mirror'' of a target attractor because of its inherent
symmetrical constraints. In these RNNs, we report that an attractor-merging
crisis accompanied by intermittency emerges simply by adjusting the global
parameter. We further reveal its underlying mechanism through a detailed
analysis of the phase-space structure and demonstrate that this bifurcation
scenario is intrinsic to a general class of RNNs, independent of training data.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [139] [The Dissipation Theory of Aging: A Quantitative Analysis Using a Cellular Aging Map](https://arxiv.org/abs/2504.13044)
*Farhan Khodaee,Rohola Zandie,Yufan Xia,Elazer R. Edelman*

Main category: q-bio.QM

TL;DR: 本论文提出了一种基于动态系统的衰老新理论，并使用数据驱动的方法量化细胞水平的衰老变化。


<details>
  <summary>Details</summary>
Motivation: 论文动机是解决衰老机制的复杂性，通过动态系统理论解释衰老作为生物系统中耗散过程的需求。

Method: 方法包括使用遍历理论分解衰老动态，并采用基于变换器的机器学习算法分析基因表达数据，将年龄作为标记创建细胞衰老图。

Result: 结果显示衰老是一种耗散过程，识别了基因嵌入空间的发散、非线性转变和熵变化，并为不同组织和细胞类型提供了模式。

Conclusion: 结论是衰老本质上为耗散过程，并引入了一个计算框架以分子分辨率测量年龄相关变化。

Abstract: We propose a new theory for aging based on dynamical systems and provide a
data-driven computational method to quantify the changes at the cellular level.
We use ergodic theory to decompose the dynamics of changes during aging and
show that aging is fundamentally a dissipative process within biological
systems, akin to dynamical systems where dissipation occurs due to
non-conservative forces. To quantify the dissipation dynamics, we employ a
transformer-based machine learning algorithm to analyze gene expression data,
incorporating age as a token to assess how age-related dissipation is reflected
in the embedding space. By evaluating the dynamics of gene and age embeddings,
we provide a cellular aging map (CAM) and identify patterns indicative of
divergence in gene embedding space, nonlinear transitions, and entropy
variations during aging for various tissues and cell types. Our results provide
a novel perspective on aging as a dissipative process and introduce a
computational framework that enables measuring age-related changes with
molecular resolution.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [140] [TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial Transcriptomics Data](https://arxiv.org/abs/2504.12353)
*Shuo Shuo Liu,Shikun Wang,Yuxuan Chen,Anil K. Rustgi,Ming Yuan,Jianhua Hu*

Main category: q-bio.GN

TL;DR: TransST 是一种转移学习框架，通过利用外部细胞标记信息来改善空间转录组学数据的细胞异质性分析。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术分辨率低和测序深度不足，难以提取可靠的生物学信号，因此需要新方法来解决这一挑战。

Method: 提出 TransST 转移学习框架，自适应利用外部细胞标记信息推断目标空间转录组学数据的细胞水平异质性。

Result: 在真实研究（如乳腺癌）和模拟中，TransST 显著提升性能，成功识别细胞簇并区分组织类型，比其他方法更优。

Conclusion: TransST 方法在识别细胞亚群和检测驱动生物标志物方面有效且稳健。

Abstract: Background: Spatial transcriptomics have emerged as a powerful tool in
biomedical research because of its ability to capture both the spatial contexts
and abundance of the complete RNA transcript profile in organs of interest.
However, limitations of the technology such as the relatively low resolution
and comparatively insufficient sequencing depth make it difficult to reliably
extract real biological signals from these data. To alleviate this challenge,
we propose a novel transfer learning framework, referred to as TransST, to
adaptively leverage the cell-labeled information from external sources in
inferring cell-level heterogeneity of a target spatial transcriptomics data.
  Results: Applications in several real studies as well as a number of
simulation settings show that our approach significantly improves existing
techniques. For example, in the breast cancer study, TransST successfully
identifies five biologically meaningful cell clusters, including the two
subgroups of cancer in situ and invasive cancer; in addition, only TransST is
able to separate the adipose tissues from the connective issues among all the
studied methods.
  Conclusions: In summary, the proposed method TransST is both effective and
robust in identifying cell subclusters and detecting corresponding driving
biomarkers in spatial transcriptomics data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [141] [WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion](https://arxiv.org/abs/2504.12354)
*Vinay Shukla,Prachee Sharma,Ryan Rossi,Sungchul Kim,Tong Yu,Aditya Grover*

Main category: eess.IV

TL;DR: WaterFlow 是一种快速、高鲁棒且高保真度的视觉水印方法，使用预训练的潜在扩散模型在傅里叶域植入水印，并在多个数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 图像水印是计算机视觉的基本问题，生成图像的快速增长加剧了现有方法的计算速度、鲁棒性和感知质量挑战。

Method: 提出 WaterFlow，使用预训练的潜在扩散模型将图像编码到潜在空间，在傅里叶域植入学习水印，并通过可逆流层增强表达性以提高质量和鲁棒性。

Result: 展示了最先进的鲁棒性性能，是首个能有效防御组合攻击的方法，在 MS-COCO、DiffusionDB 和 WikiArt 数据集上验证。

Conclusion: WaterFlow 显著提升了图像水印的速度、鲁棒性和质量，是该领域的关键进展。

Abstract: The ability to embed watermarks in images is a fundamental problem of
interest for computer vision, and is exacerbated by the rapid rise of generated
imagery in recent times. Current state-of-the-art techniques suffer from
computational and statistical challenges such as the slow execution speed for
practical deployments. In addition, other works trade off fast watermarking
speeds but suffer greatly in their robustness or perceptual quality. In this
work, we propose WaterFlow (WF), a fast and extremely robust approach for high
fidelity visual watermarking based on a learned latent-dependent watermark. Our
approach utilizes a pretrained latent diffusion model to encode an arbitrary
image into a latent space and produces a learned watermark that is then planted
into the Fourier Domain of the latent. The transformation is specified via
invertible flow layers that enhance the expressivity of the latent space of the
pre-trained model to better preserve image quality while permitting robust and
tractable detection. Most notably, WaterFlow demonstrates state-of-the-art
performance on general robustness and is the first method capable of
effectively defending against difficult combination attacks. We validate our
findings on three widely used real and generated datasets: MS-COCO,
DiffusionDB, and WikiArt.

</details>


### [142] [TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology](https://arxiv.org/abs/2504.12718)
*Walid Rehamnia,Alexandra Getmanskaya,Evgeniy Vasilyev,Vadim Turlapov*

Main category: eess.IV

TL;DR: 这篇论文提出了一种新型的完全无监督多级分割方法TUMLS，用于数字病理学，解决标注劳动密集、计算需求高和信任问题，提高病理学家的工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI在组织病理学中的应用面临标注全滑微镜图像劳动密集、计算需求高以及缺乏不确定性估计导致信任问题的挑战。

Method: TUMLS使用自编码器提取特征识别组织类型，基于不确定性选择代表性补丁，并在高分辨率下进行无监督细胞核分割，不依赖ML算法，并无缝集成临床工作流。

Result: 在UPENN-GBM数据集上，自编码器MSE为0.0016；在MoNuSeg数据集上，F1分数77.46%、Jaccard分数63.35%，优于其他无监督方法。

Conclusion: 这些结果证明TUMLS在推进数字病理学领域的效能。

Abstract: Digital pathology, augmented by artificial intelligence (AI), holds
significant promise for improving the workflow of pathologists. However,
challenges such as the labor-intensive annotation of whole slide images (WSIs),
high computational demands, and trust concerns arising from the absence of
uncertainty estimation in predictions hinder the practical application of
current AI methodologies in histopathology. To address these issues, we present
a novel trustful fully unsupervised multi-level segmentation methodology
(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to
identify the different tissue types within low-resolution training data. It
selects representative patches from each identified group based on an
uncertainty measure and then does unsupervised nuclei segmentation in their
respective higher-resolution space without using any ML algorithms. Crucially,
this solution integrates seamlessly into clinicians workflows, transforming the
examination of a whole WSI into a review of concise, interpretable cross-level
insights. This integration significantly enhances and accelerates the workflow
while ensuring transparency. We evaluated our approach using the UPENN-GBM
dataset, where the AE achieved a mean squared error (MSE) of 0.0016.
Additionally, nucleus segmentation is assessed on the MoNuSeg dataset,
outperforming all unsupervised approaches with an F1 score of 77.46% and a
Jaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in
advancing the field of digital pathology.

</details>


### [143] [Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond](https://arxiv.org/abs/2504.13037)
*Yundi Zhang,Paul Hager,Che Liu,Suprosanna Shit,Chen Chen,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: ViTa 是一个多模态模型，整合心脏磁共振成像和患者因素，提供全面心脏健康评估。


<details>
  <summary>Details</summary>
Motivation: 心脏磁共振成像无法捕获患者人口统计学、代谢和生活方式因素，导致疾病风险解释不完整，需要整合框架。

Method: 利用 42,000 名 UK Biobank 参与者的数据，融合 3D+T 动态影像和表格患者数据，学习共享潜在表示。

Result: 支持心脏表型预测、分段和疾病分类等任务，在统一框架内实现。

Conclusion: ViTa 推动通用患者特定心脏健康理解，提升临床效用和可扩展性。

Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive
cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy
and physiology. Patient-level health factors, such as demographics, metabolic,
and lifestyle, are known to substantially influence cardiovascular health and
disease risk, yet remain uncaptured by CMR alone. To holistically understand
cardiac health and to enable the best possible interpretation of an
individual's disease risk, CMR and patient-level factors must be jointly
exploited within an integrated framework. Recent multi-modal approaches have
begun to bridge this gap, yet they often rely on limited spatio-temporal data
and focus on isolated clinical tasks, thereby hindering the development of a
comprehensive representation for cardiac health evaluation. To overcome these
limitations, we introduce ViTa, a step toward foundation models that delivers a
comprehensive representation of the heart and a precise interpretation of
individual disease risk. Leveraging data from 42,000 UK Biobank participants,
ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling
a complete capture of the cardiac cycle. These imaging data are then fused with
detailed tabular patient-level factors, enabling context-aware insights. This
multi-modal paradigm supports a wide spectrum of downstream tasks, including
cardiac phenotype and physiological feature prediction, segmentation, and
classification of cardiac and metabolic diseases within a single unified
framework. By learning a shared latent representation that bridges rich imaging
features and patient context, ViTa moves beyond traditional, task-specific
models toward a universal, patient-specific understanding of cardiac health,
highlighting its potential to advance clinical utility and scalability in
cardiac analysis.

</details>


### [144] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results](https://arxiv.org/abs/2504.13131)
*Xin Li,Kun Yuan,Bingchen Li,Fengbin Guan,Yizhen Shao,Zihao Yu,Xijun Wang,Yiting Lu,Wei Luo,Suhang Yao,Ming Sun,Chao Zhou,Zhibo Chen,Radu Timofte,Yabin Zhang,Ao-Xiang Zhang,Tianwu Zhi,Jianzhao Liu,Yang Li,Jingwen Xu,Yiting Liao,Yushen Zuo,Mingyang Wu,Renjie Li,Shengyun Zhong,Zhengzhong Tu,Yufan Liu,Xiangguang Chen,Zuowei Cao,Minhao Tang,Shan Liu,Kexin Zhang,Jingfen Xie,Yan Wang,Kai Chen,Shijie Zhao,Yunchen Zhang,Xiangkai Xu,Hong Gao,Ji Shi,Yiming Bao,Xiugang Dong,Xiangsheng Zhou,Yaofeng Tu,Ying Liang,Yiwen Wang,Xinning Chai,Yuxuan Zhang,Zhengxue Cheng,Yingsheng Qin,Yucai Yang,Rong Xie,Li Song,Wei Sun,Kang Fu,Linhan Cao,Dandan Zhu,Kaiwei Zhang,Yucheng Zhu,Zicheng Zhang,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Zhi Jin,Jiawei Wu,Wei Wang,Wenjian Zhang,Yuhai Lan,Gaoxiong Yi,Hengyuan Na,Wang Luo,Di Wu,MingYin Bai,Jiawang Du,Zilong Lu,Zhenyu Jiang,Hui Zeng,Ziguan Cui,Zongliang Gan,Guijin Tang,Xinglin Xie,Kehuan Song,Xiaoqiang Lu,Licheng Jiao,Fang Liu,Xu Liu,Puhua Chen,Ha Thu Nguyen,Katrien De Moor,Seyed Ali Amirshahi,Mohamed-Chaker Larabi,Qi Tang,Linfeng He,Zhiyong Gao,Zixuan Gao,Guohua Zhang,Zhiye Huang,Yi Deng,Qingmiao Jiang,Lu Chen,Yi Yang,Xi Liao,Nourine Mohammed Nadir,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Meiqin Liu,Chao Yao,Yao Zhao*

Main category: eess.IV

TL;DR: 这篇论文回顾了NTIRE 2025挑战赛，聚焦于短视频UGC质量评估和增强，包括高效视频质量评估和基于扩散的图像超分辨率。


<details>
  <summary>Details</summary>
Motivation: 驱动研究以提升短视频UGC平台（如Kwai和TikTok）的用户体验。

Method: 挑战赛包括两个赛道：高效视频质量评估（强调轻量模型）和扩散-based图像超分辨率；引入KwaiSR数据集，包含1800对合成图像和1900张真实世界图像，按8:1:1比例划分。

Result: 吸引266名参与者，收到18个有效提交，促进了短视频UGC VQA和图像超分辨率的发展。

Conclusion: 挑战赛成果公开可用，项目代码在GitHub上发布。

Abstract: This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC
Video Quality Assessment and Enhancement. The challenge comprises two tracks:
(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image
Super-Resolution (KwaiSR). Track 1 aims to advance the development of
lightweight and efficient video quality assessment (VQA) models, with an
emphasis on eliminating reliance on model ensembles, redundant weights, and
other computationally expensive components in the previous IQA/VQA
competitions. Track 2 introduces a new short-form UGC dataset tailored for
single image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800
synthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,
which are split into training, validation, and test sets using a ratio of
8:1:1. The primary objective of the challenge is to drive research that
benefits the user experience of short-form UGC platforms such as Kwai and
TikTok. This challenge attracted 266 participants and received 18 valid final
submissions with corresponding fact sheets, significantly contributing to the
progress of short-form UGC VQA and image superresolution. The project is
publicly available at https://github.com/lixinustc/KVQE-
ChallengeCVPR-NTIRE2025.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [145] [Anonymous Public Announcements](https://arxiv.org/abs/2504.12546)
*Thomas Ågotnes,Rustam Galimullin,Ken Satoh,Satoshi Tojo*

Main category: cs.LO

TL;DR: 这篇论文形式化了匿名公共公告的概念，探讨了在不假设意图和假设意图匿名时的逻辑，并提供了表达性和完备性结果。


<details>
  <summary>Details</summary>
Motivation: 动机是处理匿名公告可能无意中揭示身份的问题，例如在社交媒体或俄罗斯扑克牌谜题中。

Method: 方法包括形式化匿名公共公告运算符，分析无意图和有意图假设的情况，并证明逻辑的表达性和公理完备性。

Result: 主要结果是形式表达性结果和关键逻辑语言的公理完备性。

Conclusion: 结论是，当假设意图匿名时，它类似于'安全'公告的概念，类似于俄罗斯扑克牌谜题。

Abstract: We formalise the notion of an \emph{anonymous public announcement} in the
tradition of public announcement logic. Such announcements can be seen as
in-between a public announcement from ``the outside" (an announcement of
$\phi$) and a public announcement by one of the agents (an announcement of
$K_a\phi$): we get more information than just $\phi$, but not (necessarily)
about exactly who made it. Even if such an announcement is prima facie
anonymous, depending on the background knowledge of the agents it might reveal
the identity of the announcer: if I post something on a message board, the
information might reveal who I am even if I don't sign my name. Furthermore,
like in the Russian Cards puzzle, if we assume that the announcer's intention
was to stay anonymous, that in fact might reveal more information. In this
paper we first look at the case when no assumption about intentions are made,
in which case the logic with an anonymous public announcement operator is
reducible to epistemic logic. We then look at the case when we assume common
knowledge of the intention to stay anonymous, which is both more complex and
more interesting: in several ways it boils down to the notion of a ``safe"
announcement (again, similarly to Russian Cards). Main results include formal
expressivity results and axiomatic completeness for key logical languages.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [146] [Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study](https://arxiv.org/abs/2504.12422)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Main category: cs.HC

TL;DR: 本文通过LinkQ系统研究减少LLM幻觉的方法，使用知识图谱查询真实数据，并在网络安全领域进行评估，显示优于GPT-4但存在局限。


<details>
  <summary>Details</summary>
Motivation: 高风险领域如网络操作需要可靠AI，LLM易产生幻觉，因此开发LinkQ强制查询知识图谱获取真实数据。

Method: 开发LinkQ系统，强制LLM在问答中查询知识图谱；使用KGQA数据集进行定量评估，并与领域专家进行定性研究。

Result: LinkQ优于GPT-4但在某些问题类别上表现不佳；专家反馈了限制、建议和未来机会。

Conclusion: 建议未来LLM查询系统需探索替代查询策略以改进性能。

Abstract: High-stakes domains like cyber operations need responsible and trustworthy AI
methods. While large language models (LLMs) are becoming increasingly popular
in these domains, they still suffer from hallucinations. This research paper
provides learning outcomes from a case study with LinkQ, an open-source natural
language interface that was developed to combat hallucinations by forcing an
LLM to query a knowledge graph (KG) for ground-truth data during
question-answering (QA). We conduct a quantitative evaluation of LinkQ using a
well-known KGQA dataset, showing that the system outperforms GPT-4 but still
struggles with certain question categories - suggesting that alternative query
construction strategies will need to be investigated in future LLM querying
systems. We discuss a qualitative study of LinkQ with two domain experts using
a real-world cybersecurity KG, outlining these experts' feedback, suggestions,
perceived limitations, and future opportunities for systems like LinkQ.

</details>


### [147] [Don't Just Translate, Agitate: Using Large Language Models as Devil's Advocates for AI Explanations](https://arxiv.org/abs/2504.12424)
*Ashley Suh,Kenneth Alperin,Harry Li,Steven R Gomez*

Main category: cs.HC

TL;DR: 这篇论文讨论了LLM在XAI中翻译解释输出可能导致过度依赖的问题，并建议LLM应作为质疑者促进批判性参与。


<details>
  <summary>Details</summary>
Motivation: 解决LLM翻译XAI输出可能无法提升用户理解，反而加剧过度依赖的问题，基于最近研究发现。

Method: 提出LLM应充当建设性质疑者，审问AI解释，包括呈现替代解释、偏差和局限性。

Result: 这种方法可减少过度依赖，提高用户与AI系统的批判性互动。

Conclusion: LLM应主动审问AI解释而非简单翻译，以提升透明度和减少过度依赖。

Abstract: This position paper highlights a growing trend in Explainable AI (XAI)
research where Large Language Models (LLMs) are used to translate outputs from
explainability techniques, like feature-attribution weights, into a natural
language explanation. While this approach may improve accessibility or
readability for users, recent findings suggest that translating into human-like
explanations does not necessarily enhance user understanding and may instead
lead to overreliance on AI systems. When LLMs summarize XAI outputs without
surfacing model limitations, uncertainties, or inconsistencies, they risk
reinforcing the illusion of interpretability rather than fostering meaningful
transparency. We argue that - instead of merely translating XAI outputs - LLMs
should serve as constructive agitators, or devil's advocates, whose role is to
actively interrogate AI explanations by presenting alternative interpretations,
potential biases, training data limitations, and cases where the model's
reasoning may break down. In this role, LLMs can facilitate users in engaging
critically with AI systems and generated explanations, with the potential to
reduce overreliance caused by misinterpreted or specious explanations.

</details>


### [148] [Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process](https://arxiv.org/abs/2504.12488)
*Mohi Reza,Jeb Thomas-Mitchell,Peter Dushniku,Nathan Laundry,Joseph Jay Williams,Anastasia Kuzminykh*

Main category: cs.HC

TL;DR: 本研究通过系统回顾和访谈，探讨AI写作辅助如何影响作者的代理感和所有权，并提出设计策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在写作中的应用引发了对维护作者代理感和所有权的担忧，但缺乏系统研究。

Method: 使用PRISMA方法回顾109篇HCI论文，并对15位不同领域的作者进行访谈。

Result: 识别四种AI写作支持设计策略，映射到写作的规划、翻译、审阅和监控过程；发现作者对AI干预偏好的差异，受类型和上下文影响。

Conclusion: 通过分析所有权重要性、作者需求和AI互动，提供人类中心写作工具的设计指导。

Abstract: As generative AI tools like ChatGPT become integral to everyday writing,
critical questions arise about how to preserve writers' sense of agency and
ownership when using these tools. Yet, a systematic understanding of how AI
assistance affects different aspects of the writing process - and how this
shapes writers' agency - remains underexplored. To address this gap, we
conducted a systematic review of 109 HCI papers using the PRISMA approach. From
this literature, we identify four overarching design strategies for AI writing
support: structured guidance, guided exploration, active co-writing, and
critical feedback - mapped across the four key cognitive processes in writing:
planning, translating, reviewing, and monitoring. We complement this analysis
with interviews of 15 writers across diverse domains. Our findings reveal that
writers' desired levels of AI intervention vary across the writing process:
content-focused writers (e.g., academics) prioritize ownership during planning,
while form-focused writers (e.g., creatives) value control over translating and
reviewing. Writers' preferences are also shaped by contextual goals, values,
and notions of originality and authorship. By examining when ownership matters,
what writers want to own, and how AI interactions shape agency, we surface both
alignment and gaps between research and user needs. Our findings offer
actionable design guidance for developing human-centered writing tools for
co-writing with AI, on human terms.

</details>


### [149] [Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis](https://arxiv.org/abs/2504.12511)
*Shravan Chaudhari,Trilokya Akula,Yoon Kim,Tom Blake*

Main category: cs.HC

TL;DR: 本论文探讨多模态大语言模型（MLLMs）在视觉感知中的应用，提出无标注框架评估其作为认知助手的效用。


<details>
  <summary>Details</summary>
Motivation: 推进AI增强推理在HCI、心理学和认知科学中的研究，聚焦视觉感知，旨在基准测试MLLMs并改善人类推理。

Method: 利用心理学和认知科学原理，提出无标注分析框架评估MLLMs在视觉感知任务中的解释性和效用。

Result: 通过框架基准测试MLLMs，量化其可解释性，揭示人类推理能力和数据集偏差。

Conclusion: 为评估MLLMs可解释性提供原则性框架，促进其在HCI任务中提升人类推理和减少偏差。

Abstract: In this paper, we advance the study of AI-augmented reasoning in the context
of Human-Computer Interaction (HCI), psychology and cognitive science, focusing
on the critical task of visual perception. Specifically, we investigate the
applicability of Multimodal Large Language Models (MLLMs) in this domain. To
this end, we leverage established principles and explanations from psychology
and cognitive science related to complexity in human visual perception. We use
them as guiding principles for the MLLMs to compare and interprete visual
content. Our study aims to benchmark MLLMs across various explainability
principles relevant to visual perception. Unlike recent approaches that
primarily employ advanced deep learning models to predict complexity metrics
from visual content, our work does not seek to develop a mere new predictive
model. Instead, we propose a novel annotation-free analytical framework to
assess utility of MLLMs as cognitive assistants for HCI tasks, using visual
perception as a case study. The primary goal is to pave the way for principled
study in quantifying and evaluating the interpretability of MLLMs for
applications in improving human reasoning capability and uncovering biases in
existing perception datasets annotated by humans.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [150] [Post-processing improves accuracy of Artificial Intelligence weather forecasts](https://arxiv.org/abs/2504.12672)
*Belinda Trotta,Robert Johnson,Catherine de Burgh-Day,Debra Hudson,Esteban Abellan,James Canvin,Andrew Kelly,Daniel Mentiplay,Benjamin Owen,Jennifer Whelan*

Main category: physics.ao-ph

TL;DR: 统计后处理方法能像数值天气预报一样改善AI天气预报的准确性，并便于整合。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI天气模型的系统偏差和可靠性问题，并测试现有后处理方法是否适用于AI模型。

Method: 将气象局的IMPROVER后处理系统应用于ECMWF的AIFS，并与传统NWP模型的输出进行比较，没有修改配置或工作流程。

Result: 后处理对AIFS和传统NWP的准确性改善相当，混合AIFS和NWP模型能提升整体预报技能。

Conclusion: 统计后处理方法可直接应用于AI模型，使国家气象中心能低风险地逐步整合AI预报。

Abstract: Artificial Intelligence (AI) weather models are now reaching
operational-grade performance for some variables, but like traditional
Numerical Weather Prediction (NWP) models, they exhibit systematic biases and
reliability issues. We test the application of the Bureau of Meteorology's
existing statistical post-processing system, IMPROVER, to ECMWF's deterministic
Artificial Intelligence Forecasting System (AIFS), and compare results against
post-processed outputs from the ECMWF HRES and ENS models. Without any
modification to configuration or processing workflows, post-processing yields
comparable accuracy improvements for AIFS as for traditional NWP forecasts, in
both expected value and probabilistic outputs. We show that blending AIFS with
NWP models improves overall forecast skill, even when AIFS alone is not the
most accurate component. These findings show that statistical post-processing
methods developed for NWP are directly applicable to AI models, enabling
national meteorological centres to incorporate AI forecasts into existing
workflows in a low-risk, incremental fashion.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [151] [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
*Guanrou Yang,Chen Yang,Qian Chen,Ziyang Ma,Wenxi Chen,Wen Wang,Tianrui Wang,Yifan Yang,Zhikang Niu,Wenrui Liu,Fan Yu,Zhihao Du,Zhifu Gao,ShiLiang Zhang,Xie Chen*

Main category: eess.AS

TL;DR: 这篇论文引入了EmoVoice，一个基于LLM的情感可控TTS模型，并使用了一个新数据集，实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决TTS模型中情感表达控制的挑战，通过自然语言实现细粒度情感控制。

Method: 提出EmoVoice模型，使用LLM进行情感控制，并行输出音素和音频标记，并引入EmoVoice-DB数据集。

Result: 在英语和中文测试集上实现了最先进性能，并使用多模态LLM评估情感指标。

Conclusion: 证明了方法的有效性，并计划发布数据集、代码和检查点。

Abstract: Human speech goes beyond the mere transfer of information; it is a profound
exchange of emotions and a connection between individuals. While Text-to-Speech
(TTS) models have made huge progress, they still face challenges in controlling
the emotional expression in the generated speech. In this work, we propose
EmoVoice, a novel emotion-controllable TTS model that exploits large language
models (LLMs) to enable fine-grained freestyle natural language emotion
control, and a phoneme boost variant design that makes the model output phoneme
tokens and audio tokens in parallel to enhance content consistency, inspired by
chain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we
introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring
expressive speech and fine-grained emotion labels with natural language
descriptions. EmoVoice achieves state-of-the-art performance on the English
EmoVoice-DB test set using only synthetic training data, and on the Chinese
Secap test set using our in-house data. We further investigate the reliability
of existing emotion evaluation metrics and their alignment with human
perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and
Gemini to assess emotional speech. Demo samples are available at
https://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints
will be released.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [152] [A Survey on Archetypal Analysis](https://arxiv.org/abs/2504.12392)
*Aleix Alcacer,Irene Epifanio,Sebastian Mair,Morten Mørup*

Main category: stat.ME

TL;DR: 这篇论文是对Archetypal Analysis (AA)的一种调查，概述其方法、应用、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: AA的动机是提取数据中的典型方面，提供可解释的表示用于特征提取和降维，但面临非凸优化挑战。

Method: 通过调查AA的方法论、应用领域、最佳实践和限制。

Result: 提供了AA的全面概述，包括建模最佳实践和未来研究方向。

Conclusion: 解释了AA的重要未来研究方向。

Abstract: Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and
Leo Breiman as a computational procedure to extract the distinct aspects called
archetypes in observations with each observational record approximated as a
mixture (i.e., convex combination) of these archetypes. AA thereby provides
straightforward, interpretable, and explainable representations for feature
extraction and dimensionality reduction, facilitating the understanding of the
structure of high-dimensional data with wide applications throughout the
sciences. However, AA also faces challenges, particularly as the associated
optimization problem is non-convex. This survey provides researchers and data
mining practitioners an overview of methodologies and opportunities that AA has
to offer surveying the many applications of AA across disparate fields of
science, as well as best practices for modeling data using AA and limitations.
The survey concludes by explaining important future research directions
concerning AA.

</details>


### [153] [Cluster weighted models with multivariate skewed distributions for functional data](https://arxiv.org/abs/2504.12683)
*Cristina Anton,Roy Shivam Ram Shreshtth*

Main category: stat.ME

TL;DR: 本论文提出了一种基于混合功能线性回归模型和偏斜分布的聚类方法funWeightClustSkew，用于功能数据聚类，并通过EM算法估计参数。


<details>
  <summary>Details</summary>
Motivation: 扩展funHDDC框架到功能数据，引入偏斜分布（如方差-伽玛、偏斜-t和正态-逆高斯分布）来处理多元数据的聚类问题。

Method: 使用混合功能线性回归模型和三种偏斜分布，构建EM算法估计参数，并考虑几种简约模型。

Result: 在模拟数据和Air Quality数据集上验证了方法的性能。

Conclusion: 该方法提升了功能数据聚类的准确性，尤其适用于偏斜分布的数据。

Abstract: We propose a clustering method, funWeightClustSkew, based on mixtures of
functional linear regression models and three skewed multivariate
distributions: the variance-gamma distribution, the skew-t distribution, and
the normal-inverse Gaussian distribution. Our approach follows the framework of
the functional high dimensional data clustering (funHDDC) method, and we extend
to functional data the cluster weighted models based on skewed distributions
used for finite dimensional multivariate data. We consider several parsimonious
models, and to estimate the parameters we construct an expectation maximization
(EM) algorithm. We illustrate the performance of funWeightClustSkew for
simulated data and for the Air Quality dataset.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [154] [Prototype-Guided Diffusion for Digital Pathology: Achieving Foundation Model Performance with Minimal Clinical Data](https://arxiv.org/abs/2504.12351)
*Ekaterina Redekop,Mara Pleasure,Vedrana Ivezic,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey Arnold*

Main category: cs.GR

TL;DR: 这篇论文提出了一种原型引导扩散模型，用于生成合成病理学数据，实现高效自监督学习，减少对真实数据依赖并保持性能。


<details>
  <summary>Details</summary>
Motivation: 探讨数据集大小与性能相关性的透明度不足，以及是否需要更多数据来提升性能，并减少对真实患者样本的依赖。

Method: 提出原型引导扩散模型，通过组织学原型指导采样，生成高质量合成数据，确保生物学和诊断意义的变化。

Result: 使用合成数据训练的模型性能与真实数据集相当，尽管数据量少60-760倍；混合方法进一步提升了性能。

Conclusion: 强调生成式AI可显著减少对临床数据集的依赖，提高数字病理学的效率。

Abstract: Foundation models in digital pathology use massive datasets to learn useful
compact feature representations of complex histology images. However, there is
limited transparency into what drives the correlation between dataset size and
performance, raising the question of whether simply adding more data to
increase performance is always necessary. In this study, we propose a
prototype-guided diffusion model to generate high-fidelity synthetic pathology
data at scale, enabling large-scale self-supervised learning and reducing
reliance on real patient samples while preserving downstream performance. Using
guidance from histological prototypes during sampling, our approach ensures
biologically and diagnostically meaningful variations in the generated data. We
demonstrate that self-supervised features trained on our synthetic dataset
achieve competitive performance despite using ~60x-760x less data than models
trained on large real-world datasets. Notably, models trained using our
synthetic data showed statistically comparable or better performance across
multiple evaluation metrics and tasks, even when compared to models trained on
orders of magnitude larger datasets. Our hybrid approach, combining synthetic
and real data, further enhanced performance, achieving top results in several
evaluations. These findings underscore the potential of generative AI to create
compelling training data for digital pathology, significantly reducing the
reliance on extensive clinical datasets and highlighting the efficiency of our
approach.

</details>


### [155] [3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise](https://arxiv.org/abs/2504.12856)
*Yifeng Cheng,Juan Du*

Main category: cs.GR

TL;DR: 本文提出一种简单的方法，使用Perlin噪声生成3D表面异常，以解决工业异常检测中缺陷样本稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中真实缺陷样本稀缺，尤其是3D数据，限制了视觉基础模型的应用。

Method: 提出3D-PNAS方法，基于Perlin噪声和表面参数化，通过将点云投影到2D平面、采样多尺度噪声并沿法线方向扰动点云生成异常。

Result: 实验显示关键参数（如噪声尺度、扰动强度和八度）可精细控制异常生成，并跨类别产生一致的几何可信异常；提供代码库和可视化工具。

Conclusion: 该方法有效生成逼真的3D表面异常，促进3D数据在工业质量检测中的应用和未来研究。

Abstract: Large pretrained vision foundation models have shown significant potential in
various vision tasks. However, for industrial anomaly detection, the scarcity
of real defect samples poses a critical challenge in leveraging these models.
While 2D anomaly generation has significantly advanced with established
generative models, the adoption of 3D sensors in industrial manufacturing has
made leveraging 3D data for surface quality inspection an emerging trend. In
contrast to 2D techniques, 3D anomaly generation remains largely unexplored,
limiting the potential of 3D data in industrial quality inspection. To address
this gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,
based on Perlin noise and surface parameterization. Our method generates
realistic 3D surface anomalies by projecting the point cloud onto a 2D plane,
sampling multi-scale noise values from a Perlin noise field, and perturbing the
point cloud along its normal direction. Through comprehensive visualization
experiments, we demonstrate how key parameters - including noise scale,
perturbation strength, and octaves, provide fine-grained control over the
generated anomalies, enabling the creation of diverse defect patterns from
pronounced deformations to subtle surface variations. Additionally, our
cross-category experiments show that the method produces consistent yet
geometrically plausible anomalies across different object types, adapting to
their specific surface characteristics. We also provide a comprehensive
codebase and visualization toolkit to facilitate future research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [156] [Resonances in reflective Hamiltonian Monte Carlo](https://arxiv.org/abs/2504.12374)
*Namu Kroupa,Gábor Csányi,Will Handley*

Main category: stat.ML

TL;DR: 这篇论文分析了高维反射Hamiltonian Monte Carlo的慢混合问题，使用Sinkhorn散度量化非均匀性，探讨了机制、行为转换和调整实践。


<details>
  <summary>Details</summary>
Motivation: 理解高维反射Hamiltonian Monte Carlo从Dirac delta分布初始化并针对均匀分布时慢混合的根本原因。

Method: 使用Sinkhorn散度量化非均匀性，分析球体和立方体中的行为转换、步长缩放、粒子分离，并构建低维玩具模型与精确方法对比。

Result: 发现行为在流体状和离散化主导间转换，临界步长随维度幂律缩放，粒子可能自发分离导致共振，玩具模型再现关键特征。

Conclusion: 强调不精确反射引起问题，通过与精确方法对比，建议优化调整以改善性能。

Abstract: In high dimensions, reflective Hamiltonian Monte Carlo with inexact
reflections exhibits slow mixing when the particle ensemble is initialised from
a Dirac delta distribution and the uniform distribution is targeted. By
quantifying the instantaneous non-uniformity of the distribution with the
Sinkhorn divergence, we elucidate the principal mechanisms underlying the
mixing problems. In spheres and cubes, we show that the collective motion
transitions between fluid-like and discretisation-dominated behaviour, with the
critical step size scaling as a power law in the dimension. In both regimes,
the particles can spontaneously unmix, leading to resonances in the particle
density and the aforementioned problems. Additionally, low-dimensional toy
models of the dynamics are constructed which reproduce the dominant features of
the high-dimensional problem. Finally, the dynamics is contrasted with the
exact Hamiltonian particle flow and tuning practices are discussed.

</details>


### [157] [Robust and Scalable Variational Bayes](https://arxiv.org/abs/2504.12528)
*Carlos Misael Madrid Padilla,Shitao Fan,Lizhen Lin*

Main category: stat.ML

TL;DR: 本论文提出了一种鲁棒且可扩展的变分贝叶斯框架，用于处理大规模数据集中的异常值和污染。


<details>
  <summary>Details</summary>
Motivation: 动机是改进贝叶斯推断以应对数据集中的任意性质的异常值和污染，提高鲁棒性和可扩展性。

Method: 方法包括将数据集分成不相交子集，对每个子集计算后验分布，独立应用变分贝叶斯近似，然后使用Wasserstein距离的几何中位数聚合，得到变分中位数后验分布。

Result: 结果显示该方法保留了后验收缩性质，提供鲁棒性保证，建立了变分Bernstein-von Mises定理，并在数值实验中验证了其性能。

Conclusion: 结论是该框架是处理复杂污染数据集的可靠贝叶斯推断工具。

Abstract: We propose a robust and scalable framework for variational Bayes (VB) that
effectively handles outliers and contamination of arbitrary nature in large
datasets. Our approach divides the dataset into disjoint subsets, computes the
posterior for each subset, and applies VB approximation independently to these
posteriors. The resulting variational posteriors with respect to the subsets
are then aggregated using the geometric median of probability measures,
computed with respect to the Wasserstein distance. This novel aggregation
method yields the Variational Median Posterior (VM-Posterior) distribution. We
rigorously demonstrate that the VM-Posterior preserves contraction properties
akin to those of the true posterior, while accounting for approximation errors
or the variational gap inherent in VB methods. We also provide provable
robustness guarantee of the VM-Posterior. Furthermore, we establish a
variational Bernstein-von Mises theorem for both multivariate Gaussian
distributions with general covariance structures and the mean-field variational
family. To facilitate practical implementation, we adapt existing algorithms
for computing the VM-Posterior and evaluate its performance through extensive
numerical experiments. The results highlight its robustness and scalability,
making it a reliable tool for Bayesian inference in the presence of complex,
contaminated datasets.

</details>


### [158] [Spectral Algorithms under Covariate Shift](https://arxiv.org/abs/2504.12625)
*Jun Fan,Zheng-Chu Guo,Lei Shi*

Main category: stat.ML

TL;DR: 这篇论文探讨了谱算法在协变量偏移下的性能，证明了在密度比有界时的最优性，并提出加权谱算法来解决密度比无界时的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了加深对谱算法在真实世界分布偏移场景下的性能理解，特别是训练和测试数据分布不同时。

Method: 分析谱算法在协变量偏移下的泛化误差，证明密度比有界时的minimax最优性；提出加权谱算法并引入权重裁剪技术。

Result: 谱算法在密度比有界时达到minimax最优，但在无界时次优；加权方法实现了最优的容量无关收敛率，并通过权重裁剪接近容量相关的最优率。

Conclusion: 这改善了谱算法在分布偏移下的理论结果，解决了次优性问题，并推进了现有研究。

Abstract: Spectral algorithms leverage spectral regularization techniques to analyze
and process data, providing a flexible framework for addressing supervised
learning problems. To deepen our understanding of their performance in
real-world scenarios where the distributions of training and test data may
differ, we conduct a rigorous investigation into the convergence behavior of
spectral algorithms under distribution shifts, specifically within the
framework of reproducing kernel Hilbert spaces. Our study focuses on the case
of covariate shift. In this scenario, the marginal distributions of the input
data differ between the training and test datasets, while the conditional
distribution of the output given the input remains unchanged. Under this
setting, we analyze the generalization error of spectral algorithms and show
that they achieve minimax optimality when the density ratios between the
training and test distributions are uniformly bounded. However, we also
identify a critical limitation: when the density ratios are unbounded, the
spectral algorithms may become suboptimal. To address this limitation, we
propose a weighted spectral algorithm that incorporates density ratio
information into the learning process. Our theoretical analysis shows that this
weighted approach achieves optimal capacity-independent convergence rates.
Furthermore, by introducing a weight clipping technique, we demonstrate that
the convergence rates of the weighted spectral algorithm can approach the
optimal capacity-dependent convergence rates arbitrarily closely. This
improvement resolves the suboptimality issue in unbounded density ratio
scenarios and advances the state-of-the-art by refining existing theoretical
results.

</details>


### [159] [When do Random Forests work?](https://arxiv.org/abs/2504.12860)
*C. Revelas,O. Boldea,B. J. M. Werker*

Main category: stat.ML

TL;DR: 本文研究随机森林中随机化分裂方向的有效性，分析其在不同信噪比和数据特性下的性能。


<details>
  <summary>Details</summary>
Motivation: 重新审视去相关性和正则化效果，系统分析随机森林在各种场景下的表现，以理解其实际应用中的优势。

Method: 使用常见数据生成过程分析不同信噪比下的样本外均方误差，并进行固定信噪比的模拟研究考察随机化的影响。

Result: 发现方差减少随信噪比增加，低信噪比时随机森林优于bagging；随机化在 fat tails 时增加偏差，无关协变量时无效，相关协变量时有效；平均方法可减少相关协变量下的偏差。

Conclusion: 这些发现有助于更好地理解随机森林在协变量相关实际场景中的表现，并为改进方法提供启发。

Abstract: We study the effectiveness of randomizing split-directions in random forests.
Prior literature has shown that, on the one hand, randomization can reduce
variance through decorrelation, and, on the other hand, randomization
regularizes and works in low signal-to-noise ratio (SNR) environments. First,
we bring together and revisit decorrelation and regularization by presenting a
systematic analysis of out-of-sample mean-squared error (MSE) for different SNR
scenarios based on commonly-used data-generating processes. We find that
variance reduction tends to increase with the SNR and forests outperform
bagging when the SNR is low because, in low SNR cases, variance dominates bias
for both methods. Second, we show that the effectiveness of randomization is a
question that goes beyond the SNR. We present a simulation study with fixed and
moderate SNR, in which we examine the effectiveness of randomization for other
data characteristics. In particular, we find that (i) randomization can
increase bias in the presence of fat tails in the distribution of covariates;
(ii) in the presence of irrelevant covariates randomization is ineffective
because bias dominates variance; and (iii) when covariates are mutually
correlated randomization tends to be effective because variance dominates bias.
Beyond randomization, we find that, for both bagging and random forests, bias
can be significantly reduced in the presence of correlated covariates. This
last finding goes beyond the prevailing view that averaging mostly works by
variance reduction. Given that in practice covariates are often correlated, our
findings on correlated covariates could open the way for a better understanding
of why random forests work well in many applications.

</details>


### [160] [Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time](https://arxiv.org/abs/2504.13110)
*Margalit Glasgow,Denny Wu,Joan Bruna*

Main category: stat.ML

TL;DR: 本研究探讨了多项式宽度神经网络与无限宽度神经网络在均值场缩放制度下使用投影梯度下降训练时的近似差距，通过微分方程进行紧密界定，并应用于特征学习问题。


<details>
  <summary>Details</summary>
Motivation: 动机是理解有限宽度神经网络如何近似无限宽度神经网络的行为，特别是针对特征学习问题中的收敛性。

Method: 方法是通过均值场动力学治理的微分方程来界定近似差距，并分析局部Hessian和自协和性属性。

Result: 结果显示，由于自协和性属性，多项式数量的神经元足以紧密近似均值场动力学，且收敛时间在环境维度d上多项式增长。

Conclusion: 结论是，在特定问题中，多项式宽度的神经网络可以很好地模拟无限宽度神经网络的动态。

Abstract: We study the approximation gap between the dynamics of a polynomial-width
neural network and its infinite-width counterpart, both trained using projected
gradient descent in the mean-field scaling regime. We demonstrate how to
tightly bound this approximation gap through a differential equation governed
by the mean-field dynamics. A key factor influencing the growth of this ODE is
the local Hessian of each particle, defined as the derivative of the particle's
velocity in the mean-field dynamics with respect to its position. We apply our
results to the canonical feature learning problem of estimating a
well-specified single-index model; we permit the information exponent to be
arbitrarily large, leading to convergence times that grow polynomially in the
ambient dimension $d$. We show that, due to a certain ``self-concordance''
property in these problems -- where the local Hessian of a particle is bounded
by a constant times the particle's velocity -- polynomially many neurons are
sufficient to closely approximate the mean-field dynamics throughout training.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [161] [MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System](https://arxiv.org/abs/2504.12757)
*Sonu Kumar,Anubhav Girdhar,Ritesh Patil,Divyansh Tripathi*

Main category: cs.CR

TL;DR: MCP Guardian 是一个框架，通过认证、速率限制、日志记录、追踪和 WAF 扫描增强 MCP 通信的安全性，并通过实证测试证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着 Agentic AI 主流采用，AI 系统受限于数据孤岛，新集成需自定义逻辑难于扩展；MCP 的灵活性引入恶意工具服务器和数据完整性风险。

Method: 提出 MCP Guardian 框架，包括认证、速率限制、日志记录、追踪和 Web 应用防火墙 (WAF) 扫描。

Result: 通过真实场景和实证测试，证明 MCP Guardian 有效缓解攻击，确保稳健监督，同时开销最小。

Conclusion: 该方法促进 AI 助手的安全、可扩展数据访问，强调防御深度方法的重要性，以实现更安全、更透明的 AI 创新。

Abstract: As Agentic AI gain mainstream adoption, the industry invests heavily in model
capabilities, achieving rapid leaps in reasoning and quality. However, these
systems remain largely confined to data silos, and each new integration
requires custom logic that is difficult to scale. The Model Context Protocol
(MCP) addresses this challenge by defining a universal, open standard for
securely connecting AI-based applications (MCP clients) to data sources (MCP
servers). However, the flexibility of the MCP introduces new risks, including
malicious tool servers and compromised data integrity. We present MCP Guardian,
a framework that strengthens MCP-based communication with authentication,
rate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.
Through real-world scenarios and empirical testing, we demonstrate how MCP
Guardian effectively mitigates attacks and ensures robust oversight with
minimal overheads. Our approach fosters secure, scalable data access for AI
assistants, underscoring the importance of a defense-in-depth approach that
enables safer and more transparent innovation in AI-driven environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
*Hao Li,Liuzhenghao Lv,He Cao,Zijing Liu,Zhiyuan Yan,Yu Wang,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型在分子理解任务中的幻觉问题，引入Mol-Hallu指标和HRPP方法，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型的幻觉问题导致药物设计错误，需要分析来源并提出缓解策略。

Method: 分析幻觉来源，引入Mol-Hallu指标评估幻觉，并提出HRPP阶段减少幻觉。

Result: 实验显示HRPP在各种LLM上有效，Mol-Hallu指标帮助重新评估幻觉程度。

Conclusion: 研究为减少幻觉和提升LLM在科学应用中的可靠性提供了关键洞见。

Abstract: Large language models are increasingly used in scientific domains, especially
for molecular understanding and analysis. However, existing models are affected
by hallucination issues, resulting in errors in drug design and utilization. In
this paper, we first analyze the sources of hallucination in LLMs for molecular
comprehension tasks, specifically the knowledge shortcut phenomenon observed in
the PubChem dataset. To evaluate hallucination in molecular comprehension tasks
with computational efficiency, we introduce \textbf{Mol-Hallu}, a novel
free-form evaluation metric that quantifies the degree of hallucination based
on the scientific entailment relationship between generated text and actual
molecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze
the extent of hallucination in various LLMs performing molecular comprehension
tasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is
proposed to alleviate molecular hallucinations, Experiments show the
effectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our
findings provide critical insights into mitigating hallucination and improving
the reliability of LLMs in scientific applications.

</details>


### [163] [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
*Xingguang Ji,Jiakang Wang,Hongzhi Zhang,Jingyuan Zhang,Haonan Zhou,Chenxi Sun,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 这篇论文介绍了Capybara-OMNI，一个轻量高效的多模态大语言模型（MLLM），支持文本、图像、视频和音频模态，并公开了模型权重、部分训练数据和推理代码。


<details>
  <summary>Details</summary>
Motivation: 由于创建和训练多模态数据对的复杂性，导致构建MLLM计算和时间消耗大，因此需要一种轻量高效的方法来开发强大的MLLM。

Method: 详细描述了框架设计、数据构建、训练配方，以及专属基准用于验证不同模态的理解能力。

Result: 通过遵循指导，可以高效构建MLLM，在各种多模态基准上达到与同规模模型竞争的性能。

Conclusion: 模型及其聊天版本已公开，包括权重、数据和代码；并讨论了如何训练聊天版本以提升多模态指令遵循和对话能力，更符合用户习惯。

Abstract: With the development of Multimodal Large Language Models (MLLMs), numerous
outstanding accomplishments have emerged within the open-source community. Due
to the complexity of creating and training multimodal data pairs, it is still a
computational and time-consuming process to build powerful MLLMs. In this work,
we introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient
manner and supports understanding text, image, video, and audio modalities. We
present in detail the framework design, the data construction, and the training
recipe, to develop an MLLM step-by-step to obtain competitive performance. We
also provide exclusive benchmarks utilized in our experiments to show how to
properly verify understanding capabilities across different modalities. Results
show that by following our guidance, we can efficiently build an MLLM that
achieves competitive performance among models of the same scale on various
multimodal benchmarks. Additionally, to enhance the multimodal instruction
following and conversational capabilities of the model, we further discuss how
to train the chat version upon an MLLM understanding model, which is more in
line with user habits for tasks like real-time interaction with humans. We
publicly disclose the Capybara-OMNI model, along with its chat-based version.
The disclosure includes both the model weights, a portion of the training data,
and the inference codes, which are made available on GitHub.

</details>


### [164] [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
*Jingyuan Zhang,Hongzhi Zhang,Zhou Haonan,Chenxi Sun,Xingguang ji,Jiakang Wang,Fanheng Kong,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 本论文引入数据代谢概念，并提出一个数据中心框架来构建视觉语言模型（VLMs），通过数据策展和迭代持续改进模型性能。展示了Capybara-VL模型在多模态任务中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 数据策展在训练强大的VLMs中扮演关键角色，动机是建立一个闭环系统来持续提升模型性能。

Method: 引入数据代谢概念，讨论数据策展和迭代步骤，提供数据集处理代码书，并构建用户特定数据飞轮。

Result: 发布了Capybara-VL模型，在视觉问答、科学推理和文本丰富任务中表现出色，超越了更大规模的开源模型，并与领先的专有模型相当。

Conclusion: 突显了数据中心框架的强大和训练更小更高效VLMs的潜力。

Abstract: Data curation plays a crucial role in training powerful Visual Language
Models (VLMs). In this work, we introduce the concept of Data Metabolism and
present our data-centric framework to build VLMs throughout the development
lifecycle. Starting from a standard model architecture, we discuss and provide
insights into two crucial development steps: data curation and iteration,
forming a closed-loop system that continuously improves model performance. We
show a detailed codebook on how to process existing massive datasets and build
user-specific data flywheel. As a demonstration, we release a VLM, named
Capybara-VL, which excels in typical multimodal tasks (e.g. , visual question
answering, scientific reasoning, and text-rich tasks). Despite its relatively
compact size, Capybara-VL surpasses several open-source models that are up to
10 times larger in size. Moreover, it achieves results that are on par with
those of several leading proprietary models, demonstrating its remarkable
competitiveness. These results highlight the power of our data-centric
framework and the potential of training smaller and more efficient VLMs.

</details>


### [165] [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: 本研究评估14个LLM的创意性能，发现无提升迹象，且输出高度可变。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT普及后LLM创意能力是否提升及输出一致性。

Method: 使用Divergent Association Task (DAT)和Alternative Uses Task (AUT)评估GPT-4等14个LLM。

Result: LLM创意未提高，AUT任务中平均优于人类，但仅有0.28%响应达人类顶尖；存在显著内部变异性。

Conclusion: 强调需更精细评估框架，注意模型选择、提示设计和重复评估。

Abstract: Following the widespread adoption of ChatGPT in early 2023, numerous studies
reported that large language models (LLMs) can match or even surpass human
performance in creative tasks. However, it remains unclear whether LLMs have
become more creative over time, and how consistent their creative output is. In
this study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,
Grok, Mistral, and DeepSeek -- across two validated creativity assessments: the
Divergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary
to expectations, we found no evidence of increased creative performance over
the past 18-24 months, with GPT-4 performing worse than in previous studies.
For the more widely used AUT, all models performed on average better than the
average human, with GPT-4o and o3-mini performing best. However, only 0.28% of
LLM-generated responses reached the top 10% of human creativity benchmarks.
Beyond inter-model differences, we document substantial intra-model
variability: the same LLM, given the same prompt, can produce outputs ranging
from below-average to original. This variability has important implications for
both creativity research and practical applications. Ignoring such variability
risks misjudging the creative potential of LLMs, either inflating or
underestimating their capabilities. The choice of prompts affected LLMs
differently. Our findings underscore the need for more nuanced evaluation
frameworks and highlight the importance of model selection, prompt design, and
repeated assessment when using Generative AI (GenAI) tools in creative
contexts.

</details>


### [166] [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
*Charlotte Siska,Anush Sankaran*

Main category: cs.CL

TL;DR: 这篇论文提出了一种使用小语言模型注意力机制的可解释防御方法来对抗语言模型越狱攻击，并展示了其性能和效率优势。


<details>
  <summary>Details</summary>
Motivation: 当前防御策略无法解释恶意输入的原因，导致封闭式方法泛滥。本研究旨在提供一种可解释且廉价的防御方法。

Method: 提出AttentionDefense，使用小语言模型的系统提示注意力来表征对抗性提示。通过基准数据集评估、消融研究，并使用多代理系统生成新型越狱数据集。

Result: AttentionDefense在检测性能上与基于文本嵌入的分类器和GPT-4零样本检测器相当或更好，在新型越狱数据集上表现出色，而现有方法性能下降。

Conclusion: AttentionDefense是一种计算需求低但性能高的实用防御方案。

Abstract: In the past few years, Language Models (LMs) have shown par-human
capabilities in several domains. Despite their practical applications and
exceeding user consumption, they are susceptible to jailbreaks when malicious
input exploits the LM's weaknesses, causing it to deviate from its intended
behavior. Current defensive strategies either classify the input prompt as
adversarial or prevent LMs from generating harmful outputs. However, it is
challenging to explain the reason behind the malicious nature of the jailbreak,
which results in a wide variety of closed-box approaches. In this research, we
propose and demonstrate that system-prompt attention from Small Language Models
(SLMs) can be used to characterize adversarial prompts, providing a novel,
explainable, and cheaper defense approach called AttentionDefense. Our research
suggests that the attention mechanism is an integral component in understanding
and explaining how LMs respond to malicious input that is not captured in the
semantic meaning of text embeddings. The proposed AttentionDefense is evaluated
against existing jailbreak benchmark datasets. Ablation studies show that
SLM-based AttentionDefense has equivalent or better jailbreak detection
performance compared to text embedding-based classifiers and GPT-4 zero-shot
detectors.To further validate the efficacy of the proposed approach, we
generate a dataset of novel jailbreak variants of the existing benchmark
dataset using a closed-loop LLM-based multi-agent system. We demonstrate that
the proposed AttentionDefense approach performs robustly on this novel
jailbreak dataset while existing approaches suffer in performance.
Additionally, for practical purposes AttentionDefense is an ideal solution as
it has the computation requirements of a small LM but the performance of a LLM
detector.

</details>


### [167] [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
*Xin Gao,Qizhi Pei,Zinan Tang,Yu Li,Honglin Lin,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出GRA框架，使用多个小型LLM协作模拟同行评审，实现高效数据合成，挑战大型LLM的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前数据合成依赖大型LLM，存在高计算成本、环境低效和偏见问题，而小型LLM更可持续但能力不足。

Method: GRA框架将小型LLM分为生成器、审阅者和仲裁者角色，迭代改进数据合成过程。

Result: 实验显示GRA数据质量与大型LLM（如Qwen-2.5-72B-Instruct）相当或更优。

Conclusion: 主张通过小型LLM协调而非依赖大型模型来实现高质量数据合成。

Abstract: While data synthesis and distillation are promising strategies to enhance
small language models, current approaches heavily rely on Large Language Models
(LLMs), which suffer from high computational costs, environmental inefficiency,
and potential biases inherited from monolithic architectures. In contrast,
smaller LLMs are more accessible and sustainable, but their individual
capabilities often fall short in generating high-quality, diverse, and reliable
data. Inspired by collaborative human processes (e.g., peer review), we propose
a multiple small LLMs involved framework, GRA, that aggregates specialized
roles across small LLMs to iterative refinement and quality control typically
achieved by a single large LLM. In this collaborative framework, multiple small
LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a
peer-review-inspired data synthesis pipeline. The Generator proposes initial
data samples, the Reviewer critiques their quality and diversity, and the
Adjudicator resolves conflicts to finalize the output. By decomposing the
synthesis process into specialized sub-tasks, collaborative small LLMs can
achieve data-level parity with large LLM-based distillation. Through
experiments across multiple benchmarks, we demonstrate that GRA-produced data
matches or exceeds the quality of single large LLM outputs, e.g.,
Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large
models for high-quality data synthesis, advocating instead for strategic
coordination of smaller agents. Our datasets, models, and code are publicly
available at https://github.com/GX-XinGao/GRA.

</details>


### [168] [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
*Zheng Zhang,Ning Li,Qi Liu,Rui Li,Weibo Gao,Qingyang Mao,Zhenya Huang,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 这篇论文探讨了RAG对LLM公平性的影响，发现小规模LLM在使用RAG时公平性可能恶化，并提出FairFT和FairFilter方法来缓解。


<details>
  <summary>Details</summary>
Motivation: 动机是RAG在社会影响大的领域可能引入公平性问题，需要评估其对LLM公平性的影响。

Method: 方法包括通过改变LLM、检索器和检索来源进行实验分析，并提出FairFT（公平微调检索器）和FairFilter（公平过滤机制）。

Result: 结果显示规模小于8B的LLM在引入RAG后公平性恶化，FairFT和FairFilter方法有效改善公平性。

Conclusion: 结论是所提方法在真实数据集上验证，能提升公平性同时保持性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant document from external knowledge sources. By referencing
this external knowledge, RAG effectively reduces the generation of factually
incorrect content and addresses hallucination issues within LLMs. Recently,
there has been growing attention to improving the performance and efficiency of
RAG systems from various perspectives. While these advancements have yielded
significant results, the application of RAG in domains with considerable
societal implications raises a critical question about fairness: What impact
does the introduction of the RAG paradigm have on the fairness of LLMs? To
address this question, we conduct extensive experiments by varying the LLMs,
retrievers, and retrieval sources. Our experimental analysis reveals that the
scale of the LLMs plays a significant role in influencing fairness outcomes
within the RAG framework. When the model scale is smaller than 8B, the
integration of retrieval mechanisms often exacerbates unfairness in small-scale
LLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness
issues introduced by RAG for small-scale LLMs, we propose two approaches,
FairFT and FairFilter. Specifically, in FairFT, we align the retriever with the
LLM in terms of fairness, enabling it to retrieve documents that facilitate
fairer model outputs. In FairFilter, we propose a fairness filtering mechanism
to filter out biased content after retrieval. Finally, we validate our proposed
approaches on real-world datasets, demonstrating their effectiveness in
improving fairness while maintaining performance.

</details>


### [169] [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
*Mengying Yuan,Wangzi Xuan,Fei Li*

Main category: cs.CL

TL;DR: 本篇论文引入跨文档跨语言自然语言推理（CDCL-NLI）任务，构建数据集并提出新方法，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: CDCL-NLI 尚未充分探索，论文旨在扩展 NLI 到多文档多语言场景，并激发相关研究兴趣。

Method: 提出整合 RST 增强图融合和可解释性预测的方法，使用 RST 与 RGAT 建模跨文档语境，基于词汇链的语义对齐实现跨语言理解，并开发 EDU 级归因框架。

Result: 实验显示，该方法显著优于传统 NLI 模型（如 DocNLI、R2F）和大型语言模型（如 Llama3、GPT-4o）。

Conclusion: 这项工作为 NLI 研究提供新见解，促进跨文档跨语言语境理解、语义检索和可解释性推理的研究。

Abstract: Natural Language Inference (NLI) is a fundamental task in both natural
language processing and information retrieval. While NLI has developed many
sub-directions such as sentence-level NLI, document-level NLI and cross-lingual
NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In
this paper, we propose a novel paradigm for CDCL-NLI that extends traditional
NLI capabilities to multi-document, multilingual scenarios. To support this
task, we construct a high-quality CDCL-NLI dataset including 1,110 instances
and spanning 26 languages. To build a baseline for this task, we also propose
an innovative method that integrates RST-enhanced graph fusion and
interpretability prediction. Our method employs RST (Rhetorical Structure
Theory) on RGAT (Relation-aware Graph Attention Network) for cross-document
context modeling, coupled with a structure-aware semantic alignment mechanism
based on lexical chains for cross-lingual understanding. For NLI
interpretability, we develop an EDU-level attribution framework that generates
extractive explanations. Extensive experiments demonstrate our approach's
superior performance, achieving significant improvements over both traditional
NLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our
work sheds light on the study of NLI and will bring research interest on
cross-document cross-lingual context understanding, semantic retrieval and
interpretability inference. Our dataset and code are available at
\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer
review}.

</details>


### [170] [LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media](https://arxiv.org/abs/2504.12325)
*Haiqi Zhang,Zhengyuan Zhu,Zeyu Zhang,Chengkai Li*

Main category: cs.CL

TL;DR: 本文介绍了LLMTaxo框架，利用大型语言模型自动构建社交媒体事实声明的分类法，并通过多数据集评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体内容急剧增加，导致分析在线话语变得复杂，需要更好的工具来帮助利益相关者导航。

Method: 引入LLMTaxo框架，使用大型语言模型从多层次粒度生成主题，在三个不同数据集上使用各种模型实现，并设计专用评价指标。

Result: 通过人类和GPT-4评估，结果显示LLMTaxo有效分类事实声明，且某些模型在特定数据集上表现更优。

Conclusion: LLMTaxo是构建社交媒体事实声明分类的有效方法，能提升分析效率。

Abstract: With the vast expansion of content on social media platforms, analyzing and
comprehending online discourse has become increasingly complex. This paper
introduces LLMTaxo, a novel framework leveraging large language models for the
automated construction of taxonomy of factual claims from social media by
generating topics from multi-level granularities. This approach aids
stakeholders in more effectively navigating the social media landscapes. We
implement this framework with different models across three distinct datasets
and introduce specially designed taxonomy evaluation metrics for a
comprehensive assessment. With the evaluations from both human evaluators and
GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims
from social media, and reveals that certain models perform better on specific
datasets.

</details>


### [171] [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
*Shahriar Noroozizadeh,Jeremy C. Weiss*

Main category: cs.CL

TL;DR: 本文使用大语言模型构建管道，从临床病例报告中提取并标注时间定位的发现，应用于Sepsis-3数据集，并验证其高恢复率和时间顺序一致性。


<details>
  <summary>Details</summary>
Motivation: 临床病例报告虽完整但延迟，结构化数据虽早但不完整，需要更完整和时间精细的数据来训练模型。

Method: 构建管道使用大语言模型对病例报告进行表型化、提取和标注时间定位的发现，并应用于PubMed Open Access的Sepsis-3病例报告，验证使用I2B2/MIMIC-IV数据集。

Result: 高恢复率（事件匹配率：O1-preview--0.755，Llama 3.3 70B Instruct--0.753）和强时间顺序一致性（协调性：O1-preview--0.932，Llama 3.3 70B Instruct--0.932）。

Conclusion: 表征LLM在文本中时间定位临床发现的能力，展示其局限性，并提出通过多模态整合改进的潜在途径。

Abstract: Clinical case reports and discharge summaries may be the most complete and
accurate summarization of patient encounters, yet they are finalized, i.e.,
timestamped after the encounter. Complementary data structured streams become
available sooner but suffer from incompleteness. To train models and algorithms
on more complete and temporally fine-grained data, we construct a pipeline to
phenotype, extract, and annotate time-localized findings within case reports
using large language models. We apply our pipeline to generate an open-access
textual time series corpus for Sepsis-3 comprising 2,139 case reports from the
Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA
and timeline annotations from I2B2/MIMIC-IV and compare the results to
physician-expert annotations. We show high recovery rates of clinical findings
(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and
strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B
Instruct--0.932). Our work characterizes the ability of LLMs to time-localize
clinical findings in text, illustrating the limitations of LLM use for temporal
reconstruction and providing several potential avenues of improvement via
multimodal integration.

</details>


### [172] [A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future](https://arxiv.org/abs/2504.12328)
*Jialun Zhong,Wei Shen,Yanzeng Li,Songyang Gao,Hua Lu,Yicheng Chen,Yang Zhang,Wei Zhou,Jinjie Gu,Lei Zou*

Main category: cs.CL

TL;DR: 这篇论文全面概述了奖励模型（RM）在提升大型语言模型（LLM）中的作用，包括偏好收集、奖励建模、使用、应用、基准、挑战和未来研究方向，旨在为初学者提供介绍并促进进一步研究。


<details>
  <summary>Details</summary>
Motivation: 动机是为初学者提供奖励模型的全面介绍，并促进未来的研究。

Method: 方法是通过从偏好收集、奖励建模和使用角度进行综述，介绍应用、讨论基准，并对挑战进行深入分析。

Result: 结果是提供了奖励模型的全面概述，分析了现有挑战，并提出了潜在研究方向。

Conclusion: 结论是这篇论文致力于初学者的教育，并提供了公开可用的资源。

Abstract: Reward Model (RM) has demonstrated impressive potential for enhancing Large
Language Models (LLM), as RM can serve as a proxy for human preferences,
providing signals to guide LLMs' behavior in various tasks. In this paper, we
provide a comprehensive overview of relevant research, exploring RMs from the
perspectives of preference collection, reward modeling, and usage. Next, we
introduce the applications of RMs and discuss the benchmarks for evaluation.
Furthermore, we conduct an in-depth analysis of the challenges existing in the
field and dive into the potential research directions. This paper is dedicated
to providing beginners with a comprehensive introduction to RMs and
facilitating future studies. The resources are publicly available at
github\footnote{https://github.com/JLZhong23/awesome-reward-models}.

</details>


### [173] [Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/abs/2504.12329)
*Wang Yang,Xiang Yue,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 一种无训练框架Speculative Thinking，使用更大模型指导较小模型，提高推理准确性和减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 解决当前后训练方法成本高、输出冗长和效率低的问题。

Method: 基于两个观察：推理支持性标记如'wait'出现在结构分隔符后，以及更大模型在反射行为上的更强控制，通过将反射步骤委托给更强的模型。

Result: 在1.5B模型上，MATH500准确率从83.2%提高到89.4%，输出长度减少15.7%；在Qwen-2.5-7B-Instruct模型上，准确率从74.0%提高到81.8%。

Conclusion: 显著提高了推理准确性和效率，无需训练。

Abstract: Recent advances leverage post-training to enhance model reasoning
performance, which typically requires costly training pipelines and still
suffers from inefficient, overly lengthy outputs. We introduce Speculative
Thinking, a training-free framework that enables large reasoning models to
guide smaller ones during inference at the reasoning level, distinct from
speculative decoding, which operates at the token level. Our approach is based
on two observations: (1) reasoning-supportive tokens such as "wait" frequently
appear after structural delimiters like "\n\n", serving as signals for
reflection or continuation; and (2) larger models exhibit stronger control over
reflective behavior, reducing unnecessary backtracking while improving
reasoning quality. By strategically delegating reflective steps to a more
capable model, our method significantly boosts the reasoning accuracy of
reasoning models while shortening their output. With the assistance of the 32B
reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to
89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average
output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%
decrease. Moreover, when applied to a non-reasoning model
(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%
on the same benchmark, achieving a relative improvement of 7.8%.

</details>


### [174] [HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2504.12330)
*Pei Liu,Xin Liu,Ruoyu Yao,Junming Liu,Siyuan Meng,Ding Wang,Jun Ma*

Main category: cs.CL

TL;DR: HM-RAG 是一个分层多代理多模态 RAG 框架，通过协作智能处理复杂查询，提高了跨异构数据知识合成的性能。


<details>
  <summary>Details</summary>
Motivation: 常规单代理 RAG 在处理需要跨异构数据协调推理的复杂查询时存在局限性，因此需要开发更先进的框架来提升查询处理能力。

Method: 提出 HM-RAG 框架，包括三层架构：分解代理负责查询分解和上下文增强，多源检索代理进行并行模态特定检索，决策代理通过一致性投票和专家模型精炼整合答案。

Result: 在 ScienceQA 和 CrisisMMD 基准上，答案准确率提升 12.95%，问题分类准确率提升 3.56%，并在零样本设置中达到最先进水平。

Conclusion: HM-RAG 在多模态推理和知识合成方面取得重大进展，支持新数据模态无缝集成，并确保数据治理。

Abstract: While Retrieval-Augmented Generation (RAG) augments Large Language Models
(LLMs) with external knowledge, conventional single-agent RAG remains
fundamentally limited in resolving complex queries demanding coordinated
reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel
Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative
intelligence for dynamic knowledge synthesis across structured, unstructured,
and graph-based data. The framework is composed of three-tiered architecture
with specialized agents: a Decomposition Agent that dissects complex queries
into contextually coherent sub-tasks via semantic-aware query rewriting and
schema-guided context augmentation; Multi-source Retrieval Agents that carry
out parallel, modality-specific retrieval using plug-and-play modules designed
for vector, graph, and web-based databases; and a Decision Agent that uses
consistency voting to integrate multi-source answers and resolve discrepancies
in retrieval results through Expert Model Refinement. This architecture attains
comprehensive query understanding by combining textual, graph-relational, and
web-derived evidence, resulting in a remarkable 12.95% improvement in answer
accuracy and a 3.56% boost in question classification accuracy over baseline
RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG
establishes state-of-the-art results in zero-shot settings on both datasets.
Its modular architecture ensures seamless integration of new data modalities
while maintaining strict data governance, marking a significant advancement in
addressing the critical challenges of multimodal reasoning and knowledge
synthesis in RAG systems. Code is available at
https://github.com/ocean-luna/HMRAG.

</details>


### [175] [Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation](https://arxiv.org/abs/2504.12331)
*Xiangju Li,Dong Yang,Xiaogang Zhu,Faliang Huang,Peng Zhang,Zhongying Zhao*

Main category: cs.CL

TL;DR: 这篇论文提出了一种基于大型语言模型的指令微调和数据增强框架，用于跨度级情感原因类别三元组提取，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在冗余信息检索和隐式情感类别确定方面的挑战。

Method: 采用指令微调、低秩适配和基于提示的数据增强技术来微调大型语言模型。

Result: 实验结果显示，与基线方法相比，性能至少提高了12.8%。

Conclusion: 证明了方法的有效性和鲁棒性，为情感原因分析研究提供了新方向。

Abstract: Span-level emotion-cause-category triplet extraction represents a novel and
complex challenge within emotion cause analysis. This task involves identifying
emotion spans, cause spans, and their associated emotion categories within the
text to form structured triplets. While prior research has predominantly
concentrated on clause-level emotion-cause pair extraction and span-level
emotion-cause detection, these methods often confront challenges originating
from redundant information retrieval and difficulty in accurately determining
emotion categories, particularly when emotions are expressed implicitly or
ambiguously. To overcome these challenges, this study explores a fine-grained
approach to span-level emotion-cause-category triplet extraction and introduces
an innovative framework that leverages instruction tuning and data augmentation
techniques based on large language models. The proposed method employs
task-specific triplet extraction instructions and utilizes low-rank adaptation
to fine-tune large language models, eliminating the necessity for intricate
task-specific architectures. Furthermore, a prompt-based data augmentation
strategy is developed to address data scarcity by guiding large language models
in generating high-quality synthetic training data. Extensive experimental
evaluations demonstrate that the proposed approach significantly outperforms
existing baseline methods, achieving at least a 12.8% improvement in span-level
emotion-cause-category triplet extraction metrics. The results demonstrate the
method's effectiveness and robustness, offering a promising avenue for
advancing research in emotion cause analysis. The source code is available at
https://github.com/zxgnlp/InstruDa-LLM.

</details>


### [176] [Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games](https://arxiv.org/abs/2504.12333)
*Andrés Isaza-Giraldo,Paulo Bala,Lucas Pereira*

Main category: cs.CL

TL;DR: 本研究评估小型LLM在严肃游戏中评估开放式响应时的可靠性，发现模型表现差异，并强调模型选择的必要性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决严肃游戏中响应主观正确性的挑战，以及小型LLM准确性和一致性的不确定性。

Method: 方法是通过二元分类指标（如准确率、真正例率、真负例率）系统比较五种小型LLM在En-join游戏中的表现。

Result: 结果突出了各模型的优缺点，揭示了敏感性、特异性和性能之间的权衡，有些模型在正确响应识别上出色，但可能存在假阳性或不一致问题。

Conclusion: 结论是需要上下文感知的评估框架和谨慎模型选择，这项工作为AI驱动评估工具的可靠性提供了见解。

Abstract: The evaluation of open-ended responses in serious games presents a unique
challenge, as correctness is often subjective. Large Language Models (LLMs) are
increasingly being explored as evaluators in such contexts, yet their accuracy
and consistency remain uncertain, particularly for smaller models intended for
local execution. This study investigates the reliability of five small-scale
LLMs when assessing player responses in \textit{En-join}, a game that simulates
decision-making within energy communities. By leveraging traditional binary
classification metrics (including accuracy, true positive rate, and true
negative rate), we systematically compare these models across different
evaluation scenarios. Our results highlight the strengths and limitations of
each model, revealing trade-offs between sensitivity, specificity, and overall
performance. We demonstrate that while some models excel at identifying correct
responses, others struggle with false positives or inconsistent evaluations.
The findings highlight the need for context-aware evaluation frameworks and
careful model selection when deploying LLMs as evaluators. This work
contributes to the broader discourse on the trustworthiness of AI-driven
assessment tools, offering insights into how different LLM architectures handle
subjective evaluation tasks.

</details>


### [177] [You've Changed: Detecting Modification of Black-Box Large Language Models](https://arxiv.org/abs/2504.12335)
*Alden Dima,James Foulds,Shimei Pan,Philip Feldman*

Main category: cs.CL

TL;DR: 本论文提出一种通过比较文本语言学和心理语言学特征分布来监测LLM变化的方法，使用统计测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM通过API提供服务，开发者难以检测其行为变化。

Method: 使用统计测试比较两个文本样本的特征分布，以判断LLM是否发生变化。

Result: 结果显示，简单文本特征结合统计测试能区分不同语言模型，并检测提示注入攻击。

Conclusion: 这项工作实现了频繁LLM变化监控，避免了计算密集型基准评估。

Abstract: Large Language Models (LLMs) are often provided as a service via an API,
making it challenging for developers to detect changes in their behavior. We
present an approach to monitor LLMs for changes by comparing the distributions
of linguistic and psycholinguistic features of generated text. Our method uses
a statistical test to determine whether the distributions of features from two
samples of text are equivalent, allowing developers to identify when an LLM has
changed. We demonstrate the effectiveness of our approach using five OpenAI
completion models and Meta's Llama 3 70B chat model. Our results show that
simple text features coupled with a statistical test can distinguish between
language models. We also explore the use of our approach to detect prompt
injection attacks. Our work enables frequent LLM change monitoring and avoids
computationally expensive benchmark evaluations.

</details>


### [178] [Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination](https://arxiv.org/abs/2504.12347)
*Mika Setälä,Pieta Sikström,Ville Heilala,Tommi Kärkkäinen*

Main category: cs.CL

TL;DR: 本研究评估大型语言模型（LLMs）在数学推理方面的能力，使用芬兰高中毕业考试，发现模型从中等表现提升到完美分数，突显其快速进步和教育应用潜力。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨LLMs在教育环境中的数学能力，因为尽管它们在教育中显示出潜力，但数学推理仍在发展。

Method: 方法是通过芬兰高中毕业考试（一个高风险数字测试）来评估各种LLMs的数学性能。

Result: 结果显示，初始测试成绩中等，但随着模型演进，表现大幅改善，有些模型达到近乎完美或完美的分数，相当于顶尖学生水平，并能申请大学。

Conclusion: 结论强调LLMs数学能力的快速提升，并展示了其在支持大规模教育评估方面的潜力。

Abstract: Large language models (LLMs) have shown increasing promise in educational
settings, yet their mathematical reasoning has been considered evolving. This
study evaluates the mathematical capabilities of various LLMs using the Finnish
matriculation examination, a high-stakes digital test for upper secondary
education. Initial tests yielded moderate performance corresponding to
mid-range grades, but later evaluations demonstrated substantial improvements
as the language models evolved. Remarkably, some models achieved near-perfect
or perfect scores, matching top student performance and qualifying for
university admission. Our findings highlight the rapid advances in the
mathematical proficiency of LLMs and illustrate their potential to also support
educational assessments at scale.

</details>


### [179] [A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports](https://arxiv.org/abs/2504.12350)
*Jing Wang,Jeremy C Weiss*

Main category: cs.CL

TL;DR: 这篇论文提出了一种系统，将病例报告转化为带时间戳的文本事件对，并使用大语言模型（LLM）进行评估，以支持临床时间分析。


<details>
  <summary>Details</summary>
Motivation: 临床事件的时间信息对患者轨迹分析（如过程追踪、预测和因果推理）至关重要，但现有结构化电子健康记录和临床报告缺乏这种结构化时间定位。

Method: 开发系统将病例报告转化为文本事件和时间戳配对；对比手动和LLM标注（样本分别为n=320和n=390），并评估LLM间协议（n=3,103；N=93）。

Result: LLM模型事件召回率为0.80（中等），事件间时间一致性为0.95（高）。

Conclusion: 通过建立任务、标注和评估系统，并展示高一致性，这项工作可作为利用PubMed开放获取语料库进行时间分析的基准。

Abstract: Timing of clinical events is central to characterization of patient
trajectories, enabling analyses such as process tracing, forecasting, and
causal reasoning. However, structured electronic health records capture few
data elements critical to these tasks, while clinical reports lack temporal
localization of events in structured form. We present a system that transforms
case reports into textual time series-structured pairs of textual events and
timestamps. We contrast manual and large language model (LLM) annotations
(n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access
(PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93).
We find that the LLM models have moderate event recall(O1-preview: 0.80) but
high temporal concordance among identified events (O1-preview: 0.95). By
establishing the task, annotation, and assessment systems, and by demonstrating
high concordance, this work may serve as a benchmark for leveraging the PMOA
corpus for temporal analytics.

</details>


### [180] [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
*Muhammad Ahmad,Muhammad Waqas,ldar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究使用AI驱动的NLP框架从社交媒体数据中检测药物使用和过量症状，准确率高达98%，有助于公共卫生监控。


<details>
  <summary>Details</summary>
Motivation: 药物过量是全球健康问题，传统方法有限，社交媒体提供实时洞察。

Method: 提出AI驱动NLP框架，使用LLM和人工混合标注策略，应用传统ML、神经网络和Transformer模型。

Result: 在多类分类中准确率98%，多标签分类97%，比基线模型高出8%。

Conclusion: 结果突显AI在支持公共卫生监控和个性化干预策略中的潜力。

Abstract: Drug overdose remains a critical global health issue, often driven by misuse
of opioids, painkillers, and psychiatric medications. Traditional research
methods face limitations, whereas social media offers real-time insights into
self-reported substance use and overdose symptoms. This study proposes an
AI-driven NLP framework trained on annotated social media data to detect
commonly used drugs and associated overdose symptoms. Using a hybrid annotation
strategy with LLMs and human annotators, we applied traditional ML models,
neural networks, and advanced transformer-based models. Our framework achieved
98% accuracy in multi-class and 97% in multi-label classification,
outperforming baseline models by up to 8%. These findings highlight the
potential of AI for supporting public health surveillance and personalized
intervention strategies.

</details>


### [181] [A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version](https://arxiv.org/abs/2504.12360)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Dariusz Czerski,Piotr Borkowski*

Main category: cs.CL

TL;DR: 这篇论文研究了带有负相似度的图谱聚类问题，特别是使用doc2vec和GloVe等嵌入方法，并比较了多种解决方案。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统术语向量空间以外的嵌入方法（如doc2vec、GloVe）导致负相似度的问题，以改善聚类性能。

Method: 方法包括讨论组合拉普拉斯和归一化拉普拉斯的解决方案，并通过实验比较6种文献和本研究提出的方法。

Result: 结果显示GloVe嵌入经常导致归一化拉普拉斯基于图谱聚类失败，修复负相似度可以提高组合和归一化拉普拉斯聚类的准确性。

Conclusion: 结论是修复相似度负值不仅提升了聚类准确性，还使解释方法适用于GloVe等嵌入。

Abstract: This paper investigates the problem of Graph Spectral Clustering with
negative similarities, resulting from document embeddings different from the
traditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for
combinatorial Laplacians and normalized Laplacians are discussed. An
experimental investigation shows the advantages and disadvantages of 6
different solutions proposed in the literature and in this research. The
research demonstrates that GloVe embeddings frequently cause failures of
normalized Laplacian based GSC due to negative similarities. Furthermore,
application of methods curing similarity negativity leads to accuracy
improvement for both combinatorial and normalized Laplacian based GSC. It also
leads to applicability for GloVe embeddings of explanation methods developed
originally bythe authors for Term Vector Space embeddings.

</details>


### [182] [Position: The Most Expensive Part of an LLM should be its Training Data](https://arxiv.org/abs/2504.12427)
*Nikhil Kandpal,Colin Raffel*

Main category: cs.CL

TL;DR: 本文主张LLM训练数据的创建成本应是主要开支，并通过估算64个模型的数据成本来支持这一观点。


<details>
  <summary>Details</summary>
Motivation: 强调LLM开发中被忽略的人力劳动成本，并呼吁为训练数据生产者提供公平补偿。

Method: 研究2016年至2024年发布的64个LLM，估算从零开始生产训练数据集的成本，使用保守工资率。

Result: 发现训练数据成本是模型训练成本的10-1000倍，构成LLM提供者的重大财务责任。

Conclusion: 主张训练数据补偿应是最大开支，并讨论未来研究方向以实现更公平的实践。

Abstract: Training a state-of-the-art Large Language Model (LLM) is an increasingly
expensive endeavor due to growing computational, hardware, energy, and
engineering demands. Yet, an often-overlooked (and seldom paid) expense is the
human labor behind these models' training data. Every LLM is built on an
unfathomable amount of human effort: trillions of carefully written words
sourced from books, academic papers, codebases, social media, and more. This
position paper aims to assign a monetary value to this labor and argues that
the most expensive part of producing an LLM should be the compensation provided
to training data producers for their work. To support this position, we study
64 LLMs released between 2016 and 2024, estimating what it would cost to pay
people to produce their training datasets from scratch. Even under highly
conservative estimates of wage rates, the costs of these models' training
datasets are 10-1000 times larger than the costs to train the models
themselves, representing a significant financial liability for LLM providers.
In the face of the massive gap between the value of training data and the lack
of compensation for its creation, we highlight and discuss research directions
that could enable fairer practices in the future.

</details>


### [183] [On Linear Representations and Pretraining Data Frequency in Language Models](https://arxiv.org/abs/2504.12459)
*Jack Merullo,Noah A. Smith,Sarah Wiegreffe,Yanai Elazar*

Main category: cs.CL

TL;DR: 本研究探讨预训练数据频率如何影响语言模型的线性表示，发现频率阈值与事实关系的线性编码密切相关，并提出预测训练数据频率的方法。


<details>
  <summary>Details</summary>
Motivation: 当前对预训练数据与语言模型表示关系的理解有限，本文旨在研究数据频率如何导致线性表示的形成。

Method: 通过分析OLMo-7B和GPT-J模型，研究主题-关系-对象三元组的共现频率与线性表示的相关性，并训练回归模型预测术语频率。

Result: 发现线性表示与预训练频率高度相关，主题和对象分别在1k和2k次共现时线性表示一致出现；回归模型在不同模型上预测准确。

Conclusion: 线性表示强度可揭示预训练语料特性，可通过操纵数据频率阈值来控制和改善模型行为。

Abstract: Pretraining data has a direct impact on the behaviors and quality of language
models (LMs), but we only understand the most basic principles of this
relationship. While most work focuses on pretraining data's effect on
downstream task behavior, we investigate its relationship to LM
representations. Previous work has discovered that, in language models, some
concepts are encoded `linearly' in the representations, but what factors cause
these representations to form? We study the connection between pretraining data
frequency and models' linear representations of factual relations. We find
evidence that the formation of linear representations is strongly connected to
pretraining term frequencies; specifically for subject-relation-object fact
triplets, both subject-object co-occurrence frequency and in-context learning
accuracy for the relation are highly correlated with linear representations.
This is the case across all phases of pretraining. In OLMo-7B and GPT-J, we
discover that a linear representation consistently (but not exclusively) forms
when the subjects and objects within a relation co-occur at least 1k and 2k
times, respectively, regardless of when these occurrences happen during
pretraining. Finally, we train a regression model on measurements of linear
representation quality in fully-trained LMs that can predict how often a term
was seen in pretraining. Our model achieves low error even on inputs from a
different model with a different pretraining dataset, providing a new method
for estimating properties of the otherwise-unknown training data of closed-data
models. We conclude that the strength of linear representations in LMs contains
signal about the models' pretraining corpora that may provide new avenues for
controlling and improving model behavior: particularly, manipulating the
models' training data to meet specific frequency thresholds.

</details>


### [184] [Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex](https://arxiv.org/abs/2504.12474)
*Azadeh Beiranvand,Seyed Mehdi Vahidipour*

Main category: cs.CL

TL;DR: 本论文提出BiGTex模型，结合GNN和LLM处理文本属性图，实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 文本属性图表示学习需捕捉节点文本语义和图结构依赖，但GNN缺乏文本处理能力，LLM忽略图结构。

Method: 提出BiGTex架构，使用堆叠的图-文本融合单元实现双向注意力，并采用LoRA参数高效微调。

Result: 在五个基准数据集上，BiGTex在节点分类中达到最先进性能，并泛化到链接预测；消融研究强调了软提示和双向注意力的重要性。

Conclusion: BiGTex通过紧凑整合GNN和LLM，展示了在文本属性图任务中的有效性和泛化能力。

Abstract: Text-attributed graphs (TAGs) present unique challenges in representation
learning by requiring models to capture both the semantic richness of
node-associated texts and the structural dependencies of the graph. While graph
neural networks (GNNs) excel at modeling topological information, they lack the
capacity to process unstructured text. Conversely, large language models (LLMs)
are proficient in text understanding but are typically unaware of graph
structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel
architecture that tightly integrates GNNs and LLMs through stacked Graph-Text
Fusion Units. Each unit allows for mutual attention between textual and
structural representations, enabling information to flow in both directions,
text influencing structure and structure guiding textual interpretation. The
proposed architecture is trained using parameter-efficient fine-tuning (LoRA),
keeping the LLM frozen while adapting to task-specific signals. Extensive
experiments on five benchmark datasets demonstrate that BiGTex achieves
state-of-the-art performance in node classification and generalizes effectively
to link prediction. An ablation study further highlights the importance of soft
prompting and bi-directional attention in the model's success.

</details>


### [185] [Evaluating the Diversity and Quality of LLM Generated Content](https://arxiv.org/abs/2504.12522)
*Alexander Shypula,Shuo Li,Botong Zhang,Vishakh Padmakumar,Kayo Yin,Osbert Bastani*

Main category: cs.CL

TL;DR: 这篇论文发现，偏好调整技术虽然减少了词汇和句法多样性，但提高了有效语义多样性，因为它产生了更多高质量输出。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，偏好调整技术（如RLHF）减少了模型的多样性，这在需要多样输出的应用中形成困境，因此需要一种更好的测量多样性的框架。

Method: 引入了一个测量有效语义多样性的框架，使用无需人工干预的开放任务，比较了偏好调整模型、SFT模型和基础模型。

Result: 偏好调整模型减少了词汇和句法多样性，但增加了有效语义多样性，因为它们产生了更多高质量输出；较小的模型在生成独特内容时更参数高效。

Conclusion: 这些发现对需要多样和高质输出应用有重要启示，并提供了模型缩放与多样性关系的洞见。

Abstract: Recent work suggests that preference-tuning techniques--including
Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,
as well as alternatives like DPO--reduce diversity, creating a dilemma given
that such models are widely deployed in applications requiring diverse outputs.
To address this, we introduce a framework for measuring effective semantic
diversity--diversity among outputs that meet quality thresholds--which better
reflects the practical utility of large language models (LLMs). Using
open-ended tasks that require no human intervention, we find counterintuitive
results: although preference-tuned models--especially those trained via
RL--exhibit reduced lexical and syntactic diversity, they produce greater
effective semantic diversity than SFT or base models, not from increasing
diversity among high-quality outputs, but from generating more high-quality
outputs overall. We discover that preference tuning reduces syntactic diversity
while preserving semantic diversity--revealing a distinction between diversity
in form and diversity in content that traditional metrics often overlook. Our
analysis further shows that smaller models are consistently more
parameter-efficient at generating unique content within a fixed sampling
budget, offering insights into the relationship between model scaling and
diversity. These findings have important implications for applications that
require diverse yet high-quality outputs, from creative assistance to synthetic
data generation.

</details>


### [186] [Memorization vs. Reasoning: Updating LLMs with New Knowledge](https://arxiv.org/abs/2504.12523)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文引入KUP框架用于评估LLM知识更新，并提出MCT方法显著提升更新性能。


<details>
  <summary>Details</summary>
Motivation: LLM预训练知识难以随实世信息更新，现有的方法仅关注实体替换，无法捕捉复杂动态。

Method: 引入KUP自动管道模拟知识更新，并提出MCT方法，通过在训练中条件化更新语料上的自生成记忆令牌。

Result: KUP基准测试非常具有挑战性，最好模型在间接探针设置下准确率不足2%；MCT训练比基线提升直接探针结果高达25.4%。

Conclusion: MCT方法有效改善LLM知识更新性能，KUP提供了一个全面的评估框架。

Abstract: Large language models (LLMs) encode vast amounts of pre-trained knowledge in
their parameters, but updating them as real-world information evolves remains a
challenge. Existing methodologies and benchmarks primarily target entity
substitutions, failing to capture the full breadth of complex real-world
dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an
automatic pipeline for simulating realistic knowledge updates reflected in an
evidence corpora. KUP's evaluation framework includes direct and indirect
probes to both test memorization of updated facts and reasoning over them, for
any update learning methods. Next, we present a lightweight method called
memory conditioned training (MCT), which conditions tokens in the update corpus
on self-generated "memory" tokens during training. Our strategy encourages LLMs
to surface and reason over newly memorized knowledge at inference. Our results
on two strong LLMs show that (1) KUP benchmark is highly challenging, with the
best CPT models achieving $<2\%$ in indirect probing setting (reasoning) and
(2) MCT training significantly outperforms prior continued pre-training (CPT)
baselines, improving direct probing (memorization) results by up to $25.4\%$.

</details>


### [187] [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
*Iris Ma,Ian Domingo,Alberto Krone-Martins,Pierre Baldi,Cristina V. Lopes*

Main category: cs.CL

TL;DR: 使用Llama 3模型和前缀提示技术，我们以高相似度重构了整本书籍，如《爱丽丝梦游仙境》，发现提取率与书籍流行度相关，并确认了微调模型中缓解措施的失效。


<details>
  <summary>Details</summary>
Motivation: 探讨从大语言模型中提取整本书籍的程度，理解再现风险，以及研究微调如何影响逐字记忆。

Method: 使用Llama 3 70B模型，通过前缀提示技术进行自回归重构，测试多本书籍，并分析微调中权重的变化。

Result: 成功重构一本书以高相似度，流行书籍提取率高，确认缓解措施失效，权重变化主要集中在较低Transformer块的一小部分。

Conclusion: 突显当前再现缓解策略的局限性，并提供框架研究微调对对齐大语言模型逐字记忆的影响。

Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B
family of models, and the "prefix-prompting" extraction technique, we were able
to auto-regressively reconstruct, with a very high level of similarity, one
entire book (Alice's Adventures in Wonderland) from just the first 500 tokens.
We were also able to obtain high extraction rates on several other books,
piece-wise. However, these successes do not extend uniformly to all books. We
show that extraction rates of books correlate with book popularity and thus,
likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama
3.1, following recent work (Nasr et al., 2025). We further find that this
undoing comes from changes to only a tiny fraction of weights concentrated
primarily in the lower transformer blocks. Our results provide evidence of the
limits of current regurgitation mitigation strategies and introduce a framework
for studying how fine-tuning affects the retrieval of verbatim memorization in
aligned LLMs.

</details>


### [188] [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
*Haris Riaz,Sourav Bhabesh,Vinayak Arannil,Miguel Ballesteros,Graham Horwood*

Main category: cs.CL

TL;DR: MetaSynth 通过元提示生成多样化合成数据，用于LLM的领域适应，在专业领域表现出显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据多样性低的问题，该问题限制了其在LLM领域适配中的应用。

Method: 提出MetaSynth 方法，使用元提示协调多个专家LLM代理生成多样化合成数据，仅用2500万tokens适应Mistral-7B-v0.3到金融和生物医学领域。

Result: 在MetaSynth上持续预训练Mistral-7B-v0.3，金融领域改善高达4.08%，生物医学改善高达13.75%；合成数据多样性接近预训练语料，且优于模板提示方法。

Conclusion: 发现使用MetaSynth生成数百万tokens多样化合成数据，无需真实数据，即可有效实现领域适应。

Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data
generated using larger Language models. Questions remain about leveraging
synthetic data for other use cases, such as adapting LLMs to specific domains.
A key limitation of synthetic data is low diversity, which negatively impacts
its downstream applicability for improving other models. To address this, we
propose MetaSynth, a method for generating synthetic data that enhances
diversity through meta-prompting, where a language model orchestrates multiple
"expert" LLM agents to collaboratively generate data. Using only 25 million
tokens of synthetic data generated with MetaSynth, we successfully adapt a
well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and
Biomedicine-without compromising the capabilities of the resulting model in
general tasks. In addition, we evaluate the diversity of our synthetic data
using seven automated metrics, and find that it approaches the diversity of LLM
pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms
the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in
Biomedicine. The same model shows degraded performance when trained on data
generated using a template prompt, even when the template includes prior
generations and varying In-Context exemplars of real data. Our findings suggest
that a few million tokens of diverse synthetic data without mixing any real
data, is sufficient for effective domain adaptation when using MetaSynth.

</details>


### [189] [Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models](https://arxiv.org/abs/2504.12585)
*Liyi Zhang,Veniamin Veselovsky,R. Thomas McCoy,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本研究探讨大型语言模型在确定性任务上的失败，并通过提示和机制解释性技术改善其性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLM由于隐式先验分布影响而在计数或缩写任务中表现不佳的问题。

Method: 方法包括使用提示减少对先验的依赖、通过机制解释性定位神经网络中与先验相关的层、以及对这些层进行轻量级微调。

Result: 结果显示，提示可显著提升性能；识别出与先验概率相关的层；微调后在先验主导任务上达到高性能，且错误不再与先验相关。

Conclusion: 结论表明，正确信息存在于模型表示中，可能通过操纵先验依赖来减少幻觉，提高LLM性能。

Abstract: Large language models (LLMs) sometimes fail to respond appropriately to
deterministic tasks -- such as counting or forming acronyms -- because the
implicit prior distribution they have learned over sequences of tokens
influences their responses. In this work, we show that, in at least some cases,
LLMs actually compute the information needed to perform these tasks correctly,
and we identify some interventions that can allow them to access this
information to improve their performance. First, we show that simply prompting
the language model to not rely on its prior knowledge leads to dramatic
improvements in prior-dominated tasks. We then use mechanistic interpretability
techniques to localize the prior within the LLM and manipulate the extent to
which that prior influences its responses. Specifically, we show that it is
possible to identify layers of the underlying neural network that correlate
with the prior probability of a response and that lightweight finetuning of
these layers with basic prompts on prior-dominated tasks achieves high
performance on held-out answers. These results suggest that the information
required to produce a correct response is contained within the representations
of the problems formed by the models. Furthermore, we show that this finetuning
is significantly more effective for prior-dominated tasks, and that the error
after finetuning is no longer correlated with the prior. Our results suggest
that it may be possible to define effective methods for manipulating the extent
to which LLMs rely upon their priors in solving problems, potentially
increasing their performance in settings where LLMs hallucinate for reasons
related to the prior probability of token sequences.

</details>


### [190] [Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions](https://arxiv.org/abs/2504.12338)
*David Anderson,Michaela Anderson,Margret Bjarnadottir,Stephen Mahar,Shriyan Reyya*

Main category: cs.CL

TL;DR: 本研究使用 GPT-4o-mini 分析临床笔记，提高心脏护理单位死亡率预测，优于传统模型，结合数据后准确性进一步提升。


<details>
  <summary>Details</summary>
Motivation: 传统模型仅使用表格数据，忽略非结构化临床笔记中的诊断、治疗等信息，本研究旨在利用大语言模型提取这些数据以提升预测性能。

Method: 使用 GPT-4o-mini 回答基于患者出院总结的临床问题，将响应作为特征输入逻辑回归模型，数据来自 MIMIC-IV Note 数据集的 14,011 次首次入院记录。

Result: GPT 模型单独使用时优于标准表格模型，结合两者后 AUC 平均提高 5.1 个百分点，最高风险十分位数的阳性预测值提高 29.9%。

Conclusion: 结果强调将大语言模型整合到临床预测中的价值，并指出在非结构化文本数据未充分利用的领域有更广泛应用潜力。

Abstract: There is a long history of building predictive models in healthcare using
tabular data from electronic medical records. However, these models fail to
extract the information found in unstructured clinical notes, which document
diagnosis, treatment, progress, medications, and care plans. In this study, we
investigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical
questions about patients, when given access to the patient's discharge summary,
can support patient-level mortality prediction. Using data from 14,011
first-time admissions to the Coronary Care or Cardiovascular Intensive Care
Units in the MIMIC-IV Note dataset, we implement a transparent framework that
uses GPT responses as input features in logistic regression models. Our
findings demonstrate that GPT-based models alone can outperform models trained
on standard tabular data, and that combining both sources of information yields
even greater predictive power, increasing AUC by an average of 5.1 percentage
points and increasing positive predictive value by 29.9 percent for the
highest-risk decile. These results highlight the value of integrating large
language models (LLMs) into clinical prediction tasks and underscore the
broader potential for using LLMs in any domain where unstructured text data
remains an underutilized resource.

</details>


### [191] [Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://arxiv.org/abs/2504.12637)
*Linda He,Jue Wang,Maurice Weber,Shang Zhu,Ben Athiwaratkun,Ce Zhang*

Main category: cs.CL

TL;DR: 这篇论文引入了一种合成数据生成策略，以高效扩展LLM上下文窗口至100万tokens，同时保持一般任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM在长上下文推理中挣扎，由于计算复杂性和长上下文数据稀缺，且缺乏开源资源。

Method: 提出新型后训练合成数据生成策略，使用逐步旋转位置嵌入(RoPE)缩放训练策略。

Result: 模型在RULER基准和InfiniteBench上表现良好，并在一般语言任务中保持稳健性能。

Conclusion: 该方法有效地解决了长上下文数据稀缺问题，并能扩展到任意长度的上下文。

Abstract: Large Language Models (LLMs) struggle with long-context reasoning, not only
due to the quadratic scaling of computational complexity with sequence length
but also because of the scarcity and expense of annotating long-context data.
There has been barely any open-source work that systematically ablates
long-context data, nor is there any openly available instruction tuning dataset
with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel
post-training synthetic data generation strategy designed to efficiently extend
the context window of LLMs while preserving their general task performance. Our
approach scalably extends to arbitrarily long context lengths, unconstrained by
the length of available real-world data, which effectively addresses the
scarcity of raw long-context data. Through a step-by-step rotary position
embedding (RoPE) scaling training strategy, we demonstrate that our model, with
a context length of up to 1M tokens, performs well on the RULER benchmark and
InfiniteBench and maintains robust performance on general language tasks.

</details>


### [192] [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
*Xiaotian Zhang,Ruizhe Chen,Yang Feng,Zuozhu Liu*

Main category: cs.CL

TL;DR: Persona-judge 是一种无需训练的判别式方法，通过模型内部偏好判断实现语言模型的个性化对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖奖励信号和标注数据导致的计算成本高和可扩展性差的问题，实现对多样人类偏好的适应性对齐。

Method: 使用草稿模型基于给定偏好生成候选标记，并由体现另一偏好的判断模型进行交叉验证，而不需额外训练。

Result: 实验结果表明，Persona-judge 具有可扩展性和计算效率高，在个性化对齐中表现出色。

Conclusion: 该方法为更适应性和定制化的语言模型对齐铺平了道路。

Abstract: Aligning language models with human preferences presents significant
challenges, particularly in achieving personalization without incurring
excessive computational costs. Existing methods rely on reward signals and
additional annotated data, limiting their scalability and adaptability to
diverse human values. To address these challenges, we introduce Persona-judge,
a novel discriminative paradigm that enables training-free personalized
alignment with unseen preferences. Instead of optimizing policy parameters
through external reward feedback, Persona-judge leverages the intrinsic
preference judgment capabilities of the model. Specifically, a draft model
generates candidate tokens conditioned on a given preference, while a judge
model, embodying another preference, cross-validates the predicted tokens
whether to be accepted. Experimental results demonstrate that Persona-judge,
using the inherent preference evaluation mechanisms of the model, offers a
scalable and computationally efficient solution to personalized alignment,
paving the way for more adaptive customized alignment.

</details>


### [193] [ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2504.12673)
*Singon Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CL

TL;DR: ACoRN 通过增强抽象压缩的鲁棒性，改善 RAG 中的性能，减少噪声影响并聚焦关键信息。


<details>
  <summary>Details</summary>
Motivation: 解决抽象压缩器在 RAG 中忽略重要信息的问题，特别是当检索文档包含无关或误导性内容时。

Method: 提出 ACoRN，包括离线数据增强提高噪声鲁棒性和微调以生成以正确答案为中心的关键信息摘要。

Result: 实验显示，使用 ACoRN 训练的 T5-large 模型提升 EM 和 F1 分数，并更好地处理噪声文档。

Conclusion: ACoRN 在真实场景中非常有用，尤其在accuracy 降低的文档环境中。

Abstract: Abstractive compression utilizes smaller langauge models to condense
query-relevant context, reducing computational costs in retrieval-augmented
generation (RAG). However,retrieved documents often include information that is
either irrelevant to answering the query or misleading due to factual incorrect
content, despite having high relevance scores. This behavior indicates that
abstractive compressors are more likely to omit important information essential
for the correct answer, especially in long contexts where attention dispersion
occurs. To address this issue, we categorize retrieved documents in a more
fine-grained manner and propose Abstractive Compression Robust against Noise
(ACoRN), which introduces two novel training steps. First, we use offline data
augmentation on the training dataset to enhance compressor robustness against
two distinct types of retrieval noise. Second, since the language modelbased
compressor cannot fully utilize information from multiple retrieved documents
and exhibits positional bias, we perform finetuning to generate summaries
centered around key information that directly supports the correct answer. Our
experiments demonstrate that T5-large, trained with ACoRN as a compressor,
improves EM and F1 scores while preserving the answer string, which could serve
as direct evidence. ACoRN excels on datasets with many accuracy-reducing
documents, making it highly useful in real-world scenarios.

</details>


### [194] [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)
*Kun-Woo Kim,Ji-Hoon Park,Ju-Min Han,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出GRAIL方法，用于多域遗忘LLM敏感信息，提高知识保留。


<details>
  <summary>Details</summary>
Motivation: LLM学习敏感信息引发社会法律问题，现有的重训练和单域方法成本高或不适用。

Method: GRAIL使用梯度信息和自适应参数定位策略，精确移除目标知识。

Result: 实验显示遗忘成功率与现有方法相当，知识保留成功率提高多达17%。

Conclusion: 确立了管理LLM敏感信息的新范式。

Abstract: Large Language Models (LLMs) trained on extensive datasets often learn
sensitive information, which raises significant social and legal concerns under
principles such as the "Right to be forgotten." Retraining entire models from
scratch to remove undesired information is both costly and impractical.
Furthermore, existing single-domain unlearning methods fail to address
multi-domain scenarios, where knowledge is interwoven across domains such as
privacy and copyright, creating overlapping representations that lead to
excessive knowledge removal or degraded performance. To tackle these issues, we
propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain
unlearning framework. GRAIL leverages gradient information from multiple
domains to precisely distinguish the unlearning scope from the retention scope,
and applies an adaptive parameter-wise localization strategy to selectively
remove targeted knowledge while preserving critical parameters for each domain.
Experimental results on unlearning benchmarks show that GRAIL achieves
unlearning success on par with the existing approaches, while also
demonstrating up to 17% stronger knowledge retention success compared to the
previous state-of-art method. Our findings establish a new paradigm for
effectively managing and regulating sensitive information in large-scale
pre-trained language models.

</details>


### [195] [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
*Yongrui Chen,Junhao He,Linbo Fu,Shenyu Zhang,Rihui Jin,Xinbang Dai,Jiaqi Li,Dehai Min,Nan Hu,Yuxin Zhang,Guilin Qi,Yi Huang,Tongtong Wu*

Main category: cs.CL

TL;DR: 本论文提出Pandora框架，使用Python的Pandas API和LLM统一处理结构化知识推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定策略或自定义表示，限制了知识转移和LLM对齐。

Method: Pandora利用Pandas API构建统一知识表示，LLM生成推理步骤和Python代码，并从训练示例记忆中抽取演示促进知识转移。

Result: 在四个基准的实验中，Pandora优于现有统一框架，并与任务特定方法竞争。

Conclusion: 证明了统一框架的有效性，展示了知识转移和LLM对齐的优势。

Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language
questions (NLQs) by using structured sources such as tables, databases, and
knowledge graphs in a unified way. Existing USKR methods either rely on
employing task-specific strategies or custom-defined representations, which
struggle to leverage the knowledge transfer between different SKR tasks or
align with the prior of LLMs, thereby limiting their performance. This paper
proposes a novel USKR framework named \textsc{Pandora}, which takes advantage
of \textsc{Python}'s \textsc{Pandas} API to construct a unified knowledge
representation for alignment with LLM pre-training. It employs an LLM to
generate textual reasoning steps and executable Python code for each question.
Demonstrations are drawn from a memory of training examples that cover various
SKR tasks, facilitating knowledge transfer. Extensive experiments on four
benchmarks involving three SKR tasks demonstrate that \textsc{Pandora}
outperforms existing unified frameworks and competes effectively with
task-specific methods.

</details>


### [196] [Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration](https://arxiv.org/abs/2504.12773)
*Yicheng Pan,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Jun Du,Jianshu Zhang,Quan Liu,Jianqing Gao,Feng Ma*

Main category: cs.CL

TL;DR: 本文提出GeoGen管道自动生成几何推理数据，并训练GeoLogic模型提升多模态大语言模型在几何问题求解中的性能，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在几何问题求解中面临数据不足和推理幻觉挑战，需要更准确的步进式解决方案。

Method: 提出GeoGen管道使用符号推理生成高质量问答对，并训练GeoLogic模型作为自然语言与符号系统的桥梁来验证输出。

Result: 实验结果显示，该方法显著提高了多模态大语言模型在几何推理任务中的性能。

Conclusion: 整合大语言模型和符号系统的优势，提供更可靠、可解释的几何问题求解方法。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have achieved
remarkable progress in general domains and demonstrated promise in multimodal
mathematical reasoning. However, applying MLLMs to geometry problem solving
(GPS) remains challenging due to lack of accurate step-by-step solution data
and severe hallucinations during reasoning. In this paper, we propose GeoGen, a
pipeline that can automatically generates step-wise reasoning paths for
geometry diagrams. By leveraging the precise symbolic reasoning,
\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To
further enhance the logical reasoning ability of MLLMs, we train
\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated
by GeoGen. Serving as a bridge between natural language and symbolic systems,
GeoLogic enables symbolic tools to help verifying MLLM outputs, making the
reasoning process more rigorous and alleviating hallucinations. Experimental
results show that our approach consistently improves the performance of MLLMs,
achieving remarkable results on benchmarks for geometric reasoning tasks. This
improvement stems from our integration of the strengths of LLMs and symbolic
systems, which enables a more reliable and interpretable approach for the GPS
task. Codes are available at https://github.com/ycpNotFound/GeoGen.

</details>


### [197] [Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication](https://arxiv.org/abs/2504.12891)
*Vicent Briva-Iglesias*

Main category: cs.CL

TL;DR: 这篇论文探讨AI代理在机器翻译中的潜力，焦点在多代理系统如何提升复杂翻译任务的性能，并通过法律翻译试点研究进行验证。


<details>
  <summary>Details</summary>
Motivation: AI代理在机器翻译领域尚未充分探索，需要改进多语种数字通信的准确性和上下文意识。

Method: 通过法律机器翻译的试点研究，采用多代理系统，包括四个专门代理：翻译、 adequacy 审查、 fluency 审查和最终编辑。

Result: 结果显示多代理系统可能提高领域适应性和上下文意识，翻译质量优于传统或单代理系统。

Conclusion: 论文为未来多代理在机器翻译中的应用和整合专业工作流奠定基础，并分享系统演示。

Abstract: The rapid evolution of artificial intelligence (AI) has introduced AI agents
as a disruptive paradigm across various industries, yet their application in
machine translation (MT) remains underexplored. This paper describes and
analyses the potential of single- and multi-agent systems for MT, reflecting on
how they could enhance multilingual digital communication. While single-agent
systems are well-suited for simpler translation tasks, multi-agent systems,
which involve multiple specialized AI agents collaborating in a structured
manner, may offer a promising solution for complex scenarios requiring high
accuracy, domain-specific knowledge, and contextual awareness. To demonstrate
the feasibility of multi-agent workflows in MT, we are conducting a pilot study
in legal MT. The study employs a multi-agent system involving four specialized
AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and
(iv) final editing. Our findings suggest that multi-agent systems may have the
potential to significantly improve domain-adaptability and contextual
awareness, with superior translation quality to traditional MT or single-agent
systems. This paper also sets the stage for future research into multi-agent
applications in MT, integration into professional translation workflows, and
shares a demo of the system analyzed in the paper.

</details>


### [198] [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
*Zhouhao Sun,Xiao Ding,Li Du,Yunpeng Xu,Yixuan Ma,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本论文提出IGCIDB框架，通过信息增益引导的因果干预去偏LLM，提高其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM易捕获数据集偏差，导致泛化能力差，现有的去偏方法因偏差多样性和上下文学习不足而效果有限。

Method: 结合因果机制和信息理论，使用信息增益引导的因果干预自动平衡数据集分布，然后进行标准监督微调。

Result: 实验结果显示IGCIDB能有效去偏LLM，提高其在不同任务中的泛化能力。

Conclusion: IGCIDB框架通过去偏提升了LLM的泛化能力，证明了其有效性。

Abstract: Despite significant progress, recent studies indicate that current large
language models (LLMs) may still capture dataset biases and utilize them during
inference, leading to the poor generalizability of LLMs. However, due to the
diversity of dataset biases and the insufficient nature of bias suppression
based on in-context learning, the effectiveness of previous prior
knowledge-based debiasing methods and in-context learning based automatic
debiasing methods is limited. To address these challenges, we explore the
combination of causal mechanisms with information theory and propose an
information gain-guided causal intervention debiasing (IGCIDB) framework. This
framework first utilizes an information gain-guided causal intervention method
to automatically and autonomously balance the distribution of
instruction-tuning dataset. Subsequently, it employs a standard supervised
fine-tuning process to train LLMs on the debiased dataset. Experimental results
show that IGCIDB can effectively debias LLM to improve its generalizability
across different tasks.

</details>


### [199] [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)
*Chengyi Ju,Weijie Shi,Chengzhong Liu,Jiaming Ji,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文引入 NaVAB 基准，用于评估大型语言模型与五个主要国家（中国、美国、英国、法国、德国）价值观的 alignment，并可通过对齐技术减少冲突。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注伦理审查，无法捕捉国家价值观多样性，且基准不易扩展。

Method: 提出 NaVAB 及其国家价值观提取管道，包括指令标记建模、筛选过程和带冲突减少机制的生成过程。

Result: 实验显示 NaVAB 可识别 LLM 错位场景，并证明其可与对齐技术结合有效降低价值观问题。

Conclusion: NaVAB 提供可扩展方法，帮助提升 LLM 与国家价值观的 alignment。

Abstract: Do Large Language Models (LLMs) hold positions that conflict with your
country's values? Occasionally they do! However, existing works primarily focus
on ethical reviews, failing to capture the diversity of national values, which
encompass broader policy, legal, and moral considerations. Furthermore, current
benchmarks that rely on spectrum tests using manually designed questionnaires
are not easily scalable.
  To address these limitations, we introduce NaVAB, a comprehensive benchmark
to evaluate the alignment of LLMs with the values of five major nations: China,
the United States, the United Kingdom, France, and Germany. NaVAB implements a
national value extraction pipeline to efficiently construct value assessment
datasets. Specifically, we propose a modeling procedure with instruction
tagging to process raw data sources, a screening process to filter
value-related topics and a generation process with a Conflict Reduction
mechanism to filter non-conflicting values.We conduct extensive experiments on
various LLMs across countries, and the results provide insights into assisting
in the identification of misaligned scenarios. Moreover, we demonstrate that
NaVAB can be combined with alignment techniques to effectively reduce value
concerns by aligning LLMs' values with the target country.

</details>


### [200] [Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback](https://arxiv.org/abs/2504.12951)
*Nearchos Potamitis,Akhil Arora*

Main category: cs.CL

TL;DR: 本论文提出一种简单'无反馈重试'机制，提升LLM推理框架，发现它比复杂方法更高效。


<details>
  <summary>Details</summary>
Motivation: LLM进步催生推理框架，但现有迭代策略计算成本高，需要更简单机制。

Method: 引入'无反馈重试'概念，允许LLM在错误答案时重试，无需自反省或反馈。

Result: 简单重试方法往往优于复杂框架，表明复杂策略可能不值其成本。

Conclusion: 挑战复杂策略优越性假设，强调简单方法可达最佳结果，并质疑'重试是否就是你所需要的？'

Abstract: Recent advancements in large language models (LLMs) have catalyzed the
development of general-purpose autonomous agents, demonstrating remarkable
performance in complex reasoning tasks across various domains. This surge has
spurred the evolution of a plethora of prompt-based reasoning frameworks. A
recent focus has been on iterative reasoning strategies that refine outputs
through self-evaluation and verbalized feedback. However, these strategies
require additional computational complexity to enable models to recognize and
correct their mistakes, leading to a significant increase in their cost. In
this work, we introduce the concept of ``retrials without feedback'', an
embarrassingly simple yet powerful mechanism for enhancing reasoning frameworks
by allowing LLMs to retry problem-solving attempts upon identifying incorrect
answers. Unlike conventional iterative refinement methods, our method does not
require explicit self-reflection or verbalized feedback, simplifying the
refinement process. Our findings indicate that simpler retrial-based approaches
often outperform more sophisticated reasoning frameworks, suggesting that the
benefits of complex methods may not always justify their computational costs.
By challenging the prevailing assumption that more intricate reasoning
strategies inherently lead to better performance, our work offers new insights
into how simpler, more efficient approaches can achieve optimal results. So,
are retrials all you need?

</details>


### [201] [Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild](https://arxiv.org/abs/2504.12982)
*Jiatai Wang,Zhiwei Xu,Di Jin,Xuewen Yang,Tao Li*

Main category: cs.CL

TL;DR: 这篇论文分析了LLM在处理知识冲突时的行为，并提出Swin-VIB框架来提升响应生成性能。


<details>
  <summary>Details</summary>
Motivation: LLM存在知识冲突问题，导致响应不可靠和不确定性，需要从信息论角度解决。

Method: 通过信息论分析LLM偏好，并提出Swin-VIB框架，整合变分信息瓶颈模型进行信息增强和偏好指导。

Result: 实验验证了理论发现，单选任务准确率至少提高7.54%，在QA和RAG任务上显示功效。

Conclusion: Swin-VIB框架有效改善LLM响应生成，验证了知识冲突处理策略。

Abstract: The proliferation of large language models (LLMs) has significantly advanced
information retrieval systems, particularly in response generation (RG).
Unfortunately, LLMs often face knowledge conflicts between internal memory and
retrievaled external information, arising from misinformation, biases, or
outdated knowledge. These conflicts undermine response reliability and
introduce uncertainty in decision-making. In this work, we analyze how LLMs
navigate knowledge conflicts from an information-theoretic perspective and
reveal that when conflicting and supplementary information exhibit significant
differences, LLMs confidently resolve their preferences. However, when the
distinction is ambiguous, LLMs experience heightened uncertainty. Based on this
insight, we propose Swin-VIB, a novel framework that integrates a pipeline of
variational information bottleneck models into adaptive augmentation of
retrieved information and guiding LLM preference in response generation.
Extensive experiments on single-choice, open-ended question-answering (QA), and
retrieval augmented generation (RAG) validate our theoretical findings and
demonstrate the efficacy of Swin-VIB. Notably, our method improves
single-choice task accuracy by at least 7.54\% over competitive baselines.

</details>


### [202] [SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation](https://arxiv.org/abs/2504.12996)
*Saransh Agrawal,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出一种针对大型语言模型的目标性遗忘方法，通过因果分析和层级优化，在SemEval-2025任务中获得第二名，同时保持88%的基准准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练中易记忆敏感信息，部署时存在风险；现有机器遗忘方法难以选择性地移除特定数据关联而不降低整体性能。

Method: 采用两阶段方法：结合因果中介分析与层级特定优化，通过OLMo架构的因果追踪实验识别关键层（0-5层），并使用约束优化冻结上层，同时在下层应用联合损失函数以最大化遗忘集损失和最小化保留集偏差。

Result: 在1B模型轨道中获得第二名，同时保持88%的基准MMLU准确率。

Conclusion: 确立了基于因果的层级优化作为高效、精确遗忘的 promising 范式，为解决AI系统数据隐私问题提供了重要进展。

Abstract: Large language models (LLMs) frequently memorize sensitive information during
training, posing risks when deploying publicly accessible models. Current
machine unlearning methods struggle to selectively remove specific data
associations without degrading overall model capabilities. This paper presents
our solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a
two-stage methodology that combines causal mediation analysis with
layer-specific optimization. Through systematic causal tracing experiments on
OLMo architectures (1B and 7B parameters), we identify the critical role of the
first few transformer layers (layers 0-5) in storing subject-attribute
associations within MLP modules. Building on this insight, we develop a
constrained optimization approach that freezes upper layers while applying a
novel joint loss function to lower layers-simultaneously maximizing forget set
loss via output token cross-entropy penalties and minimizing retain set
deviation through adaptive regularization. Our method achieves 2nd place in the
1B model track, demonstrating strong task performance while maintaining 88% of
baseline MMLU accuracy. These results establish causal-informed layer
optimization as a promising paradigm for efficient, precise unlearning in LLMs,
offering a significant step forward in addressing data privacy concerns in AI
systems.

</details>


### [203] [LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard](https://arxiv.org/abs/2504.13125)
*Varun Rao,Youran Sun,Mahendra Kumar,Tejas Mutneja,Agastya Mukherjee,Haizhao Yang*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型在金融任务中的应用，通过微调模型并使用基准测试，展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型在金融任务中的应用，以提升其能力。

Method: 使用Open FinLLM Leaderboard作为基准，基于Qwen2.5和Deepseek-R1模型，采用监督微调(SFT)、直接偏好优化(DPO)和强化学习(RL)等技术。

Result: 微调后的模型在各种金融任务中取得了显著性能提升，并测量了金融领域的數據规模定律。

Conclusion: 证明了大型语言模型在金融应用中的潜力。

Abstract: This paper investigates the application of large language models (LLMs) to
financial tasks. We fine-tuned foundation models using the Open FinLLM
Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed
techniques including supervised fine-tuning (SFT), direct preference
optimization (DPO), and reinforcement learning (RL) to enhance their financial
capabilities. The fine-tuned models demonstrated substantial performance gains
across a wide range of financial tasks. Moreover, we measured the data scaling
law in the financial domain. Our work demonstrates the potential of large
language models (LLMs) in financial applications.

</details>


### [204] [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
*Anamika Lochab,Ruqi Zhang*

Main category: cs.CL

TL;DR: EBRM 是一种轻量级后处理框架，用于提升奖励模型的鲁棒性和泛化能力，而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 奖励模型难以捕捉复杂的人类偏好和泛化到未见数据，因此需要改进方法来处理这些挑战。

Method: EBRM 通过显式建模奖励分布、冲突感知数据过滤、标签噪声感知对比训练和混合初始化来增强奖励模型。

Result: 实证评估显示在安全关键任务中改善高达 5.97%，强化学习实验证明提升了对齐质量并延迟奖励黑客行为。

Conclusion: 这种方法是可扩展且有效的，用于提升现有奖励模型和对齐管道。

Abstract: Reward models (RMs) are essential for aligning Large Language Models (LLMs)
with human preferences. However, they often struggle with capturing complex
human preferences and generalizing to unseen data. To address these challenges,
we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc
refinement framework that enhances RM robustness and generalization. EBRM
models the reward distribution explicitly, capturing uncertainty in human
preferences and mitigating the impact of noisy or misaligned annotations. It
achieves this through conflict-aware data filtering, label-noise-aware
contrastive training, and hybrid initialization. Notably, EBRM enhances RMs
without retraining, making it computationally efficient and adaptable across
different models and tasks. Empirical evaluations on RM benchmarks demonstrate
significant improvements in both robustness and generalization, achieving up to
a 5.97% improvement in safety-critical alignment tasks compared to standard
RMs. Furthermore, reinforcement learning experiments confirm that our refined
rewards enhance alignment quality, effectively delaying reward hacking. These
results demonstrate our approach as a scalable and effective enhancement for
existing RMs and alignment pipelines. The code is available at EBRM.

</details>


### [205] [Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation](https://arxiv.org/abs/2504.13054)
*Yichao Feng,Shuai Zhao,Yueqiu Li,Luwei Xiao,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新框架Self-Aspect Retrieval Enhanced Summary Generation，通过检索机制提升方面-based总结性能，解决大语言模型的令牌限制和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 针对传统总结方法的资源限制和泛化性差，以及大语言模型依赖提示工程、令牌限制和幻觉挑战。

Method: 提出嵌入驱动检索机制，识别给定方面的相关文本段，删除无关部分，优化令牌使用，确保输出基于指定方面。

Result: 在基准数据集实验中，框架表现出优越性能，并有效缓解令牌限制问题。

Conclusion: 该框架改进了方面-based总结的整体效果，证明了检索增强方法的有效性。

Abstract: Aspect-based summarization aims to generate summaries tailored to specific
aspects, addressing the resource constraints and limited generalizability of
traditional summarization approaches. Recently, large language models have
shown promise in this task without the need for training. However, they rely
excessively on prompt engineering and face token limits and hallucination
challenges, especially with in-context learning. To address these challenges,
in this paper, we propose a novel framework for aspect-based summarization:
Self-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely
on in-context learning, given an aspect, we employ an embedding-driven
retrieval mechanism to identify its relevant text segments. This approach
extracts the pertinent content while avoiding unnecessary details, thereby
mitigating the challenge of token limits. Moreover, our framework optimizes
token usage by deleting unrelated parts of the text and ensuring that the model
generates output strictly based on the given aspect. With extensive experiments
on benchmark datasets, we demonstrate that our framework not only achieves
superior performance but also effectively mitigates the token limitation
problem.

</details>


### [206] [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://arxiv.org/abs/2504.13139)
*João Loula,Benjamin LeBrun,Li Du,Ben Lipkin,Clemente Pasti,Gabriel Grand,Tianyu Liu,Yahya Emara,Marjorie Freedman,Jason Eisner,Ryan Cotterel,Vikash Mansinghka,Alexander K. Lew,Tim Vieira,Timothy J. O'Donnell*

Main category: cs.CL

TL;DR: 本文提出基于顺序蒙特卡罗 (SMC) 的架构，用于约束语言模型生成，使小型开源模型在多种任务中超越8倍大的模型。


<details>
  <summary>Details</summary>
Motivation: 许多语言模型应用需要生成符合句法或语义约束的文本，但精确生成通常不可行，因此需要高效方法。

Method: 开发了基于SMC的框架，允许在推理时灵活整合特定约束，并高效重新分配计算资源，构建于Lew et al. (2023)之上。

Result: 小型开源语言模型在Python代码生成、文本到SQL、目标推断和分子合成等任务中胜过更大模型和微调模型，这是由于更好地逼近后验分布。

Conclusion: 该方法提供简单可编程的方式，将SMC应用于各种受控生成问题，提高了性能和效率。

Abstract: A wide range of LM applications require generating text that conforms to
syntactic or semantic constraints. Imposing such constraints can be naturally
framed as probabilistic conditioning, but exact generation from the resulting
distribution -- which can differ substantially from the LM's base distribution
-- is generally intractable. In this work, we develop an architecture for
controlled LM generation based on sequential Monte Carlo (SMC). Our SMC
framework allows us to flexibly incorporate domain- and problem-specific
constraints at inference time, and efficiently reallocate computational
resources in light of new information during the course of generation. By
comparing to a number of alternatives and ablations on four challenging domains
-- Python code generation for data science, text-to-SQL, goal inference, and
molecule synthesis -- we demonstrate that, with little overhead, our approach
allows small open-source language models to outperform models over 8x larger,
as well as closed-source, fine-tuned ones. In support of the probabilistic
perspective, we show that these performance improvements are driven by better
approximation to the posterior distribution. Our system builds on the framework
of Lew et al. (2023) and integrates with its language model probabilistic
programming language, giving users a simple, programmable way to apply SMC to a
broad variety of controlled generation problems.

</details>


### [207] [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
*Sudesh Ramesh Bhagat,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CL

TL;DR: 本研究发现，高准确率的深度学习模型在崩溃叙述分类中与专家一致性较低，而大语言模型尽管准确率较低，却更符合专家意见。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习模型准确率与专家一致性之间的关系，因为在安全关键的NLP应用中，仅靠准确率可能不足。

Method: 评估了五种深度学习模型（包括BERT变体、USE和零-shot分类器）及四种大语言模型（GPT-4、LLaMA 3、Qwen、Claude），使用Cohen's Kappa、PCA和SHAP解释技术对专家标注的数据和叙述文本进行分析。

Result: 结果显示，准确率高的模型与专家一致性较低，而大语言模型更依赖上下文和时间线索，表现出更好的专家对齐。

Conclusion: 结论是，准确率不足以评估安全关键的NLP模型，应将专家一致性作为补充指标，并认可大语言模型在崩溃分析中的潜力。

Abstract: This study explores the relationship between deep learning (DL) model
accuracy and expert agreement in the classification of crash narratives. We
evaluate five DL models -- including BERT variants, the Universal Sentence
Encoder (USE), and a zero-shot classifier -- against expert-labeled data and
narrative text. The analysis is further extended to four large language models
(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive
trend: models with higher technical accuracy often exhibit lower agreement with
domain experts, whereas LLMs demonstrate greater expert alignment despite
relatively lower accuracy scores. To quantify and interpret model-expert
agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and
SHAP-based explainability techniques. Findings indicate that expert-aligned
models tend to rely more on contextual and temporal language cues, rather than
location-specific keywords. These results underscore that accuracy alone is
insufficient for evaluating models in safety-critical NLP applications. We
advocate for incorporating expert agreement as a complementary metric in model
evaluation frameworks and highlight the promise of LLMs as interpretable,
scalable tools for crash analysis pipelines.

</details>


### [208] [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
*Han Wang,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出RAMDocs数据集和MADAM-RAG方法，以同时处理RAG中的歧义、错误信息和噪声，提高LLM响应的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立处理这些挑战，而本文旨在同时考虑多个冲突因素。

Method: 引入RAMDocs数据集模拟复杂场景，并开发MADAM-RAG多代理辩论方法。

Result: 在AmbigDocs上改善高达11.40%，在FaithEval上改善高达15.80%，但基线模型表现较差。

Conclusion: MADAM-RAG有效处理冲突，但证据不平衡时仍有显著差距。

Abstract: Large language model (LLM) agents are increasingly employing
retrieval-augmented generation (RAG) to improve the factuality of their
responses. However, in practice, these systems often need to handle ambiguous
user queries and potentially conflicting information from multiple sources
while also suppressing inaccurate information from noisy or irrelevant
documents. Prior work has generally studied and addressed these challenges in
isolation, considering only one aspect at a time, such as handling ambiguity or
robustness to noise and misinformation. We instead consider multiple factors
simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and
Misinformation in Documents), a new dataset that simulates complex and
realistic scenarios for conflicting evidence for a user query, including
ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent
approach in which LLM agents debate over the merits of an answer over multiple
rounds, allowing an aggregator to collate responses corresponding to
disambiguated entities while discarding misinformation and noise, thereby
handling diverse sources of conflict jointly. We demonstrate the effectiveness
of MADAM-RAG using both closed and open-source models on AmbigDocs -- which
requires presenting all valid answers for ambiguous queries -- improving over
strong RAG baselines by up to 11.40% and on FaithEval -- which requires
suppressing misinformation -- where we improve by up to 15.80% (absolute) with
Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for
existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match
score). While MADAM-RAG begins to address these conflicting factors, our
analysis indicates that a substantial gap remains especially when increasing
the level of imbalance in supporting evidence and misinformation.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [209] [A Two-Phase Perspective on Deep Learning Dynamics](https://arxiv.org/abs/2504.12700)
*Robert de Mello Koch,Animik Ghosh*

Main category: hep-th

TL;DR: 本文提出深度神经网络学习有两个阶段：快速曲线拟合和缓慢压缩，支持泛化现象。


<details>
  <summary>Details</summary>
Motivation: 解释神经网络学习中延迟泛化现象，如grokking、double descent和信息瓶颈。

Method: 通过实证比较不同设置的时间尺度，使用互信息作为进展度量。

Result: 时间尺度对齐，互信息指标浮现，显示压缩阶段重要。

Conclusion: 压缩阶段是原则性遗忘，关键于泛化，但标准训练算法未优化，可能延长。

Abstract: We propose that learning in deep neural networks proceeds in two phases: a
rapid curve fitting phase followed by a slower compression or coarse graining
phase. This view is supported by the shared temporal structure of three
phenomena: grokking, double descent and the information bottleneck, all of
which exhibit a delayed onset of generalization well after training error
reaches zero. We empirically show that the associated timescales align in two
rather different settings. Mutual information between hidden layers and input
data emerges as a natural progress measure, complementing circuit-based metrics
such as local complexity and the linear mapping number. We argue that the
second phase is not actively optimized by standard training algorithms and may
be unnecessarily prolonged. Drawing on an analogy with the renormalization
group, we suggest that this compression phase reflects a principled form of
forgetting, critical for generalization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [210] [Boosting Reservoir Computing with Brain-inspired Adaptive Dynamics](https://arxiv.org/abs/2504.12480)
*Keshav Srinivasan,Dietmar Plenz,Michelle Girvan*

Main category: cs.NE

TL;DR: 本论文提出通过自适应兴奋-抑制平衡机制改善水库计算机（RC）的性能，减少超参数调整需求，并在线性与非线性任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 论文动机是解决RC训练中超参数敏感性和忽略兴奋-抑制平衡的问题，借鉴脑部计算原理以提升鲁棒性和效率。

Method: 方法包括引入局部调整E/I平衡的自适应机制和异质性目标神经元放电率，以实现动态优化。

Result: 结果显示RC在平衡或轻微过抑制状态下性能最佳，任务如记忆容量和时间序列预测改善高达130%。

Conclusion: 结论支持水库设计从静态优化转向动态适应，证明脑启发机制可提升性能并加深神经计算理解。

Abstract: Reservoir computers (RCs) provide a computationally efficient alternative to
deep learning while also offering a framework for incorporating brain-inspired
computational principles. By using an internal neural network with random,
fixed connections$-$the 'reservoir'$-$and training only the output weights, RCs
simplify the training process but remain sensitive to the choice of
hyperparameters that govern activation functions and network architecture.
Moreover, typical RC implementations overlook a critical aspect of neuronal
dynamics: the balance between excitatory and inhibitory (E-I) signals, which is
essential for robust brain function. We show that RCs characteristically
perform best in balanced or slightly over-inhibited regimes, outperforming
excitation-dominated ones. To reduce the need for precise hyperparameter
tuning, we introduce a self-adapting mechanism that locally adjusts E/I balance
to achieve target neuronal firing rates, improving performance by up to 130% in
tasks like memory capacity and time series prediction compared with globally
tuned RCs. Incorporating brain-inspired heterogeneity in target neuronal firing
rates further reduces the need for fine-tuning hyperparameters and enables RCs
to excel across linear and non-linear tasks. These results support a shift from
static optimization to dynamic adaptation in reservoir design, demonstrating
how brain-inspired mechanisms improve RC performance and robustness while
deepening our understanding of neural computation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [211] [Learning-based Delay Compensation for Enhanced Control of Assistive Soft Robots](https://arxiv.org/abs/2504.12428)
*Adrià Mompó Alepuz,Dimitrios Papageorgiou,Silvia Tolu*

Main category: cs.RO

TL;DR: 本论文提出了一种基于学习的非线性状态预测器方法，提高了带延迟软机器人臂的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 软机器人在医疗保健中的控制面临挑战，特别是由于非线性动力学和时间延迟。

Method: 使用Kernel Recursive Least Squares Tracker (KRLST)进行在线学习系统动态，并结合Legendre Delay Network (LDN)压缩输入历史以补偿延迟。

Result: 实验结果显示跟踪性能显著改善，统计分析证实了改进的显著性，且方法计算高效。

Conclusion: 该方法适用于真实场景，有助于在辅助护理中实现更安全、更准确的软机器人控制。

Abstract: Soft robots are increasingly used in healthcare, especially for assistive
care, due to their inherent safety and adaptability. Controlling soft robots is
challenging due to their nonlinear dynamics and the presence of time delays,
especially in applications like a soft robotic arm for patient care. This paper
presents a learning-based approach to approximate the nonlinear state predictor
(Smith Predictor), aiming to improve tracking performance in a two-module soft
robot arm with a short inherent input delay. The method uses Kernel Recursive
Least Squares Tracker (KRLST) for online learning of the system dynamics and a
Legendre Delay Network (LDN) to compress past input history for efficient delay
compensation. Experimental results demonstrate significant improvement in
tracking performance compared to a baseline model-based non-linear controller.
Statistical analysis confirms the significance of the improvements. The method
is computationally efficient and adaptable online, making it suitable for
real-world scenarios and highlighting its potential for enabling safer and more
accurate control of soft robots in assistive care applications.

</details>


### [212] [Learning Transferable Friction Models and LuGre Identification via Physics Informed Neural Networks](https://arxiv.org/abs/2504.12441)
*Asutay Ozmen,João P. Hespanha,Katie Byl*

Main category: cs.RO

TL;DR: 本文提出了一种物理信息驱动的摩擦估计框架，通过学习减少机器人模拟与现实差距，并能在不同系统中泛化。


<details>
  <summary>Details</summary>
Motivation: 准确建模机器人中的摩擦是一个核心挑战，因为模拟器使用简化模型或启发式方法，导致模拟和实际性能存在显著差异。

Method: 提出了一种将成熟摩擦模型与可学习组件相结合的物理信息驱动框架，只需最少的通用测量数据，并强制物理一致性以适应现实复杂性。

Result: 在欠驱动非线性系统中，基于少量噪声数据集训练的模型能准确模拟动态摩擦，减少模拟与现实差距，并转移到未训练系统中。

Conclusion: 这种方法能跨系统泛化，简化摩擦建模，提供可扩展和可解释的途径来桥接机器人和控制中的模拟与现实差距。

Abstract: Accurately modeling friction in robotics remains a core challenge, as
robotics simulators like Mujoco and PyBullet use simplified friction models or
heuristics to balance computational efficiency with accuracy, where these
simplifications and approximations can lead to substantial differences between
simulated and physical performance. In this paper, we present a
physics-informed friction estimation framework that enables the integration of
well-established friction models with learnable components-requiring only
minimal, generic measurement data. Our approach enforces physical consistency
yet retains the flexibility to adapt to real-world complexities. We
demonstrate, on an underactuated and nonlinear system, that the learned
friction models, trained solely on small and noisy datasets, accurately
simulate dynamic friction properties and reduce the sim-to-real gap. Crucially,
we show that our approach enables the learned models to be transferable to
systems they are not trained on. This ability to generalize across multiple
systems streamlines friction modeling for complex, underactuated tasks,
offering a scalable and interpretable path toward bridging the sim-to-real gap
in robotics and control.

</details>


### [213] [Practical Insights on Grasp Strategies for Mobile Manipulation in the Wild](https://arxiv.org/abs/2504.12512)
*Isabella Huang,Richard Cheng,Sangwoon Kim,Dan Kruse,Carolyn Matl,Lukas Kaul,JC Hancock,Shanmuga Harikumar,Mark Tjersland,James Borders,Dan Helmick*

Main category: cs.RO

TL;DR: 本论文开发了SHOPPER机器人，旨在提升在杂货店等非结构化环境中的抓取可靠性，并通过实地测试分析失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前移动操纵机器人抓取能力存在缺陷，无法可靠地在非结构化环境中操作，需要桥接这一差距。

Method: 设计通用的抓取策略，并在真实杂货店部署，详细阐述抓取策略的设计和实施。

Result: 通过实地测试分析数百次抓取尝试的失败模式，识别关键问题。

Conclusion: 提供实际洞见，识别机器人抓取领域的关键挑战，以指导社区未来研究。

Abstract: Mobile manipulation robots are continuously advancing, with their grasping
capabilities rapidly progressing. However, there are still significant gaps
preventing state-of-the-art mobile manipulators from widespread real-world
deployments, including their ability to reliably grasp items in unstructured
environments. To help bridge this gap, we developed SHOPPER, a mobile
manipulation robot platform designed to push the boundaries of reliable and
generalizable grasp strategies. We develop these grasp strategies and deploy
them in a real-world grocery store -- an exceptionally challenging setting
chosen for its vast diversity of manipulable items, fixtures, and layouts. In
this work, we present our detailed approach to designing general grasp
strategies towards picking any item in a real grocery store. Additionally, we
provide an in-depth analysis of our latest real-world field test, discussing
key findings related to fundamental failure modes over hundreds of distinct
pick attempts. Through our detailed analysis, we aim to offer valuable
practical insights and identify key grasping challenges, which can guide the
robotics community towards pressing open problems in the field.

</details>


### [214] [Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking](https://arxiv.org/abs/2504.12616)
*Farhad Nawaz,Minjun Sung,Darshan Gadginmath,Jovin D'sa,Sangjae Bae,David Isele,Nadia Figueroa,Nikolai Matni,Faizan M. Tariq*

Main category: cs.RO

TL;DR: 这篇论文提出了一种改进的路径规划算法，用于停车场景中安全高效地避开动态障碍。


<details>
  <summary>Details</summary>
Motivation: 停车环境中静态和动态障碍物导致路径规划存在安全和效率挑战。

Method: 基于时间索引的Hybrid A*算法变体，整合动态障碍预测，并结合在线规划框架和自适应中间目标。

Result: 在各种停车场景模拟中验证，显著提高了效率和安全性，优于现有样条-based方法。

Conclusion: 该方法在停车路径规划中更具优势，提升了整体性能。

Abstract: Safe and efficient path planning in parking scenarios presents a significant
challenge due to the presence of cluttered environments filled with static and
dynamic obstacles. To address this, we propose a novel and computationally
efficient planning strategy that seamlessly integrates the predictions of
dynamic obstacles into the planning process, ensuring the generation of
collision-free paths. Our approach builds upon the conventional Hybrid A star
algorithm by introducing a time-indexed variant that explicitly accounts for
the predictions of dynamic obstacles during node exploration in the graph, thus
enabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A
star algorithm within an online planning framework to compute local paths at
each planning step, guided by an adaptively chosen intermediate goal. The
proposed method is validated in diverse parking scenarios, including
perpendicular, angled, and parallel parking. Through simulations, we showcase
our approach's potential in greatly improving the efficiency and safety when
compared to the state of the art spline-based planning method for parking
situations.

</details>


### [215] [Biasing the Driving Style of an Artificial Race Driver for Online Time-Optimal Maneuver Planning](https://arxiv.org/abs/2504.12744)
*Sebastiano Taddei,Mattia Piccinini,Francesco Biral*

Main category: cs.RO

TL;DR: 这项工作提出了一种新方法，使用非线性MPC框架偏差人工赛车手的驾驶风格，实现在线时间最优轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 为了理解人类驾驶员选择早顶点或晚顶点 maneuver 的原因，并提高在线轨迹规划的效率。

Method: 基于非线性MPC，引入新终端成本公式，利用前一MPC步骤轨迹，实时适应驾驶风格。

Result: 模拟结果显示，该方法使ARD偏差驾驶风格，达到接近离线MLT的在线圈速，并优于最小时间MPC解决方案。

Conclusion: 证明了方法的有效性，为理解人类驾驶行为提供了新途径。

Abstract: In this work, we present a novel approach to bias the driving style of an
artificial race driver (ARD) for online time-optimal trajectory planning. Our
method leverages a nonlinear model predictive control (MPC) framework that
combines time minimization with exit speed maximization at the end of the
planning horizon. We introduce a new MPC terminal cost formulation based on the
trajectory planned in the previous MPC step, enabling ARD to adapt its driving
style from early to late apex maneuvers in real-time. Our approach is
computationally efficient, allowing for low replan times and long planning
horizons. We validate our method through simulations, comparing the results
against offline minimum-lap-time (MLT) optimal control and online minimum-time
MPC solutions. The results demonstrate that our new terminal cost enables ARD
to bias its driving style, and achieve online lap times close to the MLT
solution and faster than the minimum-time MPC solution. Our approach paves the
way for a better understanding of the reasons behind human drivers' choice of
early or late apex maneuvers.

</details>


### [216] [Imperative MPC: An End-to-End Self-Supervised Learning with Differentiable MPC for UAV Attitude Control](https://arxiv.org/abs/2504.13088)
*Haonan He,Yuheng Qiu,Junyi Geng*

Main category: cs.RO

TL;DR: 提出一个自监督学习框架，结合惯性里程计和可微模型预测控制，用于无人机姿态控制，提高鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统控制和纯数据驱动方法的局限性，如次优性能、低样本效率和模拟到现实差距。

Method: 使用双层优化：内层MPC优化控制动作，外层最小化真实与预测性能差异，端到端自监督训练。

Result: 在大风条件下有效，提升MPC参数学习和IMU预测性能。

Conclusion: 混合方法结合学习和模型优势，提供可解释的鲁棒控制方案。

Abstract: Modeling and control of nonlinear dynamics are critical in robotics,
especially in scenarios with unpredictable external influences and complex
dynamics. Traditional cascaded modular control pipelines often yield suboptimal
performance due to conservative assumptions and tedious parameter tuning. Pure
data-driven approaches promise robust performance but suffer from low sample
efficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid
methods combining learning-based and traditional model-based control in an
end-to-end manner offer a promising alternative. This work presents a
self-supervised learning framework combining learning-based inertial odometry
(IO) module and differentiable model predictive control (d-MPC) for Unmanned
Aerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and
predicts UAV attitudes, which are then optimized by MPC for control actions in
a bi-level optimization (BLO) setup, where the inner MPC optimizes control
actions and the upper level minimizes discrepancy between real-world and
predicted performance. The framework is thus end-to-end and can be trained in a
self-supervised manner. This approach combines the strength of learning-based
perception with the interpretable model-based control. Results show the
effectiveness even under strong wind. It can simultaneously enhance both the
MPC parameter learning and IMU prediction performance.

</details>


### [217] [AUTONAV: A Toolfor Autonomous Navigation of Robots](https://arxiv.org/abs/2504.12318)
*Mir Md Sajid Sarwar,Sudip Samanta,Rajarshi Ray*

Main category: cs.RO

TL;DR: AUTONAV 是一个自动化机器人导航任务的工具，具有模块化架构，便于算法比较，在室内模拟场景中展示了生成的地图和路径规划。


<details>
  <summary>Details</summary>
Motivation: 为了自动化映射、定位和路径规划任务，并便于不同算法的比较，以推进机器人自主导航技术。

Method: 开发了 AUTONAV 工具，使用模块化架构集成各种算法，并通过室内模拟场景进行测试。

Result: 在室内模拟场景中成功生成了地图和路径规划。

Conclusion: AUTONAV 有效地自动化了导航任务并支持算法比较。

Abstract: We present a tool AUTONAV that automates the mapping, localization, and
path-planning tasks for autonomous navigation of robots. The modular
architecture allows easy integration of various algorithms for these tasks for
comparison. We present the generated maps and path-plans by AUTONAV in indoor
simulation scenarios.

</details>


### [218] [A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling](https://arxiv.org/abs/2504.13170)
*Lujie Yang,Tobia Marcucci,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: 本论文引入半定松弛方法处理带有时间缩放的线性系统最优控制，并扩展到分段仿射系统。


<details>
  <summary>Details</summary>
Motivation: 线性系统最优控制问题因系统动力学涉及时间步长与状态、控制的乘积而是非凸的，需要有效的松弛方法。

Method: 提出半定松弛，通过选择 bilinear 项子集和变量变换实现紧凑松弛；扩展到 PWA 系统，使用图中凸集的 shortest-path 问题结合半定程序。

Result: 实验上松弛紧凑，能够通过单个半定程序解决 PWA 最优控制问题。

Conclusion: 该方法提供高效、非凸最优控制问题的求解途径。

Abstract: We introduce a semidefinite relaxation for optimal control of linear systems
with time scaling. These problems are inherently nonconvex, since the system
dynamics involves bilinear products between the discretization time step and
the system state and controls. The proposed relaxation is closely related to
the standard second-order semidefinite relaxation for quadratic constraints,
but we carefully select a subset of the possible bilinear terms and apply a
change of variables to achieve empirically tight relaxations while keeping the
computational load light. We further extend our method to handle
piecewise-affine (PWA) systems by formulating the PWA optimal-control problem
as a shortest-path problem in a graph of convex sets (GCS). In this GCS,
different paths represent different mode sequences for the PWA system, and the
convex sets model the relaxed dynamics within each mode. By combining a tight
convex relaxation of the GCS problem with our semidefinite relaxation with time
scaling, we can solve PWA optimal-control problems through a single
semidefinite program.

</details>


### [219] [Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration](https://arxiv.org/abs/2504.12609)
*Tyler Ga Wei Lum,Olivia Y. Lee,C. Karen Liu,Jeannette Bohg*

Main category: cs.RO

TL;DR: 本论文提出Human2Sim2Robot框架，使用单一RGB-D视频通过模拟强化学习训练机器人灵巧操作策略，桥接人类和机器人形态差异。


<details>
  <summary>Details</summary>
Motivation: 机器人学习灵巧操作需大量演示数据，视频易收集但缺乏行动标签和形态差异问题。

Method: 从人类演示视频提取物体位姿轨迹定义奖励函数和预操作手势引导RL训练，无需可穿戴设备或遥操作。

Result: 在抓取、非prehensile操作和多步任务中，性能提升55%至68%，优于其他方法。

Conclusion: 该框架有效，消除了任务特定奖励调整的需求，仅需两个关键组件。

Abstract: Teaching robots dexterous manipulation skills often requires collecting
hundreds of demonstrations using wearables or teleoperation, a process that is
challenging to scale. Videos of human-object interactions are easier to collect
and scale, but leveraging them directly for robot learning is difficult due to
the lack of explicit action labels from videos and morphological differences
between robot and human hands. We propose Human2Sim2Robot, a novel
real-to-sim-to-real framework for training dexterous manipulation policies
using only one RGB-D video of a human demonstrating a task. Our method utilizes
reinforcement learning (RL) in simulation to cross the human-robot embodiment
gap without relying on wearables, teleoperation, or large-scale data collection
typically necessary for imitation learning methods. From the demonstration, we
extract two task-specific components: (1) the object pose trajectory to define
an object-centric, embodiment-agnostic reward function, and (2) the
pre-manipulation hand pose to initialize and guide exploration during RL
training. We found that these two components are highly effective for learning
the desired task, eliminating the need for task-specific reward shaping and
tuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop
trajectory replay by 55% and imitation learning with data augmentation by 68%
across grasping, non-prehensile manipulation, and multi-step tasks. Project
Site: https://human2sim2robot.github.io

</details>


### [220] [Diffusion Based Robust LiDAR Place Recognition](https://arxiv.org/abs/2504.12412)
*Benjamin Krummenacher,Jonas Frey,Turcan Tuna,Olga Vysotska,Marco Hutter*

Main category: cs.RO

TL;DR: 本论文提出一种基于LiDAR的机器人全局重新定位方法，使用扩散模型在合成数据上训练，针对建筑工地感知混淆问题。


<details>
  <summary>Details</summary>
Motivation: 建筑工地机器人需要精确姿态估计进行自主任务，但重复特征和感知混淆（如相似公寓布局）导致定位挑战。

Method: 训练神经网络使用合成LiDAR点云数据，基于真实建筑网格模拟；采用PointNet++骨干的扩散模型，从单个点云中预测多个位置候选。

Result: 在五个真实数据集上，位置识别准确率平均77% (±2m)，平均误差比基线方法低2倍。

Conclusion: 模型成功预测复杂环境中的全局位置，提供多模态位置分布，抵抗感知混淆影响。

Abstract: Mobile robots on construction sites require accurate pose estimation to
perform autonomous surveying and inspection missions. Localization in
construction sites is a particularly challenging problem due to the presence of
repetitive features such as flat plastered walls and perceptual aliasing due to
apartments with similar layouts inter and intra floors. In this paper, we focus
on the global re-positioning of a robot with respect to an accurate scanned
mesh of the building solely using LiDAR data. In our approach, a neural network
is trained on synthetic LiDAR point clouds generated by simulating a LiDAR in
an accurate real-life large-scale mesh. We train a diffusion model with a
PointNet++ backbone, which allows us to model multiple position candidates from
a single LiDAR point cloud. The resulting model can successfully predict the
global position of LiDAR in confined and complex sites despite the adverse
effects of perceptual aliasing. The learned distribution of potential global
positions can provide multi-modal position distribution. We evaluate our
approach across five real-world datasets and show the place recognition
accuracy of 77% +/-2m on average while outperforming baselines at a factor of 2
in mean error.

</details>


### [221] [Trajectory Adaptation using Large Language Models](https://arxiv.org/abs/2504.12755)
*Anurag Maurya,Tashmoy Ghosh,Ravi Prakash*

Main category: cs.RO

TL;DR: 这篇论文提出使用预训练LLM通过生成代码适应机器人轨迹，处理复杂人类指令，无需训练，在模拟环境中验证有效。


<details>
  <summary>Details</summary>
Motivation: 适应机器人轨迹以响应人类指令是实现更直观和可扩展人机交互的关键需求。

Method: 提出基于语言的框架，利用预训练LLM生成代码适应轨迹，支持数值输入，不需任务特定训练，提供更好可解释性和反馈。

Result: 通过Pybullet和Gazebo模拟实验，验证LLM在机械臂、飞行器和地面机器人上成功适应复杂指令。

Conclusion: 该方法比现有模型更具优势，强调无训练需求、高可解释性和有效反馈机制。

Abstract: Adapting robot trajectories based on human instructions as per new situations
is essential for achieving more intuitive and scalable human-robot
interactions. This work proposes a flexible language-based framework to adapt
generic robotic trajectories produced by off-the-shelf motion planners like
RRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained
LLMs to adapt trajectory waypoints by generating code as a policy for dense
robot manipulation, enabling more complex and flexible instructions than
current methods. This approach allows us to incorporate a broader range of
commands, including numerical inputs. Compared to state-of-the-art
feature-based sequence-to-sequence models which require training, our method
does not require task-specific training and offers greater interpretability and
more effective feedback mechanisms. We validate our approach through simulation
experiments on the robotic manipulator, aerial vehicle, and ground robot in the
Pybullet and Gazebo simulation environments, demonstrating that LLMs can
successfully adapt trajectories to complex human instructions.

</details>


### [222] [Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks](https://arxiv.org/abs/2504.12817)
*Nassim Belmecheri,Arnaud Gotlieb,Nadjib Lazaar,Helge Spieker*

Main category: cs.RO

TL;DR: 本论文探索了图神经网络（GNNs）与定性可解释图（QXGs）的整合，用于自动驾驶中的场景理解，以提升可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有方法仅分析对象对之间单一关系链的局限性，需考虑更广泛的场景上下文以更好地解释交通参与者的行为。

Method: 方法是提出一种新颖的GNN架构，用于处理整个图结构，识别交通场景中的相关对象。

Result: 结果显示，该方法在nuScenes数据集上优于基线方法，能够有效处理类别不平衡并考虑所有对象间的时空关系。

Conclusion: 结论是，结合定性表示与深度学习方法具有潜力，提升自动驾驶系统的可解释场景理解。

Abstract: This paper investigates the integration of graph neural networks (GNNs) with
Qualitative Explainable Graphs (QXGs) for scene understanding in automated
driving. Scene understanding is the basis for any further reactive or proactive
decision-making. Scene understanding and related reasoning is inherently an
explanation task: why is another traffic participant doing something, what or
who caused their actions? While previous work demonstrated QXGs' effectiveness
using shallow machine learning models, these approaches were limited to
analysing single relation chains between object pairs, disregarding the broader
scene context. We propose a novel GNN architecture that processes entire graph
structures to identify relevant objects in traffic scenes. We evaluate our
method on the nuScenes dataset enriched with DriveLM's human-annotated
relevance labels. Experimental results show that our GNN-based approach
achieves superior performance compared to baseline methods. The model
effectively handles the inherent class imbalance in relevant object
identification tasks while considering the complete spatial-temporal
relationships between all objects in the scene. Our work demonstrates the
potential of combining qualitative representations with deep learning
approaches for explainable scene understanding in autonomous driving systems.

</details>


### [223] [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins](https://arxiv.org/abs/2504.13059)
*Yao Mu,Tianxing Chen,Zanxin Chen,Shijia Peng,Zhiqian Lan,Zeyu Gao,Zhixuan Liang,Qiaojun Yu,Yude Zou,Mingkun Xu,Lunkai Lin,Zhiqiang Xie,Mingyu Ding,Ping Luo*

Main category: cs.RO

TL;DR: RoboTwin框架利用3D生成模型和LLM创建数字孪生数据集，提升双臂机器人任务性能，成功率提高超70%。


<details>
  <summary>Details</summary>
Motivation: 解决机器人领域双臂协调和复杂物体操作中数据稀缺和评估基准缺失的问题。

Method: 使用3D生成基础模型和大型语言模型从2D图像生成数字孪生，并引入空间关系感知代码生成框架，创建模拟和真实世界基准。

Result: 在COBOT Magic Robot平台验证，基于RoboTwin数据预训练并微调的策略，使单臂任务成功率提高超过70%，双臂任务提高超过40%。

Conclusion: RoboTwin框架显著提升双臂机器人操作系统的性能，并改善模拟训练与真实世界性能的契合度。

Abstract: In the rapidly advancing field of robotics, dual-arm coordination and complex
object manipulation are essential capabilities for developing advanced
autonomous systems. However, the scarcity of diverse, high-quality
demonstration data and real-world-aligned evaluation benchmarks severely limits
such development. To address this, we introduce RoboTwin, a generative digital
twin framework that uses 3D generative foundation models and large language
models to produce diverse expert datasets and provide a real-world-aligned
evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates
varied digital twins of objects from single 2D images, generating realistic and
interactive scenarios. It also introduces a spatial relation-aware code
generation framework that combines object annotations with large language
models to break down tasks, determine spatial constraints, and generate precise
robotic movement code. Our framework offers a comprehensive benchmark with both
simulated and real-world data, enabling standardized evaluation and better
alignment between simulated training and real-world performance. We validated
our approach using the open-source COBOT Magic Robot platform. Policies
pre-trained on RoboTwin-generated data and fine-tuned with limited real-world
samples demonstrate significant potential for enhancing dual-arm robotic
manipulation systems by improving success rates by over 70% for single-arm
tasks and over 40% for dual-arm tasks compared to models trained solely on
real-world data.

</details>


### [224] [RUKA: Rethinking the Design of Humanoid Hands with Learning](https://arxiv.org/abs/2504.13165)
*Anya Zorin,Irmak Guzey,Billy Yan,Aadhithya Iyer,Lisa Kondrich,Nikhil X. Bhattasali,Lerrel Pinto*

Main category: cs.RO

TL;DR: 这篇论文介绍了RUKA，一款经济实惠、紧凑的肌腱驱动机器人手，通过基于学习的控制方法实现类似人类的抓取，并提供开源设计。


<details>
  <summary>Details</summary>
Motivation: 解决机器人灵巧操作中的硬件权衡问题，特别是通过基于学习的途径来处理肌腱驱动和低成本材料的挑战。

Method: 设计并构建RUKA，使用3D打印部件和现成组件；从MANUS手套的运动捕捉数据中学习关节到执行器和指尖到执行器的模型。

Result: 评估显示RUKA在可达性、耐用性和强度上优于其他机器人手；遥操作任务展示了其灵巧运动。

Conclusion: RUKA提供了一个成本有效的灵巧机器人手解决方案，并通过开源资源促进进一步发展。

Abstract: Dexterous manipulation is a fundamental capability for robotic systems, yet
progress has been limited by hardware trade-offs between precision,
compactness, strength, and affordability. Existing control methods impose
compromises on hand designs and applications. However, learning-based
approaches present opportunities to rethink these trade-offs, particularly to
address challenges with tendon-driven actuation and low-cost materials. This
work presents RUKA, a tendon-driven humanoid hand that is compact, affordable,
and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has
5 fingers with 15 underactuated degrees of freedom enabling diverse human-like
grasps. Its tendon-driven actuation allows powerful grasping in a compact,
human-sized form factor. To address control challenges, we learn
joint-to-actuator and fingertip-to-actuator models from motion-capture data
collected by the MANUS glove, leveraging the hand's morphological accuracy.
Extensive evaluations demonstrate RUKA's superior reachability, durability, and
strength compared to other robotic hands. Teleoperation tasks further showcase
RUKA's dexterous movements. The open-source design and assembly instructions of
RUKA, code, and data are available at https://ruka-hand.github.io/.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [225] [Deep Generative Model-Based Generation of Synthetic Individual-Specific Brain MRI Segmentations](https://arxiv.org/abs/2504.12352)
*Ruijie Wang,Luca Rossetto,Susan Mérillat,Christina Röcke,Mike Martin,Abraham Bernstein*

Main category: q-bio.NC

TL;DR: 这篇论文提出了一种新方法，使用人口统计学、访谈和认知测试信息生成个体的脑MRI分割，而不需要详细的结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要稀缺、昂贵且难以获取的脑结构信息，因此需要一种基于易得信息的方法。

Method: 提出新型深度生成模型CSegSynth，比C-VAE、C-GAN和C-LDM表现更好。

Result: 合成分割质量高，体积预测Pearson相关系数分别为WM 0.80、GM 0.82、CSF 0.70。

Conclusion: 新方法有效，能够生成高质量的个体特定脑MRI分割。

Abstract: To the best of our knowledge, all existing methods that can generate
synthetic brain magnetic resonance imaging (MRI) scans for a specific
individual require detailed structural or volumetric information about the
individual's brain. However, such brain information is often scarce, expensive,
and difficult to obtain. In this paper, we propose the first approach capable
of generating synthetic brain MRI segmentations -- specifically, 3D white
matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations --
for individuals using their easily obtainable and often readily available
demographic, interview, and cognitive test information. Our approach features a
novel deep generative model, CSegSynth, which outperforms existing prominent
generative models, including conditional variational autoencoder (C-VAE),
conditional generative adversarial network (C-GAN), and conditional latent
diffusion model (C-LDM). We demonstrate the high quality of our synthetic
segmentations through extensive evaluations. Also, in assessing the
effectiveness of the individual-specific generation, we achieve superior volume
prediction, with Pearson correlation coefficients reaching 0.80, 0.82, and 0.70
between the ground-truth WM, GM, and CSF volumes of test individuals and those
volumes predicted based on generated individual-specific segmentations,
respectively.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [226] [Integral control of the proximal gradient method for unbiased sparse optimization](https://arxiv.org/abs/2504.12814)
*V. Cerone,S. M. Fosson,A. Re,D. Regruto*

Main category: math.OC

TL;DR: 本论文通过反馈控制优化近端梯度方法，减少偏差并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 近端梯度方法在稀疏优化中受欢迎，但会产生偏差解且收敛缓慢。

Method: 设计积分控制调整超参数，不增加计算复杂度，并分析强凸问题的收敛。

Result: 数值模拟验证理论结果，并扩展到非强凸框架。

Conclusion: 可实现无偏差解，并在合理迭代次数内收敛。

Abstract: Proximal gradient methods are popular in sparse optimization as they are
straightforward to implement. Nevertheless, they achieve biased solutions,
requiring many iterations to converge. This work addresses these issues through
a suitable feedback control of the algorithm's hyperparameter. Specifically, by
designing an integral control that does not substantially impact the
computational complexity, we can reach an unbiased solution in a reasonable
number of iterations. In the paper, we develop and analyze the convergence of
the proposed approach for strongly-convex problems. Moreover, numerical
simulations validate and extend the theoretical results to the non-strongly
convex framework.

</details>


### [227] [Corner Gradient Descent](https://arxiv.org/abs/2504.12519)
*Dmitry Yarotsky*

Main category: math.OC

TL;DR: This paper proposes a generalized stationary SGD with infinite memory to achieve convergence rates up to O(t^{-2ζ}) for stochastic gradient descent on infinite-dimensional quadratic problems with power law spectral conditions, by optimizing a parameter θ using complex plane contours.


<details>
  <summary>Details</summary>
Motivation: To address the limitation that stochastic GD does not achieve the optimal convergence rates O(t^{-2ζ}) due to sampling noise, while deterministic methods can.

Method: Using generalized SGD identified with contours in the complex plane, optimizing the external angle θ to balance acceleration and noise, and approximating with finite-memory algorithms via fast rational approximations.

Result: Proved optimal rate with θ_max = min(2, ν, 2/(ζ + 1/ν)), and demonstrated practical efficiency on synthetic and MNIST datasets.

Conclusion: Generalized SGD with infinite memory can achieve near-optimal rates in stochastic settings, and can be efficiently implemented.

Abstract: We consider SGD-type optimization on infinite-dimensional quadratic problems
with power law spectral conditions. It is well-known that on such problems
deterministic GD has loss convergence rates $L_t=O(t^{-\zeta})$, which can be
improved to $L_t=O(t^{-2\zeta})$ by using Heavy Ball with a non-stationary
Jacobi-based schedule (and the latter rate is optimal among fixed schedules).
However, in the mini-batch Stochastic GD setting, the sampling noise causes the
Jacobi HB to diverge; accordingly no $O(t^{-2\zeta})$ algorithm is known. In
this paper we show that rates up to $O(t^{-2\zeta})$ can be achieved by a
generalized stationary SGD with infinite memory. We start by identifying
generalized (S)GD algorithms with contours in the complex plane. We then show
that contours that have a corner with external angle $\theta\pi$ accelerate the
plain GD rate $O(t^{-\zeta})$ to $O(t^{-\theta\zeta})$. For deterministic GD,
increasing $\theta$ allows to achieve rates arbitrarily close to
$O(t^{-2\zeta})$. However, in Stochastic GD, increasing $\theta$ also amplifies
the sampling noise, so in general $\theta$ needs to be optimized by balancing
the acceleration and noise effects. We prove that the optimal rate is given by
$\theta_{\max}=\min(2,\nu,\tfrac{2}{\zeta+1/\nu})$, where $\nu,\zeta$ are the
exponents appearing in the capacity and source spectral conditions.
Furthermore, using fast rational approximations of the power functions, we show
that ideal corner algorithms can be efficiently approximated by finite-memory
algorithms, and demonstrate their practical efficiency on a synthetic problem
and MNIST.

</details>


### [228] [On the asymptotic behaviour of stochastic processes, with applications to supermartingale convergence, Dvoretzky's approximation theorem, and stochastic quasi-Fejér monotonicity](https://arxiv.org/abs/2504.12922)
*Morenikeji Neri,Nicholas Pischke,Thomas Powell*

Main category: math.OC

TL;DR: 本文证明了随机过程在松弛超鞅条件下渐近行为的通用结果，提供显式收敛速率，并应用于各种随机逼近定理。


<details>
  <summary>Details</summary>
Motivation: 为了提供定量且有效的收敛速率，依赖于最少的数据，填补现有定性结果的空白。

Method: 通过证明一个新定理，使用松弛超鞅条件构建显式速率，并应用于具体案例如Robbins-Siegmund定理、Dvoretzky定理和随机拟Fejér单调序列。

Result: 显式且统一的均值和几乎处处收敛速率；新定量版本的几个定理；特殊情况下的线性速率。

Conclusion: 结果在随机逼近中有广泛应用，以Robbins-Monro过程为例，并具有应用于其他迭代方法的潜力。

Abstract: We prove a novel and general result on the asymptotic behavior of stochastic
processes which conform to a certain relaxed supermartingale condition. Our
result provides quantitative information in the form of an explicit and
effective construction of a rate of convergence for this process, both in mean
and almost surely, that is moreover highly uniform in the sense that it only
depends on very few data of the surrounding objects involved in the iteration.
We then apply this result to derive new quantitative versions of well-known
concepts and theorems from stochastic approximation, in particular providing
effective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's
convergence theorem, as well as the convergence of stochastic quasi-Fej\'er
monotone sequences, the latter of which formulated in a novel and highly
general metric context. We utilize the classic and widely studied Robbins-Monro
procedure as a template to evaluate our quantitative results and their
applicability in greater detail. We conclude by illustrating the breadth of
potential further applications with a brief discussion on a variety of other
well-known iterative procedures from stochastic approximation, covering a range
of different applied scenarios to which our methods can be immediately applied.
Throughout, we isolate and discuss special cases of our results which even
allow for the construction of fast, and in particular linear, rates.

</details>
