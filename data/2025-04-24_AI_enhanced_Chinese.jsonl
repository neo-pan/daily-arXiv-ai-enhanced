{"id": "2504.16115", "pdf": "https://arxiv.org/pdf/2504.16115", "abs": "https://arxiv.org/abs/2504.16115", "authors": ["Yibo Jacky Zhang", "Sanmi Koyejo"], "title": "A Framework for Objective-Driven Dynamical Stochastic Fields", "categories": ["cs.AI", "cs.LG", "cs.MA", "nlin.AO"], "comment": null, "summary": "Fields offer a versatile approach for describing complex systems composed of\ninteracting and dynamic components. In particular, some of these dynamical and\nstochastic systems may exhibit goal-directed behaviors aimed at achieving\nspecific objectives, which we refer to as $\\textit{intelligent fields}$.\nHowever, due to their inherent complexity, it remains challenging to develop a\nformal theoretical description of such systems and to effectively translate\nthese descriptions into practical applications. In this paper, we propose three\nfundamental principles -- complete configuration, locality, and purposefulness\n-- to establish a theoretical framework for understanding intelligent fields.\nMoreover, we explore methodologies for designing such fields from the\nperspective of artificial intelligence applications. This initial investigation\naims to lay the groundwork for future theoretical developments and practical\nadvances in understanding and harnessing the potential of such objective-driven\ndynamical stochastic fields.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u667a\u80fd\u573a\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u4e09\u4e2a\u539f\u5219\uff0c\u5e76\u4e3aAI\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u667a\u80fd\u573a\u7684\u590d\u6742\u6027\u4f7f\u5f97\u6b63\u5f0f\u63cf\u8ff0\u548c\u5b9e\u7528\u5e94\u7528\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u5b8c\u6574\u914d\u7f6e\u3001\u5c40\u90e8\u6027\u3001\u76ee\u7684\u6027\u539f\u5219\uff0c\u5e76\u63a2\u7d22AI\u89c6\u89d2\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u521d\u6b65\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u4e3a\u7406\u89e3\u548c\u5229\u7528\u76ee\u6807\u5bfc\u5411\u52a8\u6001\u968f\u673a\u573a\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.16209", "pdf": "https://arxiv.org/pdf/2504.16209", "abs": "https://arxiv.org/abs/2504.16209", "authors": ["Paul Zaidins", "Robert P. Goldman", "Ugur Kuter", "Dana Nau", "Mark Roberts"], "title": "HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods", "categories": ["cs.AI", "I.2.8"], "comment": "20 pages; 19 figures; To appear in the Proceedings for ICAPS 2025,\n  the 35th International Conference on Automated Planning and Schedulings", "summary": "This paper provides theoretical and empirical comparisons of three recent\nhierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our\ntheoretical results show that the three algorithms correspond to three\ndifferent definitions of the plan repair problem, leading to differences in the\nalgorithms' search spaces, the repair problems they can solve, and the kinds of\nrepairs they can make. Understanding these distinctions is important when\nchoosing a repair method for any given application.\n  Building on the theoretical results, we evaluate the algorithms empirically\nin a series of benchmark planning problems. Our empirical results provide more\ndetailed insight into the runtime repair performance of these systems and the\ncoverage of the repair problems solved, based on algorithmic properties such as\nreplanning, chronological backtracking, and backjumping over plan trees.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86SHOPFixer\u3001IPyHOPPER\u548cRewrite\u4e09\u79cd\u8ba1\u5212\u4fee\u590d\u7b97\u6cd5\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u6027\u80fd\uff0c\u5f3a\u8c03\u7b97\u6cd5\u5dee\u5f02\u5728\u9009\u62e9\u5e94\u7528\u65f6\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u7b97\u6cd5\u95f4\u5b9a\u4e49\u3001\u641c\u7d22\u7a7a\u95f4\u548c\u4fee\u590d\u80fd\u529b\u7684\u5dee\u5f02\uff0c\u5e2e\u52a9\u9009\u62e9\u9002\u5408\u7684\u5177\u4f53\u5e94\u7528\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6bd4\u8f83\u7b97\u6cd5\u5b9a\u4e49\u548c\u5b9e\u8bc1\u8bc4\u4f30\u57fa\u51c6\u89c4\u5212\u95ee\u9898\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u663e\u793a\u7b97\u6cd5\u5728\u641c\u7d22\u7a7a\u95f4\u548c\u4fee\u590d\u7c7b\u578b\u4e0a\u7684\u5dee\u5f02\uff1b\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u8fd0\u884c\u65f6\u6027\u80fd\u548c\u95ee\u9898\u8986\u76d6\u7387\u7684insights\u3002", "conclusion": "\u7406\u89e3\u8fd9\u4e9b\u533a\u522b\u5bf9\u9009\u62e9\u5408\u9002\u7684\u4fee\u590d\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.16273", "pdf": "https://arxiv.org/pdf/2504.16273", "abs": "https://arxiv.org/abs/2504.16273", "authors": ["Joseph Lee", "Tianqi Shang", "Jae Young Baik", "Duy Duong-Tran", "Shu Yang", "Lingyao Li", "Li Shen"], "title": "Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to GenAI4Health Workshop @ AAAI 2025", "summary": "Large Language Models (LLMs) have shown promise in clinical decision support,\nyet their application to triage remains underexplored. We systematically\ninvestigate the capabilities of LLMs in emergency department triage through two\nkey dimensions: (1) robustness to distribution shifts and missing data, and (2)\ncounterfactual analysis of intersectional biases across sex and race. We assess\nmultiple LLM-based approaches, ranging from continued pre-training to\nin-context learning, as well as machine learning approaches. Our results\nindicate that LLMs exhibit superior robustness, and we investigate the key\nfactors contributing to the promising LLM-based approaches. Furthermore, in\nthis setting, we identify gaps in LLM preferences that emerge in particular\nintersections of sex and race. LLMs generally exhibit sex-based differences,\nbut they are most pronounced in certain racial groups. These findings suggest\nthat LLMs encode demographic preferences that may emerge in specific clinical\ncontexts or particular combinations of characteristics.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5LLMs\u5728\u6025\u8bca\u5206\u8bca\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u504f\u5dee\uff0c\u53d1\u73b0LLMs\u9c81\u68d2\u6027\u5f3a\uff0c\u4f46\u5b58\u5728\u6027\u522b\u548c\u79cd\u65cf\u4ea4\u53c9\u504f\u597d\u3002", "motivation": "LLMs\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u6f5c\u529b\u5927\uff0c\u4f46\u5206\u8bca\u5e94\u7528\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u9c81\u68d2\u6027\u6d4b\u8bd5\u548c\u504f\u5dee\u5206\u6790\uff0c\u8bc4\u4f30\u591a\u79cdLLM\u65b9\u6cd5\u5305\u62ec\u9884\u8bad\u7ec3\u548c\u5b66\u4e60\uff0c\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "LLMs\u663e\u793a\u4f18\u8d8a\u9c81\u68d2\u6027\uff0c\u5e76\u63ed\u793a\u7279\u5b9a\u6027\u522b\u548c\u79cd\u65cf\u4ea4\u53c9\u70b9\u4e0a\u7684\u504f\u5dee\u504f\u597d\u3002", "conclusion": "LLMs\u53ef\u80fd\u7f16\u7801\u4eba\u53e3\u7edf\u8ba1\u504f\u597d\uff0c\u5728\u7279\u5b9a\u4e34\u5e8a\u60c5\u5883\u4e2d\u663e\u73b0\u3002"}}
{"id": "2504.16622", "pdf": "https://arxiv.org/pdf/2504.16622", "abs": "https://arxiv.org/abs/2504.16622", "authors": ["Christoforus Yoga Haryanto", "Emily Lomempow"], "title": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems", "categories": ["cs.AI", "cs.CY"], "comment": "Working Paper, 37 pages, 1 figure, 5 tables", "summary": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5047\u8bbe\u7684\u8ba4\u77e5\u8ba1\u7b97\u67b6\u6784\u6846\u67b6'\u8ba4\u77e5\u7845'\uff0c\u9488\u5bf92035\u5e74\u7684\u53d1\u5c55\uff0c\u65e8\u5728\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u7684\u57fa\u7840\u9650\u5236\u3002", "motivation": "\u63ed\u793a\u786e\u5b9a\u6027\u4eba\u7c7b\u7f16\u5199\u7684\u8ba1\u7b97\u67b6\u6784\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u8ba4\u77e5\u8ba1\u7b97\u7cfb\u7edf\u7684\u53d1\u5c55\u8f68\u8ff9\u3002", "method": "\u901a\u8fc7\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fa9\u8bc1\u5f0f\u5171\u540c\u8bbe\u8ba1\uff0c\u5728\u4e0d\u5bf9\u79f0\u8ba4\u77e5\u6761\u4ef6\u4e0b\u521b\u5efa\u7ed3\u6784\u5316\u6469\u64e6\u4ee5\u66b4\u9732\u76f2\u70b9\u548c\u6743\u8861\u3002", "result": "\u63d0\u51fa\u6574\u5408\u7b26\u53f7\u652f\u67b6\u3001\u6cbb\u7406\u5185\u5b58\u548c\u5bf9\u9f50\u611f\u77e5\u6267\u884c\u7684\u67b6\u6784\uff0c\u7406\u8bba\u4e0a\u4e0e\u81ea\u7531\u80fd\u91cf\u539f\u5219\u4e00\u81f4\uff0c\u63d0\u4f9b\u8ba4\u77e5\u7cfb\u7edf\u7ef4\u62a4\u8eab\u4efd\u7684\u673a\u5236\u3002", "conclusion": "\u76ee\u6807\u662f\u6784\u5efa\u9053\u5fb7\u4e0a\u53ef\u5904\u7406\u7684\u8ba4\u77e5\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u786c\u4ef6\u7ea6\u675f\u548c\u8eab\u4efd\u673a\u5236\u4fdd\u6301\u4eba\u7c7b\u5bf9\u9f50\u3002"}}
{"id": "2504.16211", "pdf": "https://arxiv.org/pdf/2504.16211", "abs": "https://arxiv.org/abs/2504.16211", "authors": ["Kunpeng Zhang", "Lei Xu", "Xinlei Yi", "Guanghui Wen", "Lihua Xie", "Tianyou Chai", "Tao Yang"], "title": "One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints", "categories": ["eess.SY", "cs.SY", "eess.SP"], "comment": "15 pages, 3 figures", "summary": "This paper considers the distributed bandit convex optimization problem with\ntime-varying constraints. In this problem, the global loss function is the\naverage of all the local convex loss functions, which are unknown beforehand.\nEach agent iteratively makes its own decision subject to time-varying\ninequality constraints which can be violated but are fulfilled in the long run.\nFor a uniformly jointly strongly connected time-varying directed graph, a\ndistributed bandit online primal-dual projection algorithm with one-point\nsampling is proposed. We show that sublinear dynamic network regret and network\ncumulative constraint violation are achieved if the path-length of the\nbenchmark also increases in a sublinear manner. In addition, an\n$\\mathcal{O}({T^{3/4 + g}})$ static network regret bound and an $\\mathcal{O}(\n{{T^{1 - {g}/2}}} )$ network cumulative constraint violation bound are\nestablished, where $T$ is the total number of iterations and $g \\in ( {0,1/4}\n)$ is a trade-off parameter. Moreover, a reduced static network regret bound\n$\\mathcal{O}( {{T^{2/3 + 4g /3}}} )$ is established for strongly convex local\nloss functions. Finally, a numerical example is presented to validate the\ntheoretical results.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5e26\u5bbd\u5728\u7ebf\u7b97\u6cd5\uff0c\u5904\u7406\u65f6\u95f4\u53d8\u5316\u7ea6\u675f\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u754c\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u672a\u77e5\u635f\u5931\u51fd\u6570\u548c\u65f6\u95f4\u53d8\u5316\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4ee3\u7406\u51b3\u7b56\u957f\u671f\u6ee1\u8db3\u7ea6\u675f\u3002", "method": "\u65b9\u6cd5\u662f\u5206\u5e03\u5f0f\u5e26\u5bbd\u5728\u7ebf\u539f-\u5bf9\u5076\u6295\u5f71\u7b97\u6cd5\uff0c\u4f7f\u7528\u4e00\u70b9\u91c7\u6837\uff0c\u9002\u7528\u4e8e\u5747\u5300\u8054\u5408\u5f3a\u8fde\u901a\u7684\u65f6\u95f4\u53d8\u5316\u6709\u5411\u56fe\u3002", "result": "\u7ed3\u679c\u5305\u62ec\u6b21\u7ebf\u6027\u52a8\u6001\u7f51\u7edc\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\uff1b\u9759\u6001\u9057\u61be\u4e3aO(T^{3/4 + g})\uff0c\u7ea6\u675f\u8fdd\u53cd\u4e3aO(T^{1 - g/2})\uff1b\u5f3a\u51f8\u635f\u5931\u4e0b\u9057\u61be\u4e3aO(T^{2/3 + 4g/3})\u3002", "conclusion": "\u7ed3\u8bba\u901a\u8fc7\u6570\u503c\u4f8b\u5b50\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2504.16109", "pdf": "https://arxiv.org/pdf/2504.16109", "abs": "https://arxiv.org/abs/2504.16109", "authors": ["Jun-Peng Jiang", "Si-Yang Liu", "Hao-Run Cai", "Qile Zhou", "Han-Jia Ye"], "title": "Representation Learning for Tabular Data: A Comprehensive Survey", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data, structured as rows and columns, is among the most prevalent\ndata types in machine learning classification and regression applications.\nModels for learning from tabular data have continuously evolved, with Deep\nNeural Networks (DNNs) recently demonstrating promising results through their\ncapability of representation learning. In this survey, we systematically\nintroduce the field of tabular representation learning, covering the\nbackground, challenges, and benchmarks, along with the pros and cons of using\nDNNs. We organize existing methods into three main categories according to\ntheir generalization capabilities: specialized, transferable, and general\nmodels. Specialized models focus on tasks where training and evaluation occur\nwithin the same data distribution. We introduce a hierarchical taxonomy for\nspecialized models based on the key aspects of tabular data -- features,\nsamples, and objectives -- and delve into detailed strategies for obtaining\nhigh-quality feature- and sample-level representations. Transferable models are\npre-trained on one or more datasets and subsequently fine-tuned on downstream\ntasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,\nor even cross-modalities such as vision and language. General models, also\nknown as tabular foundation models, extend this concept further, allowing\ndirect application to downstream tasks without fine-tuning. We group these\ngeneral models based on the strategies used to adapt across heterogeneous\ndatasets. Additionally, we explore ensemble methods, which integrate the\nstrengths of multiple tabular models. Finally, we discuss representative\nextensions of tabular learning, including open-environment tabular machine\nlearning, multimodal learning with tabular data, and tabular understanding.\nMore information can be found in the following repository:\nhttps://github.com/LAMDA-Tabular/Tabular-Survey.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u67e5\u7efc\u8ff0\u7cfb\u7edf\u4ecb\u7ecd\u4e86\u8868\u683c\u6570\u636e\u8868\u793a\u5b66\u4e60\uff0c\u5305\u62ec\u80cc\u666f\u3001\u6311\u6218\u3001\u6a21\u578b\u5206\u7c7b\u53ca\u4f18\u7f3a\u70b9\u3002", "motivation": "\u8868\u683c\u6570\u636e\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0cDNNs\u663e\u793a\u51fa\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u6f5c\u529b\uff0c\u9700\u8981\u603b\u7ed3\u73b0\u6709\u65b9\u6cd5\u4ee5\u63a8\u52a8\u53d1\u5c55\u3002", "method": "\u5c06\u6a21\u578b\u5206\u4e3a\u4e13\u4e1a\u3001\u53ef\u8f6c\u79fb\u548c\u901a\u7528\u7c7b\u578b\uff0c\u5e76\u57fa\u4e8e\u7279\u5f81\u3001\u6837\u672c\u548c\u76ee\u6807\u5efa\u7acb\u5206\u5c42\u5206\u7c7b\uff0c\u63a2\u8ba8\u96c6\u6210\u548c\u6269\u5c55\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u73af\u5883\u3001\u591a\u6a21\u6001\u5b66\u4e60\u548c\u8868\u683c\u7406\u89e3\u7684\u5e94\u7528\u3002", "conclusion": "\u5f3a\u8c03\u8868\u683c\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u8d44\u6e90\u4ed3\u5e93\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.16635", "pdf": "https://arxiv.org/pdf/2504.16635", "abs": "https://arxiv.org/abs/2504.16635", "authors": ["Fredy Pokou", "Jules Sadefo Kamdem", "Fran\u00e7ois Benhmad"], "title": "Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models", "categories": ["cs.AI", "q-fin.CP", "q-fin.RM", "q-fin.ST"], "comment": null, "summary": "In an environment of increasingly volatile financial markets, the accurate\nestimation of risk remains a major challenge. Traditional econometric models,\nsuch as GARCH and its variants, are based on assumptions that are often too\nrigid to adapt to the complexity of the current market dynamics. To overcome\nthese limitations, we propose a hybrid framework for Value-at-Risk (VaR)\nestimation, combining GARCH volatility models with deep reinforcement learning.\nOur approach incorporates directional market forecasting using the Double Deep\nQ-Network (DDQN) model, treating the task as an imbalanced classification\nproblem. This architecture enables the dynamic adjustment of risk-level\nforecasts according to market conditions. Empirical validation on daily\nEurostoxx 50 data covering periods of crisis and high volatility shows a\nsignificant improvement in the accuracy of VaR estimates, as well as a\nreduction in the number of breaches and also in capital requirements, while\nrespecting regulatory risk thresholds. The ability of the model to adjust risk\nlevels in real time reinforces its relevance to modern and proactive risk\nmanagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408GARCH\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u6846\u67b6\uff0c\u6539\u8fdb\u6ce2\u52a8\u6027\u5e02\u573a\u4e2d\u7684VaR\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5982GARCH\u5047\u8bbe\u8fc7\u4e8e\u521a\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u5e02\u573a\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u98ce\u9669\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GARCH\u6ce2\u52a8\u6a21\u578b\u4e0eDDQN\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5c06\u5e02\u573a\u65b9\u5411\u9884\u6d4b\u89c6\u4e3a\u4e0d\u5e73\u8861\u5206\u7c7b\u95ee\u9898\uff0c\u5b9e\u73b0\u98ce\u9669\u6c34\u5e73\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5728Eurostoxx 50\u6570\u636e\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793a\uff0cVaR\u4f30\u8ba1\u51c6\u786e\u6027\u63d0\u5347\uff0c\u8fdd\u89c4\u6b21\u6570\u51cf\u5c11\uff0c\u8d44\u672c\u8981\u6c42\u964d\u4f4e\uff0c\u5e76\u7b26\u5408\u76d1\u7ba1\u6807\u51c6\u3002", "conclusion": "\u6a21\u578b\u80fd\u5b9e\u65f6\u8c03\u6574\u98ce\u9669\u6c34\u5e73\uff0c\u63d0\u5347\u73b0\u4ee3\u98ce\u9669\u7ba1\u7406\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.16222", "pdf": "https://arxiv.org/pdf/2504.16222", "abs": "https://arxiv.org/abs/2504.16222", "authors": ["Matthew S. Hankins", "Jair Cert\u00f3rio", "Tzuyu Jeng", "Nuno C. Martins"], "title": "Nash Equilibrium Learning In Large Populations With First Order Payoff Modifications", "categories": ["eess.SY", "cs.GT", "cs.SY", "math.DS", "math.OC"], "comment": "6 pages, 3 figures", "summary": "We establish Nash equilibrium learning -- convergence of the population state\nto a suitably defined Nash equilibria set -- for a class of payoff dynamical\nmechanism with a first order modification. The first order payoff modification\ncan model aspects of the agents' bounded rationality, anticipatory or averaging\nterms in the payoff mechanism, or first order Pad\\'e approximations of delays.\nTo obtain our main results, we apply a combination of two nonstandard\nsystem-theoretic passivity notions.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86Nash\u5747\u8861\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u5728\u5177\u6709\u4e00\u9636\u4fee\u6b63\u7684\u6536\u76ca\u52a8\u6001\u673a\u5236\u4e0b\uff0c\u79cd\u7fa4\u72b6\u6001\u6536\u655b\u5230Nash\u5747\u8861\u96c6\u3002", "motivation": "\u52a8\u673a\u662f\u6a21\u62df\u4ee3\u7406\u4eba\u7684\u6709\u9650\u7406\u6027\u3001\u9884\u89c1\u6027\u6216\u5e73\u5747\u9879\uff0c\u4ee5\u53ca\u5ef6\u8fdf\u7684\u4e00\u9636Pad\u00e9\u8fd1\u4f3c\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u975e\u6807\u51c6\u7cfb\u7edf\u7406\u8bba\u7684\u949d\u6027\u6982\u5ff5\u3002", "result": "\u7ed3\u679c\u662f\u8bc1\u5b9e\u4e86Nash\u5747\u8861\u7684\u6536\u655b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6210\u529f\u5efa\u7acb\u4e86\u8fd9\u79cd\u6536\u655b\u673a\u5236\uff0c\u53ef\u7528\u4e8e\u5904\u7406\u73b0\u5b9e\u590d\u6742\u6027\u3002"}}
{"id": "2504.16136", "pdf": "https://arxiv.org/pdf/2504.16136", "abs": "https://arxiv.org/abs/2504.16136", "authors": ["Chiung-Yi Tseng", "Junhao Song", "Ziqian Bi", "Tianyang Wang", "Chia Xin Liang", "Ming Liu"], "title": "Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement", "categories": ["cs.LG"], "comment": null, "summary": "In the era of data-driven intelligence, the paradox of data abundance and\nannotation scarcity has emerged as a critical bottleneck in the advancement of\nmachine learning. This paper gives a detailed overview of Active Learning (AL),\nwhich is a strategy in machine learning that helps models achieve better\nperformance using fewer labeled examples. It introduces the basic concepts of\nAL and discusses how it is used in various fields such as computer vision,\nnatural language processing, transfer learning, and real-world applications.\nThe paper focuses on important research topics such as uncertainty estimation,\nhandling of class imbalance, domain adaptation, fairness, and the creation of\nstrong evaluation metrics and benchmarks. It also shows that learning methods\ninspired by humans and guided by questions can improve data efficiency and help\nmodels learn more effectively. In addition, this paper talks about current\nchallenges in the field, including the need to rebuild trust, ensure\nreproducibility, and deal with inconsistent methodologies. It points out that\nAL often gives better results than passive learning, especially when good\nevaluation measures are used. This work aims to be useful for both researchers\nand practitioners by providing key insights and proposing directions for future\nprogress in active learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6982\u8ff0\u4e86Active Learning\uff08AL\uff09\uff0c\u4e00\u79cd\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6027\u80fd\u7684\u7b56\u7565\uff0c\u6db5\u76d6\u6982\u5ff5\u3001\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u4e30\u5bcc\u4f46\u6807\u6ce8\u7a00\u7f3a\u7684\u77db\u76fe\uff0c\u4ee5\u63a8\u8fdb\u673a\u5668\u5b66\u4e60\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4ecb\u7ecdAL\u57fa\u672c\u6982\u5ff5\u3001\u8ba8\u8bba\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001NLP\u7b49\u9886\u57df\u7684\u5e94\u7528\u3001\u7126\u70b9\u8bdd\u9898\u5982\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7c7b\u4e0d\u5e73\u8861\u5904\u7406\u3002", "result": "AL\u6bd4\u88ab\u52a8\u5b66\u4e60\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u5728\u4f7f\u7528\u826f\u597d\u8bc4\u4ef7\u63aa\u65bd\u65f6\uff1b\u4eba\u7c7b\u542f\u53d1\u7684\u65b9\u6cd5\u53ef\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6d1e\u89c1\uff0c\u63d0\u51fa\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u91cd\u5efa\u4fe1\u4efb\u548c\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728", "abs": "https://arxiv.org/abs/2504.16728", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "categories": ["cs.AI", "cs.CL"], "comment": "6 pages main-text, 2 pages appendix", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System", "AI": {"tldr": "\u672c\u6587\u5f15\u5165IRIS\u7cfb\u7edf\uff0c\u4f7f\u7528LLMs\u8f85\u52a9\u79d1\u5b66\u5047\u8bbe\u751f\u6210\uff0c\u901a\u8fc7HITL\u65b9\u6cd5\u63d0\u5347\u900f\u660e\u5ea6\u548c\u53ef\u64cd\u63a7\u6027\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "LLMs\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u52a0\u901f\u53d1\u73b0\u7684\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u5047\u8bbe\u751f\u6210\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u64cd\u63a7\u6027\u3002", "method": "IRIS\u7cfb\u7edf\u91c7\u7528MCTS\u7684\u81ea\u9002\u5e94\u8ba1\u7b97\u6269\u5c55\u3001\u7ec6\u7c92\u5ea6\u53cd\u9988\u673a\u5236\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u6587\u732e\u5408\u6210\uff0c\u4ee5\u53caHITL\u65b9\u6cd5\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0cIRIS\u7cfb\u7edf\u6709\u6548\u5730\u63d0\u5347\u4e86\u7814\u7a76\u4eba\u5458\u7684\u6784\u60f3\u80fd\u529b\u3002", "conclusion": "IRIS\u7cfb\u7edf\u589e\u5f3a\u4e86\u7814\u7a76\u4eba\u5458\u7684\u63a7\u5236\u529b\u548c\u6d1e\u5bdf\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2504.16385", "pdf": "https://arxiv.org/pdf/2504.16385", "abs": "https://arxiv.org/abs/2504.16385", "authors": ["Evangelia Gkaravela", "Hang Woon Lee", "Hao Chen"], "title": "Distributed Space Resource Logistics Architecture Optimization under Economies of Scale", "categories": ["eess.SY", "cs.SY"], "comment": "27 pages, 13 figures, Journal of Spacecraft and Rockets (Accepted)", "summary": "This paper proposes an optimization framework for distributed resource\nlogistics system design to support future multimission space exploration. The\nperformance and impact of distributed In-Situ Resource Utilization (ISRU)\nsystems in facilitating space transportation are analyzed. The proposed\nframework considers technology trade studies, deployment strategy, facility\nlocation evaluation, and resource logistics after production for distributed\nISRU systems. We develop piecewise linear sizing and cost estimation models\nbased on economies of scale that can be easily integrated into network-based\nmission planning formulations. A case study on a multi-mission cislunar\nlogistics campaign is conducted to demonstrate the value of the proposed method\nand evaluate key tradeoffs to compare the performance of distributed ISRU\nsystems with traditional concentrated ISRU. Finally, a comprehensive\nsensitivity analysis is performed to assess the proposed system under varying\nconditions, comparing concentrated and distributed ISRU systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0fISRU\u7cfb\u7edf\u4f18\u5316\u6846\u67b6\uff0c\u652f\u6301\u591a\u4efb\u52a1\u7a7a\u95f4\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u5176\u4e0e\u96c6\u4e2d\u5f0fISRU\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u672a\u6765\u7684\u591a\u4efb\u52a1\u7a7a\u95f4\u63a2\u7d22\uff0c\u901a\u8fc7\u5206\u6790\u5206\u5e03\u5f0fISRU\u7cfb\u7edf\u5728\u7a7a\u95f4\u8fd0\u8f93\u4e2d\u7684\u6027\u80fd\u548c\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u6280\u672f\u6743\u8861\u7814\u7a76\u3001\u90e8\u7f72\u7b56\u7565\u3001\u8bbe\u65bd\u4f4d\u7f6e\u8bc4\u4f30\u548c\u8d44\u6e90\u7269\u6d41\uff1b\u5f00\u53d1\u57fa\u4e8e\u89c4\u6a21\u7ecf\u6d4e\u7684\u5206\u6bb5\u7ebf\u6027\u89c4\u6a21\u548c\u6210\u672c\u4f30\u7b97\u6a21\u578b\uff1b\u8fdb\u884c\u591a\u4efb\u52a1\u8fd1\u6708\u70b9\u7269\u6d41\u6848\u4f8b\u7814\u7a76\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u5206\u5e03\u5f0fISRU\u7684\u4ef7\u503c\uff0c\u8bc4\u4f30\u4e86\u5173\u952e\u6743\u8861\uff0c\u5e76\u6bd4\u8f83\u4e86\u5206\u5e03\u5f0f\u548c\u96c6\u4e2d\u5f0fISRU\u7684\u6027\u80fd\u3002", "conclusion": "\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u5206\u5e03\u5f0fISRU\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u53ef\u80fd\u4f18\u4e8e\u96c6\u4e2d\u5f0fISRU\uff0c\u4f46\u9700\u6839\u636e\u5177\u4f53\u60c5\u51b5\u6743\u8861\u3002"}}
{"id": "2504.16140", "pdf": "https://arxiv.org/pdf/2504.16140", "abs": "https://arxiv.org/abs/2504.16140", "authors": ["Max Hartman", "Lav Varshney"], "title": "SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful\nframework for learning general-purpose representations. However, these models\noften lack interpretability and suffer from inefficiencies due to dense\nembedding representations. We propose SparseJEPA, an extension that integrates\nsparse representation learning into the JEPA framework to enhance the quality\nof learned representations. SparseJEPA employs a penalty method that encourages\nlatent space variables to be shared among data features with strong semantic\nrelationships, while maintaining predictive performance. We demonstrate the\neffectiveness of SparseJEPA by training on the CIFAR-100 dataset and\npre-training a lightweight Vision Transformer. The improved embeddings are\nutilized in linear-probe transfer learning for both image classification and\nlow-level tasks, showcasing the architecture's versatility across different\ntransfer tasks. Furthermore, we provide a theoretical proof that demonstrates\nthat the grouping mechanism enhances representation quality. This was done by\ndisplaying that grouping reduces Multiinformation among latent-variables,\nincluding proofing the Data Processing Inequality for Multiinformation. Our\nresults indicate that incorporating sparsity not only refines the latent space\nbut also facilitates the learning of more meaningful and interpretable\nrepresentations. In further work, hope to further extend this method by finding\nnew ways to leverage the grouping mechanism through object-centric\nrepresentation learning.", "AI": {"tldr": "SparseJEPA improves JEPA by adding sparsity for better representations, demonstrated on CIFAR-100 and Vision Transformer with theoretical proof.", "motivation": "To address interpretability and efficiency issues in JEPA's dense embeddings.", "method": "Uses a penalty method to encourage semantic sharing in latent space, trained on CIFAR-100 and pre-trained Vision Transformer for transfer learning.", "result": "Enhanced embeddings improve performance in classification and low-level tasks; theoretically proven to reduce Multiinformation.", "conclusion": "Sparsity refines latent space for more interpretable representations; future work includes object-centric learning."}}
{"id": "2504.16736", "pdf": "https://arxiv.org/pdf/2504.16736", "abs": "https://arxiv.org/abs/2504.16736", "authors": ["Yingxuan Yang", "Huacan Chai", "Yuanyi Song", "Siyuan Qi", "Muning Wen", "Ning Li", "Junwei Liao", "Haoyi Hu", "Jianghao Lin", "Gaowei Chang", "Weiwen Liu", "Ying Wen", "Yong Yu", "Weinan Zhang"], "title": "A Survey of AI Agent Protocols", "categories": ["cs.AI"], "comment": null, "summary": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6982\u8ff0LLM\u4ee3\u7406\u901a\u4fe1\u534f\u8bae\uff0c\u5206\u7c7b\u5206\u6790\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u672a\u6765\u6311\u6218\u3002", "motivation": "LLM\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u901a\u4fe1\u534f\u8bae\uff0c\u5f71\u54cd\u4ee3\u7406\u534f\u4f5c\u548c\u6269\u5c55\u3002", "method": "\u7cfb\u7edf\u6982\u8ff0\u73b0\u6709\u534f\u8bae\uff0c\u5206\u7c7b\u4e3a\u56db\u7c7b\uff0c\u5e76\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u534f\u8bae\u5206\u7c7b\u3001\u6027\u80fd\u5206\u6790\uff08\u5b89\u5168\u3001\u53ef\u6269\u5c55\u6027\u3001\u5ef6\u8fdf\uff09\uff0c\u4ee5\u53ca\u672a\u6765\u6311\u6218\u63a2\u8ba8\u3002", "conclusion": "\u671f\u671b\u4e3a\u7814\u7a76\u8005\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u7528\u53c2\u8003\u3002"}}
{"id": "2504.16413", "pdf": "https://arxiv.org/pdf/2504.16413", "abs": "https://arxiv.org/abs/2504.16413", "authors": ["Jiayu Chen", "Takahiro Kawaguchi", "Yuichiro Yano", "Yuko Hanado", "Takayuki Ishizaki"], "title": "Hierarchical Distributed Architecture for the Least Allan Variance Atomic Timing", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "In this paper, we propose a hierarchical distributed timing architecture\nbased on an ensemble of miniature atomic clocks. The goal is to ensure\nsynchronized and accurate timing in a normal operating mode where Global\nNavigation Satellite System (GNSS) signals are available, as well as in an\nemergency operating mode during GNSS failures. At the lower level, the\nminiature atomic clocks employ a distributed control strategy that uses only\nlocal information to ensure synchronization in both modes. The resulting\nsynchronized time or generated time scale has the best frequency stability, as\nmeasured by the Allan variance, over the short control period. In the upper\nlayer, a supervisor controls the long-term behavior of the generated time\nscale. In the normal operating mode, the supervisor periodically anchors the\ngenerated time scale to the standard time based on GNSS signals, while in the\nemergency operating mode, it applies optimal floating control to reduce the\ndivergence rate of the generated time scale, which is not observable from the\nmeasurable time difference between the miniature atomic clocks. This floating\ncontrol aims to explicitly control the generated time scale to have the least\nAllan variance over the long control period. Finally, numerical examples are\nprovided to demonstrate the effectiveness and feasibility of the architecture\nin high-precision, GNSS-resilient atomic timing.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5fae\u578b\u539f\u5b50\u949f\u7684\u5206\u5c42\u5206\u5e03\u5f0f\u5b9a\u65f6\u67b6\u6784\uff0c\u786e\u4fdd\u5728GNSS\u53ef\u7528\u548c\u6545\u969c\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u540c\u6b65\u3002", "motivation": "\u52a8\u673a\u662f\u4e3aGNSS\u6545\u969c\u63d0\u4f9b\u9c81\u68d2\u7684\u540c\u6b65\u5b9a\u65f6\u7cfb\u7edf\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u63a7\u5236\u5728\u4e0d\u540c\u6a21\u5f0f\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f4e\u5c42\u5206\u5e03\u5f0f\u63a7\u5236\u4f7f\u7528\u672c\u5730\u4fe1\u606f\u540c\u6b65\u949f\u8868\uff0c\u9ad8\u5c42\u76d1\u7763\u5668\u5728\u6b63\u5e38\u6a21\u5f0f\u951a\u5b9aGNSS\uff0c\u5728\u7d27\u6025\u6a21\u5f0f\u4f7f\u7528\u6700\u4f18\u6d6e\u52a8\u63a7\u5236\u6700\u5c0f\u5316\u65f6\u6807\u53d1\u6563\u3002", "result": "\u7ed3\u679c\u901a\u8fc7\u6570\u503c\u4f8b\u5b50\u8bc1\u660e\u4e86\u67b6\u6784\u5728\u9ad8\u7cbe\u5ea6\u3001\u6297GNSS\u6545\u969c\u539f\u5b50\u5b9a\u65f6\u4e2d\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u67b6\u6784\u5728\u786e\u4fdd\u957f\u671f\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u3002"}}
{"id": "2504.16141", "pdf": "https://arxiv.org/pdf/2504.16141", "abs": "https://arxiv.org/abs/2504.16141", "authors": ["Yue Shi", "Liangxiu Han", "Xin Zhang", "Tam Sobeih", "Thomas Gaiser", "Nguyen Huu Thuy", "Dominik Behrend", "Amit Kumar Srivastava", "Krishnagopal Halder", "Frank Ewert"], "title": "Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges", "categories": ["cs.LG"], "comment": null, "summary": "Process-based models (PBMs) and deep learning (DL) are two key approaches in\nagricultural modelling, each offering distinct advantages and limitations. PBMs\nprovide mechanistic insights based on physical and biological principles,\nensuring interpretability and scientific rigour. However, they often struggle\nwith scalability, parameterisation, and adaptation to heterogeneous\nenvironments. In contrast, DL models excel at capturing complex, nonlinear\npatterns from large datasets but may suffer from limited interpretability, high\ncomputational demands, and overfitting in data-scarce scenarios.\n  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL\nframeworks, highlighting their applications in agricultural and environmental\nmodelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where\nneural networks refine process-based models, and PBM-informed DL, where\nphysical constraints guide deep learning predictions. Additionally, we conduct\na case study on crop dry biomass prediction, comparing hybrid models against\nstandalone PBMs and DL models under varying data quality, sample sizes, and\nspatial conditions. The results demonstrate that hybrid models consistently\noutperform traditional PBMs and DL models, offering greater robustness to noisy\ndata and improved generalisation across unseen locations.\n  Finally, we discuss key challenges, including model interpretability,\nscalability, and data requirements, alongside actionable recommendations for\nadvancing hybrid modelling in agriculture. By integrating domain knowledge with\nAI-driven approaches, this study contributes to the development of scalable,\ninterpretable, and reproducible agricultural models that support data-driven\ndecision-making for sustainable agriculture.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5ba1\u67e5\u4e86\u57fa\u4e8e\u8fc7\u7a0b\u7684\u6a21\u578b\uff08PBMs\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u519c\u4e1a\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u4ecb\u7ecd\u4e86\u6df7\u5408\u6846\u67b6\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u6df7\u5408\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u6574\u5408PBMs\u548cDL\u7684\u4f18\u52bf\uff0c\u4ee5\u514b\u670d\u5404\u81ea\u7684\u5c40\u9650\u6027\uff0c\u5982PBMs\u7684\u6269\u5c55\u6027\u548cDL\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5ba1\u67e5PBMs\u3001DL\u548c\u6df7\u5408\u6846\u67b6\uff0c\u5206\u7c7b\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4f5c\u7269\u5e72\u751f\u7269\u91cf\u9884\u6d4b\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u3001\u6837\u672c\u5927\u5c0f\u548c\u7a7a\u95f4\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edfPBMs\u548cDL\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u9700\u6c42\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u8fdb\u6df7\u5408\u5efa\u6a21\u7684\u5efa\u8bae\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u3002"}}
{"id": "2504.16760", "pdf": "https://arxiv.org/pdf/2504.16760", "abs": "https://arxiv.org/abs/2504.16760", "authors": ["Bartosz Piotrowski", "Witold Drzewakowski", "Konrad Staniszewski", "Piotr Mi\u0142o\u015b"], "title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies", "categories": ["cs.AI"], "comment": null, "summary": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165LiLaVe\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff0c\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u63d0\u53d6\u6b63\u786e\u6027\u4fe1\u53f7\uff0c\u63d0\u9ad8\u63a8\u7406\u4efb\u52a1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u9a8c\u8bc1\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u652f\u6301LLM\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faLiLaVe\uff0c\u901a\u8fc7\u5206\u6790\u57faLLM\u9690\u85cf\u72b6\u6001\u83b7\u53d6\u6b63\u786e\u6027\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u6216\u8bbe\u8ba1\u65b0\u7b56\u7565\u5982\u6761\u4ef6\u81ea\u4fee\u6b63\u548c\u6761\u4ef6\u591a\u6570\u6295\u7968\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8f83\u5c0fLLM\uff0c\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4eceLLM\u9690\u85cf\u72b6\u6001\u63d0\u53d6\u6f5c\u5728\u4fe1\u606f\u7684\u6709\u76ca\u6027\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u7684\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u8d44\u6e90\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16417", "pdf": "https://arxiv.org/pdf/2504.16417", "abs": "https://arxiv.org/abs/2504.16417", "authors": ["Pol Mestres", "Arnau Marzabal", "Jorge Cort\u00e9s"], "title": "Anytime Safe Reinforcement Learning", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "This paper considers the problem of solving constrained\n  reinforcement learning problems with anytime guarantees, meaning\n  that the algorithmic solution returns a safe policy regardless of\n  when it is terminated. Drawing inspiration from anytime constrained\n  optimization, we introduce Reinforcement Learning-based Safe\n  Gradient Flow (RL-SGF), an on-policy algorithm which employs\n  estimates of the value functions and their respective gradients\n  associated with the objective and safety constraints for the current\n  policy, and updates the policy parameters by solving a convex\n  quadratically constrained quadratic program. We show that if the\n  estimates are computed with a sufficiently large number of episodes\n  (for which we provide an explicit bound), safe policies are updated\n  to safe policies with a probability higher than a prescribed\n  tolerance. We also show that iterates asymptotically converge to a\n  neighborhood of a KKT point, whose size can be arbitrarily reduced\n  by refining the estimates of the value function and their gradients.\n  We illustrate the performance of RL-SGF in a navigation example.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faRL-SGF\u7b97\u6cd5\uff0c\u7528\u4e8e\u5e26\u7ea6\u675f\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u786e\u4fdd\u7b56\u7565\u5b89\u5168\u5e76\u5177\u6709anytime\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662f\u7b97\u6cd5\u968f\u65f6\u7ec8\u6b62\u65f6\u4e5f\u80fd\u8fd4\u56de\u5b89\u5168\u7b56\u7565\u3002", "method": "\u5f15\u5165RL-SGF\u7b97\u6cd5\uff0c\u4f7f\u7528\u4ef7\u503c\u51fd\u6570\u548c\u68af\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u89e3\u51b3\u51f8\u4e8c\u6b21\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u66f4\u65b0\u7b56\u7565\u53c2\u6570\u3002", "result": "\u8bc1\u660e\u4f7f\u7528\u8db3\u591fepisode\u53ef\u9ad8\u6982\u7387\u66f4\u65b0\u5b89\u5168\u7b56\u7565\uff0c\u8fed\u4ee3\u6536\u655b\u5230KKT\u70b9\u90bb\u57df\uff0c\u5e76\u5728\u5bfc\u822a\u4f8b\u5b50\u4e2d\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "RL-SGF\u7b97\u6cd5\u63d0\u4f9b\u53ef\u9760\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.16214", "pdf": "https://arxiv.org/pdf/2504.16214", "abs": "https://arxiv.org/abs/2504.16214", "authors": ["Xiao Zhang", "Yaoyao Ding", "Yang Hu", "Gennady Pekhimenko"], "title": "Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "17 pages, 24 figures", "summary": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.", "AI": {"tldr": "Hexcute \u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e tile \u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u4f18\u5316 GPU \u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u6df7\u5408\u7c7b\u578b\u8fd0\u7b97\u7b26\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u91cf\u5316\u6280\u672f\u9700\u8981\u65b0\u7684\u6df7\u5408\u6570\u636e\u7c7b\u578b\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u7b26\uff0c\u5bfc\u81f4 GPU \u4f18\u5316\u590d\u6742\u5316\uff0c\u73b0\u6709\u7684\u7f16\u8bd1\u5668\u8981\u4e48\u7f3a\u4e4f\u8868\u8fbe\u6027\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u7f16\u7a0b\u52aa\u529b\u3002", "method": "\u63d0\u51fa Hexcute \u8bed\u8a00\uff0c\u66b4\u9732\u5171\u4eab\u5185\u5b58\u548c\u5bc4\u5b58\u5668\u62bd\u8c61\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u6620\u5c04\u548c\u57fa\u4e8e\u7c7b\u578b\u63a8\u65ad\u7684\u7b97\u6cd5\u81ea\u52a8\u5408\u6210\u5e03\u5c40\u548c\u6620\u5c04\u3002", "result": "\u5bf9\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u8fd0\u7b97\u7b26\u901a\u7528\uff0c\u6bd4\u73b0\u6709\u7f16\u8bd1\u5668\u5feb 1.7-11.28 \u500d\uff0c\u5728\u7aef\u5230\u7aef\u8bc4\u4f30\u4e2d\u5feb\u8fbe 2.91 \u500d\u3002", "conclusion": "Hexcute \u5e73\u8861\u4e86\u8868\u8fbe\u6027\u548c\u5de5\u7a0b\u52aa\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u8fd0\u7b97\u7b26\u7684\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2504.16891", "pdf": "https://arxiv.org/pdf/2504.16891", "abs": "https://arxiv.org/abs/2504.16891", "authors": ["Ivan Moshkov", "Darragh Hanley", "Ivan Sorokin", "Shubham Toshniwal", "Christof Henkel", "Benedikt Schifferer", "Wei Du", "Igor Gitman"], "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Report of AIMO-2 winning submission", "summary": "This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u6211\u4eec\u8d62\u5f97AI\u6570\u5b66\u5965\u6797\u5339\u514b\u8fdb\u5c55\u59562\u7684\u83b7\u80dc\u65b9\u6848\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u96c6\u6210\u4ee3\u7801\u6267\u884c\u548c\u751f\u6210\u89e3\u51b3\u65b9\u6848\u9009\u62e9\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u52a8\u673a\u662f\u5f00\u53d1\u5148\u8fdb\u7684\u6570\u5b66\u63a8\u7406\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5305\u62ec\u5965\u6797\u5339\u514b\u7ea7\u96be\u9898\u5728\u5185\u7684\u590d\u6742\u6570\u5b66\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u652f\u67f1\uff1a\u521b\u5efa54\u4e07\u9ad8\u8d28\u91cf\u6570\u5b66\u95ee\u9898\u548c320\u4e07\u957f\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u7684\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u4e00\u79cd\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u3001\u751f\u6210\u548c\u8d28\u91cf\u8fc7\u6ee4\u5c06\u4ee3\u7801\u6267\u884c\u96c6\u6210\u5230\u957f\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u65b9\u6cd5\uff0c\u4ea7\u751f170\u4e07\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff1b\u521b\u5efa\u4e00\u79cd\u8bad\u7ec3\u6a21\u578b\u4ece\u591a\u4e2a\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6700\u6709\u524d\u666f\u65b9\u6848\u7684\u7ba1\u9053\u3002", "result": "\u7ed3\u679c\u662f\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3001\u6a21\u578b\u548cOpenMathReasoning\u6570\u636e\u96c6\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53d1\u5e03\u8d44\u6e90\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.16445", "pdf": "https://arxiv.org/pdf/2504.16445", "abs": "https://arxiv.org/abs/2504.16445", "authors": ["Michael Ruderman", "Denis Efimov"], "title": "Power-based control of output oscillations with online estimation of biased harmonics", "categories": ["eess.SY", "cs.SY"], "comment": "6 pages, 7 figures", "summary": "The recently introduced discrete power-based control (Ruderman (2024b))\nreduces largely the communication efforts in the control loop when compensating\nfor the marginally damped or even slowly diverging output oscillations. The\ncontrol commutates twice per oscillations period (at the amplitude peaks) and\nuses the measured harmonic output only. The power-based control scheme requires\nthe knowledge of the instantaneous frequency, amplitude, and bias parameters of\nthe harmonic signal. This paper extends the power-based control by the\nfinite-time estimation of the biased harmonics (Ahmed et al. (2022)). Also an\nimproved analytic calculation of the impulse weighting factor is provided. The\npower-based oscillations control with online estimation of the harmonic\nparameters is evaluated experimentally on the fifth-order actuator system with\na free hanging load under gravity and measurement noise.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u529f\u7387-based\u63a7\u5236\uff0c\u6dfb\u52a0\u6709\u9650\u65f6\u95f4\u4f30\u8ba1\u504f\u7f6e\u8c10\u6ce2\u548c\u6539\u8fdb\u6743\u91cd\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u51cf\u5c11\u63a7\u5236\u56de\u8def\u901a\u4fe1\u52aa\u529b\uff0c\u4ee5\u8865\u507f\u8fb9\u9645\u963b\u5c3c\u6216\u7f13\u6162\u53d1\u6563\u7684\u8f93\u51fa\u632f\u8361\u3002", "method": "\u4f7f\u7528\u6709\u9650\u65f6\u95f4\u4f30\u8ba1\u504f\u7f6e\u8c10\u6ce2\u3001\u6539\u8fdb\u8109\u51b2\u6743\u91cd\u56e0\u5b50\u8ba1\u7b97\uff0c\u5e76\u5728\u7ebf\u4f30\u8ba1\u8c10\u6ce2\u53c2\u6570\u3002", "result": "\u5728\u4e94\u9636\u6267\u884c\u5668\u7cfb\u7edf\u4e0a\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6d89\u53ca\u91cd\u529b\u548c\u6d4b\u91cf\u566a\u58f0\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6269\u5c55\u65b9\u6cd5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.16234", "pdf": "https://arxiv.org/pdf/2504.16234", "abs": "https://arxiv.org/abs/2504.16234", "authors": ["Rene Pilz", "Johannes Schneider"], "title": "Using Phonemes in cascaded S2S translation pipeline", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Swiss NLP Conference 2025", "summary": "This paper explores the idea of using phonemes as a textual representation\nwithin a conventional multilingual simultaneous speech-to-speech translation\npipeline, as opposed to the traditional reliance on text-based language\nrepresentations. To investigate this, we trained an open-source\nsequence-to-sequence model on the WMT17 dataset in two formats: one using\nstandard textual representation and the other employing phonemic\nrepresentation. The performance of both approaches was assessed using the BLEU\nmetric. Our findings shows that the phonemic approach provides comparable\nquality but offers several advantages, including lower resource requirements or\nbetter suitability for low-resource languages.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6bd4\u8f83\u4e86\u5728\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u4f7f\u7528\u6587\u672c\u8868\u793a\u548c\u97f3\u7d20\u8868\u793a\uff0c\u53d1\u73b0\u97f3\u7d20\u65b9\u6cd5\u8d28\u91cf\u76f8\u5f53\u4f46\u5177\u6709\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u548c\u66f4\u9002\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u97f3\u7d20\u4f5c\u4e3a\u6587\u672c\u8868\u793a\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u6587\u672c\u8868\u793a\uff0c\u52a8\u673a\u662f\u4e3a\u591a\u8bed\u8a00\u540c\u65f6\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u63d0\u4f9b\u66f4\u597d\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u652f\u6301\u3002", "method": "\u5728WMT17\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u4f7f\u7528\u6807\u51c6\u6587\u672c\u8868\u793a\u548c\u97f3\u7d20\u8868\u793a\u4e24\u79cd\u683c\u5f0f\uff0c\u5e76\u4f7f\u7528BLEU\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u97f3\u7d20\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6bd4\u7684\u8d28\u91cf\uff0c\u4f46\u5177\u6709\u66f4\u4f4e\u7684\u8d44\u6e90\u9700\u6c42\u548c\u66f4\u597d\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u9002\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u97f3\u7d20\u8868\u793a\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2504.16093", "pdf": "https://arxiv.org/pdf/2504.16093", "abs": "https://arxiv.org/abs/2504.16093", "authors": ["Yurun Ge", "Lucas B\u00f6ttcher", "Tom Chou", "Maria R. D'Orsogna"], "title": "Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model", "categories": ["q-fin.PM", "cs.AI", "math.PR", "60-08, 90-08", "G.3; I.6.1; J.4"], "comment": "15pp, 4 figs", "summary": "How to allocate limited resources to projects that will yield the greatest\nlong-term benefits is a problem that often arises in decision-making under\nuncertainty. For example, organizations may need to evaluate and select\ninnovation projects with risky returns. Similarly, when allocating resources to\nresearch projects, funding agencies are tasked with identifying the most\npromising proposals based on idiosyncratic criteria. Finally, in participatory\nbudgeting, a local community may need to select a subset of public projects to\nfund. Regardless of context, agents must estimate the uncertain values of a\npotentially large number of projects. Developing parsimonious methods to\ncompare these projects, and aggregating agent evaluations so that the overall\nbenefit is maximized, are critical in assembling the best project portfolio.\nUnlike in standard sorting algorithms, evaluating projects on the basis of\nuncertain long-term benefits introduces additional complexities. We propose\ncomparison rules based on Quicksort and the Bradley--Terry model, which\nconnects rankings to pairwise \"win\" probabilities. In our model, each agent\ndetermines win probabilities of a pair of projects based on his or her specific\nevaluation of the projects' long-term benefit. The win probabilities are then\nappropriately aggregated and used to rank projects. Several of the methods we\npropose perform better than the two most effective aggregation methods\ncurrently available. Additionally, our methods can be combined with sampling\ntechniques to significantly reduce the number of pairwise comparisons. We also\ndiscuss how the Bradley--Terry portfolio selection approach can be implemented\nin practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eBradley-Terry\u6a21\u578b\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\uff0c\u4f18\u5316\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9879\u76ee\u9009\u62e9\u548c\u6392\u540d\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5982\u521b\u65b0\u9879\u76ee\u548c\u7814\u7a76\u8d44\u52a9\uff0c\u4ee5\u6700\u5927\u5316\u957f\u671f\u6536\u76ca\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8eQuicksort\u548cBradley-Terry\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7406\u8bc4\u4f30\u9879\u76ee\u6210\u5bf9'\u83b7\u80dc'\u6982\u7387\uff0c\u805a\u5408\u6392\u540d\uff0c\u5e76\u7ed3\u5408\u91c7\u6837\u6280\u672f\u51cf\u5c11\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u663e\u793a\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u805a\u5408\u65b9\u6cd5\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u6210\u5bf9\u6bd4\u8f83\u6b21\u6570\u3002", "conclusion": "\u7ed3\u8bba\u8ba8\u8bbaBradley-Terry\u65b9\u6cd5\u5728\u5b9e\u9645\u4e2d\u7684\u5b9e\u65bd\u3002"}}
{"id": "2504.16477", "pdf": "https://arxiv.org/pdf/2504.16477", "abs": "https://arxiv.org/abs/2504.16477", "authors": ["Apostolos I. Rikos", "Wei Jiang", "Themistoklis Charalambous", "Karl H. Johansson"], "title": "Distributed Optimization with Efficient Communication, Event-Triggered Solution Enhancement, and Operation Stopping", "categories": ["eess.SY", "cs.SY", "math.OC"], "comment": null, "summary": "In modern large-scale systems with sensor networks and IoT devices it is\nessential to collaboratively solve complex problems while utilizing network\nresources efficiently. In our paper we present three distributed optimization\nalgorithms that exhibit efficient communication among nodes. Our first\nalgorithm presents a simple quantized averaged gradient procedure for\ndistributed optimization, which is shown to converge to a neighborhood of the\noptimal solution. Our second algorithm incorporates a novel event-triggered\nrefinement mechanism, which refines the utilized quantization level to enhance\nthe precision of the estimated optimal solution. It enables nodes to terminate\ntheir operation according to predefined performance guarantees. Our third\nalgorithm is tailored to operate in environments where each message consists of\nonly a few bits. It incorporates a novel event-triggered mechanism for\nadjusting the quantizer basis and quantization level, allowing nodes to\ncollaboratively decide operation termination based on predefined performance\ncriteria. We analyze the three algorithms and establish their linear\nconvergence. Finally, an application on distributed sensor fusion for target\nlocalization is used to demonstrate their favorable performance compared to\nexisting algorithms in the literature.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u9ad8\u6548\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f20\u611f\u5668\u7f51\u7edc\u548cIoT\u8bbe\u5907\uff0c\u8282\u7701\u901a\u4fe1\u8d44\u6e90\uff0c\u5e76\u7ebf\u6027\u6536\u655b\uff0c\u5e94\u7528\u4e8e\u76ee\u6807\u5b9a\u4f4d\u3002", "motivation": "\u5728\u73b0\u4ee3\u5927\u89c4\u6a21\u7cfb\u7edf\uff08\u5982\u4f20\u611f\u5668\u7f51\u7edc\u548cIoT\u8bbe\u5907\uff09\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u534f\u4f5c\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u7f51\u7edc\u8d44\u6e90\u3002", "method": "\u8bba\u6587\u5448\u73b0\u4e86\u4e09\u79cd\u5206\u5e03\u5f0f\u4f18\u5316\u7b97\u6cd5\uff1a1. \u7b80\u5355\u91cf\u5316\u5e73\u5747\u68af\u5ea6\u8fc7\u7a0b\uff1b2. \u5305\u542b\u4e8b\u4ef6\u89e6\u53d1\u7cbe\u5316\u673a\u5236\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u91cf\u5316\u7cbe\u5ea6\uff1b3. \u9488\u5bf9\u4f4e\u6bd4\u7279\u6d88\u606f\u73af\u5883\u7684\u7b97\u6cd5\uff0c\u5305\u542b\u4e8b\u4ef6\u89e6\u53d1\u673a\u5236\u8c03\u6574\u91cf\u5316\u5668\u57fa\u7840\u548c\u7ea7\u522b\u3002", "result": "\u4e09\u4e2a\u7b97\u6cd5\u5747\u663e\u793a\u7ebf\u6027\u6536\u655b\uff0c\u5728\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u878d\u5408\u76ee\u6807\u5b9a\u4f4d\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7b97\u6cd5\u9ad8\u6548\uff0c\u5141\u8bb8\u57fa\u4e8e\u6027\u80fd\u4fdd\u8bc1\u7ec8\u6b62\u64cd\u4f5c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7b97\u6cd5\u3002"}}
{"id": "2504.16238", "pdf": "https://arxiv.org/pdf/2504.16238", "abs": "https://arxiv.org/abs/2504.16238", "authors": ["L\u00e9andre Eberhard", "Nirek Sharma", "Filipp Shelobolin", "Aalok Ganesh Shanbhag"], "title": "General Post-Processing Framework for Fairness Adjustment of Machine Learning Models", "categories": ["cs.LG"], "comment": "Submitted to FAccT 2025. Does not include reviewer feedback yet", "summary": "As machine learning increasingly influences critical domains such as credit\nunderwriting, public policy, and talent acquisition, ensuring compliance with\nfairness constraints is both a legal and ethical imperative. This paper\nintroduces a novel framework for fairness adjustments that applies to diverse\nmachine learning tasks, including regression and classification, and\naccommodates a wide range of fairness metrics. Unlike traditional approaches\ncategorized as pre-processing, in-processing, or post-processing, our method\nadapts in-processing techniques for use as a post-processing step. By\ndecoupling fairness adjustments from the model training process, our framework\npreserves model performance on average while enabling greater flexibility in\nmodel development. Key advantages include eliminating the need for custom loss\nfunctions, enabling fairness tuning using different datasets, accommodating\nproprietary models as black-box systems, and providing interpretable insights\ninto the fairness adjustments. We demonstrate the effectiveness of this\napproach by comparing it to Adversarial Debiasing, showing that our framework\nachieves a comparable fairness/accuracy tradeoff on real-world datasets.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u540e\u5904\u7406\u516c\u5e73\u6027\u8c03\u6574\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5f71\u54cd\u4fe1\u8d37\u3001\u516c\u5171\u653f\u7b56\u548c\u4eba\u624d\u62db\u8058\u7b49\u5173\u952e\u9886\u57df\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u662f\u6cd5\u5f8b\u548c\u4f26\u7406\u5fc5\u8981\u3002", "method": "\u6846\u67b6\u5c06\u5185\u90e8\u5904\u7406\u6280\u672f\u9002\u5e94\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u8131\u79bb\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u7075\u6d3b\u6027\uff0c\u5982\u65e0\u9700\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u3001\u652f\u6301\u4e0d\u540c\u6570\u636e\u96c6\u548c\u9ed1\u7bb1\u6a21\u578b\u3002", "result": "\u4e0e\u5bf9\u6297\u6027\u53bb\u504f\u7f6e\u65b9\u6cd5\u6bd4\u8f83\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u76f8\u4f3c\u7684\u516c\u5e73\u6027/\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "\u6846\u67b6\u4f18\u52bf\u5305\u62ec\u6d88\u9664\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u9700\u6c42\u3001\u542f\u7528\u4e0d\u540c\u6570\u636e\u96c6\u516c\u5e73\u8c03\u6574\u3001\u9002\u5e94\u9ed1\u7bb1\u6a21\u578b\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6d1e\u5bdf\u3002"}}
{"id": "2504.16096", "pdf": "https://arxiv.org/pdf/2504.16096", "abs": "https://arxiv.org/abs/2504.16096", "authors": ["Jiaxing Xu", "Kai He", "Yue Tang", "Wei Li", "Mengcheng Lan", "Xia Dong", "Yiping Ke", "Mengling Feng"], "title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience.", "AI": {"tldr": "BrainPrompt integrates Large Language Models with Graph Neural Networks using knowledge-driven prompts to improve neurological disease diagnosis from fMRI data, enhancing prediction and interpretability.", "motivation": "Existing brain network methods rely only on imaging data, ignoring non-imaging factors, which limits predictive accuracy and interpretability.", "method": "BrainPrompt employs three types of prompts: ROI-level for brain region identity, subject-level for demographic information, and disease-level for disease progression, combined with LLMs and GNNs.", "result": "Outperforms state-of-the-art methods on fMRI datasets and extracts interpretable biomarkers consistent with neuroscience knowledge.", "conclusion": "The framework improves disease stage prediction and interpretability by leveraging multi-modal information."}}
{"id": "2504.16498", "pdf": "https://arxiv.org/pdf/2504.16498", "abs": "https://arxiv.org/abs/2504.16498", "authors": ["Ahmed A. Hassan", "Ahmad Adnan Qidan", "Taisir Elgorashi", "Jaafar Elmirghani"], "title": "LiDAL-Assisted RLNC-NOMA in OWC Systems", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "comment": null, "summary": "Optical wireless communication (OWC) is envisioned as a key enabler for\nimmersive indoor data transmission in future wireless communication networks.\nHowever, multi-user interference management arises as a challenge in dense\nindoor OWC systems composed of multiple optical access points (APs) serving\nmultiple users. In this paper, we propose a novel dual-function OWC system for\ncommunication and localization. Non-orthogonal multiple access (NOMA) with\nrandom linear network coding (RLNC) is designed for data transmission, where\nNOMA allows the serving of multiple users simultaneously through controlling\nthe power domain, and RLNC helps minimize errors that might occur during signal\nprocessing phase. This setup is assisted with a light detection and\nlocalization system (LiDAL) that can passively obtain spatio-temporal indoor\ninformation of user presence and location for dynamic-user grouping. The\ndesigned LiDAL system helps to improve the estimation of channel state\ninformation (CSI) in realistic indoor network scenarios, where the CSI of\nindoor users might be noisy and/or highly correlated. We evaluate the\nperformance of NOMA combined with RLNC by analyzing the probability of\nsuccessful decoding compared to conventional NOMA and orthogonal schemes. In\naddition, we derive the Cramer-Rao Lower Bound (CRLB) to evaluate the accuracy\nof location estimation. The results show that the proposed RLNC-NOMA improves\nthe probability of successful decoding and the overall system performance. The\nresults also show the high accuracy of the unbiased location estimator and its\nassistant in reducing the imperfection of CSI, leading to high overall system\nperformance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NOMA\u3001RLNC\u548cLiDAL\u7684\u53cc\u529f\u80fd\u5149\u5b66\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u5584\u5ba4\u5185\u591a\u7528\u6237\u5e72\u6270\u7ba1\u7406\u548c\u901a\u4fe1\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u5ba4\u5185\u5149\u5b66\u65e0\u7ebf\u901a\u4fe1\u4e2d\u591a\u7528\u6237\u5e72\u6270\u6311\u6218\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\u548c\u5b9a\u4f4d\u3002", "method": "\u4f7f\u7528NOMA\u548cRLNC\u8fdb\u884c\u6570\u636e\u4f20\u8f93\uff0c\u7ed3\u5408LiDAL\u7cfb\u7edf\u88ab\u52a8\u83b7\u53d6\u7528\u6237\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u4f18\u5316CSI\u4f30\u8ba1\u3002", "result": "RLNC-NOMA\u63d0\u9ad8\u4e86\u6210\u529f\u89e3\u7801\u6982\u7387\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u5b9a\u4f4d\u4f30\u8ba1\u51c6\u786e\u5ea6\u9ad8\uff0c\u5e76\u51cf\u5c11\u4e86CSI\u4e0d\u5b8c\u5584\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u5149\u5b66\u65e0\u7ebf\u901a\u4fe1\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5c24\u5176\u5728\u89e3\u7801\u548c\u5b9a\u4f4d\u65b9\u9762\u3002"}}
{"id": "2504.16255", "pdf": "https://arxiv.org/pdf/2504.16255", "abs": "https://arxiv.org/abs/2504.16255", "authors": ["Tina Behzad", "Mithilesh Kumar Singh", "Anthony J. Ripa", "Klaus Mueller"], "title": "FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness", "categories": ["cs.LG", "cs.CY", "cs.HC", "H.5.2; H.5.3; I.2.6"], "comment": "Accepted at ACM CSCW 2025. 30 pages total (including references and\n  supplementary material). Contains 10 figures", "summary": "The issue of fairness in decision-making is a critical one, especially given\nthe variety of stakeholder demands for differing and mutually incompatible\nversions of fairness. Adopting a strategic interaction of perspectives provides\nan alternative to enforcing a singular standard of fairness. We present a\nweb-based software application, FairPlay, that enables multiple stakeholders to\ndebias datasets collaboratively. With FairPlay, users can negotiate and arrive\nat a mutually acceptable outcome without a universally agreed-upon theory of\nfairness. In the absence of such a tool, reaching a consensus would be highly\nchallenging due to the lack of a systematic negotiation process and the\ninability to modify and observe changes. We have conducted user studies that\ndemonstrate the success of FairPlay, as users could reach a consensus within\nabout five rounds of gameplay, illustrating the application's potential for\nenhancing fairness in AI systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aFairPlay\u7684\u7f51\u7edc\u5e94\u7528\u8f6f\u4ef6\uff0c\u5141\u8bb8\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u534f\u4f5c\u53bb\u504f\u7f6e\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u7edf\u4e00\u516c\u5e73\u7406\u8bba\u7684\u60c5\u51b5\u4e0b\u8fbe\u6210\u5171\u8bc6\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u53ef\u5728\u7ea6\u4e94\u8f6e\u5185\u8fbe\u6210\u4e00\u81f4\u3002", "motivation": "\u516c\u5e73\u6027\u5728\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u516c\u5e73\u6709\u4e0d\u540c\u548c\u76f8\u4e92\u51b2\u7a81\u7684\u9700\u6c42\u65f6\u3002\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u534f\u5546\u8fc7\u7a0b\u4f7f\u5f97\u8fbe\u6210\u5171\u8bc6\u975e\u5e38\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7f51\u7edc-based\u8f6f\u4ef6\u5e94\u7528FairPlay\uff0c\u5141\u8bb8\u7528\u6237\u534f\u5546\u548c\u534f\u4f5c\u53bb\u504f\u7f6e\u6570\u636e\u96c6\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u5927\u7ea6\u4e94\u8f6e\u6e38\u620f\u4e2d\u8fbe\u6210\u5171\u8bc6\uff0c\u5c55\u793a\u4e86\u8be5\u5e94\u7528\u7684\u6210\u529f\u3002", "conclusion": "FairPlay\u6709\u6f5c\u529b\u63d0\u5347AI\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2504.16097", "pdf": "https://arxiv.org/pdf/2504.16097", "abs": "https://arxiv.org/abs/2504.16097", "authors": ["Arthur Buzelin", "Pedro Robles Dutenhefner", "Turi Rezende", "Luisa G. Porfirio", "Pedro Bento", "Yan Aquino", "Jose Fernandes", "Caio Santana", "Gabriela Miana", "Gisele L. Pappa", "Antonio Ribeiro", "Wagner Meira Jr"], "title": "A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality,\nemphasizing the critical need for efficient diagnostic tools such as\nelectrocardiograms (ECGs). Recent advancements in deep learning, particularly\ntransformers, have revolutionized ECG analysis by capturing detailed waveform\nfeatures as well as global rhythm patterns. However, traditional transformers\nstruggle to effectively capture local morphological features that are critical\nfor accurate ECG interpretation. We propose a novel Local-Global Attention ECG\nmodel (LGA-ECG) to address this limitation, integrating convolutional inductive\nbiases with global self-attention mechanisms. Our approach extracts queries by\naveraging embeddings obtained from overlapping convolutional windows, enabling\nfine-grained morphological analysis, while simultaneously modeling global\ncontext through attention to keys and values derived from the entire sequence.\nExperiments conducted on the CODE-15 dataset demonstrate that LGA-ECG\noutperforms state-of-the-art models and ablation studies validate the\neffectiveness of the local-global attention strategy. By capturing the\nhierarchical temporal dependencies and morphological patterns in ECG signals,\nthis new design showcases its potential for clinical deployment with robust\nautomated ECG classification.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LGA-ECG\u6a21\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u5377\u79ef\u548c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347ECG\u5206\u7c7b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u4ea1\u539f\u56e0\uff0c\u9700\u8981\u9ad8\u6548ECG\u8bca\u65ad\u5de5\u5177\uff1b\u4f20\u7edftransformer\u65e0\u6cd5\u6709\u6548\u6355\u83b7\u5c40\u90e8\u5f62\u6001\u7279\u5f81\u3002", "method": "\u63d0\u51faLGA-ECG\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u91cd\u53e0\u5377\u79ef\u7a97\u53e3\u4e0a\u5e73\u5747\u5d4c\u5165\u63d0\u53d6\u67e5\u8be2\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40ECG\u7279\u5f81\u3002", "result": "\u5728CODE-15\u6570\u636e\u96c6\u4e0a\uff0cLGA-ECG\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5c40\u90e8-\u5168\u5c40\u6ce8\u610f\u529b\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u6355\u83b7ECG\u4fe1\u53f7\u7684\u5c42\u6b21\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5f62\u6001\u6a21\u5f0f\uff0c\u5177\u6709\u4e34\u5e8a\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.16588", "pdf": "https://arxiv.org/pdf/2504.16588", "abs": "https://arxiv.org/abs/2504.16588", "authors": ["Defne E. Ozan", "Andrea N\u00f3voa", "Luca Magri"], "title": "Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows", "categories": ["eess.SY", "cs.LG", "cs.SY", "physics.flu-dyn"], "comment": null, "summary": "The goal of many applications in energy and transport sectors is to control\nturbulent flows. However, because of chaotic dynamics and high dimensionality,\nthe control of turbulent flows is exceedingly difficult. Model-free\nreinforcement learning (RL) methods can discover optimal control policies by\ninteracting with the environment, but they require full state information,\nwhich is often unavailable in experimental settings. We propose a\ndata-assimilated model-based RL (DA-MBRL) framework for systems with partial\nobservability and noisy measurements. Our framework employs a control-aware\nEcho State Network for data-driven prediction of the dynamics, and integrates\ndata assimilation with an Ensemble Kalman Filter for real-time state\nestimation. An off-policy actor-critic algorithm is employed to learn optimal\ncontrol strategies from state estimates. The framework is tested on the\nKuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a\nspatiotemporally chaotic flow from noisy and partial measurements.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u540c\u5316\u6a21\u578b-based RL\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u90e8\u5206\u548c\u566a\u58f0\u6d4b\u91cf\u4e2d\u63a7\u5236\u6e4d\u6d41\uff0c\u5728Kuramoto-Sivashinsky\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u63a7\u5236\u6e4d\u6d41\u5728\u80fd\u91cf\u548c\u4ea4\u901a\u9886\u57df\u91cd\u8981\uff0c\u4f46\u56e0\u6df7\u6c8c\u52a8\u529b\u5b66\u548c\u9ad8\u7ef4\u5ea6\u56f0\u96be\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u9700\u5b8c\u6574\u72b6\u6001\u4fe1\u606f\uff0c\u4f46\u5b9e\u9a8c\u4e2d\u5f80\u5f80\u4e0d\u53ef\u7528\u3002", "method": "\u63d0\u51faDA-MBRL\u6846\u67b6\uff0c\u4f7f\u7528\u63a7\u5236\u611f\u77e5Echo State Network\u9884\u6d4b\u52a8\u6001\u3001Ensemble Kalman Filter\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u3001off-policy actor-critic\u7b97\u6cd5\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728Kuramoto-Sivashinsky\u65b9\u7a0b\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u4ece\u566a\u58f0\u548c\u90e8\u5206\u6d4b\u91cf\u7a33\u5b9a\u65f6\u7a7a\u6df7\u6c8c\u6d41\u3002", "conclusion": "\u6846\u67b6\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6e4d\u6d41\u63a7\u5236\u4e2d\u6709\u6548\u3002"}}
{"id": "2504.16262", "pdf": "https://arxiv.org/pdf/2504.16262", "abs": "https://arxiv.org/abs/2504.16262", "authors": ["Junn Yong Loo", "Michelle Adeline", "Julia Kaiwen Lau", "Fang Yu Leong", "Hwa Hui Tew", "Arghya Pal", "Vishnu Monn Baskaran", "Chee-Ming Ting", "Rapha\u00ebl C. -W. Phan"], "title": "Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching", "categories": ["cs.LG"], "comment": "Accepted by Transactions on Machine Learning Research (TMLR)", "summary": "Energy-based models (EBMs) are a powerful class of probabilistic generative\nmodels due to their flexibility and interpretability. However, relationships\nbetween potential flows and explicit EBMs remain underexplored, while\ncontrastive divergence training via implicit Markov chain Monte Carlo (MCMC)\nsampling is often unstable and expensive in high-dimensional settings. In this\npaper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based\ngenerative framework that eliminates the need for implicit MCMC sampling and\ndoes not rely on auxiliary networks or cooperative training. VPFB learns an\nenergy-parameterized potential flow by constructing a flow-driven density\nhomotopy that is matched to the data distribution through a variational loss\nminimizing the Kullback-Leibler divergence between the flow-driven and marginal\nhomotopies. This principled formulation enables robust and efficient generative\nmodeling while preserving the interpretability of EBMs. Experimental results on\nimage generation, interpolation, out-of-distribution detection, and\ncompositional generation confirm the effectiveness of VPFB, showing that our\nmethod performs competitively with existing approaches in terms of sample\nquality and versatility across diverse generative modeling tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVPFB\u7684\u80fd\u91cf-based\u751f\u6210\u6846\u67b6\uff0c\u6d88\u9664\u4e86MCMC\u91c7\u6837\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u80fd\u91cf-based\u6a21\u578b\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\uff0c\u4f46\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6f5c\u5728\u6d41\u4e0e\u663e\u5f0fEBM\u7684\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faVPFB\uff0c\u901a\u8fc7\u6784\u5efa\u6d41\u9a71\u52a8\u7684\u5bc6\u5ea6\u540c\u4f26\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316KL\u6563\u5ea6\u7684\u53d8\u5206\u635f\u5931\u6765\u5339\u914d\u6570\u636e\u5206\u5e03\uff0c\u65e0\u9700\u9690\u5f0fMCMC\u91c7\u6837\u6216\u8f85\u52a9\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aVPFB\u5728\u56fe\u50cf\u751f\u6210\u3001\u63d2\u503c\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u7ec4\u5408\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u529b\u76f8\u5f53\u3002", "conclusion": "VPFB\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u751f\u6210\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u7559\u4e86EBM\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.16099", "pdf": "https://arxiv.org/pdf/2504.16099", "abs": "https://arxiv.org/abs/2504.16099", "authors": ["Luyuan Zhang", "Xidong Mu", "An Liu", "Yuanwei Liu"], "title": "Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "5 pages, 4 figures, letter", "summary": "Pinching antenna systems (PASS) have been proposed as a revolutionary\nflexible antenna technology which facilitates line-of-sight links via numerous\nlow-cost pinching antennas with adjustable activation positions over\nwaveguides. This letter proposes a two-timescale joint transmit and pinching\nbeamforming design for the maximization of sum rate of a PASS-based downlink\nmulti-user multiple input single output system. A primal dual decomposition\nmethod is developed to decouple the two-timescale problem into two\nsub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is\nproposed to solve the short-term transmit beamforming design sub-problem; 2)\nThe long-term pinching beamforming design sub-problem is tackled by adopting a\nstochastic successive convex approximation method. Simulation results\ndemonstrate that the proposed two-timescale algorithm achieves a significant\nperformance gain compared to other baselines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Pinching Antenna Systems (PASS)\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u8054\u5408\u53d1\u5c04\u548c\u634f\u5408\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\uff0c\u4ee5\u6700\u5927\u5316\u591a\u7528\u6237\u591a\u8f93\u5165\u5355\u8f93\u51fa\u7cfb\u7edf\u7684\u603b\u548c\u901f\u7387\u3002", "motivation": "PASS\u662f\u4e00\u79cd\u9769\u547d\u6027\u7684\u7075\u6d3b\u5929\u7ebf\u6280\u672f\uff0c\u80fd\u591f\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u53ef\u8c03\u8282\u6fc0\u6d3b\u4f4d\u7f6e\u7684\u634f\u5408\u5929\u7ebf\u5b9e\u73b0\u89c6\u7ebf\u94fe\u63a5\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u4ee5\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u539f\u59cb\u5bf9\u5076\u5206\u89e3\u65b9\u6cd5\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff1a1) \u77ed\u65f6\u95f4\u5c3a\u5ea6\u53d1\u5c04\u6ce2\u675f\u5f62\u6210\u91c7\u7528Karush-Kuhn-Tucker\u6307\u5bfc\u7684\u5bf9\u5076\u5b66\u4e60\u65b9\u6cd5\uff1b2) \u957f\u65f6\u95f4\u5c3a\u5ea6\u634f\u5408\u6ce2\u675f\u5f62\u6210\u4f7f\u7528\u968f\u673a\u8fde\u7eed\u51f8\u903c\u8fd1\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7b97\u6cd5\u4e0e\u57fa\u51c6\u65b9\u6848\u76f8\u6bd4\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728PASS-based\u7cfb\u7edf\u4e2d\u6709\u6548\u63d0\u9ad8\u4e86\u603b\u548c\u901f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.16815", "pdf": "https://arxiv.org/pdf/2504.16815", "abs": "https://arxiv.org/abs/2504.16815", "authors": ["Franco Angelo Torchiaro", "Gianfranco Gagliardi", "Francesco Tedesco", "Alessandro Casavola"], "title": "Distributed Unknown Input Observers for Discrete-Time Linear Time-Invariant Systems", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "This paper introduces a Distributed Unknown Input Observer (D-UIO) design\nmethodology that uses a technique called node-wise detectability decomposition\nto estimate the state of a discrete-time linear time-invariant (LTI) system in\na distributed way, even when there are noisy measurements and unknown inputs.\nIn the considered scenario, sensors are associated to nodes of an underlying\ncommunication graph. Each node has a limited scope as it can only access local\nmeasurements and share data with its neighbors. The problem of designing the\nobserver gains is divided into two separate sub-problems: (i) design local\noutput injection gains to mitigate the impact of measurement noise, and (ii)\ndesign diffusive gains to compensate for the lack of information through a\nconsensus protocol. A direct and computationally efficient synthesis strategy\nis formulated by linear matrix inequalities (LMIs) and solved via semidefinite\nprogramming. Finally, two simulative scenarios are presented to illustrate the\neffectiveness of the distributed observer when two different node-wise\ndecompositions are adopted.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u672a\u77e5\u8f93\u5165\u89c2\u6d4b\u5668\uff08D-UIO\uff09\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u8282\u70b9\u68c0\u6d4b\u5206\u89e3\u6765\u4f30\u8ba1\u79bb\u6563\u65f6\u95f4LTI\u7cfb\u7edf\u7684\u72b6\u6001\uff0c\u5373\u4f7f\u5728\u566a\u58f0\u548c\u672a\u77e5\u8f93\u5165\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u52a8\u673a\u662f\u5904\u7406\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u72b6\u6001\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u4f20\u611f\u5668\u4f4d\u4e8e\u901a\u4fe1\u56fe\u8282\u70b9\uff0c\u6bcf\u4e2a\u8282\u70b9\u4ec5\u80fd\u8bbf\u95ee\u672c\u5730\u6d4b\u91cf\u5e76\u4e0e\u90bb\u5c45\u5171\u4eab\u6570\u636e\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u8282\u70b9\u68c0\u6d4b\u5206\u89e3\uff0c\u5c06\u95ee\u9898\u5206\u4e3a\u8bbe\u8ba1\u672c\u5730\u8f93\u51fa\u6ce8\u5165\u589e\u76ca\u51cf\u8f7b\u6d4b\u91cf\u566a\u58f0\u548c\u8bbe\u8ba1\u6269\u6563\u589e\u76ca\u901a\u8fc7\u5171\u8bc6\u534f\u8bae\u8865\u507f\u4fe1\u606f\u7f3a\u5931\uff0c\u5e76\u91c7\u7528\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u548c\u534a\u5b9a\u7f16\u7a0b\u8fdb\u884c\u9ad8\u6548\u5408\u6210\u3002", "result": "\u7ed3\u679c\u901a\u8fc7\u4e24\u4e2a\u6a21\u62df\u573a\u666f\u9a8c\u8bc1\u4e86\u5206\u5e03\u5f0f\u89c2\u6d4b\u5668\u7684\u6709\u6548\u6027\uff0c\u5728\u4e0d\u540c\u8282\u70b9\u5206\u89e3\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u72b6\u6001\u4f30\u8ba1\u4e2d\u662f\u6709\u6548\u7684\u3001\u53ef\u8ba1\u7b97\u7684\uff0c\u5e76\u80fd\u5904\u7406\u566a\u58f0\u548c\u672a\u77e5\u8f93\u5165\u3002"}}
{"id": "2504.16263", "pdf": "https://arxiv.org/pdf/2504.16263", "abs": "https://arxiv.org/abs/2504.16263", "authors": ["Magnus Sieverding", "Nathan Steffen", "Kelly Cohen"], "title": "Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a performance benchmarking study of a Gradient-Optimized\nFuzzy Inference System (GF) classifier against several state-of-the-art machine\nlearning models, including Random Forest, XGBoost, Logistic Regression, Support\nVector Machines, and Neural Networks. The evaluation was conducted across five\ndatasets from the UCI Machine Learning Repository, each chosen for their\ndiversity in input types, class distributions, and classification complexity.\nUnlike traditional Fuzzy Inference Systems that rely on derivative-free\noptimization methods, the GF leverages gradient descent to significantly\nimproving training efficiency and predictive performance. Results demonstrate\nthat the GF model achieved competitive, and in several cases superior,\nclassification accuracy while maintaining high precision and exceptionally low\ntraining times. In particular, the GF exhibited strong consistency across folds\nand datasets, underscoring its robustness in handling noisy data and variable\nfeature sets. These findings support the potential of gradient optimized fuzzy\nsystems as interpretable, efficient, and adaptable alternatives to more complex\ndeep learning models in supervised learning tasks.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u68af\u5ea6\u4f18\u5316\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\uff08GF\uff09\u4e0e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u4f9d\u8d56\u65e0\u5bfc\u6570\u4f18\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22GF\u4f5c\u4e3a\u9ad8\u6548\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u5728\u4e94\u4e2aUCI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30GF\u6a21\u578b\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\uff0c\u4e0e\u968f\u673a\u68ee\u6797\u3001XGBoost\u7b49\u6a21\u578b\u6bd4\u8f83\u3002", "result": "GF\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8de8\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u68af\u5ea6\u4f18\u5316\u6a21\u7cca\u7cfb\u7edf\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2504.16100", "pdf": "https://arxiv.org/pdf/2504.16100", "abs": "https://arxiv.org/abs/2504.16100", "authors": ["Eloi Lindas", "Yannig Goude", "Philippe Ciais"], "title": "Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML"], "comment": "24 pages, 4 tables, 18 figures", "summary": "Accurate prediction of non-dispatchable renewable energy sources is essential\nfor grid stability and price prediction. Regional power supply forecasts are\nusually indirect through a bottom-up approach of plant-level forecasts,\nincorporate lagged power values, and do not use the potential of spatially\nresolved data. This study presents a comprehensive methodology for predicting\nsolar and wind power production at country scale in France using machine\nlearning models trained with spatially explicit weather data combined with\nspatial information about production sites capacity. A dataset is built\nspanning from 2012 to 2023, using daily power production data from RTE (the\nnational grid operator) as the target variable, with daily weather data from\nERA5, production sites capacity and location, and electricity prices as input\nfeatures. Three modeling approaches are explored to handle spatially resolved\nweather data: spatial averaging over the country, dimension reduction through\nprincipal component analysis, and a computer vision architecture to exploit\ncomplex spatial relationships. The study benchmarks state-of-the-art machine\nlearning models as well as hyperparameter tuning approaches based on\ncross-validation methods on daily power production data. Results indicate that\ncross-validation tailored to time series is best suited to reach low error. We\nfound that neural networks tend to outperform traditional tree-based models,\nwhich face challenges in extrapolation due to the increasing renewable capacity\nover time. Model performance ranges from 4% to 10% in nRMSE for midterm\nhorizon, achieving similar error metrics to local models established at a\nsingle-plant level, highlighting the potential of these methods for regional\npower supply forecasting.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7a7a\u95f4\u5929\u6c14\u6570\u636e\u9884\u6d4b\u6cd5\u56fd\u592a\u9633\u80fd\u548c\u98ce\u80fd\u751f\u4ea7\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e2d\u671f\u9884\u6d4b\u4e2d\u9519\u8bef\u73874%-10%\uff0c\u5c55\u793a\u4e86\u533a\u57df\u9884\u6d4b\u6f5c\u529b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e0d\u53ef\u8c03\u5ea6\u53ef\u518d\u751f\u80fd\u6e90\u5bf9\u4e8e\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u4ef7\u683c\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u7a7a\u95f4\u5206\u8fa8\u7387\u6570\u636e\u3002", "method": "\u6784\u5efa2012-2023\u5e74\u6570\u636e\u96c6\uff0c\u4f7f\u7528RTE\u7535\u529b\u6570\u636e\u4f5c\u4e3a\u76ee\u6807\uff0cERA5\u5929\u6c14\u6570\u636e\u7b49\u4f5c\u4e3a\u8f93\u5165\uff0c\u63a2\u7d22\u7a7a\u95f4\u5e73\u5747\u3001PCA\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u4ea4\u53c9\u9a8c\u8bc1\u8c03\u4f18\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u4ea4\u53c9\u9a8c\u8bc1\u6700\u6709\u6548\uff0c\u795e\u7ecf\u7f51\u7edc\u4f18\u4e8e\u6811\u6a21\u578b\uff0cnRMSE\u9519\u8bef\u7387\u57284%\u523010%\uff0c\u4e0e\u5355\u5382\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u5728\u533a\u57df\u7535\u529b\u4f9b\u5e94\u9884\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2504.16874", "pdf": "https://arxiv.org/pdf/2504.16874", "abs": "https://arxiv.org/abs/2504.16874", "authors": ["Hamed Radpour", "Markus Hofer", "Thomas Zemen"], "title": "Reconfigurable Intelligent Surface Control for a Moving Receiver", "categories": ["eess.SY", "cs.SY"], "comment": "6 pages, submitted to IEEE Global Communications (GLOBECOM25)\n  Conference", "summary": "Reconfigurable intelligent surfaces (RISs) are emerging as key enablers of\nreliable industrial automation in the millimeter-wave (mmWave) band,\nparticularly in environments with frequent line-of-sight (LoS) blockage. While\nprior works have largely focused on theoretical aspects, real-time validation\nunder user mobility remains underexplored. In this work, we propose and\nexperimentally evaluate a self-adaptive beamforming algorithm that enables RIS\nreconfiguration based on a low-rate feedback link from the mobile user\nequipment (UE) to the RIS controller. The algorithm maintains received signal\npower above a predefined threshold without requiring UE position knowledge.\nUsing a hexagonal RIS with 127 elements operating at 23.8 GHz, we validate our\napproach in a semi-anechoic environment over a 60 cm*100 cm observation area.\nThe results demonstrate up to 24 dB gain in received power compared to the\nbaseline with inactive RIS elements, highlighting the practical benefits of\nadaptive RIS control in mobile non-line-of-sight (NLoS) scenarios.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6ce2\u675f\u5f62\u6210\u7b97\u6cd5\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684RIS\uff0c\u5728\u79fb\u52a8\u573a\u666f\u4e0b\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u83b7\u5f97\u9ad8\u8fbe24dB\u7684\u63a5\u6536\u529f\u7387\u589e\u76ca\u3002", "motivation": "\u6beb\u7c73\u6ce2\u9891\u6bb5\u6613\u53d7\u89c6\u7ebf\u963b\u585e\uff0cRIS\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u4e3a\u7406\u8bba\uff0c\u5b9e\u65f6\u79fb\u52a8\u9a8c\u8bc1\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4e\u901f\u7387\u53cd\u9988\u7684\u81ea\u9002\u5e94\u6ce2\u675f\u5f62\u6210\u7b97\u6cd5\uff0c\u4e0d\u9700\u7528\u6237\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u7528127\u5143\u7d20RIS\u572823.8GHz\u534a\u6d88\u58f0\u5ba4\u73af\u5883\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63a5\u6536\u529f\u7387\u8f83\u672a\u6fc0\u6d3bRIS\u57fa\u51c6\u63d0\u5347\u9ad8\u8fbe24dB\u3002", "conclusion": "\u7a81\u663e\u81ea\u9002\u5e94RIS\u63a7\u5236\u5728\u79fb\u52a8\u975e\u89c6\u7ebf\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u76ca\u5904\u3002"}}
{"id": "2504.16268", "pdf": "https://arxiv.org/pdf/2504.16268", "abs": "https://arxiv.org/abs/2504.16268", "authors": ["Abdesslem Layeb"], "title": "Boosting Classifier Performance with Opposition-Based Data Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a novel data transformation framework based on\nOpposition-Based Learning (OBL) to boost the performance of traditional\nclassification algorithms. Originally developed to accelerate convergence in\noptimization tasks, OBL is leveraged here to generate synthetic opposite\nsamples that replace the acutely training data and improve decision boundary\nformation. We explore three OBL variants; Global OBL, Class-Wise OBL, and\nLocalized Class-Wise OBL; and integrate them with several widely used\nclassifiers, including K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments\nconducted on 26 heterogeneous and high-dimensional datasets demonstrate that\nOBL-enhanced classifiers consistently outperform their standard counterparts in\nterms of accuracy and F1-score, frequently achieving near-perfect or perfect\nclassification. Furthermore, OBL contributes to improved computational\nefficiency, particularly in SVM and LR. These findings underscore the potential\nof OBL as a lightweight yet powerful data transformation strategy for enhancing\nclassification performance, especially in complex or sparse learning\nenvironments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528Opposition-Based Learning (OBL) \u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u63d0\u5347\u4f20\u7edf\u5206\u7c7b\u7b97\u6cd5\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u51c6\u786e\u6027\u548c\u6548\u7387\u5747\u6709\u663e\u8457\u6539\u5584\u3002", "motivation": "\u63d0\u5347\u5206\u7c7b\u7b97\u6cd5\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u6216\u7a00\u758f\u6570\u636e\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u751f\u6210\u76f8\u53cd\u6837\u672c\u6539\u5584\u51b3\u7b56\u8fb9\u754c\u3002", "method": "\u5f15\u5165OBL\u6846\u67b6\uff0c\u5305\u62ecGlobal OBL\u3001Class-Wise OBL\u548cLocalized Class-Wise OBL\uff0c\u4e0eKNN\u3001SVM\u3001LR\u3001DT\u7b49\u5206\u7c7b\u5668\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff0cOBL\u589e\u5f3a\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027\u548cF1-score\u4e0a\u4f18\u4e8e\u6807\u51c6\u7248\u672c\uff0c\u8ba1\u7b97\u6548\u7387\u7279\u522b\u662fSVM\u548cLR\u5f97\u5230\u6539\u5584\u3002", "conclusion": "OBL\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7b56\u7565\uff0c\u5177\u6709\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u5b66\u4e60\u73af\u5883\u3002"}}
{"id": "2504.16101", "pdf": "https://arxiv.org/pdf/2504.16101", "abs": "https://arxiv.org/abs/2504.16101", "authors": ["Lei Kang", "Xuanshuo Fu", "Javier Vazquez-Corral", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nworldwide, highlighting the critical need for efficient and accurate diagnostic\ntools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart\nconditions; however, their manual interpretation is time-consuming and\nerror-prone. In this paper, we propose xLSTM-ECG, a novel approach that\nleverages an extended Long Short-Term Memory (xLSTM) network for multi-label\nclassification of ECG signals, using the PTB-XL dataset. To the best of our\nknowledge, this work represents the first design and application of xLSTM\nmodules specifically adapted for multi-label ECG classification. Our method\nemploys a Short-Time Fourier Transform (STFT) to convert time-series ECG\nwaveforms into the frequency domain, thereby enhancing feature extraction. The\nxLSTM architecture is specifically tailored to address the complexities of\n12-lead ECG recordings by capturing both local and global signal features.\nComprehensive experiments on the PTB-XL dataset reveal that our model achieves\nstrong multi-label classification performance, while additional tests on the\nGeorgia 12-Lead dataset underscore its robustness and efficiency. This approach\nsignificantly improves ECG classification accuracy, thereby advancing clinical\ndiagnostics and patient care. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faxLSTM-ECG\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u5c55LSTM\u7f51\u7edc\u5bf9ECG\u4fe1\u53f7\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u4ea1\u539f\u56e0\uff0c\u624b\u52a8ECG\u89e3\u91ca\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362(STFT)\u5c06ECG\u4fe1\u53f7\u8f6c\u6362\u4e3a\u9891\u57df\uff0c\u5e76\u91c7\u7528\u5b9a\u5236xLSTM\u7f51\u7edc\u9488\u5bf912-lead ECG\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\u51fa\u8272\u6027\u80fd\uff0c\u5728Georgia 12-Lead\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u663e\u8457\u63d0\u5347ECG\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u8bca\u65ad\u548c\u60a3\u8005\u62a4\u7406\uff0c\u4ee3\u7801\u5c06\u4e8e\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2504.16879", "pdf": "https://arxiv.org/pdf/2504.16879", "abs": "https://arxiv.org/abs/2504.16879", "authors": ["Puja Chaudhury", "Alexander Estornell", "Michael Everett"], "title": "Learning Verifiable Control Policies Using Relaxed Verification", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "To provide safety guarantees for learning-based control systems, recent work\nhas developed formal verification methods to apply after training ends.\nHowever, if the trained policy does not meet the specifications, or there is\nconservatism in the verification algorithm, establishing these guarantees may\nnot be possible. Instead, this work proposes to perform verification throughout\ntraining to ultimately aim for policies whose properties can be evaluated\nthroughout runtime with lightweight, relaxed verification algorithms. The\napproach is to use differentiable reachability analysis and incorporate new\ncomponents into the loss function. Numerical experiments on a quadrotor model\nand unicycle model highlight the ability of this approach to lead to learned\ncontrol policies that satisfy desired reach-avoid and invariance\nspecifications.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5206\u6790\u9a8c\u8bc1\u63a7\u5236\u7b56\u7565\uff0c\u786e\u4fdd\u5b89\u5168\u89c4\u8303\uff0c\u5e76\u5728\u673a\u5668\u4eba\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5728\u8bad\u7ec3\u540e\u9a8c\u8bc1\uff0c\u6613\u56e0\u7b56\u7565\u4e0d\u6ee1\u8db3\u89c4\u8303\u6216\u7b97\u6cd5\u4fdd\u5b88\u6027\u800c\u5931\u8d25\uff1b\u672c\u6587\u65e8\u5728\u8bad\u7ec3\u4e2d\u6301\u7eed\u9a8c\u8bc1\uff0c\u4ee5\u83b7\u5f97\u53ef\u5728\u8fd0\u884c\u65f6\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u5e76\u5c06\u65b0\u7ec4\u4ef6\u878d\u5165\u635f\u5931\u51fd\u6570\u4e2d\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u548c\u5355\u8f6e\u8f66\u6a21\u578b\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u6210\u529f\u5b66\u4e60\u51fa\u6ee1\u8db3\u53ef\u8fbe-\u907f\u5f00\u548c\u4e0d\u53d8\u6027\u89c4\u8303\u7684\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u751f\u6210\u53ef\u5728\u8fd0\u884c\u65f6\u8f7b\u677e\u9a8c\u8bc1\u7684\u5b89\u5168\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2504.16272", "pdf": "https://arxiv.org/pdf/2504.16272", "abs": "https://arxiv.org/abs/2504.16272", "authors": ["Ryan Koo", "Ian Yang", "Vipul Raheja", "Mingyi Hong", "Kwang-Sung Jun", "Dongyeop Kang"], "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Current reinforcement learning from human feedback (RLHF) pipelines for large\nlanguage model (LLM) alignment typically assign scalar rewards to sequences,\nusing the final token as a surrogate indicator for the quality of the entire\nsequence. However, this leads to sparse feedback and suboptimal token-level\ncredit assignment. In this work, we frame reward shaping as an optimization\nproblem focused on token-level credit assignment. We propose a reward-shaping\nfunction leveraging explainability methods such as SHAP and LIME to estimate\nper-token rewards from the reward model. To learn parameters of this shaping\nfunction, we employ a bilevel optimization framework that integrates Bayesian\nOptimization and policy training to handle noise from the token reward\nestimates. Our experiments show that achieving a better balance of token-level\nreward attribution leads to performance improvements over baselines on\ndownstream tasks and finds an optimal policy faster during training.\nFurthermore, we show theoretically that explainability methods that are feature\nadditive attribution functions maintain the optimal policy as the original\nreward.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u6574\u5f62\u6765\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684RLHF\uff0c\u63d0\u9ad8\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dRLHF\u7ba1\u9053\u4e2d\u6807\u91cf\u5956\u52b1\u5bfc\u81f4\u7684\u53cd\u9988\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u6b21\u4f18\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528SHAP\u548cLIME\u7684\u5956\u52b1\u6574\u5f62\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u7b56\u7565\u8bad\u7ec3\u6765\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u66f4\u597d\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u5f52\u56e0\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u8bad\u7ec3\u66f4\u5feb\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\u4fdd\u6301\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "\u4ee4\u724c\u7ea7\u5956\u52b1\u6574\u5f62\u6539\u5584RLHF\uff0c\u5177\u6709\u7ecf\u9a8c\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.16110", "pdf": "https://arxiv.org/pdf/2504.16110", "abs": "https://arxiv.org/abs/2504.16110", "authors": ["Krti Tallam"], "title": "Security-First AI: Foundations for Robust and Trustworthy Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The conversation around artificial intelligence (AI) often focuses on safety,\ntransparency, accountability, alignment, and responsibility. However, AI\nsecurity (i.e., the safeguarding of data, models, and pipelines from\nadversarial manipulation) underpins all of these efforts. This manuscript\nposits that AI security must be prioritized as a foundational layer. We present\na hierarchical view of AI challenges, distinguishing security from safety, and\nargue for a security-first approach to enable trustworthy and resilient AI\nsystems. We discuss core threat models, key attack vectors, and emerging\ndefense mechanisms, concluding that a metric-driven approach to AI security is\nessential for robust AI safety, transparency, and accountability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u5f20\u5c06AI\u5b89\u5168\u4f5c\u4e3a\u57fa\u7840\u5c42\uff0c\u4f18\u5148\u8003\u8651\u5b83\u6765\u652f\u6301AI\u7684\u5b89\u5168\u3001\u900f\u660e\u548c\u8d23\u4efb\u3002", "motivation": "AI\u5b89\u5168\u662fAI\u9886\u57df\u5b89\u5168\u3001\u900f\u660e\u3001\u8d23\u4efb\u7b49\u52aa\u529b\u7684\u57fa\u7840\uff0c\u4f46\u5f53\u524d\u8ba8\u8bba\u5f80\u5f80\u5ffd\u7565\u5b83\u3002", "method": "\u5448\u73b0AI\u6311\u6218\u7684\u5206\u5c42\u89c6\u56fe\uff0c\u533a\u5206\u5b89\u5168\u4e0e\u5b89\u5168\uff0c\u8bba\u8bc1\u5b89\u5168\u4f18\u5148\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u5a01\u80c1\u6a21\u578b\u3001\u653b\u51fb\u5411\u91cf\u548c\u9632\u5fa1\u673a\u5236\u3002", "result": "\u5f97\u51fa\u7ed3\u8bba\uff0cAI\u5b89\u5168\u9700\u8981\u91c7\u7528\u5ea6\u91cf\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "conclusion": "\u4ee5\u5ea6\u91cf\u4e3a\u57fa\u7840\u7684AI\u5b89\u5168\u65b9\u6cd5\u5bf9\u5b9e\u73b0\u7a33\u5065\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.16224", "pdf": "https://arxiv.org/pdf/2504.16224", "abs": "https://arxiv.org/abs/2504.16224", "authors": ["Hossein Gholampour", "Jonathon E. Slightam", "Logan E. Beaver"], "title": "Mass-Adaptive Admittance Control for Robotic Manipulators", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "6 pages, 7 figures", "summary": "Handling objects with unknown or changing masses is a common challenge in\nrobotics, often leading to errors or instability if the control system cannot\nadapt in real-time. In this paper, we present a novel approach that enables a\nsix-degrees-of-freedom robotic manipulator to reliably follow waypoints while\nautomatically estimating and compensating for unknown payload weight. Our\nmethod integrates an admittance control framework with a mass estimator,\nallowing the robot to dynamically update an excitation force to compensate for\nthe payload mass. This strategy mitigates end-effector sagging and preserves\nstability when handling objects of unknown weights. We experimentally validated\nour approach in a challenging pick-and-place task on a shelf with a crossbar,\nimproved accuracy in reaching waypoints and compliant motion compared to a\nbaseline admittance-control scheme. By safely accommodating unknown payloads,\nour work enhances flexibility in robotic automation and represents a\nsignificant step forward in adaptive control for uncertain environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u672a\u77e5\u8d1f\u8f7d\u8d28\u91cf\u4e0b\u53ef\u9760\u5730\u8ddf\u968f\u8def\u5f84\u70b9\uff0c\u901a\u8fc7\u6574\u5408 admittance \u63a7\u5236\u548c\u8d28\u91cf\u4f30\u8ba1\u5668\u6765\u8865\u507f\u8d1f\u8f7d\u3002", "motivation": "\u5904\u7406\u672a\u77e5\u6216\u53d8\u5316\u8d28\u91cf\u7684\u7269\u4f53\u662f\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u5e38\u89c1\u6311\u6218\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u6216\u4e0d\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u3002", "method": "\u6574\u5408 admittance \u63a7\u5236\u6846\u67b6\u4e0e\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u52a8\u6001\u66f4\u65b0\u6fc0\u52b1\u529b\u4ee5\u8865\u507f\u672a\u77e5\u8d1f\u8f7d\u8d28\u91cf\uff0c\u4ece\u800c\u51cf\u8f7b\u672b\u7aef\u6267\u884c\u5668\u4e0b\u5782\u5e76\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6311\u62e3\u548c\u653e\u7f6e\u4efb\u52a1\u4e2d\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6848\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86\u8def\u5f84\u70b9\u5230\u8fbe\u7cbe\u5ea6\u548c\u987a\u4ece\u8fd0\u52a8\u3002", "conclusion": "\u901a\u8fc7\u5b89\u5168\u5904\u7406\u672a\u77e5\u8d1f\u8f7d\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u81ea\u52a8\u5316\u7684\u7075\u6d3b\u6027\uff0c\u662f\u9002\u5e94\u4e0d\u786e\u5b9a\u73af\u5883\u63a7\u5236\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2504.16275", "pdf": "https://arxiv.org/pdf/2504.16275", "abs": "https://arxiv.org/abs/2504.16275", "authors": ["Jannis Born", "Filip Skogh", "Kahn Rhrissorrakrai", "Filippo Utro", "Nico Wagner", "Aleksandros Sobczyk"], "title": "Quantum Doubly Stochastic Transformers", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CV"], "comment": "Under Review", "summary": "At the core of the Transformer, the Softmax normalizes the attention matrix\nto be right stochastic. Previous research has shown that this often\ndestabilizes training and that enforcing the attention matrix to be doubly\nstochastic (through Sinkhorn's algorithm) consistently improves performance\nacross different tasks, domains and Transformer flavors. However, Sinkhorn's\nalgorithm is iterative, approximative, non-parametric and thus inflexible\nw.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been\nproven that DSMs can be obtained with a parametric quantum circuit, yielding a\nnovel quantum inductive bias for DSMs with no known classical analogue.\nMotivated by this, we demonstrate the feasibility of a hybrid classical-quantum\ndoubly stochastic Transformer (QDSFormer) that replaces the Softmax in the\nself-attention layer with a variational quantum circuit. We study the\nexpressive power of the circuit and find that it yields more diverse DSMs that\nbetter preserve information than classical operators. Across multiple\nsmall-scale object recognition tasks, we find that our QDSFormer consistently\nsurpasses both a standard Vision Transformer and other doubly stochastic\nTransformers. Beyond the established Sinkformer, this comparison includes a\nnovel quantum-inspired doubly stochastic Transformer (based on QR\ndecomposition) that can be of independent interest. The QDSFormer also shows\nimproved training stability and lower performance variation suggesting that it\nmay mitigate the notoriously unstable training of ViTs on small-scale data.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u53cc\u91cd\u968f\u673aTransformer\uff08QDSFormer\uff09\uff0c\u901a\u8fc7\u53d8\u5206\u91cf\u5b50\u7535\u8def\u66ff\u6362Softmax\uff0c\u4ee5\u6539\u5584\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3Transformer\u4e2dSoftmax\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5e76\u5229\u7528\u91cf\u5b50\u7535\u8def\u63d0\u4f9b\u65b0\u9896\u7684\u53cc\u91cd\u968f\u673a\u77e9\u9635\u5f52\u4e00\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u4f7f\u7528\u53d8\u5206\u91cf\u5b50\u7535\u8def\u5f3a\u5236\u6ce8\u610f\u529b\u77e9\u9635\u6210\u4e3a\u53cc\u91cd\u968f\u673a\u77e9\u9635\uff0c\u5e76\u4e0eSinkhorn\u7b97\u6cd5\u548c\u57fa\u4e8eQR\u5206\u89e3\u7684\u91cf\u5b50\u542f\u53d1\u5f0fTransformer\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "QDSFormer\u5728\u5c0f\u89c4\u6a21\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6Vision Transformer\u548c\u5176\u4ed6\u53cc\u91cd\u968f\u673aTransformer\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8f83\u4f4e\u7684\u6027\u80fd\u53d8\u5f02\u3002", "conclusion": "QDSFormer\u53ef\u80fd\u7f13\u89e3Vision Transformer\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u4f9b\u6f5c\u5728\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2504.16112", "pdf": "https://arxiv.org/pdf/2504.16112", "abs": "https://arxiv.org/abs/2504.16112", "authors": ["Myunghyun Rhee", "Joonseop Sim", "Taeyoung Ahn", "Seungyong Lee", "Daegun Yoon", "Euiseok Kim", "Kyoung Park", "Youngpyo Joo", "Hosik Kim"], "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC"], "comment": "6 pages", "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5e26\u5bbd\u5904\u7406\u5355\u5143\uff08HPU\uff09\uff0c\u901a\u8fc7\u5378\u8f7dGPU\u7684\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u63d0\u9ad8Transformer-based LLM\u5927\u6279\u91cf\u63a8\u7406\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3Transformer\u6a21\u578b\u6ce8\u610f\u529b\u5c42\u5728\u5f53\u524dGPU\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4f4e\u8fd0\u7b97\u5f3a\u5ea6\u548cKV\u7f13\u5b58\u9ad8\u5185\u5b58\u9700\u6c42\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u57fa\u4e8ePCIe FPGA\u5361\u7684HPU\u4f5c\u4e3aGPU\u7cfb\u7edf\u7684\u534f\u5904\u7406\u5668\uff0c\u5f62\u6210\u5f02\u6784\u7cfb\u7edf\uff0c\u5378\u8f7d\u5185\u5b58\u7ed1\u5b9a\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGPU-HPU\u7cfb\u7edf\u6bd4\u7eafGPU\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4.1\u500d\uff0c\u80fd\u6e90\u6548\u7387\u63d0\u53474.6\u500d\uff0c\u5e76\u652f\u6301\u6269\u5c55\u800c\u4e0d\u589e\u52a0GPU\u6570\u91cf\u3002", "conclusion": "HPU\u63d0\u5347\u4e86GPU\u8d44\u6e90\u5229\u7528\u7387\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6539\u8fdb\u3002"}}
{"id": "2504.16319", "pdf": "https://arxiv.org/pdf/2504.16319", "abs": "https://arxiv.org/abs/2504.16319", "authors": ["Connor Blais", "Md Abdul Baset Sarker", "Masudul H. Imtiaz"], "title": "Vision Controlled Orthotic Hand Exoskeleton", "categories": ["cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This paper presents the design and implementation of an AI vision-controlled\northotic hand exoskeleton to enhance rehabilitation and assistive functionality\nfor individuals with hand mobility impairments. The system leverages a Google\nCoral Dev Board Micro with an Edge TPU to enable real-time object detection\nusing a customized MobileNet\\_V2 model trained on a six-class dataset. The\nexoskeleton autonomously detects objects, estimates proximity, and triggers\npneumatic actuation for grasp-and-release tasks, eliminating the need for\nuser-specific calibration needed in traditional EMG-based systems. The design\nprioritizes compactness, featuring an internal battery. It achieves an 8-hour\nruntime with a 1300 mAh battery. Experimental results demonstrate a 51ms\ninference speed, a significant improvement over prior iterations, though\nchallenges persist in model robustness under varying lighting conditions and\nobject orientations. While the most recent YOLO model (YOLOv11) showed\npotential with 15.4 FPS performance, quantization issues hindered deployment.\nThe prototype underscores the viability of vision-controlled exoskeletons for\nreal-world assistive applications, balancing portability, efficiency, and\nreal-time responsiveness, while highlighting future directions for model\noptimization and hardware miniaturization.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f00\u53d1\u4e86AI\u89c6\u89c9\u63a7\u5236\u7684\u624b\u90e8\u5916\u9aa8\u9abc\uff0c\u63d0\u5347\u624b\u90e8\u969c\u788d\u8005\u7684\u5eb7\u590d\u8f85\u52a9\uff0c\u901a\u8fc7\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u548c\u81ea\u52a8\u6293\u53d6\uff0c\u6539\u5584\u4f20\u7edfEMG\u7cfb\u7edf\u3002", "motivation": "\u9488\u5bf9\u624b\u90e8\u8fd0\u52a8\u969c\u788d\u8005\uff0c\u63d0\u4f9b\u65e0\u9700\u7528\u6237\u6821\u51c6\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u63d0\u9ad8\u5eb7\u590d\u6548\u7387\u548c\u4fbf\u5229\u6027\u3002", "method": "\u4f7f\u7528Google Coral Dev Board Micro\u7684Edge TPU\u548c\u5b9a\u5236MobileNet_V2\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\u3001\u8ddd\u79bb\u4f30\u8ba1\u548c\u6c14\u52a8\u81f4\u52a8\uff0c\u8bbe\u8ba1\u7d27\u51d1\uff0c\u7eed\u822a8\u5c0f\u65f6\u3002", "result": "\u63a8\u7406\u901f\u5ea651ms\uff0c\u4f18\u4e8e\u4ee5\u5f80\uff0c\u4f46\u5149\u7167\u548c\u7269\u4f53\u65b9\u5411\u53d8\u5316\u65f6\u6a21\u578b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0cYOLOv11\u6f5c\u529b\u53d7\u91cf\u5316\u95ee\u9898\u5f71\u54cd\u3002", "conclusion": "\u8bc1\u660e\u89c6\u89c9\u63a7\u5236\u5916\u9aa8\u9abc\u5728\u5b9e\u9645\u8f85\u52a9\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e73\u8861\u4fbf\u643a\u6027\u548c\u6548\u7387\uff0c\u5efa\u8bae\u672a\u6765\u4f18\u5316\u6a21\u578b\u548c\u786c\u4ef6\u3002"}}
{"id": "2504.16276", "pdf": "https://arxiv.org/pdf/2504.16276", "abs": "https://arxiv.org/abs/2504.16276", "authors": ["Abhishek Jana", "Moeumu Uili", "James Atherton", "Mark O'Brien", "Joe Wood", "Leandra Brickson"], "title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD"], "comment": "16 pages, 5 figures, 4 tables", "summary": "This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7a00\u6709\u9e1f\u7c7b\u7684\u81ea\u52a8\u4e00\u51fb\u5f0f\u9e1f\u9e23\u5206\u7c7b\u7ba1\u9053\uff0c\u4f7f\u7528\u5d4c\u5165\u7a7a\u95f4\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5728\u6fd2\u5371\u9e1f\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u9e1f\u7c7b\u5206\u7c7b\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4ec5\u67091-3\u4e2a\u5f55\u97f3\u7684\u7a00\u6709\u7269\u79cd\uff0c\u8fd9\u5bf9\u76d1\u6d4b\u6fd2\u5371\u9e1f\u7c7b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5927\u578b\u9e1f\u7c7b\u7f51\u7edc\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5206\u7c7b\uff0c\u7ed3\u5408\u9884\u5904\u7406\u8fc7\u6ee4\u548c\u53bb\u566a\u6280\u672f\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u4f7f\u7528\u805a\u7c7b\u6307\u6807\u8bc4\u4f30\u3002", "result": "\u5728\u7259\u5599\u9e3d\u7684\u771f\u5b9e\u6d4b\u8bd5\u4e2d\uff0c\u53ec\u56de\u7387\u8fbe\u52301.0\uff0c\u51c6\u786e\u7387\u8fbe\u52300.95\u3002", "conclusion": "\u8be5\u5f00\u6e90\u7cfb\u7edf\u4e3a\u4fdd\u62a4\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u76d1\u6d4b\u6fd2\u4e34\u706d\u7edd\u7684\u7a00\u6709\u7269\u79cd\u3002"}}
{"id": "2504.16113", "pdf": "https://arxiv.org/pdf/2504.16113", "abs": "https://arxiv.org/abs/2504.16113", "authors": ["Xin Wang", "Xiaoqi Li"], "title": "AI-Based Vulnerability Analysis of NFT Smart Contracts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "In the research experiment of this article, our research work is divided into\nseveral stages. Firstly, we collected a large number of smart contract codes\nand classified them, identifying several common defects, including Risky\nMutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and\nPublic Burns. Secondly, we used Python to process the smart contracts. On the\none hand, we modified the file names, and on the other hand, we batched the\nprocess of the content for analysis and application. Next, we built a model of\nthe decision tree. Firstly, we carried out the feature extraction. We selected\nthe algorithm and divided the data. After comparing and processing, we chose\nthe CART classification tree to process. By gene coefficient, we analyzed and\nsorted the data, and got the initial model of the decision tree. Then, we\nintroduced the random forest model on the basis of the decision tree. From\nabstracting the same amount of samples to selecting features randomly.From\nadjusting and optimizing parameters to completing the construction of the\nforest model. Finally, we compared and analyzed the decision tree, random\nforest, and self-built model in the paper and drew general conclusions.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u68c0\u6d4b\u667a\u80fd\u5408\u7ea6\u7f3a\u9677\uff0c\u5982Risky Mutably Porxy\u7b49\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u5b58\u5728\u5e38\u89c1\u7f3a\u9677\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u6536\u96c6\u5e76\u5206\u7c7b\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\uff0c\u4f7f\u7528Python\u5904\u7406\u6570\u636e\uff0c\u6784\u5efaCART\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u3002", "result": "\u6bd4\u8f83\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u548c\u81ea\u5efa\u6a21\u578b\uff0c\u5206\u6790\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u5f97\u51fa\u6a21\u578b\u6bd4\u8f83\u7684\u4e00\u822c\u7ed3\u8bba\uff0c\u53ef\u80fd\u63a8\u8350\u968f\u673a\u68ee\u6797\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.16328", "pdf": "https://arxiv.org/pdf/2504.16328", "abs": "https://arxiv.org/abs/2504.16328", "authors": ["Nicholas P. Nurre", "Ehsan Taheri"], "title": "Eigendecomposition Parameterization of Penalty Matrices for Enhanced Control Design: Aerospace Applications", "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "comment": "39 pages, 18 figures", "summary": "Modern control algorithms require tuning of square weight/penalty matrices\nappearing in quadratic functions/costs to improve performance and/or stability\noutput. Due to simplicity in gain-tuning and enforcing positive-definiteness,\ndiagonal penalty matrices are used extensively in control methods such as\nlinear quadratic regulator (LQR), model predictive control, and Lyapunov-based\ncontrol. In this paper, we propose an eigendecomposition approach to\nparameterize penalty matrices, allowing positive-definiteness with non-zero\noff-diagonal entries to be implicitly satisfied, which not only offers notable\ncomputational and implementation advantages, but broadens the class of\nachievable controls. We solve three control problems: 1) a variation of\nZermelo's navigation problem, 2) minimum-energy spacecraft attitude control\nusing both LQR and Lyapunov-based methods, and 3) minimum-fuel and minimum-time\nLyapunov-based low-thrust trajectory design. Particle swarm optimization is\nused to optimize the decision variables, which will parameterize the penalty\nmatrices. The results demonstrate improvements of up to 65% in the performance\nobjective in the example problems utilizing the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7279\u5f81\u5206\u89e3\u53c2\u6570\u5316\u60e9\u7f5a\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u63a7\u5236\u7b97\u6cd5\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u514b\u670d\u4f20\u7edf\u5bf9\u89d2\u60e9\u7f5a\u77e9\u9635\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5141\u8bb8\u975e\u96f6\u975e\u5bf9\u89d2\u5143\u7d20\u6765\u63d0\u5347\u6027\u80fd\u548c\u6269\u5c55\u63a7\u5236\u8303\u56f4\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528\u7279\u5f81\u5206\u89e3\u53c2\u6570\u5316\u60e9\u7f5a\u77e9\u9635\uff0c\u786e\u4fdd\u6b63\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u53d8\u91cf\uff0c\u5e94\u7528\u4e8eZermelo\u5bfc\u822a\u3001\u822a\u5929\u5668\u63a7\u5236\u548c\u8f68\u8ff9\u8bbe\u8ba1\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe65%\uff0c\u5728\u591a\u4e2a\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\u548c\u66f4\u5e7f\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2504.16277", "pdf": "https://arxiv.org/pdf/2504.16277", "abs": "https://arxiv.org/abs/2504.16277", "authors": ["Neha Hulkund", "Alaa Maalouf", "Levi Cai", "Daniel Yang", "Tsun-Hsuan Wang", "Abigail O'Neil", "Timm Haucke", "Sandeep Mukherjee", "Vikram Ramaswamy", "Judy Hansen Shen", "Gabriel Tseng", "Mike Walmsley", "Daniela Rus", "Ken Goldberg", "Hannah Kerner", "Irene Chen", "Yogesh Girdhar", "Sara Beery"], "title": "DataS^3: Dataset Subset Selection for Specialization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In many real-world machine learning (ML) applications (e.g. detecting broken\nbones in x-ray images, detecting species in camera traps), in practice models\nneed to perform well on specific deployments (e.g. a specific hospital, a\nspecific national park) rather than the domain broadly. However, deployments\noften have imbalanced, unique data distributions. Discrepancy between the\ntraining distribution and the deployment distribution can lead to suboptimal\nperformance, highlighting the need to select deployment-specialized subsets\nfrom the available training data. We formalize dataset subset selection for\nspecialization (DS3): given a training set drawn from a general distribution\nand a (potentially unlabeled) query set drawn from the desired\ndeployment-specific distribution, the goal is to select a subset of the\ntraining data that optimizes deployment performance.\n  We introduce DataS^3; the first dataset and benchmark designed specifically\nfor the DS3 problem. DataS^3 encompasses diverse real-world application\ndomains, each with a set of distinct deployments to specialize in. We conduct a\ncomprehensive study evaluating algorithms from various families--including\ncoresets, data filtering, and data curation--on DataS^3, and find that\ngeneral-distribution methods consistently fail on deployment-specific tasks.\nAdditionally, we demonstrate the existence of manually curated\n(deployment-specific) expert subsets that outperform training on all available\ndata with accuracy gains up to 51.3 percent. Our benchmark highlights the\ncritical role of tailored dataset curation in enhancing performance and\ntraining efficiency on deployment-specific distributions, which we posit will\nonly become more important as global, public datasets become available across\ndomains and ML models are deployed in the real world.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faDS3\u95ee\u9898\u548cDataS^3\u57fa\u51c6\uff0c\u663e\u793a\u9488\u5bf9\u90e8\u7f72\u7279\u5b9a\u6570\u636e\u96c6\u5b50\u96c6\u9009\u62e9\u53ef\u663e\u8457\u63d0\u5347ML\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5b9e\u9645ML\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u9700\u5728\u7279\u5b9a\u90e8\u7f72\uff08\u5982\u533b\u9662\u6216\u516c\u56ed\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u9009\u62e9\u90e8\u7f72\u4e13\u7528\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u3002", "method": "\u5f15\u5165DataS^3\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u8bc4\u4f30coresets\u3001data filtering\u548cdata curation\u7b49\u7b97\u6cd5\u3002", "result": "\u901a\u7528\u65b9\u6cd5\u5931\u8d25\uff0c\u624b\u52a8curation\u5b50\u96c6\u53ef\u63d0\u5347\u51c6\u786e\u7387\u9ad8\u8fbe51.3%\u3002", "conclusion": "\u5f3a\u8c03\u5b9a\u5236\u6570\u636e\u96c6curation\u5728\u63d0\u5347\u90e8\u7f72\u6027\u80fd\u548c\u6548\u7387\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u968f\u7740\u516c\u5171\u6570\u636e\u96c6\u589e\u591a\u548c\u6a21\u578b\u90e8\u7f72\uff0c\u5c06\u66f4\u91cd\u8981\u3002"}}
{"id": "2504.16116", "pdf": "https://arxiv.org/pdf/2504.16116", "abs": "https://arxiv.org/abs/2504.16116", "authors": ["Miracle Master", "Rainy Sun", "Anya Reese", "Joey Ouyang", "Alex Chen", "Winter Dong", "Frank Li", "James Yi", "Garry Zhao", "Tony Ling", "Hobert Wong", "Lowes Yang"], "title": "DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nprogress on a wide range of natural language processing tasks. However, their\neffectiveness in specialized and rapidly evolving domains such as Web3 remains\nunderexplored. In this paper, we introduce DMind Benchmark, a novel framework\nthat systematically tests LLMs across nine key categories encompassing\nblockchain fundamentals, infrastructure, smart contract analysis, decentralized\nfinance (DeFi), decentralized autonomous organizations (DAOs), non-fungible\ntokens (NFTs), token economics, meme concepts, and security vulnerabilities.\n  DMind Benchmark goes beyond conventional multiple-choice questions by\nincorporating domain-specific subjective tasks (e.g., smart contract code\nauditing and repair, numeric reasoning on on-chain data, and fill-in\nassessments), thereby capturing real-world complexities and stress-testing\nmodel adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,\nClaude, and Gemini series) on DMind Benchmark, uncovering performance gaps in\nWeb3-specific reasoning and application, particularly in emerging areas like\ntoken economics and meme concepts. Even the strongest models face significant\nchallenges in identifying subtle security vulnerabilities and analyzing complex\nDeFi mechanisms. To foster progress in this area, we publicly release our\nbenchmark dataset, evaluation pipeline, and annotated results at\nhttp://www.dmind.ai, offering a valuable resource for advancing specialized\ndomain adaptation and the development of more robust Web3-enabled LLMs.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165DMind Benchmark\u6846\u67b6\uff0c\u8bc4\u4f30LLM\u5728Web3\u9886\u57df\u7684\u6027\u80fd\uff0c\u6db5\u76d6\u4e5d\u4e2a\u7c7b\u522b\u7684\u4e3b\u89c2\u4efb\u52a1\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u3002", "motivation": "LLM\u5728\u4e00\u822cNLP\u4efb\u52a1\u4e0a\u8fdb\u6b65\u663e\u8457\uff0c\u4f46\u5bf9Web3\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1DMind Benchmark\u6846\u67b6\uff0c\u5305\u62ec\u533a\u5757\u94fe\u57fa\u7840\u3001DeFi\u7b49\u4e5d\u4e2a\u7c7b\u522b\uff0c\u5e76\u4f7f\u7528\u4e3b\u89c2\u4efb\u52a1\u5982\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u548c\u6570\u503c\u63a8\u7406\u3002", "result": "\u8bc4\u4f3015\u4e2aLLM\uff0c\u53d1\u73b0\u6a21\u578b\u5728Web3\u7279\u5b9a\u63a8\u7406\u4e0a\u5b58\u5728\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u4ee3\u5e01\u7ecf\u6d4e\u5b66\u548c\u5b89\u5168\u6f0f\u6d1e\u8bc6\u522b\u65b9\u9762\u3002", "conclusion": "\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u7ba1\u9053\u548c\u7ed3\u679c\uff0c\u4ee5\u4fc3\u8fdbWeb3-enabled LLM\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.16369", "pdf": "https://arxiv.org/pdf/2504.16369", "abs": "https://arxiv.org/abs/2504.16369", "authors": ["Yu Mei", "Xinyu Zhou", "Shuyang Yu", "Vaibhav Srivastava", "Xiaobo Tan"], "title": "Fast Online Adaptive Neural MPC via Meta-Learning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Data-driven model predictive control (MPC) has demonstrated significant\npotential for improving robot control performance in the presence of model\nuncertainties. However, existing approaches often require extensive offline\ndata collection and computationally intensive training, limiting their ability\nto adapt online. To address these challenges, this paper presents a fast online\nadaptive MPC framework that leverages neural networks integrated with\nModel-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot\nadaptation of residual dynamics - capturing the discrepancy between nominal and\ntrue system behavior - using minimal online data and gradient steps. By\nembedding these meta-learned residual models into a computationally efficient\nL4CasADi-based MPC pipeline, the proposed method enables rapid model\ncorrection, enhances predictive accuracy, and improves real-time control\nperformance. We validate the framework through simulation studies on a Van der\nPol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show\nsignificant gains in adaptation speed and prediction accuracy over both nominal\nMPC and nominal MPC augmented with a freshly initialized neural network,\nunderscoring the effectiveness of our approach for real-time adaptive robot\ncontrol.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5feb\u901f\u5728\u7ebf\u81ea\u9002\u5e94MPC\u6846\u67b6\uff0c\u4f7f\u7528MAML\u548c\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8MPC\u65b9\u6cd5\u9700\u5927\u91cf\u79bb\u7ebf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u65e0\u6cd5\u5728\u7ebf\u9002\u5e94\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528MAML\u8fdb\u884c\u5c11\u6837\u672c\u9002\u5e94\u6b8b\u5dee\u52a8\u6001\uff0c\u5e76\u5d4c\u5165L4CasADi-based MPC\u7ba1\u9053\uff0c\u5b9e\u73b0\u5feb\u901f\u6a21\u578b\u4fee\u6b63\u3002", "result": "\u6a21\u62df\u9a8c\u8bc1\u5728Van der Pol\u632f\u8361\u5668\u3001Cart-Pole\u548c2D quadrotor\u4e0a\uff0c\u663e\u793a\u6bd4\u6807\u51c6MPC\u66f4\u597d\u7684\u9002\u5e94\u901f\u5ea6\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u5b9e\u65f6\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2504.16283", "pdf": "https://arxiv.org/pdf/2504.16283", "abs": "https://arxiv.org/abs/2504.16283", "authors": ["Jaya Narain", "Amrit Romana", "Vikramjit Mitra", "Colin Lea", "Shirley Ren"], "title": "Affect Models Have Weak Generalizability to Atypical Speech", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Speech and voice conditions can alter the acoustic properties of speech,\nwhich could impact the performance of paralinguistic models for affect for\npeople with atypical speech. We evaluate publicly available models for\nrecognizing categorical and dimensional affect from speech on a dataset of\natypical speech, comparing results to datasets of typical speech. We\ninvestigate three dimensions of speech atypicality: intelligibility, which is\nrelated to pronounciation; monopitch, which is related to prosody, and\nharshness, which is related to voice quality. We look at (1) distributional\ntrends of categorical affect predictions within the dataset, (2) distributional\ncomparisons of categorical affect predictions to similar datasets of typical\nspeech, and (3) correlation strengths between text and speech predictions for\nspontaneous speech for valence and arousal. We find that the output of affect\nmodels is significantly impacted by the presence and degree of speech\natypicalities. For instance, the percentage of speech predicted as sad is\nsignificantly higher for all types and grades of atypical speech when compared\nto similar typical speech datasets. In a preliminary investigation on improving\nrobustness for atypical speech, we find that fine-tuning models on\npseudo-labeled atypical speech data improves performance on atypical speech\nwithout impacting performance on typical speech. Our results emphasize the need\nfor broader training and evaluation datasets for speech emotion models, and for\nmodeling approaches that are robust to voice and speech differences.", "AI": {"tldr": "\u8bed\u97f3\u5f02\u5e38\u4f1a\u5f71\u54cd\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u53ef\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u8bed\u97f3\u548c\u58f0\u97f3\u6761\u4ef6\u53ef\u80fd\u6539\u53d8\u8bed\u97f3\u5c5e\u6027\uff0c\u5f71\u54cd\u5bf9\u5f02\u5e38\u8bed\u97f3\u7684\u60c5\u611f\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u5176\u5f71\u54cd\u3002", "method": "\u5728\u5f02\u5e38\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u60c5\u611f\u8bc6\u522b\u6a21\u578b\uff0c\u6bd4\u8f83\u5178\u578b\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u8c03\u67e5\u8bed\u97f3\u53ef\u61c2\u5ea6\u3001\u5355\u8c03\u6027\u548c\u7c97\u7cd9\u5ea6\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u7b7e\u6570\u636e\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u6a21\u578b\u8f93\u51fa\u53d7\u5f02\u5e38\u8bed\u97f3\u5f71\u54cd\u663e\u8457\uff0c\u4f8b\u5982\u5bf9\u5f02\u5e38\u8bed\u97f3\u9884\u6d4b'\u60b2\u4f24'\u6bd4\u4f8b\u66f4\u9ad8\uff1b\u5fae\u8c03\u540e\uff0c\u5f02\u5e38\u8bed\u97f3\u6027\u80fd\u63d0\u5347\uff0c\u4e0d\u5f71\u54cd\u5178\u578b\u8bed\u97f3\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5bf9\u8bed\u97f3\u5dee\u5f02\u66f4\u9c81\u68d2\u7684\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2504.16117", "pdf": "https://arxiv.org/pdf/2504.16117", "abs": "https://arxiv.org/abs/2504.16117", "authors": ["Sridevi Polavaram", "Xin Zhou", "Meenu Ravi", "Mohammad Zarei", "Anmol Srivastava"], "title": "Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted to IEEE Conference for Artificial Intelligence, 2025", "summary": "Vision systems are increasingly deployed in critical domains such as\nsurveillance, law enforcement, and transportation. However, their\nvulnerabilities to rare or unforeseen scenarios pose significant safety risks.\nTo address these challenges, we introduce Context-Awareness and\nInterpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive\ndiscovery framework for failure cases (or CP - Critical Phenomena) detection\nand formalization. CAIRO by design incentivizes human-in-the-loop for testing\nand evaluation of criticality that arises from misdetections, adversarial\nattacks, and hallucinations in AI black-box models. Our robust analysis of\nobject detection model(s) failures in automated driving systems (ADS) showcases\nscalable and interpretable ways of formalizing the observed gaps between camera\nperception and real-world contexts, resulting in test cases stored as explicit\nknowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,\nlogical reasoning, and accountability.", "AI": {"tldr": "CAIRO\u6846\u67b6\u901a\u8fc7\u672c\u4f53\u548c\u4eba\u7c7b\u8f85\u52a9\u65b9\u6cd5\u68c0\u6d4bAI\u89c6\u89c9\u7cfb\u7edf\u6545\u969c\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u7cfb\u7edf\u5bf9\u7a00\u6709\u573a\u666f\u7684\u8106\u5f31\u6027\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5f15\u5165CAIRO\u6846\u67b6\uff0c\u4f7f\u7528\u672c\u4f53\u548c\u4eba\u7c7b-in-the-loop\u68c0\u6d4b\u53ca\u6b63\u5f0f\u5316\u6545\u969c\u6848\u4f8b\u4e3a\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5206\u6790\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6545\u969c\uff0c\u751f\u6210\u53ef\u5171\u4eab\u3001\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "CAIRO\u63d0\u5347AI\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u5b89\u5168\u6027\u548c\u95ee\u8d23\u6027\u3002"}}
{"id": "2504.16581", "pdf": "https://arxiv.org/pdf/2504.16581", "abs": "https://arxiv.org/abs/2504.16581", "authors": ["Vijeth Hebbar", "C\u00e9dric Langbort"], "title": "Revisiting Regret Benchmarks in Online Non-Stochastic Control", "categories": ["math.OC", "cs.SY", "eess.SY"], "comment": null, "summary": "In the online non-stochastic control problem, an agent sequentially selects\ncontrol inputs for a linear dynamical system when facing unknown and\nadversarially selected convex costs and disturbances. A common metric for\nevaluating control policies in this setting is policy regret, defined relative\nto the best-in-hindsight linear feedback controller. However, for general\nconvex costs, this benchmark may be less meaningful since linear controllers\ncan be highly suboptimal. To address this, we introduce an alternative, more\nsuitable benchmark--the performance of the best fixed input. We show that this\nbenchmark can be viewed as a natural extension of the standard benchmark used\nin online convex optimization and propose a novel online control algorithm that\nachieves sublinear regret with respect to this new benchmark. We also discuss\nthe connections between our method and the original one proposed by Agarwal et\nal. in their seminal work introducing the online non-stochastic control\nproblem, and compare the performance of both approaches through numerical\nsimulations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9488\u5bf9\u5728\u7ebf\u975e\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u5f15\u5165\u6700\u4f73\u56fa\u5b9a\u8f93\u5165\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u7b97\u6cd5\u5b9e\u73b0\u5b50\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u539f\u57fa\u51c6\uff08\u6700\u4f73\u4e8b\u540e\u7ebf\u6027\u53cd\u9988\u63a7\u5236\u5668\uff09\u5bf9\u4e00\u822c\u51f8\u6210\u672c\u53ef\u80fd\u4e0d\u5408\u9002\uff0c\u56e0\u4e3a\u7ebf\u6027\u63a7\u5236\u5668\u53ef\u80fd\u9ad8\u5ea6\u6b21\u4f18\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u5728\u7ebf\u63a7\u5236\u7b97\u6cd5\uff0c\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u6700\u4f73\u56fa\u5b9a\u8f93\u5165\u7684\u5b50\u7ebf\u6027\u9057\u61be\u3002", "result": "\u7b97\u6cd5\u5728\u65b0\u57fa\u51c6\u4e0b\u5b9e\u73b0\u4e86\u5b50\u7ebf\u6027\u9057\u61be\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4e0eAgarwal et al.\u5de5\u4f5c\u7684\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u6bd4\u8f83\u6027\u80fd\u3002"}}
{"id": "2504.16318", "pdf": "https://arxiv.org/pdf/2504.16318", "abs": "https://arxiv.org/abs/2504.16318", "authors": ["Kisung You"], "title": "Semantics at an Angle: When Cosine Similarity Works Until It Doesn't", "categories": ["cs.LG"], "comment": null, "summary": "Cosine similarity has become a standard metric for comparing embeddings in\nmodern machine learning. Its scale-invariance and alignment with model training\nobjectives have contributed to its widespread adoption. However, recent studies\nhave revealed important limitations, particularly when embedding norms carry\nmeaningful semantic information. This informal article offers a reflective and\nselective examination of the evolution, strengths, and limitations of cosine\nsimilarity. We highlight why it performs well in many settings, where it tends\nto break down, and how emerging alternatives are beginning to address its blind\nspots. We hope to offer a mix of conceptual clarity and practical perspective,\nespecially for quantitative scientists who think about embeddings not just as\nvectors, but as geometric and philosophical objects.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u53cd\u601d\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u6f14\u53d8\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u65b0\u5174\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u5d4c\u5165\u8303\u6570\u643a\u5e26\u8bed\u4e49\u4fe1\u606f\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5b9a\u91cf\u79d1\u5b66\u5bb6\u63d0\u4f9b\u6982\u5ff5\u548c\u5b9e\u7528\u89c6\u89d2\u3002", "method": "\u65b9\u6cd5\u662f\u975e\u6b63\u5f0f\u7684\u9009\u62e9\u6027\u8003\u5bdf\uff0c\u5305\u62ec\u56de\u987e\u6f14\u53d8\u3001\u5206\u6790\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u63a2\u7d22\u65b0\u5174\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u7ed3\u679c\u7a81\u51fa\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u8bb8\u591a\u573a\u666f\u4e2d\u7684\u826f\u597d\u6027\u80fd\u3001\u5176\u5931\u6548\u70b9\uff0c\u4ee5\u53ca\u65b0\u5174\u65b9\u6cd5\u5982\u4f55\u5f00\u59cb\u89e3\u51b3\u5176\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5e0c\u671b\u901a\u8fc7\u6df7\u5408\u6982\u5ff5\u6e05\u6670\u548c\u5b9e\u7528\u89c6\u89d2\uff0c\u5e2e\u52a9\u79d1\u5b66\u5bb6\u66f4\u597d\u5730\u7406\u89e3\u5d4c\u5165\u3002"}}
{"id": "2504.16118", "pdf": "https://arxiv.org/pdf/2504.16118", "abs": "https://arxiv.org/abs/2504.16118", "authors": ["Milad Rahmati"], "title": "Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As cyber threats continue to evolve, securing edge networks has become\nincreasingly challenging due to their distributed nature and resource\nlimitations. Many AI-driven threat detection systems rely on complex deep\nlearning models, which, despite their high accuracy, suffer from two major\ndrawbacks: lack of interpretability and high computational cost. Black-box AI\nmodels make it difficult for security analysts to understand the reasoning\nbehind their predictions, limiting their practical deployment. Moreover,\nconventional deep learning techniques demand significant computational\nresources, rendering them unsuitable for edge devices with limited processing\npower. To address these issues, this study introduces an Explainable and\nLightweight AI (ELAI) framework designed for real-time cyber threat detection\nin edge networks. Our approach integrates interpretable machine learning\nalgorithms with optimized lightweight deep learning techniques, ensuring both\ntransparency and computational efficiency. The proposed system leverages\ndecision trees, attention-based deep learning, and federated learning to\nenhance detection accuracy while maintaining explainability. We evaluate ELAI\nusing benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing\nits performance across diverse cyberattack scenarios. Experimental results\ndemonstrate that the proposed framework achieves high detection rates with\nminimal false positives, all while significantly reducing computational demands\ncompared to traditional deep learning methods. The key contributions of this\nwork include: (1) a novel interpretable AI-based cybersecurity model tailored\nfor edge computing environments, (2) an optimized lightweight deep learning\napproach for real-time cyber threat detection, and (3) a comprehensive analysis\nof explainability techniques in AI-driven cybersecurity applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u8f7b\u91cf\u7ea7\u7684AI\u6846\u67b6\uff08ELAI\uff09\uff0c\u7528\u4e8e\u8fb9\u7f18\u7f51\u7edc\u7684\u5b9e\u65f6\u7f51\u7edc\u5a01\u80c1\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAI\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u5a01\u80c1\u4e0d\u65ad\u6f14\u53d8\uff0c\u8fb9\u7f18\u7f51\u7edc\u56e0\u5206\u5e03\u5f0f\u7279\u6027\u548c\u8d44\u6e90\u9650\u5236\u800c\u96be\u4ee5\u5b89\u5168\uff1bAI\u9a71\u52a8\u7684\u5a01\u80c1\u68c0\u6d4b\u7cfb\u7edf\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5f15\u5165ELAI\u6846\u67b6\uff0c\u6574\u5408\u51b3\u7b56\u6811\u7b49\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u4e14\u9ad8\u6548\u7684\u5b9e\u65f6\u5a01\u80c1\u68c0\u6d4b\u3002", "result": "\u5728CICIDS\u548cUNSW-NB15\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7387\u3001\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a\u9488\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u7684\u521b\u65b0\u53ef\u89e3\u91caAI\u6a21\u578b\u3001\u4f18\u5316\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5bf9AI\u5b89\u5168\u5e94\u7528\u4e2d\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7684\u5168\u9762\u5206\u6790\u3002"}}
{"id": "2504.16916", "pdf": "https://arxiv.org/pdf/2504.16916", "abs": "https://arxiv.org/abs/2504.16916", "authors": ["Hsin-Jung Yang", "Mahsa Khosravi", "Benjamin Walt", "Girish Krishnan", "Soumik Sarkar"], "title": "Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The 7th Annual Learning for Dynamics & Control Conference (L4DC) 2025", "summary": "Soft continuum arms (SCAs) soft and deformable nature presents challenges in\nmodeling and control due to their infinite degrees of freedom and non-linear\nbehavior. This work introduces a reinforcement learning (RL)-based framework\nfor visual servoing tasks on SCAs with zero-shot sim-to-real transfer\ncapabilities, demonstrated on a single section pneumatic manipulator capable of\nbending and twisting. The framework decouples kinematics from mechanical\nproperties using an RL kinematic controller for motion planning and a local\ncontroller for actuation refinement, leveraging minimal sensing with visual\nfeedback. Trained entirely in simulation, the RL controller achieved a 99.8%\nsuccess rate. When deployed on hardware, it achieved a 67% success rate in\nzero-shot sim-to-real transfer, demonstrating robustness and adaptability. This\napproach offers a scalable solution for SCAs in 3D visual servoing, with\npotential for further refinement and expanded applications.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u8f6f\u8fde\u7eed\u81c2\u7684\u89c6\u89c9\u4f3a\u670d\uff0c\u652f\u6301\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u8f6c\u79fb\u3002", "motivation": "\u89e3\u51b3\u8f6f\u8fde\u7eed\u81c2\u7684\u65e0\u7a77\u81ea\u7531\u5ea6\u548c\u975e\u7ebf\u6027\u884c\u4e3a\u5728\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165RL-based\u6846\u67b6\uff0c\u89e3\u8026\u8fd0\u52a8\u5b66\u548c\u673a\u68b0\u5c5e\u6027\uff0c\u4f7f\u7528RL\u63a7\u5236\u5668\u548c\u672c\u5730\u63a7\u5236\u5668\uff0c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u53cd\u9988\uff0c\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u3002", "result": "\u6a21\u62df\u6210\u529f\u738799.8%\uff0c\u786c\u4ef6\u96f6\u6837\u672c\u8f6c\u79fb\u6210\u529f\u738767%\u3002", "conclusion": "\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u6269\u5c55\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.16360", "pdf": "https://arxiv.org/pdf/2504.16360", "abs": "https://arxiv.org/abs/2504.16360", "authors": ["Mao Wang", "Tao Wu", "Xingping Xian", "Shaojie Qiao", "Weina Niu", "Canyixing Cui"], "title": "Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graphs effectively characterize relational data, driving graph representation\nlearning methods that uncover underlying predictive information. As\nstate-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end\nlearning for diverse tasks. Recent disentangled graph representation learning\nenhances interpretability by decoupling independent factors in graph data.\nHowever, existing methods often implicitly and coarsely characterize graph\nstructures, limiting structural pattern analysis within the graph. This paper\nproposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to\naddress this limitation. We view graphs as node-centric subgraphs, where each\nsubgraph acts as a structural factor encoding position-specific information.\nThis transforms graph prediction into structural pattern recognition. Inspired\nby CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a\nconvolutional operator, computing similarities between subgraphs and learnable\ngraph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert\nspace, representing graphs as point sets. Disentangled representations emerge\nfrom projecting subgraphs onto task-optimized filters, which adaptively capture\nrelevant structural patterns via gradient descent. Crucially, GOMK incorporates\nlocal correspondences in similarity measurement, resolving the trade-off\nbetween differentiability and accuracy in graph kernels. Experiments validate\nthat GOMKCN achieves superior accuracy and interpretability in graph pattern\nmining and prediction. The framework advances the theoretical foundation for\ndisentangled graph representation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGOMKCN\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u56fe\u7ed3\u6784\u8868\u793a\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9690\u5f0f\u7c97\u7cd9\u8868\u5f81\u56fe\u7ed3\u6784\uff0c\u9650\u5236\u6a21\u5f0f\u5206\u6790\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u89e3\u8026\u5408\u5b66\u4e60\u3002", "method": "GOMKCN\u5c06\u56fe\u89c6\u4e3a\u8282\u70b9\u4e2d\u5fc3\u5b50\u56fe\uff0c\u4f7f\u7528GOMK\u5377\u79ef\u64cd\u4f5c\u7b26\u5728Hilbert\u7a7a\u95f4\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u8fc7\u6ee4\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGOMKCN\u5728\u56fe\u6a21\u5f0f\u6316\u6398\u548c\u9884\u6d4b\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6846\u67b6\u63a8\u8fdb\u89e3\u8026\u5408\u56fe\u8868\u793a\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.16120", "pdf": "https://arxiv.org/pdf/2504.16120", "abs": "https://arxiv.org/abs/2504.16120", "authors": ["Chaima Njeh", "Ha\u00effa Nakouri", "Fehmi Jaafar"], "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content", "categories": ["cs.CR", "cs.AI"], "comment": "This paper is under revision in the International Journal of\n  Information Security", "summary": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165BART-Corrective Model\uff0c\u901a\u8fc7\u751f\u6210\u540e\u4fee\u6b63\u673a\u5236\u51cf\u5c11LLM\u4e2d\u7684\u504f\u5dee\u548c\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u663e\u793a\u51fa\u663e\u8457\u5b89\u5168\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3LLM\u6f5c\u5728\u504f\u5dee\u548c\u6709\u5bb3\u5185\u5bb9\u7684\u62c5\u5fe7\uff0c\u63d0\u4f9b\u4e00\u79cd\u5b9e\u7528\u6570\u636e\u4e2d\u5fc3\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u6a21\u578b\u5fae\u8c03\u6216\u63d0\u793a\u5de5\u7a0b\u3002", "method": "\u4f7f\u7528BART-Corrective Model\u7684\u751f\u6210\u540e\u4fee\u6b63\u673a\u5236\uff0c\u8fd9\u662f\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u6709\u6bd2\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff1aGPT-4\u6bd2\u6027\u548c\u8d8a\u72f1\u5206\u6570\u51cf\u5c1115%\u548c21%\uff0cPaLM2\u51cf\u5c1128%\u548c5%\uff0cMistral-7B\u51cf\u5c11\u7ea626%\u548c23%\uff0cGemma-2b-it\u51cf\u5c1111.1%\u548c19%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347LLM\u7684\u5b89\u5168\u6027\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002"}}
{"id": "2504.16923", "pdf": "https://arxiv.org/pdf/2504.16923", "abs": "https://arxiv.org/abs/2504.16923", "authors": ["Jacob Levy", "Jason Gibson", "Bogdan Vlahov", "Erica Tevere", "Evangelos Theodorou", "David Fridovich-Keil", "Patrick Spieler"], "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "High-speed off-road autonomous driving presents unique challenges due to\ncomplex, evolving terrain characteristics and the difficulty of accurately\nmodeling terrain-vehicle interactions. While dynamics models used in\nmodel-based control can be learned from real-world data, they often struggle to\ngeneralize to unseen terrain, making real-time adaptation essential. We propose\na novel framework that combines a Kalman filter-based online adaptation scheme\nwith meta-learned parameters to address these challenges. Offline meta-learning\noptimizes the basis functions along which adaptation occurs, as well as the\nadaptation parameters, while online adaptation dynamically adjusts the onboard\ndynamics model in real time for model-based control. We validate our approach\nthrough extensive experiments, including real-world testing on a full-scale\nautonomous off-road vehicle, demonstrating that our method outperforms baseline\napproaches in prediction accuracy, performance, and safety metrics,\nparticularly in safety-critical scenarios. Our results underscore the\neffectiveness of meta-learned dynamics model adaptation, advancing the\ndevelopment of reliable autonomous systems capable of navigating diverse and\nunseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA", "AI": {"tldr": "A framework combining Kalman filter-based adaptation and meta-learning improves dynamics models for high-speed off-road autonomous driving, enhancing accuracy and safety.", "motivation": "Challenges in high-speed off-road driving, such as complex terrain and poor model generalization, necessitate real-time adaptation of dynamics models.", "method": "Uses offline meta-learning to optimize basis functions and parameters, combined with online Kalman filter adaptation for real-time dynamics model adjustment in model-based control.", "result": "Experiments, including real-world tests on a full-scale vehicle, show superior prediction accuracy, performance, and safety compared to baselines.", "conclusion": "Meta-learned adaptation effectively advances reliable autonomous systems for diverse and unseen environments."}}
{"id": "2504.16415", "pdf": "https://arxiv.org/pdf/2504.16415", "abs": "https://arxiv.org/abs/2504.16415", "authors": ["Neharika Jali", "Eshika Pathak", "Pranay Sharma", "Guannan Qu", "Gauri Joshi"], "title": "Natural Policy Gradient for Average Reward Non-Stationary RL", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider the problem of non-stationary reinforcement learning (RL) in the\ninfinite-horizon average-reward setting. We model it by a Markov Decision\nProcess with time-varying rewards and transition probabilities, with a\nvariation budget of $\\Delta_T$. Existing non-stationary RL algorithms focus on\nmodel-based and model-free value-based methods. Policy-based methods despite\ntheir flexibility in practice are not theoretically well understood in\nnon-stationary RL. We propose and analyze the first model-free policy-based\nalgorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient\nmethod with a restart based exploration for change and a novel interpretation\nof learning rates as adapting factors. Further, we present a bandit-over-RL\nbased parameter-free algorithm BORL-NS-NAC that does not require prior\nknowledge of the variation budget $\\Delta_T$. We present a dynamic regret of\n$\\tilde{\\mathscr O}(|S|^{1/2}|A|^{1/2}\\Delta_T^{1/6}T^{5/6})$ for both\nalgorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of\nthe state and action spaces. The regret analysis leverages a novel adaptation\nof the Lyapunov function analysis of NAC to dynamic environments and\ncharacterizes the effects of simultaneous updates in policy, value function\nestimate and changes in the environment.", "AI": {"tldr": "This paper proposes the first model-free policy-based algorithm for non-stationary reinforcement learning and provides a dynamic regret bound.", "motivation": "To address the lack of theoretical understanding of policy-based methods in non-stationary RL.", "method": "Proposes NS-NAC, a policy gradient method with restart-based exploration, and BORL-NS-NAC, a bandit-over-RL based parameter-free algorithm.", "result": "Achieves a dynamic regret of \\tilde{\\mathscr O}(|S|^{1/2}|A|^{1/2}\\Delta_T^{1/6}T^{5/6}).", "conclusion": "The regret analysis adapts Lyapunov function methods to handle dynamic environments and simultaneous updates."}}
{"id": "2504.16122", "pdf": "https://arxiv.org/pdf/2504.16122", "abs": "https://arxiv.org/abs/2504.16122", "authors": ["Xuhui Zhou", "Zhe Su", "Sophie Feng", "Jiaxu Zhou", "Jen-tse Huang", "Hsien-Te Kao", "Spencer Lynch", "Svitlana Volkova", "Tongshuang Sherry Wu", "Anita Woolley", "Hao Zhu", "Maarten Sap"], "title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation", "categories": ["cs.CY", "cs.AI"], "comment": "The first author and the second author contributed equally", "summary": "Social simulation through large language model (LLM) agents is a promising\napproach to explore and validate hypotheses related to social science questions\nand LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable\nsocial simulation system that addresses the technical barriers of current\nframeworks while enabling practitioners to generate multi-turn and multi-party\nLLM-based interactions with customizable evaluation metrics for hypothesis\ntesting. SOTOPIA-S4 comes as a pip package that contains a simulation engine,\nan API server with flexible RESTful APIs for simulation management, and a web\ninterface that enables both technical and non-technical users to design, run,\nand analyze simulations without programming. We demonstrate the usefulness of\nSOTOPIA-S4 with two use cases involving dyadic hiring negotiation and\nmulti-party planning scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SOTOPIA-S4\uff0c\u4e00\u4e2a\u5feb\u901f\u3001\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u793e\u4f1a\u6a21\u62df\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u4ee3\u7406\u8fdb\u884c\u591a\u8f6e\u591a\u65b9\u4ea4\u4e92\u548c\u5047\u8bbe\u6d4b\u8bd5\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u5f53\u524d\u6846\u67b6\u7684\u6280\u672f\u969c\u788d\uff0c\u5e76\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u81ea\u5b9a\u4e49\u8bc4\u4f30\u6307\u6807\u6765\u6d4b\u8bd5\u793e\u4f1a\u79d1\u5b66\u95ee\u9898\u548cLLM\u884c\u4e3a\u5047\u8bbe\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2apip\u5305\uff0c\u5305\u62ec\u6a21\u62df\u5f15\u64ce\u3001API\u670d\u52a1\u5668\u548c\u7f51\u7edc\u754c\u9762\uff0c\u5141\u8bb8\u7528\u6237\u65e0\u9700\u7f16\u7a0b\u8bbe\u8ba1\u3001\u8fd0\u884c\u548c\u5206\u6790\u6a21\u62df\u3002", "result": "\u7ed3\u679c\u662f\u901a\u8fc7\u4e8c\u5143\u62db\u8058\u8c08\u5224\u548c\u591a\u65b9\u89c4\u5212\u573a\u666f\u7684\u7528\u4f8b\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662fSOTOPIA-S4\u6709\u6548\u63d0\u5347\u4e86\u793e\u4f1a\u6a21\u62df\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u7387\uff0c\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2504.16430", "pdf": "https://arxiv.org/pdf/2504.16430", "abs": "https://arxiv.org/abs/2504.16430", "authors": ["Andrew Ilyas", "Logan Engstrom"], "title": "MAGIC: Near-Optimal Data Attribution for Deep Learning", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "comment": null, "summary": "The goal of predictive data attribution is to estimate how adding or removing\na given set of training datapoints will affect model predictions. In convex\nsettings, this goal is straightforward (i.e., via the infinitesimal jackknife).\nIn large-scale (non-convex) settings, however, existing methods are far less\nsuccessful -- current methods' estimates often only weakly correlate with\nground truth. In this work, we present a new data attribution method (MAGIC)\nthat combines classical methods and recent advances in metadifferentiation to\n(nearly) optimally estimate the effect of adding or removing training data on\nmodel predictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5MAGIC\uff0c\u7528\u4e8e\u5728\u975e\u51f8\u673a\u5668\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u66f4\u597d\u5730\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u51f8\u8bbe\u7f6e\u4e2d\u6548\u679c\u8f83\u5dee\uff0c\u800c\u51f8\u8bbe\u7f6e\u4e2d\u5bb9\u6613\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6570\u636e\u5f52\u56e0\u4f30\u8ba1\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u65b9\u6cd5\u548c\u5143\u5fae\u5206\u6765\u4f18\u5316\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u7684\u6dfb\u52a0\u6216\u79fb\u9664\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "result": "MAGIC\u65b9\u6cd5\u80fd\u8fd1\u4f3c\u6700\u4f18\u5730\u4f30\u8ba1\u6570\u636e\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u8fdb\u4e86\u975e\u51f8\u8bbe\u7f6e\u4e0b\u7684\u9884\u6d4b\u6570\u636e\u5f52\u56e0\u6027\u80fd\u3002"}}
{"id": "2504.16128", "pdf": "https://arxiv.org/pdf/2504.16128", "abs": "https://arxiv.org/abs/2504.16128", "authors": ["Stanley Mugisha", "Rashid Kisitu", "Florence Tushabe"], "title": "Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "12 pages and 4 figures", "summary": "Integrating deep learning applications into agricultural IoT systems faces a\nserious challenge of balancing the high accuracy of Vision Transformers (ViTs)\nwith the efficiency demands of resource-constrained edge devices. Large\ntransformer models like the Swin Transformers excel in plant disease\nclassification by capturing global-local dependencies. However, their\ncomputational complexity (34.1 GFLOPs) limits applications and renders them\nimpractical for real-time on-device inference. Lightweight models such as\nMobileNetV3 and TinyML would be suitable for on-device inference but lack the\nrequired spatial reasoning for fine-grained disease detection. To bridge this\ngap, we propose a hybrid knowledge distillation framework that synergistically\ntransfers logit and attention knowledge from a Swin Transformer teacher to a\nMobileNetV3 student model. Our method includes the introduction of adaptive\nattention alignment to resolve cross-architecture mismatch (resolution,\nchannels) and a dual-loss function optimizing both class probabilities and\nspatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled\nMobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%\nreduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU\nand 86ms/image on smartphone CPUs). Key innovations include IoT-centric\nvalidation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching\nattention maps. Comparative experiments show significant improvements over\nstandalone CNNs and prior distillation methods, with a 3.5% accuracy gain over\nMobileNetV3 baselines. Significantly, this work advances real-time,\nenergy-efficient crop monitoring in precision agriculture and demonstrates how\nwe can attain ViT-level diagnostic precision on edge devices. Code and models\nwill be made available for replication after acceptance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06Swin Transformer\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230MobileNetV3\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u690d\u7269\u75c5\u5bb3\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u4e1aIoT\u4e2d\u7684\u6311\u6218\uff1a\u5927\u578b\u6a21\u578b\u5982Swin Transformer\u51c6\u786e\u7387\u9ad8\u4f46\u8ba1\u7b97\u590d\u6742\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\uff1b\u8f7b\u91cf\u7ea7\u6a21\u578b\u5982MobileNetV3\u7f3a\u4e4f\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5bf9\u9f50\u548c\u53cc\u635f\u5931\u51fd\u6570\uff0c\u4eceSwin Transformer\u6559\u5e08\u6a21\u578b\u5411MobileNetV3\u5b66\u751f\u6a21\u578b\u8f6c\u79fblogit\u548c\u6ce8\u610f\u529b\u77e5\u8bc6\u3002", "result": "\u5728lantVillage-Tomato\u6570\u636e\u96c6\u4e0a\uff0c\u84b8\u998f\u540eMobileNetV3\u51c6\u786e\u7387\u8fbe92.4%\uff0c\u6bd4Swin-L\u768495.9%\u7565\u4f4e\uff0c\u4f46\u8ba1\u7b97\u91cf\u51cf\u5c1195%\uff0c\u5ef6\u8fdf\u964d\u4f4e\u81f3PC CPU 23ms\u548c\u667a\u80fd\u624b\u673aCPU 86ms/\u56fe\u50cf\u3002", "conclusion": "\u6b64\u5de5\u4f5c\u63a8\u8fdb\u5b9e\u65f6\u3001\u8282\u80fd\u7684\u4f5c\u7269\u76d1\u6d4b\uff0c\u5c55\u793a\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0Vision Transformer\u7ea7\u522b\u8bca\u65ad\u7cbe\u5ea6\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.16431", "pdf": "https://arxiv.org/pdf/2504.16431", "abs": "https://arxiv.org/abs/2504.16431", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Yizhe Zhang", "James Thornton", "Zijing Ou", "Joshua Susskind", "Navdeep Jaitly"], "title": "Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion is a promising framework for modeling and generating\ndiscrete data. In this work, we present Target Concrete Score Matching (TCSM),\na novel and versatile objective for training and fine-tuning discrete diffusion\nmodels. TCSM provides a general framework with broad applicability. It supports\npre-training discrete diffusion models directly from data samples, and many\nexisting discrete diffusion approaches naturally emerge as special cases of our\nmore general TCSM framework. Furthermore, the same TCSM objective extends to\npost-training of discrete diffusion models, including fine-tuning using reward\nfunctions or preference data, and distillation of knowledge from pre-trained\nautoregressive models. These new capabilities stem from the core idea of TCSM,\nestimating the concrete score of the target distribution, which resides in the\noriginal (clean) data space. This allows seamless integration with reward\nfunctions and pre-trained models, which inherently only operate in the clean\ndata space rather than the noisy intermediate spaces of diffusion processes.\nOur experiments on language modeling tasks demonstrate that TCSM matches or\nsurpasses current methods. Additionally, TCSM is versatile, applicable to both\npre-training and post-training scenarios, offering greater flexibility and\nsample efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTCSM\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u79bb\u6563\u6570\u636e\u4e0a\u6709\u524d\u666f\uff0c\u4f46\u65b9\u6cd5\u6709\u9650\uff1bTCSM\u7684\u52a8\u673a\u662f\u63d0\u4f9b\u901a\u7528\u6846\u67b6\uff0c\u652f\u6301\u76f4\u63a5\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u3002", "method": "TCSM\u901a\u8fc7\u4f30\u8ba1\u76ee\u6807\u5206\u5e03\u7684concrete score\u8bad\u7ec3\u6a21\u578b\uff0c\u5141\u8bb8\u4e0e\u5956\u52b1\u51fd\u6570\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\uff0cTCSM\u5339\u914d\u6216\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u597d\u7075\u6d3b\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "TCSM\u662f\u591a\u529f\u80fd\u7684\u6846\u67b6\uff0c\u63d0\u5347\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.16129", "pdf": "https://arxiv.org/pdf/2504.16129", "abs": "https://arxiv.org/abs/2504.16129", "authors": ["Junwei Liao", "Muning Wen", "Jun Wang", "Weinan Zhang"], "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "comment": "36 pages", "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM-based Multi-Agent Systems\u7684Multi-Agent Reinforcement Fine-Tuning\uff08MARFT\uff09\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u4ee3\u7406\u667a\u80fd\u65b9\u9762\u6709\u6548\uff0c\u4f46\u9488\u5bf9LLM-based Multi-Agent Systems\u7684\u5fae\u8c03\u7814\u7a76\u6709\u9650\uff0c\u4e14\u76f4\u63a5\u5e94\u7528\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u9610\u8ff0\u4e86MARFT\u7684\u6982\u5ff5\u57fa\u7840\u3001\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u533a\u522b\u3001\u6838\u5fc3\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684MARFT\u6846\u67b6\uff0c\u5e76\u516c\u5f00\u4e86\u5f00\u6e90\u4ee3\u7801\uff0c\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u524d\u666f\u548c\u6f5c\u5728\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u63a8\u8fdbMARFT\u5411\u66f4\u575a\u97e7\u548c\u9002\u5e94\u6027\u4ee3\u7406\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2504.16432", "pdf": "https://arxiv.org/pdf/2504.16432", "abs": "https://arxiv.org/abs/2504.16432", "authors": ["Ziran Liang", "Rui An", "Wenqi Fan", "Yanghui Rao", "Yuxuan Liang"], "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities.", "AI": {"tldr": "This paper proposes iTFKAN, an interpretable time series forecasting model that balances high performance and interpretability.", "motivation": "Time series data is predictable, but current deep learning methods lack interpretability, which is essential for safety-critical applications like autonomous driving and healthcare.", "method": "iTFKAN uses model symbolization for interpretability and employs prior knowledge injection and time-frequency synergy learning to handle complex time series data.", "result": "Experimental results demonstrate that iTFKAN achieves strong forecasting performance with high interpretability.", "conclusion": "iTFKAN is an effective and trustworthy model for time series forecasting in critical domains."}}
{"id": "2504.16130", "pdf": "https://arxiv.org/pdf/2504.16130", "abs": "https://arxiv.org/abs/2504.16130", "authors": ["Pengju Ren", "Ri-gui Zhou", "Yaochong Li"], "title": "A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures", "summary": "Raman spectroscopy serves as a powerful and reliable tool for analyzing the\nchemical information of substances. The integration of Raman spectroscopy with\ndeep learning methods enables rapid qualitative and quantitative analysis of\nmaterials. Most existing approaches adopt supervised learning methods. Although\nsupervised learning has achieved satisfactory accuracy in spectral analysis, it\nis still constrained by costly and limited well-annotated spectral datasets for\ntraining. When spectral annotation is challenging or the amount of annotated\ndata is insufficient, the performance of supervised learning in spectral\nmaterial identification declines. In order to address the challenge of feature\nextraction from unannotated spectra, we propose a self-supervised learning\nparadigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.\nSMAE does not require any spectral annotations during pre-training. By randomly\nmasking and then reconstructing the spectral information, the model learns\nessential spectral features. The reconstructed spectra exhibit certain\ndenoising properties, improving the signal-to-noise ratio (SNR) by more than\ntwofold. Utilizing the network weights obtained from masked pre-training, SMAE\nachieves clustering accuracy of over 80% for 30 classes of isolated bacteria in\na pathogenic bacterial dataset, demonstrating significant improvements compared\nto classical unsupervised methods and other state-of-the-art deep clustering\nmethods. After fine-tuning the network with a limited amount of annotated data,\nSMAE achieves an identification accuracy of 83.90% on the test set, presenting\ncompetitive performance against the supervised ResNet (83.40%).", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSMAE\uff08\u57fa\u4e8eMasked AutoEncoder\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u7528\u4e8eRaman\u5149\u8c31\u5206\u6790\uff0c\u63d0\u9ad8\u65e0\u6807\u6ce8\u6570\u636e\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u6570\u636e\u6709\u9650\uff1b\u6807\u6ce8\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u76d1\u7763\u65b9\u6cd5\u89e3\u51b3\u7279\u5f81\u63d0\u53d6\u6311\u6218\u3002", "method": "\u63d0\u51faSMAE\uff0c\u901a\u8fc7\u968f\u673amasking\u5149\u8c31\u4fe1\u606f\u5e76\u91cd\u5efa\uff0c\u5b66\u4e60\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u53bb\u566a\u6548\u679c\u3002", "result": "\u91cd\u5efa\u5149\u8c31SNR\u63d0\u9ad8\u4e24\u500d\u4ee5\u4e0a\uff1b\u65e0\u76d1\u7763\u805a\u7c7b\u51c6\u786e\u7387\u8d85\u8fc780%\uff1b\u5fae\u8c03\u540e\u8bc6\u522b\u51c6\u786e\u738783.90%\uff0c\u4e0e\u76d1\u7763ResNet\u7ade\u4e89\u3002", "conclusion": "SMAE\u5728Raman\u5149\u8c31\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u6570\u636e\u6807\u6ce8\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2504.16438", "pdf": "https://arxiv.org/pdf/2504.16438", "abs": "https://arxiv.org/abs/2504.16438", "authors": ["Charlie Hou", "Mei-Yu Wang", "Yige Zhu", "Daniel Lazar", "Giulia Fanti"], "title": "Private Federated Learning using Preference-Optimized Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "Spotlight presentation at SynthData Workshop ICLR25", "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as a preference ranking. Our algorithm, Preference Optimization for\nPrivate Client Data (POPri) harnesses client feedback using preference\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPOPri\u7b97\u6cd5\uff0c\u4f7f\u7528\u504f\u597d\u4f18\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u57fa\u51c6LargeFedBench\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u5584\u3002", "motivation": "\u6700\u8fd1\u7814\u7a76\u663e\u793a\u5dee\u5206\u9690\u79c1\u8054\u90a6\u5b66\u4e60\u53ef\u80fd\u88abDP\u5408\u6210\u6570\u636e\u65b9\u6cd5\u8d85\u8d8a\uff0c\u672c\u6587\u6d1e\u89c1\u5c06\u5ba2\u6237\u7aef\u53cd\u9988\u89c6\u4e3a\u504f\u597d\u6392\u540d\u3002", "method": "\u63d0\u51faPOPri\u7b97\u6cd5\uff0c\u901a\u8fc7Direct Preference Optimization (DPO) \u7b49\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u5fae\u8c03LLMs\uff0c\u751f\u6210DP\u5408\u6210\u6570\u636e\u3002", "result": "POPri\u5c06\u5168\u9690\u79c1\u4e0e\u975e\u9690\u79c1\u8bbe\u7f6e\u7684next-token\u9884\u6d4b\u51c6\u786e\u7387\u5dee\u8ddd\u7f29\u5c0f\u591a\u8fbe68%\uff0c\u4f18\u4e8e\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u768452%\u548cDP\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u768410%\u3002", "conclusion": "POPri\u663e\u8457\u63d0\u5347DP\u5408\u6210\u6570\u636e\u6548\u7528\uff0c\u5e76\u53d1\u5e03LargeFedBench\u57fa\u51c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.16131", "pdf": "https://arxiv.org/pdf/2504.16131", "abs": "https://arxiv.org/abs/2504.16131", "authors": ["Samuel Yen-Chi Chen", "Zhiding Liang"], "title": "Introduction to Quantum Machine Learning and Quantum Architecture Search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG", "cs.NE"], "comment": "ISCAS 2025 Tutorial", "summary": "Recent advancements in quantum computing (QC) and machine learning (ML) have\nfueled significant research efforts aimed at integrating these two\ntransformative technologies. Quantum machine learning (QML), an emerging\ninterdisciplinary field, leverages quantum principles to enhance the\nperformance of ML algorithms. Concurrently, the exploration of systematic and\nautomated approaches for designing high-performance quantum circuit\narchitectures for QML tasks has gained prominence, as these methods empower\nresearchers outside the quantum computing domain to effectively utilize\nquantum-enhanced tools. This tutorial will provide an in-depth overview of\nrecent breakthroughs in both areas, highlighting their potential to expand the\napplication landscape of QML across diverse fields.", "AI": {"tldr": "\u672c\u6559\u7a0b\u6982\u8ff0\u4e86\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ee5\u53ca\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u7a81\u7834\u53ca\u5176\u5728\u591a\u6837\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u6574\u5408\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u4f7f\u975e\u91cf\u5b50\u4e13\u5bb6\u80fd\u591f\u6709\u6548\u5229\u7528\u91cf\u5b50\u5de5\u5177\u3002", "method": "\u63d0\u4f9b\u6df1\u5165\u6559\u7a0b\uff0c\u6db5\u76d6\u7cfb\u7edf\u5316\u548c\u81ea\u52a8\u5316\u7684\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u7a81\u51fa\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6269\u5c55\u5e94\u7528\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u83b7\u5f97\u5e94\u7528\u3002"}}
{"id": "2504.16447", "pdf": "https://arxiv.org/pdf/2504.16447", "abs": "https://arxiv.org/abs/2504.16447", "authors": ["Jeesuk Shin", "Cheolwoong Kim", "Sunwoong Yang", "Minseo Lee", "Sung Joong Kim", "Joongoo Jeon"], "title": "Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module", "categories": ["cs.LG"], "comment": "40 pages, 12 figures. Jeesuk Shin and Cheolwoong Kim contributed\n  equally to this work. Sung Joong Kim and Joongoo Jeon are co-corresponding\n  authors", "summary": "Severe accidents (SAs) in nuclear power plants have been analyzed using\nthermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes\nefficiently simulate the progression of SAs, while they still have inherent\nlimitations due to their inconsistent finite difference schemes. The use of\nempirical schemes incorporating both implicit and explicit formulations\ninherently induces unidirectional coupling in multi-physics analyses. The\nobjective of this study is to develop a novel numerical method for TH system\ncodes using physics-informed neural network (PINN). They have shown strength in\nsolving multi-physics due to the innate feature of neural networks-automatic\ndifferentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for\nthe control volume approach-based system codes. NA-PINN addresses the issue of\nspatial governing equation variation by assigning an individual network to each\nnodalization of the system code, such that spatial information is excluded from\nboth the input and output domains, and each subnetwork learns to approximate a\npurely temporal solution. In this phase, we evaluated the accuracy of the PINN\nmethods for the hydrodynamic module. In the 6 water tank simulation, PINN and\nNA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It\nshould be noted that only NA-PINN demonstrated acceptable accuracy. To the best\nof the authors' knowledge, this is the first study to successfully implement a\nsystem code using PINN. Our future work involves extending NA-PINN to a\nmulti-physics solver and developing it in a surrogate manner.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8282\u70b9\u5206\u914d\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08NA-PINN\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u6838\u7535\u7ad9\u4e25\u91cd\u4e8b\u6545\u7684\u70ed\u6db2\u529b\u5b66\u6a21\u62df\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u70ed\u6db2\u529b\u5b66\u4ee3\u7801\u7684\u5c40\u9650\u6027\uff0c\u5982\u4e0d\u4e00\u81f4\u7684\u6709\u9650\u5dee\u5206\u65b9\u6848\u548c\u591a\u7269\u7406\u5206\u6790\u4e2d\u7684\u5355\u5411\u8026\u5408\uff0c\u63d0\u51fa\u4f7f\u7528PINN\u6765\u63d0\u5347\u6a21\u62df\u80fd\u529b\u3002", "method": "\u63d0\u51faNA-PINN\u65b9\u6cd5\uff0c\u4e3a\u7cfb\u7edf\u4ee3\u7801\u7684\u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u72ec\u7acb\u795e\u7ecf\u7f51\u7edc\uff0c\u6392\u9664\u7a7a\u95f4\u4fe1\u606f\uff0c\u4e13\u6ce8\u4e8e\u65f6\u95f4\u89e3\u7684\u903c\u8fd1\u3002", "result": "\u57286\u6c34\u7bb1\u6a21\u62df\u4e2d\uff0cNA-PINN\u7684\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u4e3a0.007\uff0c\u800cPINN\u4e3a1.678\uff0c\u4ec5NA-PINN\u8fbe\u5230\u53ef\u63a5\u53d7\u51c6\u786e\u6027\uff0c\u8fd9\u662f\u9996\u6b21\u4f7f\u7528PINN\u5b9e\u73b0\u7cfb\u7edf\u4ee3\u7801\u3002", "conclusion": "NA-PINN\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5230\u591a\u7269\u7406\u6c42\u89e3\u5668\u5e76\u5f00\u53d1\u4e3a\u4ee3\u7406\u6a21\u578b\u3002"}}
{"id": "2504.16132", "pdf": "https://arxiv.org/pdf/2504.16132", "abs": "https://arxiv.org/abs/2504.16132", "authors": ["Andrew M. Olney", "Sidney K. D'Mello", "Natalie Person", "Whitney Cade", "Patrick Hays", "Claire W. Dempsey", "Blair Lehman", "Betsy Williams", "Art Graesser"], "title": "Efficacy of a Computer Tutor that Models Expert Human Tutors", "categories": ["cs.CY", "cs.AI", "I.2.4; I.2.7; K.3.1"], "comment": "Shortened version of this paper has been accepted to AIED 2025", "summary": "Tutoring is highly effective for promoting learning. However, the\ncontribution of expertise to tutoring effectiveness is unclear and continues to\nbe debated. We conducted a 9-week learning efficacy study of an intelligent\ntutoring system (ITS) for biology modeled on expert human tutors with two\ncontrol conditions: human tutors who were experts in the domain but not in\ntutoring and a no-tutoring condition. All conditions were supplemental to\nclassroom instruction, and students took learning tests immediately before and\nafter tutoring sessions as well as delayed tests 1-2 weeks later. Analysis\nusing logistic mixed-effects modeling indicates significant positive effects on\nthe immediate post-test for the ITS (d =.71) and human tutors (d =.66) which\nare in the 99th percentile of meta-analytic effects, as well as significant\npositive effects on the delayed post-test for the ITS (d =.36) and human tutors\n(d =.39). We discuss implications for the role of expertise in tutoring and the\ndesign of future studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u8f85\u5bfc\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u5747\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6210\u7ee9\u3002", "motivation": "\u5c3d\u7ba1\u8f85\u5bfc\u5bf9\u4fc3\u8fdb\u5b66\u4e60\u975e\u5e38\u6709\u6548\uff0c\u4f46\u4e13\u5bb6\u77e5\u8bc6\u5bf9\u8f85\u5bfc\u6709\u6548\u6027\u7684\u8d21\u732e\u4ecd\u4e0d\u6e05\u695a\u4e14\u5b58\u5728\u4e89\u8bae\u3002", "method": "\u5f00\u5c55\u4e869\u5468\u7684\u5b66\u4e60\u6548\u80fd\u7814\u7a76\uff0c\u5305\u62ec\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u3001\u9886\u57df\u4e13\u5bb6\u4f46\u975e\u8f85\u5bfc\u4e13\u5bb6\u7684\u4eba\u7c7b\u8f85\u5bfc\u548c\u65e0\u8f85\u5bfc\u6761\u4ef6\uff0c\u8865\u5145\u8bfe\u5802\u6559\u5b66\uff0c\u901a\u8fc7\u524d\u540e\u6d4b\u8bd5\u548c\u5ef6\u8fdf\u6d4b\u8bd5\uff0c\u4f7f\u7528logistic\u6df7\u5408\u6548\u5e94\u6a21\u578b\u5206\u6790\u3002", "result": "\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u548c\u4eba\u7c7b\u8f85\u5bfc\u5728\u5373\u65f6\u540e\u6d4b\u4e0a\u5747\u6709\u663e\u8457\u6b63\u6548\u5e94\uff08ITS d=0.71\uff0c\u4eba tutor d=0.66\uff09\uff0c\u5728\u5ef6\u8fdf\u540e\u6d4b\u4e0a\u4e5f\u6709\u663e\u8457\u6b63\u6548\u5e94\uff08ITS d=0.36\uff0c\u4eba tutor d=0.39\uff09\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4e13\u5bb6\u77e5\u8bc6\u5728\u8f85\u5bfc\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2504.16450", "pdf": "https://arxiv.org/pdf/2504.16450", "abs": "https://arxiv.org/abs/2504.16450", "authors": ["Rubing Yang", "Pratik Chaudhari"], "title": "An Effective Gram Matrix Characterizes Generalization in Deep Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We derive a differential equation that governs the evolution of the\ngeneralization gap when a deep network is trained by gradient descent. This\ndifferential equation is controlled by two quantities, a contraction factor\nthat brings together trajectories corresponding to slightly different datasets,\nand a perturbation factor that accounts for them training on different\ndatasets. We analyze this differential equation to compute an ``effective Gram\nmatrix'' that characterizes the generalization gap after training in terms of\nthe alignment between this Gram matrix and a certain initial ``residual''.\nEmpirical evaluations on image classification datasets indicate that this\nanalysis can predict the test loss accurately. Further, at any point during\ntraining, the residual predominantly lies in the subspace of the effective Gram\nmatrix with the smallest eigenvalues. This indicates that the training process\nis benign, i.e., it does not lead to significant deterioration of the\ngeneralization gap (which is zero at initialization). The alignment between the\neffective Gram matrix and the residual is different for different datasets and\narchitectures. The match/mismatch of the data and the architecture is primarily\nresponsible for good/bad generalization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a8\u5bfc\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u4e2d\u6cdb\u5316\u95f4\u9699\u6f14\u5316\uff0c\u5e76\u901a\u8fc7\u6709\u6548Gram\u77e9\u9635\u5206\u6790\u9884\u6d4b\u6d4b\u8bd5\u635f\u5931\uff0c\u5f3a\u8c03\u6570\u636e\u4e0e\u67b6\u6784\u5339\u914d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6cdb\u5316\u95f4\u9699\u6f14\u5316\u673a\u5236\uff0c\u4ee5\u89e3\u91ca\u4e3a\u4ec0\u4e48\u67d0\u4e9b\u6a21\u578b\u6cdb\u5316\u826f\u597d\u6216\u4e0d\u826f\u3002", "method": "\u63a8\u5bfc\u63a7\u5236\u6cdb\u5316\u95f4\u9699\u6f14\u5316\u7684\u5fae\u5206\u65b9\u7a0b\uff0c\u8ba1\u7b97\u6709\u6548Gram\u77e9\u9635\uff0c\u5206\u6790\u8f68\u8ff9\u548c\u6b8b\u5dee\u5bf9\u9f50\u3002", "result": "\u51c6\u786e\u9884\u6d4b\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u635f\u5931\uff0c\u6b8b\u5dee\u4e3b\u8981\u4f4d\u4e8e\u6700\u5c0f\u7279\u5f81\u503c\u5b50\u7a7a\u95f4\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u826f\u6027\u65e0\u663e\u8457\u6cdb\u5316\u9000\u5316\u3002", "conclusion": "\u6570\u636e\u4e0e\u67b6\u6784\u7684\u5339\u914d/\u4e0d\u5339\u914d\u662f\u826f\u597d/\u4e0d\u826f\u6cdb\u5316\u7684\u4e3b\u8981\u56e0\u7d20\u3002"}}
{"id": "2504.16133", "pdf": "https://arxiv.org/pdf/2504.16133", "abs": "https://arxiv.org/abs/2504.16133", "authors": ["Milad Leyli-abadi", "Ricardo J. Bessa", "Jan Viebahn", "Daniel Boos", "Clark Borst", "Alberto Castagna", "Ricardo Chavarriaga", "Mohamed Hassouna", "Bruno Lemetayer", "Giulia Leto", "Antoine Marot", "Maroua Meddeb", "Manuel Meyer", "Viola Schiaffonati", "Manuel Schneider", "Toni Waefler"], "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The interaction between humans and AI in safety-critical systems presents a\nunique set of challenges that remain partially addressed by existing\nframeworks. These challenges stem from the complex interplay of requirements\nfor transparency, trust, and explainability, coupled with the necessity for\nrobust and safe decision-making. A framework that holistically integrates human\nand AI capabilities while addressing these concerns is notably required,\nbridging the critical gaps in designing, deploying, and maintaining safe and\neffective systems. This paper proposes a holistic conceptual framework for\ncritical infrastructures by adopting an interdisciplinary approach. It\nintegrates traditionally distinct fields such as mathematics, decision theory,\ncomputer science, philosophy, psychology, and cognitive engineering and draws\non specialized engineering domains, particularly energy, mobility, and\naeronautics. The flexibility in its adoption is also demonstrated through its\ninstantiation on an already existing framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u6574\u4f53\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u6574\u5408\u4eba\u7c7b\u548cAI\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u672a\u80fd\u5145\u5206\u5904\u7406\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u51b3\u7b56\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6574\u5408\u6570\u5b66\u3001\u51b3\u7b56\u7406\u8bba\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u3001\u8ba4\u77e5\u5de5\u7a0b\u7b49\u9886\u57df\uff0c\u5e76\u5e94\u7528\u4e8e\u80fd\u6e90\u3001\u4ea4\u901a\u548c\u822a\u7a7a\u9886\u57df\uff1b\u5e76\u901a\u8fc7\u5728\u73b0\u6709\u6846\u67b6\u4e0a\u7684\u5b9e\u4f8b\u5316\u5c55\u793a\u7075\u6d3b\u6027\u3002", "result": "\u6846\u67b6\u88ab\u63d0\u51fa\u5e76\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u6210\u679c\u3002", "conclusion": "\u9700\u8981\u4e00\u4e2a\u6574\u4f53\u6846\u67b6\u6765\u6865\u63a5\u5b89\u5168\u7cfb\u7edf\u8bbe\u8ba1\u3001\u90e8\u7f72\u548c\u7ef4\u62a4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u65b9\u6cd5\u5b9e\u73b0\u3002"}}
{"id": "2504.16501", "pdf": "https://arxiv.org/pdf/2504.16501", "abs": "https://arxiv.org/abs/2504.16501", "authors": ["Seungyoon Choi", "Sein Kim", "Hongseok Kang", "Wonjoong Kim", "Chanyoung Park"], "title": "Dynamic Time-aware Continual User Representation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Traditional user modeling (UM) approaches have primarily focused on designing\nmodels for a single specific task, but they face limitations in generalization\nand adaptability across various tasks. Recognizing these challenges, recent\nstudies have shifted towards continual learning (CL)-based universal user\nrepresentation learning aiming to develop a single model capable of handling\nmultiple tasks. Despite advancements, existing methods are in fact evaluated\nunder an unrealistic scenario that does not consider the passage of time as\ntasks progress, which overlooks newly emerged items that may change the item\ndistribution of previous tasks. In this paper, we introduce a practical\nevaluation scenario on which CL-based universal user representation learning\napproaches should be evaluated, which takes into account the passage of time as\ntasks progress. Then, we propose a novel framework Dynamic Time-aware continual\nuser representation learner, named DITTO, designed to alleviate catastrophic\nforgetting despite continuous shifts in item distribution, while also allowing\nthe knowledge acquired from previous tasks to adapt to the current shifted item\ndistribution. Through our extensive experiments, we demonstrate the superiority\nof DITTO over state-of-the-art methods under a practical evaluation scenario.\nOur source code is available at\nhttps://github.com/seungyoon-Choi/DITTO_official.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDITTO\u6846\u67b6\uff0c\u5904\u7406\u7528\u6237\u5efa\u6a21\u4e2d\u65f6\u95f4\u611f\u77e5\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7528\u6237\u5efa\u6a21\u65b9\u6cd5\u4efb\u52a1\u7279\u5b9a\uff0c\u7f3a\u4e4f\u6cdb\u5316\uff1b\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u65f6\u95f4\u6d41\u901d\u548c\u9879\u76ee\u5206\u5e03\u53d8\u5316\u3002", "method": "\u5f15\u5165\u5b9e\u9645\u8bc4\u4f30\u573a\u666f\uff0c\u5e76\u63d0\u51faDITTO\u6846\u67b6\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u5e76\u9002\u5e94\u9879\u76ee\u5206\u5e03\u52a8\u6001\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDITTO\u5728\u8003\u8651\u65f6\u95f4\u56e0\u7d20\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DITTO\u6709\u6548\u89e3\u51b3\u7528\u6237\u8868\u793a\u5b66\u4e60\u95ee\u9898\uff0c\u9002\u5e94\u65f6\u95f4\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u6e90\u4ee3\u7801\u3002"}}
{"id": "2504.16138", "pdf": "https://arxiv.org/pdf/2504.16138", "abs": "https://arxiv.org/abs/2504.16138", "authors": ["Iyngkarran Kumar", "Sam Manning"], "title": "Trends in Frontier AI Model Count: A Forecast to 2028", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Governments are starting to impose requirements on AI models based on how\nmuch compute was used to train them. For example, the EU AI Act imposes\nrequirements on providers of general-purpose AI with systemic risk, which\nincludes systems trained using greater than $10^{25}$ floating point operations\n(FLOP). In the United States' AI Diffusion Framework, a training compute\nthreshold of $10^{26}$ FLOP is used to identify \"controlled models\" which face\na number of requirements. We explore how many models such training compute\nthresholds will capture over time. We estimate that by the end of 2028, there\nwill be between 103-306 foundation models exceeding the $10^{25}$ FLOP\nthreshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding\nthe $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion\nFramework (90% CI). We also find that the number of models exceeding these\nabsolute compute thresholds each year will increase superlinearly -- that is,\neach successive year will see more new models captured within the threshold\nthan the year before. Thresholds that are defined with respect to the largest\ntraining run to date (for example, such that all models within one order of\nmagnitude of the largest training run to date are captured by the threshold)\nsee a more stable trend, with a median forecast of 14-16 models being captured\nby this definition annually from 2025-2028.", "AI": {"tldr": "\u672c\u6587\u4f30\u8ba1\u653f\u5e9cAI\u6cd5\u89c4\u7684\u8ba1\u7b97\u9608\u503c\uff08\u5982EU AI Act\u768410^25 FLOP\u548cUS\u6846\u67b6\u768410^26 FLOP\uff09\u5c06\u6355\u83b7\u591a\u5c11\u6a21\u578b\uff0c\u5e76\u9884\u6d4b\u52302028\u5e74\u6a21\u578b\u6570\u91cf\u7684\u589e\u957f\u8d8b\u52bf\uff0c\u5f3a\u8c03\u7edd\u5bf9\u9608\u503c\u6355\u83b7\u91cf\u8d85\u7ebf\u6027\u589e\u52a0\uff0c\u800c\u76f8\u5bf9\u9608\u503c\u66f4\u7a33\u5b9a\u3002", "motivation": "\u653f\u5e9c\u5f00\u59cb\u57fa\u4e8eAI\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u91cf\u65bd\u52a0\u6cd5\u89c4\uff0c\u9700\u8981\u9884\u6d4b\u8fd9\u4e9b\u9608\u503c\u5bf9\u6a21\u578b\u6355\u83b7\u7684\u5f71\u54cd\uff0c\u4ee5\u8bc4\u4f30\u653f\u7b56\u6709\u6548\u6027\u548c\u6f5c\u5728\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u548c\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u6570\u636e\u6a21\u578bforecast\u672a\u6765\u6a21\u578b\u6570\u91cf\u548c\u8d8b\u52bf\uff0c\u5206\u6790\u7edd\u5bf9\u548c\u76f8\u5bf9\u9608\u503c\u7684\u6355\u83b7\u6548\u679c\u3002", "result": "\u9884\u6d4b\u52302028\u5e74\u5e95\uff0c\u5c06\u6709103-306\u4e2a\u6a21\u578b\u8d85\u8fc710^25 FLOP\u9608\u503c\uff0c45-148\u4e2a\u8d85\u8fc710^26 FLOP\u9608\u503c\uff1b\u6a21\u578b\u6570\u91cf\u6bcf\u5e74\u8d85\u7ebf\u6027\u589e\u52a0\uff0c\u800c\u76f8\u5bf9\u9608\u503c\u6bcf\u5e74\u6355\u83b7\u4e2d\u4f4d\u657014-16\u4e2a\u6a21\u578b\u3002", "conclusion": "\u7edd\u5bf9\u8ba1\u7b97\u9608\u503c\u5c06\u5bfc\u81f4\u6355\u83b7\u6a21\u578b\u6570\u91cf\u6025\u5267\u589e\u52a0\uff0c\u800c\u4ee5\u6700\u5927\u8bad\u7ec3\u8fd0\u884c\u4e3a\u57fa\u51c6\u7684\u76f8\u5bf9\u9608\u503c\u80fd\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u8d8b\u52bf\u3002"}}
{"id": "2504.16506", "pdf": "https://arxiv.org/pdf/2504.16506", "abs": "https://arxiv.org/abs/2504.16506", "authors": ["Ruxue Shi", "Yili Wang", "Mengnan Du", "Xu Shen", "Xin Wang"], "title": "A Comprehensive Survey of Synthetic Tabular Data Generation", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5bf9\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u5168\u9762\u8c03\u67e5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8c03\u67e5\u7684\u7a7a\u767d\uff0c\u5305\u62ec\u5404\u79cd\u751f\u6210\u65b9\u6cd5\u3001\u7ba1\u9053\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u6570\u636e\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u95ee\u9898\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ee5\u53ca\u73b0\u6709\u8c03\u67e5\u7684\u788e\u7247\u5316\uff0c\u7279\u522b\u662fLLM\u548c\u6269\u6563\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u5206\u7c7b\u6cd5\u3001\u8be6\u7ec6\u63cf\u8ff0\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u7ba1\u9053\uff08\u5305\u62ec\u5408\u6210\u3001\u540e\u5904\u7406\u548c\u8bc4\u4f30\uff09\u3001\u4ee5\u53ca\u8bc6\u522b\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u65b9\u6cd5\u6bd4\u8f83\u5206\u6790\u3001\u6311\u6218\u8bc6\u522b\u548c\u5f00\u653e\u7814\u7a76\u95ee\u9898\u7684\u6982\u8ff0\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u9886\u57df\u7684\u6f14\u53d8\u3001\u65b9\u6cd5\u95f4\u4e92\u52a8\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.16139", "pdf": "https://arxiv.org/pdf/2504.16139", "abs": "https://arxiv.org/abs/2504.16139", "authors": ["Sridharan Sankaran"], "title": "Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) reshapes industries and societies, ensuring\nits trustworthiness-through mitigating ethical risks like bias, opacity, and\naccountability deficits-remains a global challenge. International Organization\nfor Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to\nfoster responsible development by embedding fairness, transparency, and risk\nmanagement into AI systems. However, their effectiveness varies across diverse\nregulatory landscapes, from the EU's risk-based AI Act to China's\nstability-focused measures and the U.S.'s fragmented state-led initiatives.\nThis paper introduces a novel Comparative Risk-Impact Assessment Framework to\nevaluate how well ISO standards address ethical risks within these contexts,\nproposing enhancements to strengthen their global applicability. By mapping ISO\nstandards to the EU AI Act and surveying regulatory frameworks in ten\nregions-including the UK, Canada, India, Japan, Singapore, South Korea, and\nBrazil-we establish a baseline for ethical alignment. The framework, applied to\ncase studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO\nstandards falter in enforcement (e.g., Colorado) and undervalue region-specific\nrisks like privacy (China). We recommend mandatory risk audits, region-specific\nannexes, and a privacy-focused module to enhance ISO's adaptability. This\napproach not only synthesizes global trends but also offers a replicable tool\nfor aligning standardization with ethical imperatives, fostering\ninteroperability and trust in AI worldwide. Policymakers and standards bodies\ncan leverage these insights to evolve AI governance, ensuring it meets diverse\nsocietal needs as the technology advances.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6bd4\u8f83\u98ce\u9669\u5f71\u54cd\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30ISO AI\u6807\u51c6\u5728\u5168\u7403\u4e0d\u540c\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5efa\u8bae\u6539\u8fdb\u4ee5\u589e\u5f3a\u5176\u9002\u7528\u6027\u3002", "motivation": "AI\u7684\u4fe1\u4efb\u6027\u9762\u4e34\u5168\u7403\u6311\u6218\uff0c\u5305\u62ec\u504f\u89c1\u3001\u4e0d\u900f\u660e\u548c\u8d23\u4efb\u7f3a\u5931\uff0cISO\u6807\u51c6\u65e8\u5728\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u53d1\u5c55\uff0c\u4f46\u5176\u6548\u679c\u56e0\u76d1\u7ba1\u5dee\u5f02\u800c\u5f02\u3002", "method": "\u5f15\u5165\u6bd4\u8f83\u98ce\u9669\u5f71\u54cd\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06ISO\u6807\u51c6\u6620\u5c04\u5230\u6b27\u76dfAI\u6cd5\u6848\uff0c\u8c03\u67e5\u5341\u4e2a\u5730\u533a\u7684\u76d1\u7ba1\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u6b27\u76df\u3001\u7f8e\u56fd\u79d1\u7f57\u62c9\u591a\u548c\u4e2d\u56fd\u7b49\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6846\u67b6\u63ed\u793aISO\u6807\u51c6\u7684\u5dee\u8ddd\uff0c\u5982\u5728\u6267\u884c\u529b\u4e0a\u4e0d\u8db3\uff08\u4f8b\u5982\u79d1\u7f57\u62c9\u591a\uff09\u548c\u4f4e\u4f30\u5730\u533a\u7279\u5b9a\u98ce\u9669\uff08\u5982\u4e2d\u56fd\u7684\u9690\u79c1\u98ce\u9669\uff09\u3002", "conclusion": "\u63a8\u8350\u5f3a\u5236\u98ce\u9669\u5ba1\u8ba1\u3001\u5730\u533a\u7279\u5b9a\u9644\u4ef6\u548c\u9690\u79c1\u6a21\u5757\uff0c\u4ee5\u63d0\u9ad8ISO\u6807\u51c6\u7684\u9002\u5e94\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u590d\u5236\u5de5\u5177\u6765\u5bf9\u9f50\u6807\u51c6\u4e0e\u4f26\u7406\u8981\u6c42\uff0c\u4fc3\u8fdb\u5168\u7403AI\u6cbb\u7406\u3002"}}
{"id": "2504.16553", "pdf": "https://arxiv.org/pdf/2504.16553", "abs": "https://arxiv.org/abs/2504.16553", "authors": ["Mohammad Mahdi Abedi", "David Pardo", "Tariq Alkhalifah"], "title": "Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations", "categories": ["cs.LG", "physics.comp-ph", "physics.geo-ph"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have shown promise in solving\npartial differential equations (PDEs), including the frequency-domain Helmholtz\nequation. However, standard training of PINNs using gradient descent (GD)\nsuffers from slow convergence and instability, particularly for high-frequency\nwavefields. For scattered acoustic wavefield simulation based on Helmholtz\nequation, we derive a hybrid optimization framework that accelerates training\nconvergence by embedding a least-squares (LS) solver directly into the GD loss\nfunction. This formulation enables optimal updates for the linear output layer.\nOur method is applicable with or without perfectly matched layers (PML), and we\nprovide practical tensor-based implementations for both scenarios. Numerical\nexperiments on benchmark velocity models demonstrate that our approach achieves\nfaster convergence, higher accuracy, and improved stability compared to\nconventional PINN training. In particular, our results show that the\nLS-enhanced method converges rapidly even in cases where standard GD-based\ntraining fails. The LS solver operates on a small normal matrix, ensuring\nminimal computational overhead and making the method scalable for large-scale\nwavefield simulations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\u5668\u5d4c\u5165\u68af\u5ea6\u4e0b\u964d\u635f\u5931\u51fd\u6570\u7684\u6df7\u5408\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u52a0\u901fPhysics-Informed Neural Networks\u5728\u6c42\u89e3Helmholtz\u65b9\u7a0b\u65f6\u7684\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u6807\u51c6PINNs\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u5728\u9ad8\u9891\u6ce2\u573a\u6c42\u89e3\u65f6\u6536\u655b\u6162\u4e14\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\u5668\u76f4\u63a5\u5d4c\u5165GD\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u7ebf\u6027\u8f93\u51fa\u5c42\uff0c\u5e76\u63d0\u4f9b\u6709\u65e0\u5b8c\u7f8e\u5339\u914d\u5c42(PML)\u7684\u5f20\u91cf\u5b9e\u73b0\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u6536\u655b\u3001\u66f4\u51c6\u786e\u3001\u66f4\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u6807\u51c6GD\u5931\u8d25\u65f6\u4e5f\u80fd\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5c0f\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6ce2\u573a\u6a21\u62df\u3002"}}
{"id": "2504.16559", "pdf": "https://arxiv.org/pdf/2504.16559", "abs": "https://arxiv.org/abs/2504.16559", "authors": ["Adam Izdebski", "Jan Olszewski", "Pankhil Gawade", "Krzysztof Koras", "Serra Korkmaz", "Valentin Rauscher", "Jakub M. Tomczak", "Ewa Szczurek"], "title": "Unified Molecule Generation and Property Prediction", "categories": ["cs.LG", "q-bio.QM", "68T01", "I.2.1"], "comment": "17 pages, 4 figures", "summary": "Modeling the joint distribution of the data samples and their properties\nallows to construct a single model for both data generation and property\nprediction, with synergistic capabilities reaching beyond purely generative or\npredictive models. However, training joint models presents daunting\narchitectural and optimization challenges. Here, we propose Hyformer, a\ntransformer-based joint model that successfully blends the generative and\npredictive functionalities, using an alternating attention mask together with a\nunified pre-training scheme. We show that Hyformer rivals other joint models,\nas well as state-of-the-art molecule generation and property prediction models.\nAdditionally, we show the benefits of joint modeling in downstream tasks of\nmolecular representation learning, hit identification and antimicrobial peptide\ndesign.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Hyformer\uff0c\u4e00\u4e2a\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u8054\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u6570\u636e\u751f\u6210\u548c\u5c5e\u6027\u9884\u6d4b\uff0c\u5728\u5206\u5b50\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5efa\u6a21\u6570\u636e\u6837\u672c\u53ca\u5176\u5c5e\u6027\u7684\u8054\u5408\u5206\u5e03\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u5355\u4e00\u6a21\u578b\uff0c\u65e2\u80fd\u751f\u6210\u6570\u636e\u53c8\u80fd\u9884\u6d4b\u5c5e\u6027\uff0c\u6bd4\u7eaf\u751f\u6210\u6216\u9884\u6d4b\u6a21\u578b\u66f4\u6709\u534f\u540c\u6548\u5e94\uff0c\u4f46\u8bad\u7ec3\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faHyformer\uff0c\u4f7f\u7528\u4ea4\u66ff\u6ce8\u610f\u529b\u63a9\u7801\u548c\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u65b9\u6848\u3002", "result": "Hyformer\u5728\u5206\u5b50\u751f\u6210\u548c\u5c5e\u6027\u9884\u6d4b\u4e2d\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\uff0c\u5e76\u5728\u5206\u5b50\u8868\u793a\u5b66\u4e60\u3001\u547d\u4e2d\u8bc6\u522b\u548c\u6297\u83cc\u80bd\u8bbe\u8ba1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u76ca\u5904\u3002", "conclusion": "\u8054\u5408\u5efa\u6a21\u901a\u8fc7Hyformer\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2504.16142", "pdf": "https://arxiv.org/pdf/2504.16142", "abs": "https://arxiv.org/abs/2504.16142", "authors": ["Hangxu Liu", "Yaojie Sun", "Yu Wang"], "title": "A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, non-intrusive load monitoring (NILM) technology has\nattracted much attention in the related research field by virtue of its unique\nadvantage of utilizing single meter data to achieve accurate decomposition of\ndevice-level energy consumption. Cutting-edge methods based on machine learning\nand deep learning have achieved remarkable results in load decomposition\naccuracy by fusing time-frequency domain features. However, these methods\ngenerally suffer from high computational costs and huge memory requirements,\nwhich become the main obstacles for their deployment on resource-constrained\nmicrocontroller units (MCUs). To address these challenges, this study proposes\nan innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain\nand systematically compares and analyzes the performance of six machine\nlearning techniques in home electricity scenarios. Through complete\nexperimental validation on edge MCUs, this scheme successfully achieves a\nrecognition accuracy of 95%. Meanwhile, this study deeply optimizes the\nfrequency domain feature extraction process, which effectively reduces the\nrunning time by 55.55% and the storage overhead by about 34.6%. The algorithm\nperformance will be further optimized in future research work. Considering that\nthe elimination of voltage transformer design can significantly reduce the\ncost, the subsequent research will focus on this direction, and is committed to\nproviding more cost-effective solutions for the practical application of NILM,\nand providing a solid theoretical foundation and feasible technical paths for\nthe design of efficient NILM systems in edge computing environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u7684\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b(NILM)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u8fb9\u7f18\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u8d1f\u8f7d\u5206\u89e3\u3002", "motivation": "\u73b0\u6709NILM\u65b9\u6cd5\u867d\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5185\u5b58\u9700\u6c42\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u4e0a\u3002", "method": "\u63d0\u51fa\u65f6\u9891\u57dfDTW\u7b97\u6cd5\uff0c\u5e76\u6bd4\u8f83\u516d\u79cd\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u540c\u65f6\u4f18\u5316\u9891\u57df\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u3002", "result": "\u5728\u8fb9\u7f18MCU\u4e0a\u5b9e\u73b095%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1155.55%\uff0c\u5b58\u50a8\u5f00\u9500\u51cf\u5c11\u7ea634.6%\u3002", "conclusion": "\u7b97\u6cd5\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u672a\u6765\u7126\u70b9\u662f\u6d88\u9664\u7535\u538b\u53d8\u538b\u5668\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u66f4\u7ecf\u6d4e\u5b9e\u60e0\u7684NILM\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.16580", "pdf": "https://arxiv.org/pdf/2504.16580", "abs": "https://arxiv.org/abs/2504.16580", "authors": ["Ignacio Peis", "Batuhan Koyuncu", "Isabel Valera", "Jes Frellsen"], "title": "Hyper-Transforming Latent Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce a novel generative framework for functions by integrating\nImplicit Neural Representations (INRs) and Transformer-based hypernetworks into\nlatent variable models. Unlike prior approaches that rely on MLP-based\nhypernetworks with scalability limitations, our method employs a\nTransformer-based decoder to generate INR parameters from latent variables,\naddressing both representation capacity and computational efficiency. Our\nframework extends latent diffusion models (LDMs) to INR generation by replacing\nstandard decoders with a Transformer-based hypernetwork, which can be trained\neither from scratch or via hyper-transforming-a strategy that fine-tunes only\nthe decoder while freezing the pre-trained latent space. This enables efficient\nadaptation of existing generative models to INR-based representations without\nrequiring full retraining.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u751f\u6210\u6846\u67b6\uff0c\u5c06\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INRs)\u548cTransformer-based\u8d85\u7f51\u7edc\u96c6\u6210\u5230\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528Transformer-based\u89e3\u7801\u5668\u751f\u6210INR\u53c2\u6570\uff0c\u63d0\u9ad8\u8868\u793a\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u6269\u5c55\u6f5c\u5728\u6269\u6563\u6a21\u578b(LDMs)\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56MLP-based\u8d85\u7f51\u7edc\uff0c\u5b58\u5728\u53ef\u4f38\u7f29\u6027\u9650\u5236\uff0c\u56e0\u6b64\u672c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528Transformer-based\u89e3\u7801\u5668\u4ece\u6f5c\u5728\u53d8\u91cf\u751f\u6210INR\u53c2\u6570\uff0c\u6269\u5c55LDMs\u66ff\u6362\u6807\u51c6\u89e3\u7801\u5668\uff0c\u53ef\u4ece\u5934\u8bad\u7ec3\u6216\u901a\u8fc7hyper-transforming\u7b56\u7565\u5fae\u8c03\u89e3\u7801\u5668\u3002", "result": "\u63d0\u9ad8\u4e86\u8868\u793a\u5bb9\u91cf\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u73b0\u6709\u751f\u6210\u6a21\u578b\u5230INR-based\u8868\u793a\u7684\u6709\u6548\u9002\u5e94\uff0c\u65e0\u9700\u5b8c\u6574\u91cd\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u4f38\u7f29\u7684INR\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2504.16144", "pdf": "https://arxiv.org/pdf/2504.16144", "abs": "https://arxiv.org/abs/2504.16144", "authors": ["Ahmed El Fekih Zguir", "Ferda Ofli", "Muhammad Imran"], "title": "Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Natural disasters often result in a surge of social media activity, including\nrequests for assistance, offers of help, sentiments, and general updates. To\nenable humanitarian organizations to respond more efficiently, we propose a\nfine-grained hierarchical taxonomy to systematically organize crisis-related\ninformation about requests and offers into three critical dimensions: supplies,\nemergency personnel, and actions. Leveraging the capabilities of Large Language\nModels (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)\nthat retrieves class-specific labeled examples from an embedding database to\nenhance the model's performance in detecting and classifying posts. Beyond\nclassification, we assess the actionability of messages to prioritize posts\nrequiring immediate attention. Extensive experiments demonstrate that our\napproach outperforms baseline prompting strategies, effectively identifying and\nprioritizing actionable requests and offers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u5206\u5c42\u5206\u7c7b\u6cd5\u548cQuery-Specific Few-shot Learning\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u68c0\u6d4b\u548c\u5206\u7c7b\u707e\u5bb3\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u5e76\u8bc4\u4f30\u884c\u52a8\u6027\uff0c\u4ee5\u63d0\u5347\u4eba\u9053\u7ec4\u7ec7\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u5f15\u53d1\u793e\u4ea4\u5a92\u4f53\u6d3b\u52a8\u6fc0\u589e\uff0c\u5305\u62ec\u6c42\u52a9\u548c\u63d0\u4f9b\u5e2e\u52a9\u4fe1\u606f\uff0c\u52a8\u673a\u662f\u5e2e\u52a9\u4eba\u9053\u7ec4\u7ec7\u66f4\u6709\u6548\u5730\u54cd\u5e94\u707e\u5bb3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5206\u7c7b\u6cd5\u7ec4\u7ec7\u8bf7\u6c42\u548c\u63d0\u4f9b\u4fe1\u606f\uff1b\u5f15\u5165QSF Learning\uff0c\u4f7f\u7528LLM\u4ece\u5d4c\u5165\u6570\u636e\u5e93\u68c0\u7d22\u7279\u5b9a\u7c7b\u522b\u793a\u4f8b\u63d0\u5347\u68c0\u6d4b\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u6d88\u606f\u884c\u52a8\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u53ef\u884c\u52a8\u8bf7\u6c42\u548c\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u793e\u4ea4\u5a92\u4f53\u5728\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u4fe1\u606f\u5904\u7406\u548c\u4f18\u5148\u7ea7\u7ba1\u7406\u3002"}}
{"id": "2504.16585", "pdf": "https://arxiv.org/pdf/2504.16585", "abs": "https://arxiv.org/abs/2504.16585", "authors": ["Xiaofei Wu", "Rongmei Liang"], "title": "Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "In large-scale supervised learning, penalized logistic regression (PLR)\neffectively addresses the overfitting problem by introducing regularization\nterms yet its performance still depends on efficient variable selection\nstrategies. This paper theoretically demonstrates that label noise stemming\nfrom manual labeling, which is solely related to classification difficulty,\nrepresents a type of beneficial noise for variable selection in PLR. This\nbenefit is reflected in a more accurate estimation of the selected non-zero\ncoefficients when compared with the case where only truth labels are used.\nUnder large-scale settings, the sample size for PLR can become very large,\nmaking it infeasible to store on a single machine. In such cases, distributed\ncomputing methods are required to handle PLR model with manual labeling. This\npaper presents a partition-insensitive parallel algorithm founded on the ADMM\n(alternating direction method of multipliers) algorithm to address PLR by\nincorporating manual labeling. The partition insensitivity of the proposed\nalgorithm refers to the fact that the solutions obtained by the algorithm will\nnot change with the distributed storage of data. In addition, the algorithm has\nglobal convergence and a sublinear convergence rate. Experimental results\nindicate that, as compared with traditional variable selection classification\ntechniques, the PLR with manually-labeled noisy data achieves higher estimation\nand classification accuracy across multiple large-scale datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u60e9\u7f5a\u5316\u903b\u8f91\u56de\u5f52\uff08PLR\uff09\u4e2d\u624b\u52a8\u6807\u6ce8\u6807\u7b7e\u566a\u58f0\u5bf9\u53d8\u91cf\u9009\u62e9\u7684\u6709\u76ca\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eADMM\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5927\u5c3a\u5ea6\u6570\u636e\u96c6\u7684\u5904\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3PLR\u5728\u53d8\u91cf\u9009\u62e9\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u624b\u52a8\u6807\u6ce8\u5bfc\u81f4\u7684\u6807\u7b7e\u566a\u58f0\u5982\u4f55\u63d0\u5347\u7cfb\u6570\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "method": "\u7406\u8bba\u8bc1\u660e\u6807\u7b7e\u566a\u58f0\u7684\u76ca\u5904\uff0c\u5e76\u5f00\u53d1\u5206\u533a\u4e0d\u654f\u611f\u7684ADMM\u5e76\u884c\u7b97\u6cd5\uff0c\u5177\u6709\u5168\u5c40\u6536\u655b\u548c\u6b21\u7ebf\u6027\u6536\u655b\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u624b\u52a8\u6807\u6ce8\u566a\u58f0\u6570\u636e\u7684PLR\u5728\u591a\u4e2a\u5927\u578b\u6570\u636e\u96c6\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u66f4\u9ad8\u7684\u4f30\u8ba1\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u6807\u7b7e\u566a\u58f0\u53ef\u6539\u5584PLR\u7684\u53d8\u91cf\u9009\u62e9\u6027\u80fd\uff0c\u65b0\u7b97\u6cd5\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u3002"}}
{"id": "2504.16145", "pdf": "https://arxiv.org/pdf/2504.16145", "abs": "https://arxiv.org/abs/2504.16145", "authors": ["Jingchao Wang", "Hong Wang", "Wenlong Zhang", "Kunhua Ji", "Dingjiang Huang", "Yefeng Zheng"], "title": "Progressive Language-guided Visual Learning for Multi-Task Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL", "AI": {"tldr": "This paper proposes the PLVL framework for multi-task visual grounding, which progressively integrates language information into visual learning and uses a multi-task head for collaborative prediction, achieving superior performance on REC and RES tasks.", "motivation": "To address limitations in existing methods, such as insufficient language injection into visual features and ineffective exploitation of relationships between Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES) tasks.", "method": "Proposes the PLVL framework that mines inherent visual features, progressively injects language information without additional cross-modal fusion modules, and designs a multi-task head for joint prediction of REC and RES.", "result": "Extensive experiments on benchmark datasets demonstrate that PLVL outperforms representative methods in both REC and RES tasks.", "conclusion": "The PLVL framework significantly enhances the accuracy and efficiency of multi-task visual grounding by fully utilizing language guidance and task collaborations."}}
{"id": "2504.16624", "pdf": "https://arxiv.org/pdf/2504.16624", "abs": "https://arxiv.org/abs/2504.16624", "authors": ["Leo Henry", "Thomas Neele", "Mohammad Mousavi", "Matteo Sammartino"], "title": "Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement", "categories": ["cs.LG", "cs.FL"], "comment": null, "summary": "Active automata learning infers automaton models of systems from behavioral\nobservations, a technique successfully applied to a wide range of domains.\nCompositional approaches for concurrent systems have recently emerged. We take\na significant step beyond available results, including those by the authors,\nand develop a general technique for compositional learning of a synchronizing\nparallel system with an unknown decomposition. Our approach automatically\nrefines the global alphabet into component alphabets while learning the\ncomponent models. We develop a theoretical treatment of distributions of\nalphabets, i.e., sets of possibly overlapping component alphabets. We\ncharacterize counter-examples that reveal inconsistencies with global\nobservations, and show how to systematically update the distribution to restore\nconsistency. We present a compositional learning algorithm implementing these\nideas, where learning counterexamples precisely correspond to distribution\ncounterexamples under well-defined conditions. We provide an implementation,\ncalled CoalA, using the state-of-the-art active learning library LearnLib. Our\nexperiments show that in more than 630 subject systems, CoalA delivers orders\nof magnitude improvements (up to five orders) in membership queries and in\nsystems with significant concurrency, it also achieves better scalability in\nthe number of equivalence queries.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u5f0f\u4e3b\u52a8\u81ea\u52a8\u673a\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u672a\u77e5\u5206\u89e3\u7684\u540c\u6b65\u5e76\u884c\u7cfb\u7edf\uff0c\u81ea\u52a8\u7ec6\u5316\u5b57\u6bcd\u8868\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u73b0\u6709\u7ec4\u5408\u5f0f\u65b9\u6cd5\u5728\u5904\u7406\u5e76\u53d1\u7cfb\u7edf\u672a\u77e5\u5206\u89e3\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u5b57\u6bcd\u8868\u5206\u5e03\u7406\u8bba\u3001\u5904\u7406\u53cd\u4f8b\u4ee5\u66f4\u65b0\u5206\u5e03\uff0c\u5e76\u5b9e\u73b0\u540d\u4e3aCoalA\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u7528LearnLib\u5e93\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728630\u591a\u4e2a\u7cfb\u7edf\u4e2d\uff0c\u6210\u5458\u67e5\u8be2\u51cf\u5c11\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728\u9ad8\u5e76\u53d1\u7cfb\u7edf\u4e2d\u7b49\u4ef7\u67e5\u8be2\u53ef\u4f38\u7f29\u6027\u66f4\u597d\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5e76\u53d1\u7cfb\u7edf\u6a21\u578b\u5b66\u4e60\u7684\u6548\u7387\u548c\u53ef\u4f38\u7f29\u6027\u3002"}}
{"id": "2504.16148", "pdf": "https://arxiv.org/pdf/2504.16148", "abs": "https://arxiv.org/abs/2504.16148", "authors": ["Danial Hooshyar", "Gustav \u0160\u00edr", "Yeongwook Yang", "Eve Kikas", "Raija H\u00e4m\u00e4l\u00e4inen", "Tommi K\u00e4rkk\u00e4inen", "Dragan Ga\u0161evi\u0107", "Roger Azevedo"], "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Despite significant advancements in AI-driven educational systems and ongoing\ncalls for responsible AI for education, several critical issues remain\nunresolved -- acting as the elephant in the room within AI in education,\nlearning analytics, educational data mining, learning sciences, and educational\npsychology communities. This critical analysis identifies and examines nine\npersistent challenges that continue to undermine the fairness, transparency,\nand effectiveness of current AI methods and applications in education. These\ninclude: (1) the lack of clarity around what AI for education truly means --\noften ignoring the distinct purposes, strengths, and limitations of different\nAI families -- and the trend of equating it with domain-agnostic,\ncompany-driven large language models; (2) the widespread neglect of essential\nlearning processes such as motivation, emotion, and (meta)cognition in\nAI-driven learner modelling and their contextual nature; (3) limited\nintegration of domain knowledge and lack of stakeholder involvement in AI\ndesign and development; (4) continued use of non-sequential machine learning\nmodels on temporal educational data; (5) misuse of non-sequential metrics to\nevaluate sequential models; (6) use of unreliable explainable AI methods to\nprovide explanations for black-box models; (7) ignoring ethical guidelines in\naddressing data inconsistencies during model training; (8) use of mainstream AI\nmethods for pattern discovery and learning analytics without systematic\nbenchmarking; and (9) overemphasis on global prescriptions while overlooking\nlocalised, student-specific recommendations. Supported by theoretical and\nempirical research, we demonstrate how hybrid AI methods -- specifically\nneural-symbolic AI -- can address the elephant in the room and serve as the\nfoundation for responsible, trustworthy AI systems in education.", "AI": {"tldr": "\u672c\u6587\u8bc6\u522b\u5e76\u5206\u6790AI\u5728\u6559\u80b2\u4e2d\u7684\u4e5d\u4e2a\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u6df7\u5408AI\u65b9\u6cd5\u5982\u795e\u7ecf\u7b26\u53f7AI\u6765\u89e3\u51b3\uff0c\u4ee5\u63d0\u5347\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1AI\u6559\u80b2\u7cfb\u7edf\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5982\u5ffd\u7565\u5b66\u4e60\u8fc7\u7a0b\u548c\u4f26\u7406\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u8d1f\u8d23\u4efb\u7684AI\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\u8bc6\u522b\u4e5d\u4e2a\u6311\u6218\uff0c\u5e76\u5efa\u8bae\u91c7\u7528\u6df7\u5408AI\u65b9\u6cd5\u5982\u795e\u7ecf\u7b26\u53f7AI\u3002", "result": "\u8bc1\u660e\u6df7\u5408AI\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u5347AI\u5728\u6559\u80b2\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408AI\u65b9\u6cd5\u5e94\u4f5c\u4e3a\u6559\u80b2\u4e2d\u8d1f\u8d23\u4efb\u3001\u503c\u5f97\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u57fa\u7840\u3002"}}
{"id": "2504.16628", "pdf": "https://arxiv.org/pdf/2504.16628", "abs": "https://arxiv.org/abs/2504.16628", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 6 figure, Multiobjective Alignment of LLMs", "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165ParetoHqD\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u65b9\u5411\u548cPareto\u524d\u6cbf\u6570\u636e\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u7684\u504f\u597d\u8868\u793a\u4e0d\u5f53\u548c\u5956\u52b1\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5c06\u504f\u597d\u8868\u793a\u4e3a\u76ee\u6807\u7a7a\u95f4\u65b9\u5411\uff0c\u89c6Pareto\u524d\u6cbf\u6570\u636e\u4e3a\u9ad8\u8d28\u91cf\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u9488\u5bf9\u7279\u5b9a\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aParetoHqD\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4e94\u4e2a\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "ParetoHqD\u6709\u6548\u63d0\u5347\u591a\u76ee\u6807\u5bf9\u9f50\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2504.16152", "pdf": "https://arxiv.org/pdf/2504.16152", "abs": "https://arxiv.org/abs/2504.16152", "authors": ["Mohammad Molaee", "Nasrollah Moghadam Charkari"], "title": "Heterogeneous networks in drug-target interaction prediction", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "18 pages, 5 figures, 10 tables", "summary": "Drug discovery requires a tremendous amount of time and cost. Computational\ndrug-target interaction prediction, a significant part of this process, can\nreduce these requirements by narrowing the search space for wet lab\nexperiments. In this survey, we provide comprehensive details of graph machine\nlearning-based methods in predicting drug-target interaction, as they have\nshown promising results in this field. These details include the overall\nframework, main contribution, datasets, and their source codes. The selected\npapers were mainly published from 2020 to 2024. Prior to discussing papers, we\nbriefly introduce the datasets commonly used with these methods and\nmeasurements to assess their performance. Finally, future challenges and some\ncrucial areas that need to be explored are discussed.", "AI": {"tldr": "\u672c\u8c03\u67e5\u603b\u7ed3\u4e862020-2024\u5e74\u95f4\u4f7f\u7528\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u836f\u7269-\u9776\u70b9\u4ea4\u4e92\u7684\u6846\u67b6\u3001\u8d21\u732e\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u8fc7\u7a0b\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u8ba1\u7b97\u65b9\u6cd5\u53ef\u7f29\u5c0f\u5b9e\u9a8c\u641c\u7d22\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be6\u7ec6\u63cf\u8ff0\u6846\u67b6\u3001\u4e3b\u8981\u8d21\u732e\u3001\u5e38\u7528\u6570\u636e\u96c6\u548c\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u65b9\u6cd5\u5728\u836f\u7269-\u9776\u70b9\u4ea4\u4e92\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u524d\u666f\u6027\u7ed3\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u6311\u6218\u3002", "conclusion": "\u63a2\u8ba8\u4e86\u672a\u6765\u6311\u6218\u548c\u9700\u8981\u63a2\u7d22\u7684\u5173\u952e\u9886\u57df\u3002"}}
{"id": "2504.16639", "pdf": "https://arxiv.org/pdf/2504.16639", "abs": "https://arxiv.org/abs/2504.16639", "authors": ["Haoran Chen", "Jiapeng Liu", "Jiafan Wang", "Wenjun Shi"], "title": "DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Traditional Partial Least Squares Regression (PLSR) models frequently\nunderperform when handling data characterized by uneven categories. To address\nthe issue, this paper proposes a Data Augmentation Partial Least Squares\nRegression (DAPLSR) model via manifold optimization. The DAPLSR model\nintroduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase\nthe number of samples and utilizes the Value Difference Metric (VDM) to select\nthe nearest neighbor samples that closely resemble the original samples for\ngenerating synthetic samples. In solving the model, in order to obtain a more\naccurate numerical solution for PLSR, this paper proposes a manifold\noptimization method that uses the geometric properties of the constraint space\nto improve model degradation and optimization. Comprehensive experiments show\nthat the proposed DAPLSR model achieves superior classification performance and\noutstanding evaluation metrics on various datasets, significantly outperforming\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDAPLSR\u6a21\u578b\uff0c\u901a\u8fc7SMOTE\u548cVDM\u589e\u5f3a\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u6d41\u5f62\u4f18\u5316\u63d0\u5347PLSR\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfPLSR\u5728\u5904\u7406\u4e0d\u5747\u8861\u7c7b\u522b\u6570\u636e\u65f6\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528SMOTE\u8fc7\u91c7\u6837\u548cVDM\u9009\u62e9\u90bb\u5c45\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u5e76\u91c7\u7528\u6d41\u5f62\u4f18\u5316\u65b9\u6cd5\u5229\u7528\u7ea6\u675f\u7a7a\u95f4\u51e0\u4f55\u5c5e\u6027\u6c42\u89e3PLSR\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\uff0cDAPLSR\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u548c\u8bc4\u4f30\u6307\u6807\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAPLSR\u6a21\u578b\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4f18\u5316\u6280\u672f\u6709\u6548\u6539\u5584\u4e86PLSR\u7684\u5904\u7406\u4e0d\u5747\u8861\u6570\u636e\u80fd\u529b\u3002"}}
