<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 43]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 3]
- [math.OC](#math.OC) [Total: 5]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.RO](#cs.RO) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Framework for the Assurance of AI-Enabled Systems](https://arxiv.org/abs/2504.16937)
*Ariel S. Kapusta,David Jin,Peter M. Teague,Robert A. Houston,Jonathan B. Elliott,Grace Y. Park,Shelby S. Holdren*

Main category: cs.AI

TL;DR: This paper proposes a claims-based framework for AI assurance in the DOD to balance rapid deployment with risk management.


<details>
  <summary>Details</summary>
Motivation: The DOD aims to accelerate AI development but faces challenges like technical, security, and ethical issues that hinder adoption, necessitating trustworthiness.

Method: Proposes a claims-based framework including a process for AI assurance, relevant definitions, and key considerations.

Result: Provides a mechanism for faster AI deployment with rigorous evaluation and risk management, ensuring mission goals are met without unacceptable risks.

Conclusion: The framework offers an efficient way for the DOD to field AI capabilities while maintaining trust and minimizing risks.

Abstract: The United States Department of Defense (DOD) looks to accelerate the
development and deployment of AI capabilities across a wide spectrum of defense
applications to maintain strategic advantages. However, many common features of
AI algorithms that make them powerful, such as capacity for learning,
large-scale data ingestion, and problem-solving, raise new technical, security,
and ethical challenges. These challenges may hinder adoption due to uncertainty
in development, testing, assurance, processes, and requirements.
Trustworthiness through assurance is essential to achieve the expected value
from AI.
  This paper proposes a claims-based framework for risk management and
assurance of AI systems that addresses the competing needs for faster
deployment, successful adoption, and rigorous evaluation. This framework
supports programs across all acquisition pathways provide grounds for
sufficient confidence that an AI-enabled system (AIES) meets its intended
mission goals without introducing unacceptable risks throughout its lifecycle.
The paper's contributions are a framework process for AI assurance, a set of
relevant definitions to enable constructive conversations on the topic of AI
assurance, and a discussion of important considerations in AI assurance. The
framework aims to provide the DOD a robust yet efficient mechanism for swiftly
fielding effective AI capabilities without overlooking critical risks or
undermining stakeholder trust.

</details>


### [2] [Rational Inference in Formal Concept Analysis](https://arxiv.org/abs/2504.16938)
*Lucas Carr,Nicholas Leisegang,Thomas Meyer,Sergei Obiedkov*

Main category: cs.AI

TL;DR: 这篇论文将KLM框架的非单调推理应用于形式概念分析（FCA），以处理异常数据，并证明其忠实于原框架，同时提供更上下文化的推理。


<details>
  <summary>Details</summary>
Motivation: 标准FCA中的蕴涵无法有效处理错误或异常数据，非单调推理在FCA中尚未充分研究，因此需要引入KLM框架。

Method: 通过在FCA中构建KLM框架的结构。

Result: 该构建忠实于原KLM框架的原则，并在FCA中实现更上下文化的推理，允许得出更相关的结论。

Conclusion: FCA中的非单调推理比命题逻辑更适合处理实际异常，提供更相关和上下文化的推理结果。

Abstract: Defeasible conditionals are a form of non-monotonic inference which enable
the expression of statements like "if $\phi$ then normally $\psi$". The KLM
framework defines a semantics for the propositional case of defeasible
conditionals by construction of a preference ordering over possible worlds. The
pattern of reasoning induced by these semantics is characterised by consequence
relations satisfying certain desirable properties of non-monotonic reasoning.
In FCA, implications are used to describe dependencies between attributes.
However, these implications are unsuitable to reason with erroneous data or
data prone to exceptions. Until recently, the topic of non-monotonic inference
in FCA has remained largely uninvestigated. In this paper, we provide a
construction of the KLM framework for defeasible reasoning in FCA and show that
this construction remains faithful to the principle of non-monotonic inference
described in the original framework. We present an additional argument that,
while remaining consistent with the original ideas around non-monotonic
reasoning, the defeasible reasoning we propose in FCA offers a more contextual
view on inference, providing the ability for more relevant conclusions to be
drawn when compared to the propositional case.

</details>


### [3] [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
*Emre Can Acikgoz,Cheng Qian,Hongru Wang,Vardhan Dongre,Xiusi Chen,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: This survey paper analyzes LLM-driven Conversational Agents, organizing capabilities into reasoning, monitoring, and control dimensions, and identifies future research directions.


<details>
  <summary>Details</summary>
Motivation: To address open questions about capabilities, limitations, and paths forward for Conversational Agents, aiming to advance towards human-level intelligence and AGI.

Method: Systematically analyzing agents by defining three dimensions (Reasoning, Monitor, Control), introducing a taxonomy, classifying recent work, and identifying research gaps.

Result: Identified critical gaps and key directions including realistic evaluations, long-term reasoning, self-evolution, collaborative tasks, personalization, and proactivity.

Conclusion: Provides a structured foundation, highlights limitations, and offers insights for advancing towards AGI.

Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational
AI from traditional dialogue systems into sophisticated agents capable of
autonomous actions, contextual awareness, and multi-turn interactions with
users. Yet, fundamental questions about their capabilities, limitations, and
paths forward remain open. This survey paper presents a desideratum for
next-generation Conversational Agents - what has been achieved, what challenges
persist, and what must be done for more scalable systems that approach
human-level intelligence. To that end, we systematically analyze LLM-driven
Conversational Agents by organizing their capabilities into three primary
dimensions: (i) Reasoning - logical, systematic thinking inspired by human
intelligence for decision making, (ii) Monitor - encompassing self-awareness
and user interaction monitoring, and (iii) Control - focusing on tool
utilization and policy following. Building upon this, we introduce a novel
taxonomy by classifying recent work on Conversational Agents around our
proposed desideratum. We identify critical research gaps and outline key
directions, including realistic evaluations, long-term multi-turn reasoning
skills, self-evolution capabilities, collaborative and multi-agent task
completion, personalization, and proactivity. This work aims to provide a
structured foundation, highlight existing limitations, and offer insights into
potential future research directions for Conversational Agents, ultimately
advancing progress toward Artificial General Intelligence (AGI). We maintain a
curated repository of papers at:
https://github.com/emrecanacikgoz/awesome-conversational-agents.

</details>


### [4] [A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs](https://arxiv.org/abs/2504.17006)
*Jalal Arabneydi,Saiful Islam,Srijita Das,Sai Krishna Gottipati,William Duguay,Cloderic Mars,Matthew E. Taylor,Matthew Guzdial,Antoine Fagette,Younes Zerouali*

Main category: cs.AI

TL;DR: 本文提出一种多层级人机交互深度强化学习算法，并通过无人机场景验证其在决策问题中的优势。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习的流行，人机交互方法可革新决策方式并促进人机合作。

Method: 提出新型多层级分层人机交互DRL算法，包括自学习、模仿学习和转移学习，并整合奖励、动作和演示三种人类输入。

Result: 实验显示人机交互加速训练、提升性能，建议作为梯度指导降低方差，建议量需适中避免过拟合或欠拟合。

Conclusion: 讨论人机交互在复杂问题中的挑战、权衡和优势，并展示在真实场景中人机合作的作用。

Abstract: With the growing popularity of deep reinforcement learning (DRL),
human-in-the-loop (HITL) approach has the potential to revolutionize the way we
approach decision-making problems and create new opportunities for human-AI
collaboration. In this article, we introduce a novel multi-layered hierarchical
HITL DRL algorithm that comprises three types of learning: self learning,
imitation learning and transfer learning. In addition, we consider three forms
of human inputs: reward, action and demonstration. Furthermore, we discuss main
challenges, trade-offs and advantages of HITL in solving complex problems and
how human information can be integrated in the AI solution systematically. To
verify our technical results, we present a real-world unmanned aerial vehicles
(UAV) problem wherein a number of enemy drones attack a restricted area. The
objective is to design a scalable HITL DRL algorithm for ally drones to
neutralize the enemy drones before they reach the area. To this end, we first
implement our solution using an award-winning open-source HITL software called
Cogment. We then demonstrate several interesting results such as (a) HITL leads
to faster training and higher performance, (b) advice acts as a guiding
direction for gradient methods and lowers variance, and (c) the amount of
advice should neither be too large nor too small to avoid over-training and
under-training. Finally, we illustrate the role of human-AI cooperation in
solving two real-world complex scenarios, i.e., overloaded and decoy attacks.

</details>


### [5] [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
*Balaji Rao,William Eiers,Carlo Lipizzi*

Main category: cs.AI

TL;DR: 本论文提出一个框架，使用LLM生成形式化证明来验证软件代码，特别是LLM生成的代码。


<details>
  <summary>Details</summary>
Motivation: 动机是LLM生成代码的兴起增加了形式化验证需求，且定理证明是评估LLM推理能力的基准。

Method: 方法包括一个框架：生成自然语言语句、LLM生成形式化证明、启发式模块构建证明。训练采用SFT确保语法正确，然后RL确保证明可验证。

Result: 结果在miniF2F-test基准和Isabelle上验证，并应用于AWS S3访问政策用例，还整理了基于FVEL_ER的数据集。

Conclusion: 结论是该框架提升了LLM在形式化证明中的性能，并为未来研究提供了数据集。

Abstract: Formally verifying properties of software code has been a highly desirable
task, especially with the emergence of LLM-generated code. In the same vein,
they provide an interesting avenue for the exploration of formal verification
and mechanistic interpretability. Since the introduction of code-specific
models, despite their successes in generating code in Lean4 and Isabelle, the
task of generalized theorem proving still remains far from being fully solved
and will be a benchmark for reasoning capability in LLMs. In this work, we
introduce a framework that generates whole proofs in a formal language to be
used within systems that utilize the power of built-in tactics and
off-the-shelf automated theorem provers. Our framework includes 3 components:
generating natural language statements of the code to be verified, an LLM that
generates formal proofs for the given statement, and a module employing
heuristics for building the final proof. To train the LLM, we employ a 2-stage
fine-tuning process, where we first use SFT-based training to enable the model
to generate syntactically correct Isabelle code and then RL-based training that
encourages the model to generate proofs verified by a theorem prover. We
validate our framework using the miniF2F-test benchmark and the Isabelle proof
assistant and design a use case to verify the correctness of the AWS S3 bucket
access policy code. We also curate a dataset based on the
FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.

</details>


### [6] [Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments](https://arxiv.org/abs/2504.17087)
*Yuran Li,Jama Hussein Mohamud,Chongren Sun,Di Wu,Benoit Boulet*

Main category: cs.AI

TL;DR: 本论文提出三阶段元判断选择管道，使用多代理LLM改进LLM评估判断选择，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: LLM任务复杂，评估挑战大；人类评估效率低且有偏差；现有研究忽略偏差和判断选择问题。

Method: 提出三阶段管道：1) 与GPT-4和专家开发评分标准，2) 使用三个LLM代理评分，3) 阈值过滤低分判断，引入多代理协作。

Result: 在JudgeBench数据集上，改进15.55%比原始判断，8.37%比单代理基线。

Conclusion: 证明LLM作为元判断潜力，并为LLM判断强化学习偏好数据集研究奠基。

Abstract: Large language models (LLMs) are being widely applied across various fields,
but as tasks become more complex, evaluating their responses is increasingly
challenging. Compared to human evaluators, the use of LLMs to support
performance evaluation offers a more efficient alternative. However, most
studies focus mainly on aligning LLMs' judgments with human preferences,
overlooking the existence of biases and mistakes in human judgment.
Furthermore, how to select suitable LLM judgments given multiple potential LLM
responses remains underexplored. To address these two aforementioned issues, we
propose a three-stage meta-judge selection pipeline: 1) developing a
comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM
agents to score judgments, and 3) applying a threshold to filter out
low-scoring judgments. Compared to methods using a single LLM as both judge and
meta-judge, our pipeline introduces multi-agent collaboration and a more
comprehensive rubric. Experimental results on the JudgeBench dataset show about
15.55\% improvement compared to raw judgments and about 8.37\% improvement over
the single-agent baseline. Our work demonstrates the potential of LLMs as
meta-judges and lays the foundation for future research on constructing
preference datasets for LLM-as-a-judge reinforcement learning.

</details>


### [7] [AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models](https://arxiv.org/abs/2504.17179)
*Mohammad Zarei,Melanie A Jutras,Eliana Evans,Mike Tan,Omid Aaramoon*

Main category: cs.AI

TL;DR: 本论文提出使用生成AI技术模拟自动驾驶车辆的稀有故障模式，提高其鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在训练后仍难以检测稀有故障模式，导致长尾挑战，需要提升其安全性和可靠性。

Method: 提取对象分割掩码，创建环境掩码，结合文本提示和Stable Diffusion模型生成规避检测图像，并产生自然语言描述。

Result: 生成图像暴露AI漏洞，提高车辆鲁棒性，并指导模型训练和测试。

Conclusion: 该方法增强自动驾驶系统的安全性和可靠性，辅助开发者与政策制定者改进。

Abstract: Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately
detect objects and interpret their surroundings. However, even when trained
using millions of miles of real-world data, AVs are often unable to detect rare
failure modes (RFMs). The problem of RFMs is commonly referred to as the
"long-tail challenge", due to the distribution of data including many instances
that are very rarely seen. In this paper, we present a novel approach that
utilizes advanced generative and explainable AI techniques to aid in
understanding RFMs. Our methods can be used to enhance the robustness and
reliability of AVs when combined with both downstream model training and
testing. We extract segmentation masks for objects of interest (e.g., cars) and
invert them to create environmental masks. These masks, combined with carefully
crafted text prompts, are fed into a custom diffusion model. We leverage the
Stable Diffusion inpainting model guided by adversarial noise optimization to
generate images containing diverse environments designed to evade object
detection models and expose vulnerabilities in AI systems. Finally, we produce
natural language descriptions of the generated RFMs that can guide developers
and policymakers to improve the safety and reliability of AV systems.

</details>


### [8] [Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning](https://arxiv.org/abs/2504.17282)
*Lynn Cherif,Flemming Kondrup,David Venuto,Ankit Anand,Doina Precup,Khimya Khetarpal*

Main category: cs.AI

TL;DR: 这篇论文提出CoGA方法，使用预训练视觉语言模型生成代码约束动作空间，提高强化学习代理在网页GUI任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 动机是解决低数据环境下代理样本效率低的问题，尤其在稀疏奖励和大型动作空间中。

Method: 方法是CoGA，利用VLMs生成代码通过意图完成函数和自动化管道，确定可负担动作集。

Result: 结果显示CoGA样本效率高出几个数量级，能泛化，并在少量专家演示时性能优于或相当于是行为克隆。

Conclusion: 结论是CoGA是一种有效的样本高效学习方法，证明了约束动作空间的益处。

Abstract: Agents that can autonomously navigate the web through a graphical user
interface (GUI) using a unified action space (e.g., mouse and keyboard actions)
can require very large amounts of domain-specific expert demonstrations to
achieve good performance. Low sample efficiency is often exacerbated in
sparse-reward and large-action-space environments, such as a web GUI, where
only a few actions are relevant in any given situation. In this work, we
consider the low-data regime, with limited or no access to expert behavior. To
enable sample-efficient learning, we explore the effect of constraining the
action space through $\textit{intent-based affordances}$ -- i.e., considering
in any situation only the subset of actions that achieve a desired outcome. We
propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$,
a method that leverages pre-trained vision-language models (VLMs) to generate
code that determines affordable actions through implicit intent-completion
functions and using a fully-automated program generation and verification
pipeline. These programs are then used in-the-loop of a reinforcement learning
agent to return a set of affordances given a pixel observation. By greatly
reducing the number of actions that an agent must consider, we demonstrate on a
wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$
$\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,
$\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of
tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared
with behavior cloning when a small number of expert demonstrations is
available.

</details>


### [9] [AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining](https://arxiv.org/abs/2504.17295)
*Shahrzad Khayatbashi,Viktor Sjölind,Anders Granåker,Amin Jalali*

Main category: cs.AI

TL;DR: 本论文通过对象中心过程挖掘（OCPM）分析了在保险行业中使用大语言模型（LLM）自动化索赔部分识别的影响，展示了其提升可伸缩性但也引入新动态的优势和局限。


<details>
  <summary>Details</summary>
Motivation: 评估AI自动化如何影响业务过程数字转型，特别是传统和AI增强过程的共存，需要数据驱动分析。

Method: 采用OCPM方法，在保险案例中部署LLM自动化索赔识别任务。

Result: LLM显著提升操作容量，但引入新过程动态需优化；OCPM证明了其实用性及其限制。

Conclusion: 突显OCPM在AI驱动过程分析中的优势和挑战。

Abstract: Recent advancements in Artificial Intelligence (AI), particularly Large
Language Models (LLMs), have enhanced organizations' ability to reengineer
business processes by automating knowledge-intensive tasks. This automation
drives digital transformation, often through gradual transitions that improve
process efficiency and effectiveness. To fully assess the impact of such
automation, a data-driven analysis approach is needed - one that examines how
traditional and AI-enhanced process variants coexist during this transition.
Object-Centric Process Mining (OCPM) has emerged as a valuable method that
enables such analysis, yet real-world case studies are still needed to
demonstrate its applicability. This paper presents a case study from the
insurance sector, where an LLM was deployed in production to automate the
identification of claim parts, a task previously performed manually and
identified as a bottleneck for scalability. To evaluate this transformation, we
apply OCPM to assess the impact of AI-driven automation on process scalability.
Our findings indicate that while LLMs significantly enhance operational
capacity, they also introduce new process dynamics that require further
refinement. This study also demonstrates the practical application of OCPM in a
real-world setting, highlighting its advantages and limitations.

</details>


### [10] [Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2504.17356)
*Weiliang Zhang,Xiaohan Huang,Yi Du,Ziyue Qiao,Qingqing Long,Zhen Meng,Yuanchun Zhou,Meng Xiao*

Main category: cs.AI

TL;DR: 这篇论文提出HRLFS方法，使用强化学习和大型语言模型优化特征选择，提高机器学习性能并加速运行。


<details>
  <summary>Details</summary>
Motivation: 当前RL-based特征选择方法效率低下，如每个特征一个代理和数据集复杂性问题，需要改进。

Method: HRLFS使用LLM-based混合状态提取器捕获特征特性，进行聚类并构建分层代理探索特征子空间。

Result: 实验证明HRLFS提升下游ML性能，减少代理数量，加速运行时间，并展示高效、可扩展和鲁棒性。

Conclusion: HRLFS优于现有RL方法，在性能和效率上取得显著改进。

Abstract: Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.

</details>


### [11] [Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation](https://arxiv.org/abs/2504.17402)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisarkka,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本研究评估LLM在领域特定本体生成中的通用性，实验显示DeepSeek和o1-preview模型在不同领域性能一致。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在领域特定本体生成任务中的适用程度，因为其潜力尚不明确。

Method: 使用DeepSeek和o1-preview模型，从95个competency questions和user stories生成本体，涉及6个不同领域。

Result: 两个LLM在所有领域的性能高度一致，证明其通用化能力。

Conclusion: 强调LLM可实现可扩展的领域无关本体构建，并为进一步研究自动推理和知识表示技术奠定基础。

Abstract: Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.

</details>


### [12] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Feifei Zhao,Yuwei Wang,Enmeng Lu,Dongcheng Zhao,Bing Han,Haibo Tong,Yao Liang,Dongqi Liang,Kang Sun,Lei Wang,Yitao Liang,Chao Liu,Yaodong Yang,Yi Zeng*

Main category: cs.AI

TL;DR: 这篇论文重新定义超对齐为人类-人工智能共同对齐，提出整合外部监督和内在主动对齐的框架，以实现可持续共生社会。


<details>
  <summary>Details</summary>
Motivation: 人工智能可能超越人类控制并导致灾难性后果，因此需要探索更安全的多维超对齐方法。

Method: 提出一个框架，包括基于人类决策的外部监督（如可解释自动评估）和基于自我意识、自我反思、同理心的内在主动对齐，通过迭代互动实现对齐。

Result: 框架整合外部和内在方法，促进人类-人工智能共同对齐，支撑安全AGI和ASI的发展。

Conclusion: 通过这种整合，人类和人工智能可共同构建可持续的共生社会，确保AI为人类福祉服务。

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>


### [13] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/abs/2504.17531)
*Justus Flerlage,Ilja Behnke,Odej Kao*

Main category: cs.AI

TL;DR: 这篇论文探讨使用大型语言模型（LLM）生成代码以实现用户意图的工作流，并通过GPT-4o-mini展示了其可行性。


<details>
  <summary>Details</summary>
Motivation: 动机是重新评估AI进步下用户与设备交互机制，从依赖高级应用转向AI生成的工作流，以更好地处理用户意图。

Method: 方法包括使用用户意图提示LLM（如GPT-4o-mini）生成代码，并通过简化API执行工作流。

Result: 结果显示该方法总体可行，且GPT-4o-mini在生成代码导向工作流方面表现出色。

Conclusion: 结论是混合人类-AI工作流具有前景，AI可有效实现用户定义的意图。

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code, which is tantamount to the generation of workflows
comprising a multitude of interdependent steps. This development represents a
significant progression in the realm of hybrid workflows, where human and
artificial intelligence collaborate to address user intentions, with the former
responsible for defining these intentions and the latter for implementing the
solutions to address them. In this paper, we investigate the feasibility of
generating and executing workflows through code generation that results from
prompting an LLM with a concrete user intention, such as \emph{Please send my
car title to my insurance company}, and a simplified application programming
interface for a GUI-less operating system. We provide in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate a general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [14] [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TL;DR: 这篇论文引入了一个五维审计模型来评估大型语言模型的伦理推理，并通过基准测试展示了模型性能的差异。


<details>
  <summary>Details</summary>
Motivation: 生成AI模型日益整合到高风险领域，需要鲁棒的方法来评估其伦理推理。

Method: 使用多电池提示方法，包括新颖的伦理困境，对七个主要LLM进行基准测试，基于五维模型（分析质量、伦理考虑广度、解释深度、一致性和决断力）评估。

Result: 模型在伦理决策上趋同，但解释严谨度和道德优先级存在差异；Chain-of-Thought提示和优化推理模型显著提升了性能。

Conclusion: 引入了一种可扩展的AI伦理基准测试方法，并强调AI可补充人类道德推理。

Abstract: As generative AI models become increasingly integrated into high-stakes
domains, the need for robust methods to evaluate their ethical reasoning
becomes increasingly important. This paper introduces a five-dimensional audit
model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth
of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic
of leading large language models (LLMs). Drawing on traditions from applied
ethics and higher-order thinking, we present a multi-battery prompt approach,
including novel ethical dilemmas, to probe the models' reasoning across diverse
contexts. We benchmark seven major LLMs finding that while models generally
converge on ethical decisions, they vary in explanatory rigor and moral
prioritization. Chain-of-Thought prompting and reasoning-optimized models
significantly enhance performance on our audit metrics. This study introduces a
scalable methodology for ethical benchmarking of AI systems and highlights the
potential for AI to complement human moral reasoning in complex decision-making
contexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [A Novel Graph Transformer Framework for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.16961)
*Binon Teji,Swarup Roy*

Main category: cs.LG

TL;DR: 本文提出GT-GRN模型，使用嵌入技术和图变换器改进基因调控网络推断。


<details>
  <summary>Details</summary>
Motivation: 现有GRN推断方法易受噪声影响，需整合多网络先验知识和位置信息以提高准确性。

Method: 采用自编码器嵌入基因表达，BERT编码GRN结构和位置信息，整合到图变换器模型GT-GRN中。

Result: 实验显示GT-GRN在准确性和鲁棒性上优于现有方法。

Conclusion: 该方法有效利用网络拓扑和信息编码，提升GRN推断性能。

Abstract: The inference of gene regulatory networks (GRNs) is a foundational stride
towards deciphering the fundamentals of complex biological systems. Inferring a
possible regulatory link between two genes can be formulated as a link
prediction problem. Inference of GRNs via gene coexpression profiling data may
not always reflect true biological interactions, as its susceptibility to noise
and misrepresenting true biological regulatory relationships. Most GRN
inference methods face several challenges in the network reconstruction phase.
Therefore, it is important to encode gene expression values, leverege the prior
knowledge gained from the available inferred network structures and positional
informations of the input network nodes towards inferring a better and more
confident GRN network reconstruction. In this paper, we explore the integration
of multiple inferred networks to enhance the inference of Gene Regulatory
Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene
expression patterns directly from raw data, preserving intricate biological
signals. Then, we embed the prior knowledge from GRN structures transforming
them into a text-like representation using random walks, which are then encoded
with a masked language model, BERT, to generate global embeddings for each gene
across all networks. Additionally, we embed the positional encodings of the
input gene networks to better identify the position of each unique gene within
the graph. These embeddings are integrated into graph transformer-based model,
termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the
topological structure of the ground truth network while incorporating the
enriched encoded information. Experimental results demonstrate that GT-GRN
significantly outperforms existing GRN inference methods, achieving superior
accuracy and highlighting the robustness of our approach.

</details>


### [16] [Backslash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TL;DR: 本文介绍了一种名为Backslash的训练时压缩方法，使用率失真优化（RDO），在不损失准确性的前提下减少60%-90%的内存使用。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的训练后压缩研究广泛，但训练过程中的压缩尚未充分探索，本文旨在填补这一空白。

Method: 提出Rate-Constrained Training（Backslash）方法，基于率失真优化，允许在模型准确性和复杂度之间灵活权衡。

Result: 实验显示，内存使用减少60%-90%无准确性损失，比训练后压缩更有效；还提升泛化能力、抗剪枝鲁棒性，并加速边缘设备推理。

Conclusion: Backslash方法高度通用，可改善模型性能、增强鲁棒性和简化网络。

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (Backslash), a novel training-time
compression approach based on rate-distortion optimization (RDO). Backslash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that Backslash can
reduce memory usage by 60\% - 90\% without accuracy loss and provides
significant compression gain compared to compression after training. Moreover,
Backslash proves to be highly versatile: it enhances generalization with small
Lagrange multipliers, improves model robustness to pruning (maintaining
accuracy even at 80\% pruning rates), and enables network simplification for
accelerated inference on edge devices.

</details>


### [17] [STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction](https://arxiv.org/abs/2504.16970)
*Yin Wang,Chunlin Gong,Xiang Wu,Hanleran Zhang*

Main category: cs.LG

TL;DR: 本研究提出基于数据驱动的SST预测框架，使用相空间重构和STFM方法，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: SST预测对生产规划至关重要，但海洋系统非线性挑战大，现有方法计算复杂或数据需求高。

Method: 通过相空间重构构建吸引子对，并设计STFM揭示SST动态。

Result: 方法高效，需少量训练数据，在测试中实现高准确性。

Conclusion: 数据驱动方法证明在SST预测中有效且优越。

Abstract: The sea surface temperature (SST), a key environmental parameter, is crucial
to optimizing production planning, making its accurate prediction a vital
research topic. However, the inherent nonlinearity of the marine dynamic system
presents significant challenges. Current forecasting methods mainly include
physics-based numerical simulations and data-driven machine learning
approaches. The former, while describing SST evolution through differential
equations, suffers from high computational complexity and limited
applicability, whereas the latter, despite its computational benefits, requires
large datasets and faces interpretability challenges. This study presents a
prediction framework based solely on data-driven techniques. Using phase space
reconstruction, we construct initial-delay attractor pairs with a mathematical
homeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover
their intrinsic connections. Unlike conventional models, our method captures
SST dynamics efficiently through phase space reconstruction and achieves high
prediction accuracy with minimal training data in comparative tests

</details>


### [18] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/abs/2504.16972)
*Hossein Ahmadi,Sajjad Emdadi Mahdimahalleh,Arman Farahat,Banafsheh Saffari*

Main category: cs.LG

TL;DR: 这篇综述回顾了在无监督信号分析中应用自编码器和视觉变压器的最新进展，涵盖架构、应用和趋势。


<details>
  <summary>Details</summary>
Motivation: 未标记时间序列数据的快速增长推动了无监督学习的发展，特别是在无线通信、雷达、生物医学工程和物联网领域。

Method: 通过合成自编码器和视觉变压器的架构、应用和新兴趋势，探讨其在信号分析中的作用。

Result: 模型实现了特征提取、异常检测和分类，突出了混合架构和自监督学习的优势，但面临可解释性、可扩展性和领域泛化挑战。

Conclusion: 提供了一个桥接方法创新与实际应用的路线图，以开发稳健的自适应信号智能模型。

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>


### [19] [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)
*Pratyush Maini,Sachin Goyal,Dylan Sam,Alex Robey,Yash Savani,Yiding Jiang,Andy Zou,Zacharcy C. Lipton,J. Zico Kolter*

Main category: cs.LG

TL;DR: 本论文提出数据中心预训练框架，从一开始提升大型语言模型的安全性，减少有害输出。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在高风险环境中生成有害内容的风险，因为事后对齐方法不稳定。

Method: 包括训练安全分类器过滤数据、生成合成安全数据集、创建特定数据集并注入标记以引导模型。

Result: 将攻击成功率从38.8%降至8.4%，无性能退化。

Conclusion: 安全预训练方法有效降低有害生成风险。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. We present a data-centric
pretraining framework that builds safety into the model from the start. Our
contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled
examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset
to date (100B tokens) generated via recontextualization of harmful web data;
(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into
refusal dialogues and web-style educational material; (iv) Harmfulness-Tag
annotations injected during pretraining to flag unsafe content and steer away
inference from harmful generations; and (v) safety evaluations measuring base
model behavior before instruction tuning. Our safety-pretrained models reduce
attack success rates from 38.8% to 8.4% with no performance degradation on
standard LLM safety benchmarks.

</details>


### [20] [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
*Amin Karbasi,Omar Montasser,John Sous,Grigoris Velegkas*

Main category: cs.LG

TL;DR: 这篇论文分析了是否可能自动检测大型语言模型（LLMs）的幻觉，发现没有专家反馈时不可能，但有反馈时可能，并支持RLHF等方法。


<details>
  <summary>Details</summary>
Motivation: 受Gold-Angluin框架启发，探讨自动检测LLMs幻觉的可行性。

Method: 引入理论框架，建立幻觉检测与语言识别的等价性，并研究专家标记反馈的影响。

Result: 没有专家标记反馈时，幻觉检测在大多数语言集合中不可能；有反馈时，对所有可数语言集合成为可能。

Conclusion: 强调专家标记示例在训练幻觉检测器中的关键作用，并为反馈-based 方法如RLHF提供理论支持。

Abstract: Is automated hallucination detection possible? In this work, we introduce a
theoretical framework to analyze the feasibility of automatically detecting
hallucinations produced by large language models (LLMs). Inspired by the
classical Gold-Angluin framework for language identification and its recent
adaptation to language generation by Kleinberg and Mullainathan, we investigate
whether an algorithm, trained on examples drawn from an unknown target language
$K$ (selected from a countable collection) and given access to an LLM, can
reliably determine whether the LLM's outputs are correct or constitute
hallucinations.
  First, we establish an equivalence between hallucination detection and the
classical task of language identification. We prove that any hallucination
detection method can be converted into a language identification method, and
conversely, algorithms solving language identification can be adapted for
hallucination detection. Given the inherent difficulty of language
identification, this implies that hallucination detection is fundamentally
impossible for most language collections if the detector is trained using only
correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the
detector with both positive examples (correct statements) and negative examples
(explicitly labeled incorrect statements), dramatically changes this
conclusion. Under this enriched training regime, automated hallucination
detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in
training hallucination detectors and provide theoretical support for
feedback-based methods, such as reinforcement learning with human feedback
(RLHF), which have proven critical for reliable LLM deployment.

</details>


### [21] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/abs/2504.17028)
*Iman Khadir,Shane Stevenson,Henry Li,Kyle Krick,Abram Burrows,David Hall,Stan Posey,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 这篇论文展示了如何通过GPU和NVIDIA的FourCastNetv2等免费AI模型，使AI驱动的全球天气预报在大学研究群体中实现民主化。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限大学群体复制天气预报结果的挑战，并促进AI数值天气预报（NWP）的民主化。

Method: 利用FourCastNetv2 API进行预测，并使用NVIDIA硬件训练FourCastNet模型；探讨数据管理、训练效率和模型验证。

Result: 证明了使用有限GPU资源的可行性，突出了NVIDIA A100的能力、限制以及优势与挑战。

Conclusion: 可作为指南，帮助其他大学群体开发AI天气预报程序，促进AI NWP在数字经济中的普及。

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>


### [22] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma*

Main category: cs.LG

TL;DR: 本论文提出Conformalized GAN框架，通过整合conformal prediction方法，提供合成数据的统计保证和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型缺乏严格统计保证，限制了在关键领域如医疗和金融的应用。

Method: 将Inductive Conformal Prediction、Mondrian Conformal Prediction等方法整合到GAN中，形成Conformalized GAN (cGAN)。

Result: cGAN提高了合成数据的校准性能和统计保真度，提供可证明的有限样本有效性。

Conclusion: 该框架通过数学证明确保渐进效率，支持在高风险领域可靠应用。

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [23] [Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks](https://arxiv.org/abs/2504.17065)
*Sahar Bagherkhani,Jackson Christopher Earls,Franco De Flaviis,Pierre Baldi*

Main category: cs.LG

TL;DR: This paper uses CNNs to reconstruct near-field electromagnetic distributions from far-field data, achieving low errors and demonstrating deep learning's potential.


<details>
  <summary>Details</summary>
Motivation: To enable electromagnetic field reconstruction for applications like antenna diagnostics without explicit analytical transformations.

Method: Convolutional Neural Networks trained on paired far-field and near-field data, evaluated with mean squared error (MSE).

Result: Best model achieves training error of 0.0199 and test error of 0.3898, with visual comparisons showing effective capture of field behavior.

Conclusion: Highlights the potential of deep learning in electromagnetic field reconstruction.

Abstract: Electromagnetic field reconstruction is crucial in many applications,
including antenna diagnostics, electromagnetic interference analysis, and
system modeling. This paper presents a deep learning-based approach for
Far-Field to Near-Field (FF-NF) transformation using Convolutional Neural
Networks (CNNs). The goal is to reconstruct near-field distributions from the
far-field data of an antenna without relying on explicit analytical
transformations. The CNNs are trained on paired far-field and near-field data
and evaluated using mean squared error (MSE). The best model achieves a
training error of 0.0199 and a test error of 0.3898. Moreover, visual
comparisons between the predicted and true near-field distributions demonstrate
the model's effectiveness in capturing complex electromagnetic field behavior,
highlighting the potential of deep learning in electromagnetic field
reconstruction.

</details>


### [24] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng,Yicheng Yang,Hao Zhuo,Tim Menzies*

Main category: cs.LG

TL;DR: 使用倾向得分匹配改善机器学习模型的公平性评估和偏见缓解方法。


<details>
  <summary>Details</summary>
Motivation: 许多模型在公平度量下仍不公平，训练和测试数据的随机采样可能导致测试数据中存在偏见，扭曲公平评估。

Method: 提出FairMatch后处理方法，通过倾向得分匹配识别测试集中的控制和处理组对，调整子群体决策阈值；对无法匹配样本使用基于公平性损失函数的概率校准。

Result: 实验结果显示，可精确定位无偏见子集，并显著减少剩余数据的偏见，同时不牺牲预测性能。

Conclusion: 倾向得分匹配提供原则性方法，提升公平性评估和缓解，而不影响预测性能。

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>


### [25] [In-Context Learning can distort the relationship between sequence likelihoods and biological fitness](https://arxiv.org/abs/2504.17068)
*Pranav Kantroo,Günter P. Wagner,Benjamin B. Machta*

Main category: cs.LG

TL;DR: 语言模型的上下文学习会扭曲生物序列适应性预测，特别是对重复基序的序列。


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型在生物序列预测中存在的潜在缺陷，以提高其准确性。

Method: 使用不同架构的蛋白质语言模型，训练于掩码语言建模任务，并测试重复基序序列。

Result: 基于变压器的模型易受影响，原因在于查找操作覆盖先验知识，并延伸至不完美重复和RNA特征。

Conclusion: 此现象表明语言模型在生物应用中可能不可靠，需要进一步优化。

Abstract: Language models have emerged as powerful predictors of the viability of
biological sequences. During training these models learn the rules of the
grammar obeyed by sequences of amino acids or nucleotides. Once trained, these
models can take a sequence as input and produce a likelihood score as an
output; a higher likelihood implies adherence to the learned grammar and
correlates with experimental fitness measurements. Here we show that in-context
learning can distort the relationship between fitness and likelihood scores of
sequences. This phenomenon most prominently manifests as anomalously high
likelihood scores for sequences that contain repeated motifs. We use protein
language models with different architectures trained on the masked language
modeling objective for our experiments, and find transformer-based models to be
particularly vulnerable to this effect. This behavior is mediated by a look-up
operation where the model seeks the identity of the masked position by using
the other copy of the repeated motif as a reference. This retrieval behavior
can override the model's learned priors. This phenomenon persists for
imperfectly repeated sequences, and extends to other kinds of biologically
relevant features such as reversed complement motifs in RNA sequences that fold
into hairpin structures.

</details>


### [26] [Sparse Phased Array Optimization Using Deep Learning](https://arxiv.org/abs/2504.17073)
*David Lu,Lior Maman,Jackson Earls,Amir Boag,Pierre Baldi*

Main category: cs.LG

TL;DR: 这篇论文使用深度学习优化稀疏相控阵列设计，减少栅瓣，提升信号性能。


<details>
  <summary>Details</summary>
Motivation: 天线阵列用于增强无线通信、雷达等领域的信号强度、方向性和干扰抑制，但面临非凸优化挑战。

Method: 采用神经网络近似非凸成本函数，通过梯度下降优化天线坐标，并加入惩罚机制处理约束。

Result: 在十种初始配置上，成本减少411%至643%，平均改善552%，显著降低旁瓣水平。

Conclusion: 该方法促进精确波束形成和干扰抑制，提升无线和雷达系统的效率与清晰度。

Abstract: Antenna arrays are widely used in wireless communication, radar systems,
radio astronomy, and military defense to enhance signal strength, directivity,
and interference suppression. We introduce a deep learning-based optimization
approach that enhances the design of sparse phased arrays by reducing grating
lobes. This approach begins by generating sparse array configurations to
address the non-convex challenges and extensive degrees of freedom inherent in
array design. We use neural networks to approximate the non-convex cost
function that estimates the energy ratio between the main and side lobes. This
differentiable approximation facilitates cost function minimization through
gradient descent, optimizing the antenna elements' coordinates and leading to
an improved layout. Additionally, we incorporate a tailored penalty mechanism
that includes various physical and design constraints into the optimization
process, enhancing its robustness and practical applicability. We demonstrate
the effectiveness of our method by applying it to the ten array configurations
with the lowest initial costs, achieving further cost reductions ranging from
411% to 643%, with an impressive average improvement of 552%. By significantly
reducing side lobe levels in antenna arrays, this breakthrough paves the way
for ultra-precise beamforming, enhanced interference mitigation, and
next-generation wireless and radar systems with unprecedented efficiency and
clarity.

</details>


### [27] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely,Otto Lamminpää,Steffen Mauceri,Sean M. R. Crowell,Christopher W. O'Dell,Gregory R. McGarragh*

Main category: cs.LG

TL;DR: 本论文提出一种基于扩散的算法，用于从卫星数据快速准确地估算温室气体浓度，提供更好的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 当前方法计算开销大、收敛问题多、不确定性估计不准，未来数据量剧增，需要高效算法以实现实时全球碳监测，支持气候政策。

Method: 提出扩散-based方法，灵活检索高斯或非高斯后验分布，应用于NASA OCO-2光谱仪。

Result: 相比Optimal Estimation，计算速度大幅提升，并提供更可靠的不确定性估计。

Conclusion: 这种方法有助于实现近连续实时全球监测温室气体，对气候政策制定具有重要影响。

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [28] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi,C. Martin-Barreiro,X. Cabezas*

Main category: cs.LG

TL;DR: 本论文提出一种结合Transformer和GRU的混合深度学习模型，提高加密货币价格预测准确性。


<details>
  <summary>Details</summary>
Motivation: 为了整合Transformer捕捉长程模式和GRU处理短程序列趋势的优势，提升金融预测性能。

Method: 开发混合模型应用于比特币和以太坊价格预测，使用历史数据与多种机器学习模型比较，并通过MSE、RMSE、MAE、MAPE等指标及统计测试评估。

Result: 混合模型在准确性指标上优于RBFN、GRNN、BiLSTM和BiGRU等模型。

Conclusion: 证明混合模型有效，支持其在加密货币市场实时决策和金融分析中的应用。

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>


### [29] [GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs](https://arxiv.org/abs/2504.17099)
*Martin Boeckling,Heiko Paulheim,Sarah Detzler*

Main category: cs.LG

TL;DR: 这篇论文提出了一种整合几何信息的RDF2Vec变体，用于学习位置感知的实体嵌入，并通过基准数据集评估证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中存在大量空间实体，但现有实体表示学习方法未充分考虑其几何信息。

Method: 引入RDF2Vec变体，通过从地理节点洪泛图扩展节点，并使用空间权重偏置图游走。

Result: 在多个基准数据集上，该方法优于非位置感知的RDF2Vec和GeoTransE。

Conclusion: 整合几何信息可提升实体嵌入性能，证明位置感知方法的重要性。

Abstract: Many knowledge graphs contain a substantial number of spatial entities, such
as cities, buildings, and natural landmarks. For many of these entities, exact
geometries are stored within the knowledge graphs. However, most existing
approaches for learning entity representations do not take these geometries
into account. In this paper, we introduce a variant of RDF2Vec that
incorporates geometric information to learn location-aware embeddings of
entities. Our approach expands different nodes by flooding the graph from
geographic nodes, ensuring that each reachable node is considered. Based on the
resulting flooded graph, we apply a modified version of RDF2Vec that biases
graph walks using spatial weights. Through evaluations on multiple benchmark
datasets, we demonstrate that our approach outperforms both non-location-aware
RDF2Vec and GeoTransE.

</details>


### [30] [Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks](https://arxiv.org/abs/2504.17109)
*Zhaobin Mo,Xiangyi Liao,Dominik A. Karbowski,Yanbing Wang*

Main category: cs.LG

TL;DR: 这篇论文使用时空图神经网络（ST-GNN）和Shapley值来识别交通拥堵前兆，并通过Interstate-24数据证明路网拓扑和急刹车是主要因素。


<details>
  <summary>Details</summary>
Motivation: 理解和预测交通拥堵前兆对于提高道路安全和交通流管理至关重要。

Method: 提出结合ST-GNN与Shapley值的创新方法，并扩展Shapley解释到时空设置，以识别和解释交通拥堵前兆。

Result: 在Interstate-24数据上演示，确认路网拓扑和急刹车是导致交通拥堵的主要因素。

Conclusion: 该方法成功桥接黑箱神经网络预测与可解释原因，提升了交通管理的安全性和效率。

Abstract: Understanding and predicting the precursors of traffic breakdowns is critical
for improving road safety and traffic flow management. This paper presents a
novel approach combining spatiotemporal graph neural networks (ST-GNNs) with
Shapley values to identify and interpret traffic breakdown precursors. By
extending Shapley explanation methods to a spatiotemporal setting, our proposed
method bridges the gap between black-box neural network predictions and
interpretable causes. We demonstrate the method on the Interstate-24 data, and
identify that road topology and abrupt braking are major factors that lead to
traffic breakdowns.

</details>


### [31] [Scalable Permutation-Aware Modeling for Temporal Set Prediction](https://arxiv.org/abs/2504.17140)
*Ashish Ranjan,Ayush Agarwal,Shalin Barot,Sushant Kumar*

Main category: cs.LG

TL;DR: 这篇论文提出一个可扩展的框架，用于时间序列集预测，通过使用置换等变和置换不变变换，减少计算开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂架构，计算开销大，影响可扩展性，因此需要更高效的解决方案。

Method: 引入一个新框架，利用置换等变和置换不变变换来高效建模集动态。

Result: 在多个公共基准上实验显示，该方法在评价指标上达到或超过最先进模型，同时显著减少训练和推理时间。

Conclusion: 该模型有效实现了高效和可扩展的时间序列集预测。

Abstract: Temporal set prediction involves forecasting the elements that will appear in
the next set, given a sequence of prior sets, each containing a variable number
of elements. Existing methods often rely on intricate architectures with
substantial computational overhead, which hampers their scalability. In this
work, we introduce a novel and scalable framework that leverages
permutation-equivariant and permutation-invariant transformations to
efficiently model set dynamics. Our approach significantly reduces both
training and inference time while maintaining competitive performance.
Extensive experiments on multiple public benchmarks show that our method
achieves results on par with or superior to state-of-the-art models across
several evaluation metrics. These results underscore the effectiveness of our
model in enabling efficient and scalable temporal set prediction.

</details>


### [32] [OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection](https://arxiv.org/abs/2504.17160)
*Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 本论文引入OUI指标，用于监控DNN训练动态，指导Weight Decay超参数选择，无需验证数据，提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 动机是改进超参数调优，特别是在Weight Decay上，监控过拟合和欠拟合，而不依赖验证集。

Method: 方法是通过引入OUI并在DenseNet-BC-100、EfficientNet-B0和ResNet-34上实验，监控训练过程。

Result: 结果显示OUI更快收敛，与更好泛化相关，帮助早期识别最佳WD值。

Conclusion: 结论是OUI可精确调优WD，提高性能，代码开源可复现。

Abstract: We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for
monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying
optimal regularization hyperparameters. Specifically, we validate that OUI can
effectively guide the selection of the Weight Decay (WD) hyperparameter by
indicating whether a model is overfitting or underfitting during training
without requiring validation data. Through experiments on DenseNet-BC-100 with
CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,
we show that maintaining OUI within a prescribed interval correlates strongly
with improved generalization and validation scores. Notably, OUI converges
significantly faster than traditional metrics such as loss or accuracy,
enabling practitioners to identify optimal WD (hyperparameter) values within
the early stages of training. By leveraging OUI as a reliable indicator, we can
determine early in training whether the chosen WD value leads the model to
underfit the training data, overfit, or strike a well-balanced trade-off that
maximizes validation scores. This enables more precise WD tuning for optimal
performance on the tested datasets and DNNs. All code for reproducing these
experiments is available at https://github.com/AlbertoFdezHdez/OUI.

</details>


### [33] [A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation](https://arxiv.org/abs/2504.17196)
*Jiawen Hou,Hao Wu*

Main category: cs.LG

TL;DR: 本论文提出TATSI方法，通过结合L2范数和smooth L1范数，提高交通速度数据插值的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 交通管理系统中，速度数据常因传感器故障等缺失，现有的L2范数方法鲁棒性不足。

Method: 提出TATSI，使用L2和smooth L1范数损失函数及SLF-NMU算法进行非负潜变量分析。

Result: 实验在三个真实数据集上显示，TATSI更准确捕捉时间模式，提供最佳缺失数据插值。

Conclusion: TATSI在交通速度数据插值中表现出高准确性和鲁棒性。

Abstract: In intelligent transportation systems (ITS), traffic management departments
rely on sensors, cameras, and GPS devices to collect real-time traffic data.
Traffic speed data is often incomplete due to sensor failures, data
transmission delays, or occlusions, resulting in missing speed data in certain
road segments. Currently, tensor decomposition based methods are extensively
utilized, they mostly rely on the $L_2$-norm to construct their learning
objectives, which leads to reduced robustness in the algorithms. To address
this, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which
combines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,
thereby achieving both high accuracy and robust performance in imputing missing
time-varying traffic speed data. TATSI adopts a single latent factor-dependent,
nonnegative, and multiplicative update (SLF-NMU) approach, which serves as an
efficient solver for performing nonnegative latent factor analysis (LFA) on a
tensor. Empirical studies on three real-world time-varying traffic speed
datasets demonstrate that, compared with state-of-the-art traffic speed
predictors, TATSI more precisely captures temporal patterns, thereby yielding
the most accurate imputations for missing traffic speed data.

</details>


### [34] [Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2504.17210)
*Junfei Wang,Darshana Upadhyay,Marzia Zaman,Pirathayini Srikantha*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于物理信息和Denoising Diffusion Probabilistic Models (DDPMs)的生成框架，用于合成可行的电力流数据，以解决智能电网中数据驱动模块对高质量数据的需求。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和操作约束，真实世界的数据往往有限，因此需要一种方法来合成高质量的电力流数据。

Method: 提出了一种基于DDPMs的生成框架，结合辅助训练和物理信息损失函数，确保生成的数据在统计上真实并符合电力系统可行性。

Result: 在IEEE 14-bus和30-bus基准系统上评估，模型能够捕捉关键分布特性、在分布外场景下泛化，并优于基线模型在可行性、多样性和统计特征准确性方面。

Conclusion: 这项工作突出了将生成模型集成到数据驱动的电力系统应用中的潜力。

Abstract: Many data-driven modules in smart grid rely on access to high-quality power
flow data; however, real-world data are often limited due to privacy and
operational constraints. This paper presents a physics-informed generative
framework based on Denoising Diffusion Probabilistic Models (DDPMs) for
synthesizing feasible power flow data. By incorporating auxiliary training and
physics-informed loss functions, the proposed method ensures that the generated
data exhibit both statistical fidelity and adherence to power system
feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark
systems, demonstrating its ability to capture key distributional properties and
generalize to out-of-distribution scenarios. Comparative results show that the
proposed model outperforms three baseline models in terms of feasibility,
diversity, and accuracy of statistical features. This work highlights the
potential of integrating generative modelling into data-driven power system
applications.

</details>


### [35] [Enhancing Variational Autoencoders with Smooth Robust Latent Encoding](https://arxiv.org/abs/2504.17219)
*Hyomin Lee,Minseon Kim,Sangwon Jang,Jongheon Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 本文引入SRL-VAE，通过对抗训练提升VAE的生成质量和鲁棒性，同时保持保真度。


<details>
  <summary>Details</summary>
Motivation: 对抗训练被忽略，因为可能降低生成模型的保真度，本文挑战这一观点，旨在证明它能同时提升质量和鲁棒性。

Method: 提出SRL-VAE框架，通过对抗扰动平滑潜在空间，促进泛化表示，并正则化以维持保真度，作为预训练VAE的后处理步骤。

Result: 实验显示SRL-VAE改善图像重建、文本引导编辑的质量和对特定攻击的鲁棒性，计算开销低。

Conclusion: 建立新范式，证明对抗训练能增强生成模型的保真度和鲁棒性。

Abstract: Variational Autoencoders (VAEs) have played a key role in scaling up
diffusion-based generative models, as in Stable Diffusion, yet questions
regarding their robustness remain largely underexplored. Although adversarial
training has been an established technique for enhancing robustness in
predictive models, it has been overlooked for generative models due to concerns
about potential fidelity degradation by the nature of trade-offs between
performance and robustness. In this work, we challenge this presumption,
introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training
framework that boosts both generation quality and robustness. In contrast to
conventional adversarial training, which focuses on robustness only, our
approach smooths the latent space via adversarial perturbations, promoting more
generalizable representations while regularizing with originality
representation to sustain original fidelity. Applied as a post-training step on
pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal
computational overhead. Experiments show that SRL-VAE improves both generation
quality, in image reconstruction and text-guided image editing, and robustness,
against Nightshade attacks and image editing attacks. These results establish a
new paradigm, showing that adversarial training, once thought to be detrimental
to generative models, can instead enhance both fidelity and robustness.

</details>


### [36] [Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification](https://arxiv.org/abs/2504.17232)
*Nivedita M,Yasmeen Shajitha S*

Main category: cs.LG

TL;DR: 这项研究提出了一种集成的机器学习框架，用于高级交通分析，结合时间序列预测、分类和计算机视觉技术。


<details>
  <summary>Details</summary>
Motivation: 为了改进智能交通系统，实现实时监控、事故预防和资源优化。

Method: 使用ARIMA(2,0,1)模型进行交通预测、XGBoost分类器进行事故严重程度分类、CNN进行交通图像分类。

Result: ARIMA模型MAE为2.1、XGBoost在平衡数据上准确率100%、CNN准确率92%、框架优于基线模型，并识别了天气和道路基础设施等影响因素。

Conclusion: 框架的模块化设计支持在智能城市系统中部署，促进智能交通系统的演进。

Abstract: This study proposes an integrated machine learning framework for advanced
traffic analysis, combining time-series forecasting, classification, and
computer vision techniques. The system utilizes an ARIMA(2,0,1) model for
traffic prediction (MAE: 2.1), an XGBoost classifier for accident severity
classification (100% accuracy on balanced data), and a Convolutional Neural
Network (CNN) for traffic image classification (92% accuracy). Tested on
diverse datasets, the framework outperforms baseline models and identifies key
factors influencing accident severity, including weather and road
infrastructure. Its modular design supports deployment in smart city systems
for real-time monitoring, accident prevention, and resource optimization,
contributing to the evolution of intelligent transportation systems.

</details>


### [37] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou,Simin Fan,Martin Jaggi,Jie Fu*

Main category: cs.LG

TL;DR: 本文提出 NeuralGrok，一种通过学习梯度变换加速 Transformer 在算术任务中泛化的新方法。


<details>
  <summary>Details</summary>
Motivation: 针对 Grokking 现象（过拟合后实现泛化），本文旨在加速泛化和提高训练稳定性。

Method: 使用辅助模块（如 MLP 块）动态调节梯度分量影响，采用双层优化算法。

Result: 实验显示 NeuralGrok 显著加速泛化，减少模型复杂性，并优于传统正则化方法；引入 Absolute Gradient Entropy (AGE) 指标解释泛化机制。

Conclusion: 提供 Transformer Grokking 现象的洞见，深化对泛化原理的理解。

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>


### [38] [Targeted AMP generation through controlled diffusion with efficient embeddings](https://arxiv.org/abs/2504.17247)
*Diogo Soares,Leon Hetzel,Paulina Szymczak,Fabian Theis,Stephan Günnemann,Ewa Szczurek*

Main category: cs.LG

TL;DR: 这篇论文介绍了OmegAMP框架，使用基于扩散的生成模型改进抗菌肽发现，提高可控性和减少假阳性，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习-based抗菌肽发现的挑战，如实验命中率低和对可控性及高效建模的需求。

Method: 采用扩散-based生成模型、低维嵌入、可控机制和减少假阳性率的分类器进行候选生成和过滤。

Result: 实现针对特定属性的抗菌肽生成，最大化多样性并保持数据分布忠实，表现优异。

Conclusion: 显著提升计算框架在对抗抗生素耐药性方面的潜力。

Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as low experimental hit rates as well as the need for nuanced
controllability and efficient modeling of peptide properties. To address these
challenges, we introduce OmegAMP, a framework that leverages a diffusion-based
generative model with efficient low-dimensional embeddings, precise
controllability mechanisms, and novel classifiers with drastically reduced
false positive rates for candidate filtering. OmegAMP enables the targeted
generation of AMPs with specific physicochemical properties, activity profiles,
and species-specific effectiveness. Moreover, it maximizes sample diversity
while ensuring faithfulness to the underlying data distribution during
generation. We demonstrate that OmegAMP achieves state-of-the-art performance
across all stages of the AMP discovery pipeline, significantly advancing the
potential of computational frameworks in combating antimicrobial resistance.

</details>


### [39] [Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation](https://arxiv.org/abs/2504.17709)
*Stefan Jonas,Angela Meyer*

Main category: cs.LG

TL;DR: 本论文提出使用CycleGAN-based domain mapping处理风力涡轮机数据稀缺问题，提高故障诊断性能。


<details>
  <summary>Details</summary>
Motivation: 减少风力涡轮机停机时间需要智能监测，但数据驱动模型需大量训练数据，数据稀缺会导致诊断不可靠。

Method: 采用CycleGAN-based domain mapping，使数据稀缺风力涡轮机的SCADA样本类似于有丰富训练数据的样本。

Result: 在7个不同风力涡轮机上测试，F1-score改善达+10.3%（1个月数据）和+16.8%（2周数据），优于传统微调。

Conclusion: 该方法能更早、更可靠地诊断故障，适用于新风电场，并开辟新研究方向。

Abstract: Intelligent condition monitoring of wind turbines is essential for reducing
downtimes. Machine learning models trained on wind turbine operation data are
commonly used to detect anomalies and, eventually, operation faults. However,
data-driven normal behavior models (NBMs) require a substantial amount of
training data, as NBMs trained with scarce data may result in unreliable fault
diagnosis. To overcome this limitation, we present a novel generative deep
learning approach to make SCADA samples from one wind turbine lacking training
data resemble SCADA data from wind turbines with representative training data.
Through CycleGAN-based domain mapping, our method enables the application of an
NBM trained on an existing wind turbine to one with severely limited data. We
demonstrate our approach on field data mapping SCADA samples across 7
substantially different WTs. Our findings show significantly improved fault
diagnosis in wind turbines with scarce data. Our method achieves the most
similar anomaly scores to an NBM trained with abundant data, outperforming NBMs
trained on scarce training data with improvements of +10.3% in F1-score when 1
month of training data is available and +16.8% when 2 weeks are available. The
domain mapping approach outperforms conventional fine-tuning at all considered
degrees of data scarcity, ranging from 1 to 8 weeks of training data. The
proposed technique enables earlier and more reliable fault diagnosis in newly
installed wind farms, demonstrating a novel and promising research direction to
improve anomaly detection when faced with training data scarcity.

</details>


### [40] [Group Downsampling with Equivariant Anti-aliasing](https://arxiv.org/abs/2504.17258)
*Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 本文推广了群等变CNN的降采样操作，引入抗混叠，改进了准确性和等变性。


<details>
  <summary>Details</summary>
Motivation: 为了将均匀降采样扩展到群等变架构中，以增加感受野、学习高级特征、减少内存/计算，并处理一般有限群信号。

Method: 提出算法选择子群进行降采样，并基于带限性研究抗混叠方法，推广了经典采样理论，对于循环群信号可恢复标准低通滤波和子采样。

Result: 实验显示在图像分类任务中，准确性提升，等变性更好地保留，模型大小减小。

Conclusion: 提出的降采样操作在G-等变网络中有效，提高了性能并保持了等变性。

Abstract: Downsampling layers are crucial building blocks in CNN architectures, which
help to increase the receptive field for learning high-level features and
reduce the amount of memory/computation in the model. In this work, we study
the generalization of the uniform downsampling layer for group equivariant
architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature
maps) on general finite groups with anti-aliasing. This involves the following:
(a) Given a finite group and a downsampling rate, we present an algorithm to
form a suitable choice of subgroup. (b) Given a group and a subgroup, we study
the notion of bandlimited-ness and propose how to perform anti-aliasing.
Notably, our method generalizes the notion of downsampling based on classical
sampling theory. When the signal is on a cyclic group, i.e., periodic, our
method recovers the standard downsampling of an ideal low-pass filter followed
by a subsampling operation. Finally, we conducted experiments on image
classification tasks demonstrating that the proposed downsampling operation
improves accuracy, better preserves equivariance, and reduces model size when
incorporated into G-equivariant networks

</details>


### [41] [Symbolic Representation for Any-to-Any Generative Tasks](https://arxiv.org/abs/2504.17261)
*Jiaqi Chen,Xiaoye Zhu,Yue Wang,Tianyang Liu,Xinhui Chen,Ying Chen,Chak Tou Leong,Yifei Ke,Joseph Liu,Yiwen Yuan,Julian McAuley,Li-jia Li*

Main category: cs.LG

TL;DR: 本论文提出了一种符号化生成任务描述语言和推理引擎，能够高效灵活地处理多模态任务。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型依赖大规模训练和隐式表示，计算成本高且灵活性差，因此本框架使用显式符号表示来解决这些问题。

Method: 框架包括函数、参数和拓扑逻辑三个核心元素，利用预训练语言模型无训练地映射自然语言指令到符号工作流。

Result: 在12个多模态生成任务上表现优异，内容质量匹配或优于现有最先进模型，并提供更高的效率、可编辑性和中断性。

Conclusion: 符号任务表示为生成AI提供了成本效益和可扩展的基础。

Abstract: We propose a symbolic generative task description language and a
corresponding inference engine capable of representing arbitrary multimodal
tasks as structured symbolic flows. Unlike conventional generative models that
rely on large-scale training and implicit neural representations to learn
cross-modal mappings, often at high computational cost and with limited
flexibility, our framework introduces an explicit symbolic representation
comprising three core primitives: functions, parameters, and topological logic.
Leveraging a pre-trained language model, our inference engine maps natural
language instructions directly to symbolic workflows in a training-free manner.
Our framework successfully performs over 12 diverse multimodal generative
tasks, demonstrating strong performance and flexibility without the need for
task-specific tuning. Experiments show that our method not only matches or
outperforms existing state-of-the-art unified models in content quality, but
also offers greater efficiency, editability, and interruptibility. We believe
that symbolic task representations provide a cost-effective and extensible
foundation for advancing the capabilities of generative AI.

</details>


### [42] [Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy](https://arxiv.org/abs/2504.17274)
*Siddharth Vishwanath,Jonathan Hehir*

Main category: cs.LG

TL;DR: 这篇论文探讨了在ε-边局部差分隐私下从图中恢复潜在信息的问题，并展示了如何通过调整统计推断实现有效恢复。


<details>
  <summary>Details</summary>
Motivation: 动机是处理图数据中边信息隐私问题，即使对数据管理者保密，使用局部差分隐私机制。

Method: 方法是通过分析隐私机制引起的几何扭曲，并针对广义随机点积图调整统计推断程序。

Result: 结果显示，可以一致恢复潜在位置，且近乎最优；还能恢复潜在位置的几何和拓扑信息。

Conclusion: 结论是，该框架扩展了隐私保护方法，适用于更丰富的模型和推理任务。

Abstract: We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.

</details>


### [43] [HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks](https://arxiv.org/abs/2504.17276)
*Ke-Jia Chen,Wenhui Mu,Zheng Liu*

Main category: cs.LG

TL;DR: 本论文提出HeRB方法，针对GNNs的结构不平衡问题，通过减少异质性和转移同质知识，实验显示在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: GNNs在图数据表示中进步显著，但结构不平衡问题未被先前解决方案充分考虑异质性，导致效果不足。

Method: 提出HeRB，包括异质性减少增强模块（减少类间边、增加类内边）和同质知识转移机制（从头节点向尾节点传递信息）。

Result: 实验结果显示HeRB在两个同质和六个异质数据集上性能优越，消融研究验证了两个组件的有效性。

Conclusion: HeRB通过处理异质性有效改善GNNs的结构不平衡问题，提升了整体性能。

Abstract: Recent research has witnessed the remarkable progress of Graph Neural
Networks (GNNs) in the realm of graph data representation. However, GNNs still
encounter the challenge of structural imbalance. Prior solutions to this
problem did not take graph heterophily into account, namely that connected
nodes process distinct labels or features, thus resulting in a deficiency in
effectiveness. Upon verifying the impact of heterophily on solving the
structural imbalance problem, we propose to rectify the heterophily first and
then transfer homophilic knowledge. To the end, we devise a method named HeRB
(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two
innovative components: 1) A heterophily-lessening augmentation module which
serves to reduce inter-class edges and increase intra-class edges; 2) A
homophilic knowledge transfer mechanism to convey homophilic information from
head nodes to tail nodes. Experimental results demonstrate that HeRB achieves
superior performance on two homophilic and six heterophilic benchmark datasets,
and the ablation studies further validate the efficacy of two proposed
components.

</details>


### [44] [ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders](https://arxiv.org/abs/2504.17277)
*Zongliang Ji,Andre Carlos Kajdacsy-Balla Amaral,Anna Goldenberg,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 本论文提出ExOSITO方法，通过off-policy学习和特权信息优化ICU实验室测试订购，减少成本并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: ICU实验室测试订购最小子集具有挑战性，需要平衡信息可用性和减少临床负担及成本，许多医院过度订购测试，旨在减轻资源和环境负担。

Method: 开发ExOSITO方法，结合off-policy学习和特权信息，将问题建模为因果bandit，使用离线数据和基于临床规则的奖励函数，整合临床知识与观察数据。

Result: 学到的政策函数提供可解释临床信息，减少成本而不遗漏重要测试，优于医生的政策和现有方法。

Conclusion: 该方法为临床医生提供可解释辅助工具，优化实验室测试订购，降低成本并提高效率。

Abstract: Ordering a minimal subset of lab tests for patients in the intensive care
unit (ICU) can be challenging. Care teams must balance between ensuring the
availability of the right information and reducing the clinical burden and
costs associated with each lab test order. Most in-patient settings experience
frequent over-ordering of lab tests, but are now aiming to reduce this burden
on both hospital resources and the environment. This paper develops a novel
method that combines off-policy learning with privileged information to
identify the optimal set of ICU lab tests to order. Our approach, EXplainable
Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO)
creates an interpretable assistive tool for clinicians to order lab tests by
considering both the observed and predicted future status of each patient. We
pose this problem as a causal bandit trained using offline data and a reward
function derived from clinically-approved rules; we introduce a novel learning
framework that integrates clinical knowledge with observational data to bridge
the gap between the optimal and logging policies. The learned policy function
provides interpretable clinical information and reduces costs without omitting
any vital lab orders, outperforming both a physician's policy and prior
approaches to this practical problem.

</details>


### [45] [The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes](https://arxiv.org/abs/2504.17300)
*Wencong You,Daniel Lowd*

Main category: cs.LG

TL;DR: 这篇论文提出了一种隐蔽的后门攻击方法，使文本分类器在触发器存在时预测预定义标签，但不易被人类检测，并通过人类评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击使用显眼触发器，易被人类标注者发现并过滤，因此需要开发更隐蔽的攻击以提高成功率。

Method: 作者提出AttrBkd框架，包括三种制作隐蔽触发属性的策略，如从基线攻击中提取细粒度属性，并进行人类评估来检验攻击的隐蔽性和有效性。

Result: 人类评估显示，AttrBkd比基线攻击具有更高攻击成功率和更低检测率，并揭示自动指标与人类判断不一致。

Conclusion: 后门攻击可通过设计隐蔽触发器实现逃避人类检测，同时保持高有效性。

Abstract: Backdoor attacks on text classifiers can cause them to predict a predefined
label when a particular "trigger" is present. Prior attacks often rely on
triggers that are ungrammatical or otherwise unusual, leading to conspicuous
attacks. As a result, human annotators, who play a critical role in curating
training data in practice, can easily detect and filter out these unnatural
texts during manual inspection, reducing the risk of such attacks. We argue
that a key criterion for a successful attack is for text with and without
triggers to be indistinguishable to humans. However, prior work neither
directly nor comprehensively evaluated attack subtlety and invisibility with
human involvement. We bridge the gap by conducting thorough human evaluations
to assess attack subtlety. We also propose \emph{AttrBkd}, consisting of three
recipes for crafting subtle yet effective trigger attributes, such as
extracting fine-grained attributes from existing baseline backdoor attacks. Our
human evaluations find that AttrBkd with these baseline-derived attributes is
often more effective (higher attack success rate) and more subtle (fewer
instances detected by humans) than the original baseline backdoor attacks,
demonstrating that backdoor attacks can bypass detection by being inconspicuous
and appearing natural even upon close inspection, while still remaining
effective. Our human annotation also provides information not captured by
automated metrics used in prior work, and demonstrates the misalignment of
these metrics with human judgment.

</details>


### [46] [Machine learning-based condition monitoring of powertrains in modern electric drives](https://arxiv.org/abs/2504.17305)
*Dinan Li,Panagiotis Kakosimos,Luca Peretti*

Main category: cs.LG

TL;DR: 本文利用工业驱动中的现有数据，开发了一个数据驱动的电力模块热模型，使用机器学习预测温度。


<details>
  <summary>Details</summary>
Motivation: 动机是提升工业系统的智能性，通过数据分析优化资产性能。

Method: 方法包括设计测试台，训练和验证各种机器学习模型（如线性模型和深度神经网络），在静态和动态操作条件下。

Result: 结果是通过评估指标，比较不同方法的性能，以确定适合工业嵌入式系统的模型。

Conclusion: 结论是，机器学习可以有效地估计电力模块温度，提高工业系统优化。

Abstract: The recent technological advances in digitalization have revolutionized the
industrial sector. Leveraging data analytics has now enabled the collection of
deep insights into the performance and, as a result, the optimization of
assets. Industrial drives, for example, already accumulate all the necessary
information to control electric machines. These signals include but are not
limited to currents, frequency, and temperature. Integrating machine learning
(ML) models responsible for predicting the evolution of those directly
collected or implicitly derived parameters enhances the smartness of industrial
systems even further. In this article, data already residing in most modern
electric drives has been used to develop a data-driven thermal model of a power
module. A test bench has been designed and used specifically for training and
validating the thermal digital twin undergoing various static and dynamic
operating profiles. Different approaches, from traditional linear models to
deep neural networks, have been implemented to emanate the best ML model for
estimating the case temperature of a power module. Several evaluation metrics
were then used to assess the investigated methods' performance and
implementation in industrial embedded systems.

</details>


### [47] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao,Qiang Zhang,Chenrong Li*

Main category: cs.LG

TL;DR: 这篇论文提出了一种不需要偏置标注的鲁棒学习方法，通过样本再加权平衡类别条件分布，减少虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 虚假相关性导致模型错误预测，是鲁棒泛化的关键挑战；现有方法依赖昂贵的偏置标注或资源密集型预训练模型。

Method: 重新定义虚假相关性为类别条件分布的不平衡，提出样本再加权策略减少虚假因素和标签之间的互信息。

Result: 实验显示方法性能优异，媲美依赖偏置监督的方法。

Conclusion: 该方法有效消除了对偏置标注的需要，提供了一种简单有效的鲁棒学习方案。

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>


### [48] [Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization](https://arxiv.org/abs/2504.17355)
*Xiaohan Huang,Dongjie Wang,Zhiyuan Ning,Ziyue Qiao,Qingqing Long,Haowei Zhu,Yi Du,Min Wu,Yuanchun Zhou,Meng Xiao*

Main category: cs.LG

TL;DR: 本文提出TCTO框架，通过图驱动的多代理强化学习自动优化特征工程，提高下游机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有框架忽略转换步骤间的动态依赖，导致效率低下，因此需开发更智能的自动特征工程方法。

Method: 使用协作多代理强化学习框架，基于演化交互图进行路径优化，包括图修剪、回溯和子图重用。

Result: 实验和案例研究显示TCTO在多种数据集上性能优越。

Conclusion: TCTO框架有效提升特征工程自动化、减少冗余并提高适应性。

Abstract: Feature transformation methods aim to find an optimal mathematical
feature-feature crossing process that generates high-value features and
improves the performance of downstream machine learning tasks. Existing
frameworks, though designed to mitigate manual costs, often treat feature
transformations as isolated operations, ignoring dynamic dependencies between
transformation steps. To address the limitations, we propose TCTO, a
collaborative multi-agent reinforcement learning framework that automates
feature engineering through graph-driven path optimization. The framework's
core innovation lies in an evolving interaction graph that models features as
nodes and transformations as edges. Through graph pruning and backtracking, it
dynamically eliminates low-impact edges, reduces redundant operations, and
enhances exploration stability. This graph also provides full traceability to
empower TCTO to reuse high-utility subgraphs from historical transformations.
To demonstrate the efficacy and adaptability of our approach, we conduct
comprehensive experiments and case studies, which show superior performance
across a range of datasets.

</details>


### [49] [Doubly Adaptive Social Learning](https://arxiv.org/abs/2504.17370)
*Marco Carpentiero,Virginia Bordignon,Vincenzo Matta,Ali H. Sayed*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为A²SL的策略，用于社会学习中处理动态漂移问题，确保代理在假设和模型变化时也能正确学习。


<details>
  <summary>Details</summary>
Motivation: 现有社会学习方法在动态漂移下可能导致错误决策，因此需要一种适应性的策略来跟踪变化。

Method: 提出A²SL策略，包括随机梯度下降更新跟踪决策模型漂移和适应性信念更新跟踪真实假设变化，通过两个适应参数控制错误概率。

Result: 证明了在足够小的适应参数下，代理能一致学习，错误假设概率收敛到与参数相关的值。

Conclusion: 通过理论分析和实验（合成数据和真实数据）验证了策略的有效性。

Abstract: In social learning, a network of agents assigns probability scores (beliefs)
to some hypotheses of interest, which rule the generation of local streaming
data observed by each agent. Belief formation takes place by means of an
iterative two-step procedure where: i) the agents update locally their beliefs
by using some likelihood model; and ii) the updated beliefs are combined with
the beliefs of the neighboring agents, using a pooling rule. This procedure can
fail to perform well in the presence of dynamic drifts, leading the agents to
incorrect decision making. Here, we focus on the fully online setting where
both the true hypothesis and the likelihood models can change over time. We
propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy,
which infuses social learning with the necessary adaptation capabilities. This
goal is achieved by exploiting two adaptation stages: i) a stochastic gradient
descent update to learn and track the drifts in the decision model; ii) and an
adaptive belief update to track the true hypothesis changing over time. These
stages are controlled by two adaptation parameters that govern the evolution of
the error probability for each agent. We show that all agents learn
consistently for sufficiently small adaptation parameters, in the sense that
they ultimately place all their belief mass on the true hypothesis. In
particular, the probability of choosing the wrong hypothesis converges to
values on the order of the adaptation parameters. The theoretical analysis is
illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$
strategy to a social learning problem in the online setting using real data.

</details>


### [50] [Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware](https://arxiv.org/abs/2504.17403)
*Hans Rosenberger,Rodrigo Fischer,Johanna S. Fröhlich,Ali Bereyhi,Ralf R. Müller*

Main category: cs.LG

TL;DR: 这篇论文提出了一种神经网络压缩方案，通过减少FPGA上推理的计算量来提高资源效率。


<details>
  <summary>Details</summary>
Motivation: 由于最先进的神经网络规模不断增大，实现资源高效的部署变得越来越重要。

Method: 通过结合正则化训练的剪枝、权重共享和线性计算编码（LCC），优化减少推理所需的加法运算，并针对硬件友好设计。

Result: 在简单多层感知器和大型深度神经网络如ResNet-34上实现了竞争性的性能。

Conclusion: 该方案有效降低了神经网络推理的计算需求，适用于各种规模的模型。

Abstract: As state of the art neural networks (NNs) continue to grow in size, their
resource-efficient implementation becomes ever more important. In this paper,
we introduce a compression scheme that reduces the number of computations
required for NN inference on reconfigurable hardware such as FPGAs. This is
achieved by combining pruning via regularized training, weight sharing and
linear computation coding (LCC). Contrary to common NN compression techniques,
where the objective is to reduce the memory used for storing the weights of the
NNs, our approach is optimized to reduce the number of additions required for
inference in a hardware-friendly manner. The proposed scheme achieves
competitive performance for simple multilayer perceptrons, as well as for large
scale deep NNs such as ResNet-34.

</details>


### [51] [Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks](https://arxiv.org/abs/2504.17421)
*Yang Liu,Bingjie Yan,Tianyuan Zou,Jianqing Zhang,Zixuan Gu,Jianbing Ding,Xidong Wang,Jingyi Li,Xiaozhou Ye,Ye Ouyang,Qiang Yang,Ya-Qin Zhang*

Main category: cs.LG

TL;DR: 这篇论文主张大语言模型（LLM）和小型模型（SM）协同工作，以加速LLM在私有领域的适应，并探讨挑战和机会，同时倡导行业驱动的研究。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLM需要大量数据和计算资源的问题，通过与SM协作来提高效率和针对特定领域。

Method: 方法包括探索各种模型协作策略，并识别潜在挑战和机会。

Result: 结果强调了这种协同方法可以解锁AI新潜力，并提出了多目标基准的建议。

Conclusion: 结论是推动以真实私有数据集和应用为基础的行业驱动研究。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
they require vast amounts of data and computational resources. In contrast,
smaller models (SMs), while less powerful, can be more efficient and tailored
to specific domains. In this position paper, we argue that taking a
collaborative approach, where large and small models work synergistically, can
accelerate the adaptation of LLMs to private domains and unlock new potential
in AI. We explore various strategies for model collaboration and identify
potential challenges and opportunities. Building upon this, we advocate for
industry-driven research that prioritizes multi-objective benchmarks on
real-world private datasets and applications.

</details>


### [52] [CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning](https://arxiv.org/abs/2504.17448)
*Jun Zhang,Jue Wang,Huan Li,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TL;DR: 本文提出CHASe方法，用于联邦主动学习（FAL），解决数据分布异质性和模型波动问题，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有FAL方法未考虑客户端数据分布异质性和参数波动，导致模型准确性下降。

Method: CHASe通过跟踪训练中决策边界的不一致性、用新对齐损失校准模型、以及数据冻结和子集采样机制来选择数据。

Result: 实验显示CHASe在有效性和效率上优于基线方法，在多种数据集、模型复杂度和异质设置中验证。

Conclusion: CHASe有效地解决了FAL中的异质性挑战，提升了模型性能。

Abstract: Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.

</details>


### [53] [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
*Jun Zhang,Jue Wang,Huan Li,Lidan Shou,Ke Chen,Gang Chen,Qin Xie,Guiming Xie,Xuejian Gong*

Main category: cs.LG

TL;DR: 简而言之，该论文提出HMI系统，通过分层知识管理在单个GPU上高效服务多达10,000个预训练语言模型，仅有微小准确率损失。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型计算需求高，需要专用硬件，尤其在多租户环境中，资源效率是一个重大挑战。

Method: 方法包括将PLM知识分为一般、领域特定和任务特定，构建分层PLM减少内存使用，通过知识树和参数交换管理知识，并优化系统如分层预取和批处理矩阵乘法。

Result: 实验结果显示，HMI可在单个GPU上服务10,000个hBERTs和hGPTs，仅有微不足道的准确率损失。

Conclusion: 该方法有效地提升了多租户环境中PLM的资源利用率，证明了分层知识管理的可行性。

Abstract: The significant computational demands of pretrained language models (PLMs),
which often require dedicated hardware, present a substantial challenge in
serving them efficiently, especially in multi-tenant environments. To address
this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant
Inference system, designed to manage tenants with distinct PLMs
resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM
knowledge into general, domain-specific, and task-specific. Leveraging insights
on knowledge acquisition across different model layers, we construct
hierarchical PLMs (hPLMs) by extracting and storing knowledge at different
levels, significantly reducing GPU memory usage per tenant. Secondly, we
establish hierarchical knowledge management for hPLMs generated by various
tenants in HMI. We manage domain-specific knowledge with acceptable storage
increases by constructing and updating domain-specific knowledge trees based on
frequency. We manage task-specific knowledge within limited GPU memory through
parameter swapping. Finally, we propose system optimizations to enhance
resource utilization and inference throughput. These include fine-grained
pipelining via hierarchical knowledge prefetching to overlap CPU and I/O
operations with GPU computations, and optimizing parallel implementations with
batched matrix multiplications. Our experimental results demonstrate that the
proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a
single GPU, with only a negligible compromise in accuracy.

</details>


### [54] [Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience](https://arxiv.org/abs/2504.17461)
*Vipin Singh,Tianheng Ling,Teodor Chiaburu,Felix Biessmann*

Main category: cs.LG

TL;DR: 本论文提出使用机器学习模型预测城市联合下水系统（CSS）行为，以应对气候变化引起的极端降雨，比较全局和本地模型的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 气候变化增加极端降雨频率，传统物理模型维护成本高且适应性差，机器学习提供更高效和灵活的替代方案。

Method: 提出一个协议评估神经网络架构，包括预测性能、模型复杂性、鲁棒性；比较全局模型（使用所有信息）和本地模型（仅用附近传感器数据）；引入错误模型评估对网络中断或攻击的弹性。

Result: 全局模型预测性能更高，但本地模型在去中心化场景中提供足够弹性；具有更长预测地平线的模型对数据扰动更鲁棒。

Conclusion: 这些发现有助于开发可解释和可靠的机器学习解决方案，促进可持续的城市废水管理，并提供GitHub实现。

Abstract: Climate change increases the frequency of extreme rainfall, placing a
significant strain on urban infrastructures, especially Combined Sewer Systems
(CSS). Overflows from overburdened CSS release untreated wastewater into
surface waters, posing environmental and public health risks. Although
traditional physics-based models are effective, they are costly to maintain and
difficult to adapt to evolving system dynamics. Machine Learning (ML)
approaches offer cost-efficient alternatives with greater adaptability. To
systematically assess the potential of ML for modeling urban infrastructure
systems, we propose a protocol for evaluating Neural Network architectures for
CSS time series forecasting with respect to predictive performance, model
complexity, and robustness to perturbations. In addition, we assess model
performance on peak events and critical fluctuations, as these are the key
regimes for urban wastewater management. To investigate the feasibility of
lightweight models suitable for IoT deployment, we compare global models, which
have access to all information, with local models, which rely solely on nearby
sensor readings. Additionally, to explore the security risks posed by network
outages or adversarial attacks on urban infrastructure, we introduce error
models that assess the resilience of models. Our results demonstrate that while
global models achieve higher predictive performance, local models provide
sufficient resilience in decentralized scenarios, ensuring robust modeling of
urban infrastructure. Furthermore, models with longer native forecast horizons
exhibit greater robustness to data perturbations. These findings contribute to
the development of interpretable and reliable ML solutions for sustainable
urban wastewater management. The implementation is available in our GitHub
repository.

</details>


### [55] [GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework](https://arxiv.org/abs/2504.17471)
*Yacine Belal,Mohamed Maouche,Sonia Ben Mokhtar,Anthony Simonet-Boulogne*

Main category: cs.LG

TL;DR: GRANITE 是一个框架，用于在动态稀疏图上进行鲁棒分散式学习，抵抗 Byzantine 攻击。


<details>
  <summary>Details</summary>
Motivation: 现有 Gossip Learning 方法在动态图上收敛快，但对 Byzantine 攻击的鲁棒性不足，尤其是当攻击者操纵 RPS 协议时。

Method: GRANITE 包括 History-aware Byzantine-resilient Peer Sampling (HaPS) 和 Adaptive Probabilistic Threshold (APT) 两个组件，HaPS 通过跟踪历史标识符减少对手影响，APT 通过估计 Byzantine 存在设置聚合阈值。

Result: GRANITE 在高达 30% Byzantine 节点时保持收敛，提高学习速度，并在比当前理论稀疏 9 倍的图上表现良好。

Conclusion: GRANITE 有效提升了 Gossip Learning 在 Byzantine 攻击下的鲁棒性和效率。

Abstract: Gossip Learning (GL) is a decentralized learning paradigm where users
iteratively exchange and aggregate models with a small set of neighboring
peers. Recent GL approaches rely on dynamic communication graphs built and
maintained using Random Peer Sampling (RPS) protocols. Thanks to graph
dynamics, GL can achieve fast convergence even over extremely sparse
topologies. However, the robustness of GL over dy- namic graphs to Byzantine
(model poisoning) attacks remains unaddressed especially when Byzantine nodes
attack the RPS protocol to scale up model poisoning. We address this issue by
introducing GRANITE, a framework for robust learning over sparse, dynamic
graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two
key components (i) a History-aware Byzantine-resilient Peer Sampling protocol
(HaPS), which tracks previously encountered identifiers to reduce adversarial
influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which
leverages an estimate of Byzantine presence to set aggregation thresholds with
formal guarantees. Empirical results confirm that GRANITE maintains convergence
with up to 30% Byzantine nodes, improves learning speed via adaptive filtering
of poisoned models and obtains these results in up to 9 times sparser graphs
than dictated by current theory.

</details>


### [56] [Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning](https://arxiv.org/abs/2504.17490)
*Mingqi Yuan,Qi Wang,Guozheng Ma,Bo Li,Xin Jin,Yunbo Wang,Xiaokang Yang,Wenjun Zeng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本论文引入Plasticine框架，用于基准测试深度强化学习中的可塑性优化，提供缓解方法、评估指标和学习场景。


<details>
  <summary>Details</summary>
Motivation: 开发终身学习代理对人工通用智能至关重要，但深度强化学习系统常因可塑性损失而适应能力下降，且缺乏统一基准和评估协议。

Method: 引入Plasticine开源框架，包含13种以上缓解方法、10种评估指标，以及从标准到开放式环境的非平稳学习场景。

Result: 框架使研究人员能系统量化可塑性损失、评估缓解策略，并分析不同情境下的可塑性动态。

Conclusion: Plasticine填补了深度强化学习可塑性优化基准的空白，可从GitHub获取。

Abstract: Developing lifelong learning agents is crucial for artificial general
intelligence. However, deep reinforcement learning (RL) systems often suffer
from plasticity loss, where neural networks gradually lose their ability to
adapt during training. Despite its significance, this field lacks unified
benchmarks and evaluation protocols. We introduce Plasticine, the first
open-source framework for benchmarking plasticity optimization in deep RL.
Plasticine provides single-file implementations of over 13 mitigation methods,
10 evaluation metrics, and learning scenarios with increasing non-stationarity
levels from standard to open-ended environments. This framework enables
researchers to systematically quantify plasticity loss, evaluate mitigation
strategies, and analyze plasticity dynamics across different contexts. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/Plasticine.

</details>


### [57] [Prototype-enhanced prediction in graph neural networks for climate applications](https://arxiv.org/abs/2504.17492)
*Nawid Keshtmand,Elena Fillola,Jeffrey Nicholas Clark,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.LG

TL;DR: 这篇论文提出使用原型来改进数据驱动仿真器的方法，通过将输出近似作为输入提升预测质量，应用于大气扩散模拟以监测温室气体排放。


<details>
  <summary>Details</summary>
Motivation: 动机是减少物理模拟的计算开销和运行时间，同时提高高维输出质量，特别是针对温室气体排放监测。

Method: 方法涉及将原型作为额外输入，与基线模型比较；原型可随机或通过k-means等数据驱动方式选择。

Result: 结果显示，使用原型模型性能更好，即使原型少或随机；k-means选择可提高某些指标近10%。

Conclusion: 结论是，原型方法能显著改善仿真预测，数据驱动选择进一步增强效果。

Abstract: Data-driven emulators are increasingly being used to learn and emulate
physics-based simulations, reducing computational expense and run time. Here,
we present a structured way to improve the quality of these high-dimensional
emulated outputs, through the use of prototypes: an approximation of the
emulator's output passed as an input, which informs the model and leads to
better predictions. We demonstrate our approach to emulate atmospheric
dispersion, key for greenhouse gas emissions monitoring, by comparing a
baseline model to models trained using prototypes as an additional input. The
prototype models achieve better performance, even with few prototypes and even
if they are chosen at random, but we show that choosing the prototypes through
data-driven methods (k-means) can lead to almost 10\% increased performance in
some metrics.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [58] [Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline](https://arxiv.org/abs/2504.16979)
*Masoud Tafavvoghi,Lars Ailo Bongo,André Berli Delgado,Nikita Shvetsov,Anders Sildnes,Line Moi,Lill-Tove Rasmussen Busund,Kajsa Møllersen*

Main category: q-bio.QM

TL;DR: 本研究在QuPath中构建了一个端到端TILs评估管道，使用像素分类器和StarDist模型自动评估乳腺癌H&E染色全滑图像中的TILs，与病理学家评分一致，Cohen's kappa为0.71。


<details>
  <summary>Details</summary>
Motivation: 展示易于访问的工具可以自动执行复杂任务，以简化TILs评估。

Method: 训练像素分类器分割肿瘤、肿瘤相关基质和其他组织；应用预训练StarDist模型检测细胞；训练二元分类器区分TILs；计算TIL密度并分类为低、中、高水平。

Result: 在外部测试集上，Cohen's kappa为0.71，与病理学家评分一致。

Conclusion: 确认现有软件可以为乳腺癌H&E染色WSI中的TILs评估提供实用解决方案。

Abstract: In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)
assessment pipeline within QuPath, demonstrating the potential of easily
accessible tools to perform complex tasks in a fully automatic fashion. First,
we trained a pixel classifier to segment tumor, tumor-associated stroma, and
other tissue compartments in breast cancer H&E-stained whole-slide images (WSI)
to isolate tumor-associated stroma for subsequent analysis. Next, we applied a
pre-trained StarDist deep learning model in QuPath for cell detection and used
the extracted cell features to train a binary classifier distinguishing TILs
from other cells. To evaluate our TILs assessment pipeline, we calculated the
TIL density in each WSI and categorized them as low, medium, or high TIL
levels. Our pipeline was evaluated against pathologist-assigned TIL scores,
achieving a Cohen's kappa of 0.71 on the external test set, corroborating
previous research findings. These results confirm that existing software can
offer a practical solution for the assessment of TILs in H&E-stained WSIs of
breast cancer.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)
*Zhenhailong Wang,Senthil Purushwalkam,Caiming Xiong,Silvio Savarese,Heng Ji,Ran Xu*

Main category: cs.CV

TL;DR: 本论文提出DyMU框架，动态减少视觉语言模型计算负担，无需训练，保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉变换器固定长度输出低效问题，提高模型适应性和效率。

Method: 包括动态令牌合并(DToMe)基于图像复杂度合并相似令牌，以及虚拟令牌取消合并(VTU)重建注意力动态模拟完整序列。

Result: 实验显示视觉令牌减少32%-85%，在图像和视频任务中性能与完整模型相当，适用于多种架构。

Conclusion: 方法训练免费、动态适应，提供用户计算成本控制，提升VLM实用性。

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>


### [60] [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)
*Rishav Pramanik,Antoine Poupon,Juan A. Rodriguez,Masih Aminbeidokhti,David Vazquez,Christopher Pal,Zhaozheng Yin,Marco Pedersoli*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，通过任意顺序生成图像patch来推断内容和位置，并微调模型，提高图像质量，优于传统raster-scan方法。


<details>
  <summary>Details</summary>
Motivation: 传统raster-scan顺序不尊重图像内容的因果关系，例如生成日落场景时可能先生成云后生成太阳，但云的颜色应依赖太阳的颜色。

Method: 首先训练模型在任意给定顺序生成patch，然后推断每个patch的内容和位置，其次使用这些顺序微调模型以提升图像质量。

Result: 在两个数据集上，实验显示新方法产生更好图像质量，训练成本类似，且无需额外标注。

Conclusion: 新方法证明了任意顺序生成patch的优越性，能在类似成本下提高图像生成性能。

Abstract: Autoregressive patch-based image generation has recently shown competitive
results in terms of image quality and scalability. It can also be easily
integrated and scaled within Vision-Language models. Nevertheless,
autoregressive models require a defined order for patch generation. While a
natural order based on the dictation of the words makes sense for text
generation, there is no inherent generation order that exists for image
generation. Traditionally, a raster-scan order (from top-left to bottom-right)
guides autoregressive image generation models. In this paper, we argue that
this order is suboptimal, as it fails to respect the causality of the image
content: for instance, when conditioned on a visual description of a sunset, an
autoregressive model may generate clouds before the sun, even though the color
of clouds should depend on the color of the sun and not the inverse. In this
work, we show that first by training a model to generate patches in
any-given-order, we can infer both the content and the location (order) of each
patch during generation. Secondly, we use these extracted orders to finetune
the any-given-order model to produce better-quality images. Through our
experiments, we show on two datasets that this new generation method produces
better images than the traditional raster-scan approach, with similar training
costs and no extra annotations.

</details>


### [61] [A Comprehensive Review on RNA Subcellular Localization Prediction](https://arxiv.org/abs/2504.17162)
*Cece Zhang,Xuehuan Zhu,Nick Peterson,Jieqiong Wang,Shibiao Wan*

Main category: cs.CV

TL;DR: 这篇论文综述了基于AI的方法预测RNA亚细胞定位，涵盖各种RNA类型和序列、图像、混合方法，并讨论挑战与机会。


<details>
  <summary>Details</summary>
Motivation: RNA定位对生物功能至关重要，传统方法耗时且昂贵，因此需要AI/ML提供高效预测。

Method: 回顾AI-based方法，包括序列、图像和混合方法，用于RNA定位预测。

Result: AI方法可加速RNA研究、揭示分子途径并指导疾病治疗；讨论数据稀缺等挑战。

Conclusion: 旨在为研究人员提供资源，开发RNA定位创新解决方案。

Abstract: The subcellular localization of RNAs, including long non-coding RNAs
(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,
plays a critical role in determining their biological functions. For instance,
lncRNAs are predominantly associated with chromatin and act as regulators of
gene transcription and chromatin structure, while mRNAs are distributed across
the nucleus and cytoplasm, facilitating the transport of genetic information
for protein synthesis. Understanding RNA localization sheds light on processes
like gene expression regulation with spatial and temporal precision. However,
traditional wet lab methods for determining RNA localization, such as in situ
hybridization, are often time-consuming, resource-demanding, and costly. To
overcome these challenges, computational methods leveraging artificial
intelligence (AI) and machine learning (ML) have emerged as powerful
alternatives, enabling large-scale prediction of RNA subcellular localization.
This paper provides a comprehensive review of the latest advancements in
AI-based approaches for RNA subcellular localization prediction, covering
various RNA types and focusing on sequence-based, image-based, and hybrid
methodologies that combine both data types. We highlight the potential of these
methods to accelerate RNA research, uncover molecular pathways, and guide
targeted disease treatments. Furthermore, we critically discuss the challenges
in AI/ML approaches for RNA subcellular localization, such as data scarcity and
lack of benchmarks, and opportunities to address them. This review aims to
serve as a valuable resource for researchers seeking to develop innovative
solutions in the field of RNA subcellular localization and beyond.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [62] [Parameter Estimation in ODE Models with Certified Polynomial System Solving](https://arxiv.org/abs/2504.17268)
*Alexander Demin,Alexey Ovchinnikov,Fabrice Rouillier*

Main category: cs.SC

TL;DR: 这篇论文比较了两种多项式求解器在理性ODE系统参数估计中的性能：homotopy continuation和基于RUR的认证求解器。


<details>
  <summary>Details</summary>
Motivation: 参数估计是理性ODE系统中的重要且具有挑战性的任务，需要从观察数据中恢复参数值。

Method: 使用基于微分代数和理性插值的参数估计方法，并比较HomotopyContinuation.jl的homotopy continuation求解器和基于理性单变量表示(RUR)的认证求解器。

Result: 结果显示，RUR求解器可以处理一些homotopy方法无法解决的例子，反之亦然。

Conclusion: 这强调了在多项式系统求解中选择适当求解器的必要性，以提高参数估计的效率和可行性。

Abstract: We consider dynamical models given by rational ODE systems. Parameter
estimation is an important and challenging task of recovering parameter values
from observed data. Recently, a method based on differential algebra and
rational interpolation was proposed to express parameter estimation in terms of
polynomial system solving. Typically, polynomial system solving is a
bottleneck, hence the choice of the polynomial solver is crucial. In this
contribution, we compare two polynomial system solvers applied to parameter
estimation: homotopy continuation solver from HomotopyContinuation.jl and our
new implementation of a certified solver based on rational univariate
representation (RUR) and real root isolation. We show how the new RUR solver
can tackle examples that are out of reach for the homotopy methods and vice
versa.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [63] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)
*Felix Kares,Timo Speith,Hanwei Zhang,Markus Langer*

Main category: cs.HC

TL;DR: 本研究比较了LIME、Grad-CAM和Guided Backpropagation三种显著性图方法，使用主观、客观和数学评估，发现这些方法在评估结果上不一致，并讨论了XAI的含义。


<details>
  <summary>Details</summary>
Motivation: 解决如何最好地评估显著性图的开放问题，通过比较不同评估方法。

Method: 采用166名参与者的组间研究，测试三种显著性图方法在主观（信任、满意度）、客观（能力提升）和数学指标方面的表现。

Result: 在信任和满意度方面无差异；Grad-CAM在提升用户能力方面最佳；Guided Backpropagation在数学指标方面最优；一些指标与用户理解相关，但往往与直觉相反。

Conclusion: 评估方法之间没有一致性，强调在XAI中需要互补使用用户研究和数学指标。

Abstract: Saliency maps are a popular approach for explaining classifications of
(convolutional) neural networks. However, it remains an open question as to how
best to evaluate salience maps, with three families of evaluation methods
commonly being used: subjective user measures, objective user measures, and
mathematical metrics. We examine three of the most popular saliency map
approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between
subject study (N=166) across these families of evaluation methods. We test 1)
for subjective measures, if the maps differ with respect to user trust and
satisfaction; 2) for objective measures, if the maps increase users' abilities
and thus understanding of a model; 3) for mathematical metrics, which map
achieves the best ratings across metrics; and 4) whether the mathematical
metrics can be associated with objective user measures. To our knowledge, our
study is the first to compare several salience maps across all these evaluation
methods$-$with the finding that they do not agree in their assessment (i.e.,
there was no difference concerning trust and satisfaction, Grad-CAM improved
users' abilities best, and Guided Backpropagation had the most favorable
mathematical metrics). Additionally, we show that some mathematical metrics
were associated with user understanding, although this relationship was often
counterintuitive. We discuss these findings in light of general debates
concerning the complementary use of user studies and mathematical metrics in
the evaluation of explainable AI (XAI) approaches.

</details>


### [64] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)
*Ayushi Agrawal,Aditya Kondai,Kavita Vemuri*

Main category: cs.HC

TL;DR: 本研究发现AI面部评估工具会增加自物化和负面情绪，并存在性别差异。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对自物化、自尊和情绪的心理影响，以及性别差异。

Method: 使用两个样本测试批评性和中性版本的工具，通过量表和统计分析（如U检验）评估。

Result: 高自物化与低自尊相关，工具引发负面情绪，女性更易数字增强和感知社会情绪。

Conclusion: AI工具可能强化社会偏见，需要负责任设计，未来研究数据意识形态影响。

Abstract: AI-powered facial assessment tools are reshaping how individuals evaluate
appearance and internalize social judgments. This study examines the
psychological impact of such tools on self-objectification, self-esteem, and
emotional responses, with attention to gender differences. Two samples used
distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9
years), and another more neutral (N=51; M=19.9 years). Participants completed
validated self-objectification and self-esteem scales and custom items
measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and
perceived social emotion (PSE). Results revealed consistent links between high
self-objectification, low self-esteem, and increased appearance enhancement
behaviors across both versions. Despite softer framing, the newer tool still
evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit
feedback may reinforce appearance-related insecurities. Gender differences
emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital
enhancement and less likely to perceive emotional impact in others. These
findings reveal how AI tools may unintentionally reinforce and amplify existing
social biases and underscore the critical need for responsible AI design and
development. Future research will investigate how human ideologies embedded in
the training data of such tools shape their evaluative outputs, and how these,
in turn, influence user attitudes and decisions.

</details>


### [65] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)
*Robert Kaufman*

Main category: cs.HC

TL;DR: 这篇论文通过三个实证研究探讨自动驾驶车辆如何通过透明、可适应和个性化的通信系统满足骑手需求，提升信任和性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆如何满足骑手资讯需求的未解决问题，考虑到不同人、目标和情境的差异，以及当前研究设计的统一性不足。

Method: 使用三个实证研究：1. 识别最佳通信策略；2. 分析故障通信的后果；3. 运用机器学习预测信任的个人因素。

Result: 发现需要任务敏感、模态适当的通信；强调上下文和个人因素的重要性，支持个性化AV设计。

Conclusion: 主张开发透明、可适应和个性化的AV系统，为设计师、研究者和政策制定者提供洞见，并指导未来人机交互研究。

Abstract: Unresolved questions about how autonomous vehicles (AVs) should meet the
informational needs of riders hinder real-world adoption. Complicating our
ability to satisfy rider needs is that different people, goals, and driving
contexts have different criteria for what constitutes interaction success.
Unfortunately, most human-AV research and design today treats all people and
situations uniformly. It is crucial to understand how an AV should communicate
to meet rider needs, and how communications should change when the human-AV
complex system changes. I argue that understanding the relationships between
different aspects of the human-AV system can help us build improved and
adaptable AV communications. I support this argument using three empirical
studies. First, I identify optimal communication strategies that enhance
driving performance, confidence, and trust for learning in extreme driving
environments. Findings highlight the need for task-sensitive,
modality-appropriate communications tuned to learner cognitive limits and
goals. Next, I highlight the consequences of deploying faulty communication
systems and demonstrate the need for context-sensitive communications. Third, I
use machine learning (ML) to illuminate personal factors predicting trust in
AVs, emphasizing the importance of tailoring designs to individual traits and
concerns. Together, this dissertation supports the necessity of transparent,
adaptable, and personalized AV systems that cater to individual needs, goals,
and contextual demands. By considering the complex system within which human-AV
interactions occur, we can deliver valuable insights for designers,
researchers, and policymakers. This dissertation also provides a concrete
domain to study theories of human-machine joint action and situational
awareness, and can be used to guide future human-AI interaction research.
[shortened for arxiv]

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [66] [S2Vec: Self-Supervised Geospatial Embeddings](https://arxiv.org/abs/2504.16942)
*Shushman Choudhury,Elad Aharoni,Chandrakumari Suvarna,Iveel Tsogsuren,Abdul Rahman Kreidieh,Chun-Ta Lu,Neha Arora*

Main category: cs.SI

TL;DR: 这篇论文引入S2Vec框架，通过自监督学习生成通用的地理空间嵌入，提升地理空间人工智能应用性能。


<details>
  <summary>Details</summary>
Motivation: 可扩展的通用建成环境表示对于地理空间人工智能应用至关重要。

Method: S2Vec使用S2 Geometry库将区域划分为S2单元，将特征栅格化为图像，并应用掩码自动编码学习嵌入。

Result: 在三个社会经济预测任务上，S2Vec性能与最先进图像嵌入竞争相当，结合多模态融合可提升性能。

Conclusion: 结果突显S2Vec能有效学习地理空间表示，并与其他数据模式互补。

Abstract: Scalable general-purpose representations of the built environment are crucial
for geospatial artificial intelligence applications. This paper introduces
S2Vec, a novel self-supervised framework for learning such geospatial
embeddings. S2Vec uses the S2 Geometry library to partition large areas into
discrete S2 cells, rasterizes built environment feature vectors within cells as
images, and applies masked autoencoding on these rasterized images to encode
the feature vectors. This approach yields task-agnostic embeddings that capture
local feature characteristics and broader spatial relationships. We evaluate
S2Vec on three large-scale socioeconomic prediction tasks, showing its
competitive performance against state-of-the-art image-based embeddings. We
also explore the benefits of combining S2Vec embeddings with image-based
embeddings downstream, showing that such multimodal fusion can often improve
performance. Our results highlight how S2Vec can learn effective
general-purpose geospatial representations and how it can complement other data
modalities in geospatial artificial intelligence.

</details>


### [67] [MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation](https://arxiv.org/abs/2504.16946)
*Xiaotong Ye,Nicolas Bougie,Toshihiko Yamasaki,Narimasa Watanabe*

Main category: cs.SI

TL;DR: 本论文提出了一种可扩展的城市代理模拟框架，解决了现有方法在交通选择简化与计算资源需求上的问题，通过虚拟城市和调查实现对4000多个代理的真实行为模拟，并进行多层次分析。


<details>
  <summary>Details</summary>
Motivation: 现有生成式代理方法在模拟城市交通选择时过于简化，且计算资源需求过高，无法有效处理大规模人口模拟。

Method: 构建一个包含多种功能建筑和交通方式的虚拟城市；通过广泛调查建模行为选择和流动性偏好；引入一个可扩展的模拟框架来模拟代理行为。

Result: 成功模拟超过4000个代理；通过微观和宏观分析评估行为真实性；实验包括从运动模式预测人群密度和识别代理群体中的车辆偏好趋势。

Conclusion: 该框架提高了城市行为模拟的真实性和可扩展性，并通过实验提供了对城市动态和趋势的洞见。

Abstract: Generative agents offer promising capabilities for simulating realistic urban
behaviors. However, existing methods oversimplify transportation choices in
modern cities, and require prohibitive computational resources for large-scale
population simulation. To address these limitations, we first present a virtual
city that features multiple functional buildings and transportation modes.
Then, we conduct extensive surveys to model behavioral choices and mobility
preferences among population groups. Building on these insights, we introduce a
simulation framework that captures the complexity of urban mobility while
remaining scalable, enabling the simulation of over 4,000 agents. To assess the
realism of the generated behaviors, we perform a series of micro and
macro-level analyses. Beyond mere performance comparison, we explore insightful
experiments, such as predicting crowd density from movement patterns and
identifying trends in vehicle preferences across agent demographics.

</details>


### [68] [SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments](https://arxiv.org/abs/2504.16947)
*Dachun Sun,You Lyu,Jinning Li,Yizhuo Chen,Tianshi Wang,Tomoyoshi Kimura,Tarek Abdelzaher*

Main category: cs.SI

TL;DR: 这篇论文介绍了SCRAG框架，它结合大型语言模型和检索增强生成技术来预测社区对社交媒体帖子的响应。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大型语言模型依赖静态数据和产生幻觉的问题，以在动态社交媒体环境中提供更准确的社区响应预测。

Method: 方法是通过整合LLM与RAG技术，检索目标社区的历史响应和外部知识（如新闻文章），来预测新帖子的响应。

Result: 结果显示，在X平台上的六种场景实验中，平均提高了10%以上的关键评估指标，并通过示例证明了其捕捉多样意识形态和细微差别的有效性。

Conclusion: 结论是这项工作提供了一个社会计算工具，用于需要准确社区响应洞察的应用。

Abstract: This paper introduces SCRAG, a prediction framework inspired by social
computing, designed to forecast community responses to real or hypothetical
social media posts. SCRAG can be used by public relations specialists (e.g., to
craft messaging in ways that avoid unintended misinterpretations) or public
figures and influencers (e.g., to anticipate social responses), among other
applications related to public sentiment prediction, crisis management, and
social what-if analysis. While large language models (LLMs) have achieved
remarkable success in generating coherent and contextually rich text, their
reliance on static training data and susceptibility to hallucinations limit
their effectiveness at response forecasting in dynamic social media
environments. SCRAG overcomes these challenges by integrating LLMs with a
Retrieval-Augmented Generation (RAG) technique rooted in social computing.
Specifically, our framework retrieves (i) historical responses from the target
community to capture their ideological, semantic, and emotional makeup, and
(ii) external knowledge from sources such as news articles to inject
time-sensitive context. This information is then jointly used to forecast the
responses of the target community to new posts or narratives. Extensive
experiments across six scenarios on the X platform (formerly Twitter), tested
with various embedding models and LLMs, demonstrate over 10% improvements on
average in key evaluation metrics. A concrete example further shows its
effectiveness in capturing diverse ideologies and nuances. Our work provides a
social computing tool for applications where accurate and concrete insights
into community responses are crucial.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [69] [Singular Arcs in Optimal Control: Closed-loop Implementations without Workarounds](https://arxiv.org/abs/2504.17093)
*Nikilesh Ramesh,Ross Drummond,Pablo Rodolfo Baldivieso Monasterios,Yuanbo Nie*

Main category: math.OC

TL;DR: 本文展示了使用IRM方法解决带有奇异弧的优化控制问题，无需特殊处理，在经济模型预测控制框架下取得了可靠的结果。


<details>
  <summary>Details</summary>
Motivation: 优化控制问题中奇异弧处理复杂，尤其在线闭环应用中不切实际，需要更有效的解决方案。

Method: 采用Integrated Residual Methods (IRM)来求解优化控制问题，并抑制奇异弧引起的波动。

Result: IRM无需特殊处理奇异弧，能够可靠求解问题，抑制波动，且闭环结果与解析最优解相符。

Conclusion: IRM是处理奇异弧优化控制问题的一种高效方法，适用于实际工程应用。

Abstract: Singular arcs emerge in the solutions of Optimal Control Problems (OCPs) when
the optimal inputs on some finite time intervals cannot be directly obtained
via the optimality conditions. Solving OCPs with singular arcs often requires
tailored treatments, suitable for offline trajectory optimization. This
approach can become increasingly impractical for online closed-loop
implementations, especially for large-scale engineering problems. Recent
development of Integrated Residual Methods (IRM) have indicated their
suitability for handling singular arcs; the convergence of error measures in
IRM automatically suppresses singular arc-induced fluctuations and leads to
non-fluctuating solutions more suitable for practical problems. Through several
examples, we demonstrate the advantages of solving OCPs with singular arcs
using {IRM} under an economic model predictive control framework. In
particular, the following observations are made: (i) IRM does not require
special treatment for singular arcs, (ii) it solves the OCPs reliably with
singular arc fluctuation suppressed, and (iii) the closed-loop results closely
match the analytic optimal solutions.

</details>


### [70] [Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems](https://arxiv.org/abs/2504.17102)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: math.OC

TL;DR: 本文提出了一种学习和验证神经网络收缩度量的方法，用于离散时间非线性动力系统，解决了非光滑动态挑战，并通过示例验证。


<details>
  <summary>Details</summary>
Motivation: 收缩度量在控制理论中对稳定性、鲁棒性和收敛性分析至关重要，但复杂非线性系统缺乏可扩展工具，尤其神经网络控制器引入非光滑动态。

Method: 建立了新的充分条件，仅假设动态连续性，使用α,β-CROWN验证器高效验证，并开发从采样数据学习合成神经收缩度量的方法。

Result: 成功为各种非线性示例合成和验证了神经网络收缩度量。

Conclusion: 该方法有效，并通过示例验证了其可行性。

Abstract: Contraction metrics are crucial in control theory because they provide a
powerful framework for analyzing stability, robustness, and convergence of
various dynamical systems. However, identifying these metrics for complex
nonlinear systems remains an open challenge due to the lack of scalable and
effective tools. This paper explores the approach of learning verifiable
contraction metrics parametrized as neural networks (NNs) for discrete-time
nonlinear dynamical systems. While prior works on formal verification of
contraction metrics for general nonlinear systems have focused on convex
optimization methods (e.g. linear matrix inequalities, etc) under the
assumption of continuously differentiable dynamics, the growing prevalence of
NN-based controllers, often utilizing ReLU activations, introduces challenges
due to the non-smooth nature of the resulting closed-loop dynamics. To bridge
this gap, we establish a new sufficient condition for establishing formal
neural contraction metrics for general discrete-time nonlinear systems assuming
only the continuity of the dynamics. We show that from a computational
perspective, our sufficient condition can be efficiently verified using the
state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN, which scales
up non-convex neural network verification via novel integration of symbolic
linear bound propagation and branch-and-bound. Built upon our analysis tool, we
further develop a learning method for synthesizing neural contraction metrics
from sampled data. Finally, our approach is validated through the successful
synthesis and verification of NN contraction metrics for various nonlinear
examples.

</details>


### [71] [Advancing Frontiers of Path Integral Theory for Stochastic Optimal Control](https://arxiv.org/abs/2504.17154)
*Apurva Patil*

Main category: math.OC

TL;DR: 本论文使用路径积分控制框架，通过采样方法高效解决高维随机最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法如动态规划在高维非线性系统中因维数灾难而难以处理，针对不确定性系统提出可扩展替代方案。

Method: 重构SOC问题为随机轨迹期望，利用Monte Carlo采样和GPU并行化实现策略合成。

Result: 应用于六类SOC问题，包括机会约束SOC等，并提供离散时间样本复杂度分析。

Conclusion: 为复杂不确定环境中的模拟器驱动自治奠定基础。

Abstract: Stochastic Optimal Control (SOC) problems arise in systems influenced by
uncertainty, such as autonomous robots or financial models. Traditional methods
like dynamic programming are often intractable for high-dimensional, nonlinear
systems due to the curse of dimensionality. This dissertation explores the path
integral control framework as a scalable, sampling-based alternative. By
reformulating SOC problems as expectations over stochastic trajectories, it
enables efficient policy synthesis via Monte Carlo sampling and supports
real-time implementation through GPU parallelization.
  We apply this framework to six classes of SOC problems: Chance-Constrained
SOC, Stochastic Differential Games, Deceptive Control, Task Hierarchical
Control, Risk Mitigation of Stealthy Attacks, and Discrete-Time LQR. A sample
complexity analysis for the discrete-time case is also provided. These
contributions establish a foundation for simulator-driven autonomy in complex,
uncertain environments.

</details>


### [72] [Obtaining Structural Network Controllability with Higher-Order Local Dynamics](https://arxiv.org/abs/2504.17417)
*Marco Peruzzo,Giacomo Baggio,Francesco Ticozzi*

Main category: math.OC

TL;DR: This paper studies how replacing some first-order systems with higher-order ones in a network improves controllability, showing fewer modifications are needed and identifying networks where homogeneous dynamics suffice.


<details>
  <summary>Details</summary>
Motivation: To explore how introducing higher-order dynamics affects controllability in networks of identical first-order linear systems, aiming to reduce modifications for structural controllability.

Method: Establishes a correspondence between state controllability in first-order networks and output controllability in higher-order networks, compares modification efficiency, and characterizes X-networks with homogeneous dynamics.

Result: Higher-order dynamics require fewer changes for structural controllability than heterogeneous first-order subsystems; identifies X-networks where homogeneous dynamics achieve output controllability.

Conclusion: Higher-order subsystems enhance controllability more efficiently, and in certain topologies, structural output controllability can be achieved without heterogeneous dynamics.

Abstract: We consider a network of identical, first-order linear systems, and
investigate how replacing a subset of the systems composing the network with
higher-order ones, either taken to be generic or specifically designed, may
affect its controllability. After establishing a correspondence between state
controllability in networks of first-order systems with output controllability
in networks of higher-order systems, we show that adding higher-order dynamics
may require significantly fewer subsystem modifications to achieve structural
controllability, when compared to first-order heterogeneous subsystems.
Furthermore, we characterize the topology of networks (which we call
X-networks) in which the introduction of heterogeneous local dynamics is not
necessary for structural output controllability, as the latter can be attained
by suitable higher-order subsystems with homogeneous internal dynamics.

</details>


### [73] [Recursive feasibility for stochastic MPC and the rationale behind fixing flat tires](https://arxiv.org/abs/2504.17718)
*Mirko Fiacchini,Martina Mammarella,Fabrizio Dabbene*

Main category: math.OC

TL;DR: 这篇论文提出了一种基于测量状态初始化的随机模型预测控制（SMPC）方案，用于处理受无界扰动的线性系统。


<details>
  <summary>Details</summary>
Motivation: 由于无界噪声导致机会约束违反概率非零，论文旨在设计确保递归可行性的SMPC方案。

Method: 引入基于椭球的概率可达集和约束松弛，并采用测量状态初始化策略。

Result: 证明了闭环机会约束满足、最小松弛下的递归可行性，以及松弛需求随时间消失，轨迹趋向无约束LQR不变区域；通过数值例子展示了优越性。

Conclusion: 该SMPC方案在测量状态条件下确保递归可行性，并优于开环初始化方案。

Abstract: In this paper, we address the problem of designing stochastic model
predictive control (SMPC) schemes for linear systems affected by unbounded
disturbances. The contribution of the paper is rooted in a measured-state
initialization strategy. First, due to the nonzero probability of violating
chance-constraints in the case of unbounded noise, we introduce
ellipsoidal-based probabilistic reachable sets and we include constraint
relaxations to recover recursive feasibility conditioned to the measured state.
Second, we prove that the solution of this novel SMPC scheme guarantees
closed-loop chance constraints satisfaction under minimum relaxation. Last, we
demonstrate that, in expectation, the need of relaxing the constraints vanishes
over time, which leads the closed-loop trajectories steered towards the
unconstrained LQR invariant region. This novel SMPC scheme is proven to satisfy
the recursive feasibility conditioned to the state realization, and its
superiority with respect to open-loop initialization schemes is shown through
numerical examples.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [74] [Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy](https://arxiv.org/abs/2504.17124)
*Ming Du,Mark Wolfman,Chengjun Sun,Shelly D. Kelly,Mathew J. Cherukara*

Main category: physics.app-ph

TL;DR: 这篇论文提出了一种注入知识的贝叶斯优化方法，用于XANES光谱的自适应数据采集，提高效率。


<details>
  <summary>Details</summary>
Motivation: XANES光谱采集需要许多能量点，耗时长，现有的自适应方法缺乏领域特定知识，因此需要结合光谱结构知识来优化。

Method: 使用注入知识的贝叶斯优化方法，考虑吸收边和预边峰等光谱特征，进行自适应采样。

Result: 方法仅用15-20%的测量点重建光谱，峰能量错误小于0.03 eV，吸收边错误小于0.1 eV，RMS错误小于0.005；在电池材料和催化剂上验证了静态和动态测量。

Conclusion: 该方法提高了数据采集效率，增强了自动化，减少了采样错误，并提升了动态实验的时间分辨率。

Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful
technique for characterizing the chemical state and symmetry of individual
elements within materials, but requires collecting data at many energy points
which can be time-consuming. While adaptive sampling methods exist for
efficiently collecting spectroscopic data, they often lack domain-specific
knowledge about XANES spectra structure. Here we demonstrate a
knowledge-injected Bayesian optimization approach for adaptive XANES data
collection that incorporates understanding of spectral features like absorption
edges and pre-edge peaks. We show this method accurately reconstructs the
absorption edge of XANES spectra using only 15-20% of the measurement points
typically needed for conventional sampling, while maintaining the ability to
determine the x-ray energy of the sharp peak after absorption edge with errors
less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and
overall root-mean-square errors less than 0.005 compared to compared to
traditionally sampled spectra. Our experiments on battery materials and
catalysts demonstrate the method's effectiveness for both static and dynamic
XANES measurements, improving data collection efficiency and enabling better
time resolution for tracking chemical changes. This approach advances the
degree of automation in XANES experiments reducing the common errors of under-
or over-sampling points in near the absorption edge and enabling dynamic
experiments that require high temporal resolution or limited measurement time.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [75] [Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation](https://arxiv.org/abs/2504.17114)
*Valentin Langer,Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 本研究提出多器官分割-based IDIF方法，改善[18F]FDG PET动力学建模，减少肝和肺MSE。


<details>
  <summary>Details</summary>
Motivation: 传统IDIFs仅从主动脉获取，忽略了解剖变异和血管贡献，需要整合多器官血供来源。

Method: 使用高分辨CT分割肝、肺、肾、膀胱，整合主动脉、门静脉、肺动脉和输尿管的IDIFs进行动力学建模。

Result: 在九名患者数据上，肝脏MSE减少13.39%，肺部减少10.42%。

Conclusion: 证明多个IDIFs可提升解剖建模，并促进动态PET在临床的整合。

Abstract: Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron
emission tomography (PET) requires anatomically constrained modelling of
image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from
the aorta, neglecting anatomical variations and complex vascular contributions.
This study proposes a multi-organ segmentation-based approach that integrates
IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using
high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we
incorporate organ-specific blood supply sources to improve kinetic modelling.
Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,
resulting in a mean squared error (MSE) reduction of $13.39\%$ for the liver
and $10.42\%$ for the lungs. These initial results highlight the potential of
multiple IDIFs in improving anatomical modelling and fully leveraging dynamic
PET imaging. This approach could facilitate the integration of tracer kinetic
modelling into clinical routine.

</details>


### [76] [Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET](https://arxiv.org/abs/2504.17122)
*Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 本研究提出基于隐式神经表示(INRs)的动态PET成像方法，提高葡萄糖代谢参数估计的效率和精度，并整合CT解剖先验。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算密集且受限于分辨率，DNNs需大量数据，因此开发INRs以减少数据需求和提升性能。

Method: 使用INRs学习连续函数，结合3D CT基础模型的解剖先验，进行个性化动力学参数估计。

Result: 在PET/CT数据集上，与现有DNNs相比，显示更高空间分辨率、更低均方误差和更好解剖一致性，尤其在肿瘤和血管化区域。

Conclusion: INRs适用于肿瘤特征化、分割和预后评估，提供数据高效的动力学建模方法。

Abstract: Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables
non-invasive quantification of glucose metabolism through kinetic analysis,
often modelled by the two-tissue compartment model (TCKM). However, voxel-wise
kinetic parameter estimation using conventional methods is computationally
intensive and limited by spatial resolution. Deep neural networks (DNNs) offer
an alternative but require large training datasets and significant
computational resources. To address these limitations, we propose a
physiological neural representation based on implicit neural representations
(INRs) for personalized kinetic parameter estimation. INRs, which learn
continuous functions, allow for efficient, high-resolution parametric imaging
with reduced data requirements. Our method also integrates anatomical priors
from a 3D CT foundation model to enhance robustness and precision in kinetic
modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset
and compare it to state-of-the-art DNNs. Results demonstrate superior spatial
resolution, lower mean-squared error, and improved anatomical consistency,
particularly in tumour and highly vascularized regions. Our findings highlight
the potential of INRs for personalized, data-efficient tracer kinetic
modelling, enabling applications in tumour characterization, segmentation, and
prognostic assessment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
*Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Amit Agarwal*

Main category: cs.CL

TL;DR: 这篇论文比较了在低资源印度语言NER中的标记化方法，发现SentencePiece优于BPE。


<details>
  <summary>Details</summary>
Motivation: 解决BPE在处理低资源印度语言NER时形态复杂性的限制。

Method: 使用IndicBERT系统比较BPE、SentencePiece和字符级标记化策略，评估NER在阿萨姆语、孟加拉语、马拉地语、奥里亚语、桑塔利语、曼尼普尔语和信德语中的固有特性和外部性能，包括微调和零样本跨语言转移。

Result: SentencePiece在零样本设置中表现更好，尤其在实体一致性和泛化方面优于BPE，对极低资源和形态丰富的语言如桑塔利语和曼尼普尔语更有效。

Conclusion: 推荐SentencePiece作为多语言和低资源印度NLP应用中NER的更有效标记化策略。

Abstract: Tokenization is a critical component of Natural Language Processing (NLP),
especially for low resource languages, where subword segmentation influences
vocabulary structure and downstream task accuracy. Although Byte Pair Encoding
(BPE) is a standard tokenization method in multilingual language models, its
suitability for Named Entity Recognition (NER) in low resource Indic languages
remains underexplored due to its limitations in handling morphological
complexity. In this work, we systematically compare BPE, SentencePiece, and
Character Level tokenization strategies using IndicBERT for NER tasks in low
resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as
extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We
assess both intrinsic linguistic properties tokenization efficiency, out of
vocabulary (OOV) rates, and morphological preservation as well as extrinsic
downstream performance, including fine tuning and zero shot cross lingual
transfer.
  Our experiments show that SentencePiece is a consistently better performing
approach than BPE for NER in low resource Indic Languages, particularly in zero
shot cross lingual settings, as it better preserves entity consistency. While
BPE provides the most compact tokenization form, it is not capable of
generalization because it misclassifies or even fails to recognize entity
labels when tested on unseen languages. In contrast, SentencePiece constitutes
a better linguistic structural preservation model, benefiting extremely low
resource and morphologically rich Indic languages, such as Santali and
Manipuri, for superior entity recognition, as well as high generalization
across scripts, such as Sindhi, written in Arabic. The results point to
SentencePiece as the more effective tokenization strategy for NER within
multilingual and low resource Indic NLP applications.

</details>


### [78] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg,Shaina Raza,Shebuti Rayana,Xingyi Liu,Sunghwan Sohn*

Main category: cs.CL

TL;DR: 本调查探讨小型语言模型在医疗保健中的应用，提供分类框架和实验结果，展示其在资源受限环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型的数据隐私和资源限制问题，小型语言模型被提出作为可扩展的医疗解决方案。

Method: 通过构建分类框架、时间线分析和实验结果编译，涵盖NLP任务、架构基础、适应技术和压缩方法。

Result: 展示了小型语言模型在医疗保健NLP任务中的变革性进展和优化创新。

Conclusion: 为医疗专业人员提供资源，支持未来研究，并提供GitHub仓库。

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>


### [79] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
*Chanhee Park,Hyeonseok Moon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 这篇论文介绍了MIRAGE数据集，用于评估RAG系统，包括新指标和实验，以评估检索和生成组件。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统评估的挑战，由于缺乏针对组件特定评估的基准。

Method: 开发了包含7560个实例的问题回答数据集和37800个检索池，引入了新的适应性指标（如噪声易感性、上下文可接受性等），并通过各种检索器-LLM配置进行全面实验。

Result: 通过实验提供了关于模型最佳配对和RAG系统动态的新见解。

Conclusion: 数据集和代码公开可用，便于研究者集成和定制。

Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective
method for enhancing the generative capabilities of Large Language Models
(LLMs) through the incorporation of external knowledge. However, the evaluation
of RAG systems remains a challenge, due to the intricate interplay between
retrieval and generation components. This limitation has resulted in a scarcity
of benchmarks that facilitate a detailed, component-specific assessment. In
this work, we present MIRAGE, a Question Answering dataset specifically
designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped
to a retrieval pool of 37,800 entries, enabling an efficient and precise
evaluation of both retrieval and generation tasks. We also introduce novel
evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions
such as noise vulnerability, context acceptability, context insensitivity, and
context misinterpretation. Through comprehensive experiments across various
retriever-LLM configurations, we provide new insights into the optimal
alignment of model pairs and the nuanced dynamics within RAG systems. The
dataset and evaluation code are publicly available, allowing for seamless
integration and customization in diverse research settings\footnote{The MIRAGE
code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [80] [Can deep neural networks learn biological vision?](https://arxiv.org/abs/2504.16940)
*Drew Linsley,Pinyuan Feng,Thomas Serre*

Main category: q-bio.NC

TL;DR: DNNs在提升识别准确性后与灵长类神经响应不再一致，建议视觉科学独立于AI，开发以生物视觉为导向的算法。


<details>
  <summary>Details</summary>
Motivation: DNNs与灵长类视觉特征的分化引发了对更好生物视觉模型的需求，推动视觉科学从AI基准转向生物启发。

Method: 提出设计以生物视觉系统为导向的算法，使用更接近人类视觉的数据、训练和目标。

Result: 预测下一代深度学习模型将采用更生物化的数据和训练方式。

Conclusion: 视觉科学应脱离AI，专注于开发以生物为基础的视觉模型。

Abstract: Deep neural networks (DNNs) once showed increasing alignment with primate
neural responses as they improved on computer vision benchmarks. This trend
raised the exciting possibility that better models of biological vision would
come as a byproduct of the deep learning revolution in artificial intelligence.
However, the trend has reversed over recent years as DNNs have scaled to human
or superhuman recognition accuracy, a divergence that may stem from modern DNNs
learning to rely on different visual features than primates to solve tasks.
Where will better computational models of biological vision come from? We
propose that vision science must break from artificial intelligence to develop
algorithms that are designed with biological visual systems in mind instead of
internet data benchmarks. We predict that the next generation of deep learning
models of biological vision will be trained with data diets, training routines,
and objectives that are closer to those that shape human vision than those that
are in use today.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [81] [Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models](https://arxiv.org/abs/2504.17077)
*Dongjin Seo,Soobin Um,Sangbin Lee,Jong Chul Ye,Haejun Chung*

Main category: physics.optics

TL;DR: AdjointDiffusion 是一种高效的光子器件设计框架，整合伴随梯度和扩散模型，减少模拟需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统逆设计和深度学习方法的局限性，如复杂几何、制造约束和高模拟次数。

Method: 训练扩散网络于合成数据集，并在推理中注入伴随梯度指导生成高性能结构。

Result: 优于现有优化器，使用约200次模拟，提高效率和可制造性。

Conclusion: 提供简化的、模拟高效的设计管道，并开源实现。

Abstract: Designing free-form photonic devices is fundamentally challenging due to the
vast number of possible geometries and the complex requirements of fabrication
constraints. Traditional inverse-design approaches--whether driven by human
intuition, global optimization, or adjoint-based gradient methods--often
involve intricate binarization and filtering steps, while recent deep learning
strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To
overcome these limitations, we present AdjointDiffusion, a physics-guided
framework that integrates adjoint sensitivity gradients into the sampling
process of diffusion models. AdjointDiffusion begins by training a diffusion
network on a synthetic, fabrication-aware dataset of binary masks. During
inference, we compute the adjoint gradient of a candidate structure and inject
this physics-based guidance at each denoising step, steering the generative
process toward high figure-of-merit (FoM) solutions without additional
post-processing. We demonstrate our method on two canonical photonic design
problems--a bent waveguide and a CMOS image sensor color router--and show that
our method consistently outperforms state-of-the-art nonlinear optimizers (such
as MMA and SLSQP) in both efficiency and manufacturability, while using orders
of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning
approaches (approximately 10^5 to 10^6). By eliminating complex binarization
schedules and minimizing simulation overhead, AdjointDiffusion offers a
streamlined, simulation-efficient, and fabrication-aware pipeline for
next-generation photonic device design. Our open-source implementation is
available at https://github.com/dongjin-seo2020/AdjointDiffusion.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [82] [Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks](https://arxiv.org/abs/2504.17029)
*Jeffrey Smith,Taisei Fujii,Jesse Cranney,Charles Gretton*

Main category: astro-ph.IM

TL;DR: 本文使用机器学习从单张波前传感器图像估计Fried参数r0，以优化自适应光学系统，实现快速准确的实时控制。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致天文观测图像模糊，Fried参数r0是关键控制参数，用于提升自适应光学和自由空间光学通信性能。

Method: 开发数据驱动方法，应用计算机视觉机器学习，从Shack-Hartmann或pyramid波前传感器图像估计r0，使用COMPASS模拟工具评估各种条件。

Result: 准确估计r0至几毫米精度，推理时间0.83ms，适用于开环和闭环AO配置，适合实时控制。

Conclusion: 提供经济高效的实时控制解决方案，展示了在实际仪器中的潜力。

Abstract: Atmospheric turbulence degrades the quality of astronomical observations in
ground-based telescopes, leading to distorted and blurry images. Adaptive
Optics (AO) systems are designed to counteract these effects, using atmospheric
measurements captured by a wavefront sensor to make real-time corrections to
the incoming wavefront. The Fried parameter, r0, characterises the strength of
atmospheric turbulence and is an essential control parameter for optimising the
performance of AO systems and more recently sky profiling for Free Space
Optical (FSO) communication channels. In this paper, we develop a novel
data-driven approach, adapting machine learning methods from computer vision
for Fried parameter estimation from a single Shack-Hartmann or pyramid
wavefront sensor image. Using these data-driven methods, we present a detailed
simulation-based evaluation of our approach using the open-source COMPASS AO
simulation tool to evaluate both the Shack-Hartmann and pyramid wavefront
sensors. Our evaluation is over a range of guide star magnitudes, and realistic
noise, atmospheric and instrument conditions. Remarkably, we are able to
develop a single network-based estimator that is accurate in both open and
closed-loop AO configurations. Our method accurately estimates the Fried
parameter from a single WFS image directly from AO telemetry to a few
millimetres. Our approach is suitable for real time control, exhibiting 0.83ms
r0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby
demonstrating a compelling economic solution for use in real-time instrument
control.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [83] [Analyzing Value Functions of States in Parametric Markov Chains](https://arxiv.org/abs/2504.17020)
*Kasper Engelen,Guillermo A. Pérez,Shrisha Rao*

Main category: cs.LO

TL;DR: This paper proposes an efficient algorithm to collapse equivalence classes in parametric Markov chains, reducing size and speeding up verification, with empirical evidence.


<details>
  <summary>Details</summary>
Motivation: To simplify the coETR-complete verification of parametric Markov chains by using monotonicity and easier-to-check properties.

Method: Reduces monotonicity to comparing reachability probabilities and uses an algorithm to collapse same-value equivalence classes.

Result: Empirical results show size reductions in benchmarks and faster checks for monotonicity and parameter lifting.

Conclusion: The algorithm can be used as a fast pre-processing step to improve practical verification efficiency.

Abstract: Parametric Markov chains (pMC) are used to model probabilistic systems with
unknown or partially known probabilities. Although (universal) pMC verification
for reachability properties is known to be coETR-complete, there have been
efforts to approach it using potentially easier-to-check properties such as
asking whether the pMC is monotonic in certain parameters. In this paper, we
first reduce monotonicity to asking whether the reachability probability from a
given state is never less than that of another given state. Recent results for
the latter property imply an efficient algorithm to collapse same-value
equivalence classes, which in turn preserves verification results and
monotonicity. We implement our algorithm to collapse "trivial" equivalence
classes in the pMC and show empirical evidence for the following: First, the
collapse gives reductions in size for some existing benchmarks and significant
reductions on some custom benchmarks; Second, the collapse speeds up existing
algorithms to check monotonicity and parameter lifting, and hence can be used
as a fast pre-processing step in practice.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [84] [Intrinsic Barriers to Explaining Deep Foundation Models](https://arxiv.org/abs/2504.16948)
*Zhen Tan,Huan Liu*

Main category: cs.CY

TL;DR: 这篇论文探讨深度基础模型（DFMs）的解释性挑战，质疑这些困难是暂时的还是源于内在障碍，并分析其对验证和治理的影响。


<details>
  <summary>Details</summary>
Motivation: 为了确保DFMs的信任、安全和问责性，论文动机是探讨解释困难是否源于模型本身的内在特性。

Method: 通过考察DFMs的基本特征并审视当前解释性方法的局限性，来评估解释的可行性。

Result: 结果表明，解释DFMs可能面临内在障碍，这对验证和治理提出了新挑战。

Conclusion: 结论是，必须采用新的方法来处理DFMs的验证和治理，以应对这些内在障碍。

Abstract: Deep Foundation Models (DFMs) offer unprecedented capabilities but their
increasing complexity presents profound challenges to understanding their
internal workings-a critical need for ensuring trust, safety, and
accountability. As we grapple with explaining these systems, a fundamental
question emerges: Are the difficulties we face merely temporary hurdles,
awaiting more sophisticated analytical techniques, or do they stem from
\emph{intrinsic barriers} deeply rooted in the nature of these large-scale
models themselves? This paper delves into this critical question by examining
the fundamental characteristics of DFMs and scrutinizing the limitations
encountered by current explainability methods when confronted with this
inherent challenge. We probe the feasibility of achieving satisfactory
explanations and consider the implications for how we must approach the
verification and governance of these powerful technologies.

</details>


### [85] [Approaches to Responsible Governance of GenAI in Organizations](https://arxiv.org/abs/2504.17044)
*Dhari Gandhi,Himanshu Joshi,Lucas Hartman,Shabnam Hassani*

Main category: cs.CY

TL;DR: 这篇论文通过文献综述和讨论，提出GenAI的责任治理框架，以平衡创新和风险。


<details>
  <summary>Details</summary>
Motivation: GenAI快速发展带来机会和挑战，需要整合负责任治理处理伦理、责任和社会影响。

Method: 采用文献综述、现有治理框架和行业圆桌讨论的方法。

Result: 识别核心原则，推荐可适应风险评估工具、持续监控和跨部门合作，并提供了Responsible GenAI Guide (ResAI)。

Conclusion: 为组织提供结构化基础，帮助GenAI举措与伦理、法律和操作最佳实践对齐。

Abstract: The rapid evolution of Generative AI (GenAI) has introduced unprecedented
opportunities while presenting complex challenges around ethics,
accountability, and societal impact. This paper draws on a literature review,
established governance frameworks, and industry roundtable discussions to
identify core principles for integrating responsible GenAI governance into
diverse organizational structures. Our objective is to provide actionable
recommendations for a balanced, risk-based governance approach that enables
both innovation and oversight. Findings emphasize the need for adaptable risk
assessment tools, continuous monitoring practices, and cross-sector
collaboration to establish trustworthy GenAI. These insights provide a
structured foundation and Responsible GenAI Guide (ResAI) for organizations to
align GenAI initiatives with ethical, legal, and operational best practices.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [86] [Path Integral Methods for Synthesizing and Preventing Stealthy Attacks in Nonlinear Cyber-Physical Systems](https://arxiv.org/abs/2504.17118)
*Apurva Patil,Kyle Morgenstein,Luis Sentis,Takashi Tanaka*

Main category: eess.SY

TL;DR: 这篇论文研究非线性网络物理系统中隐秘攻击的合成和缓解，使用KL散度量化隐秘性，通过路径积分方法和最小最大控制问题验证。


<details>
  <summary>Details</summary>
Motivation: 量化攻击者在保持隐秘性和破坏系统性能之间的权衡，使用KL散度捕捉这种trade-off。

Method: 首先用路径积分方法合成最坏情况隐秘攻击；其次制定最小最大KL控制问题，求解零和博弈，通过路径积分和蒙特卡洛模拟计算策略。

Result: 在unicycle导航和巡航控制问题中验证，展示了攻击者隐秘驱动系统进入不安全区域，以及控制器适应对抗最坏攻击。

Conclusion: 方法有效，提供对抗隐秘攻击的框架。

Abstract: This paper studies the synthesis and mitigation of stealthy attacks in
nonlinear cyber-physical systems (CPS). To quantify stealthiness, we employ the
Kullback-Leibler (KL) divergence, a measure rooted in hypothesis testing and
detection theory, which captures the trade-off between an attacker's desire to
remain stealthy and her goal of degrading system performance. First, we
synthesize the worst-case stealthy attack in nonlinear CPS using the path
integral approach. Second, we consider how a controller can mitigate the impact
of such stealthy attacks by formulating a minimax KL control problem, yielding
a zero-sum game between the attacker and the controller. Again, we leverage a
path integral-based solution that computes saddle-point policies for both
players through Monte Carlo simulations. We validate our approach using
unicycle navigation and cruise control problems, demonstrating how an attacker
can covertly drive the system into unsafe regions, and how the controller can
adapt her policy to combat the worst-case attacks.

</details>


### [87] [PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games](https://arxiv.org/abs/2504.17128)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: 这篇论文提出PACE框架，用于处理不完全信息下的两玩家线性二次微分游戏，实现实时成本参数估计和控制策略适应。


<details>
  <summary>Details</summary>
Motivation: 解决代理不完全信息问题，尤其是不知道对方成本函数的挑战，常见于多代理控制、人机交互和一般和微分游戏的近似方法。

Method: 提出基于模型的PACE框架，每个代理将对等方视为学习代理，建模其学习动态，并用于推断成本函数参数。

Result: 提供参数估计收敛和系统状态稳定的理论保证；数值研究显示，与假设完全信息的方法相比，PACE在稳定性和收敛速度上表现更好。

Conclusion: PACE框架允许代理基于观察实时推断对方目标并动态适应控制策略，证明其有效性。

Abstract: In this paper, we address the problem of a two-player linear quadratic
differential game with incomplete information, a scenario commonly encountered
in multi-agent control, human-robot interaction (HRI), and approximation
methods for solving general-sum differential games. While solutions to such
linear differential games are typically obtained through coupled Riccati
equations, the complexity increases when agents have incomplete information,
particularly when neither is aware of the other's cost function. To tackle this
challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework
for learning the cost parameters of the other agent. In PACE, each agent treats
its peer as a learning agent rather than a stationary optimal agent, models
their learning dynamics, and leverages this dynamic to infer the cost function
parameters of the other agent. This approach enables agents to infer each
other's objective function in real time based solely on their previous state
observations and dynamically adapt their control policies. Furthermore, we
provide a theoretical guarantee for the convergence of parameter estimation and
the stability of system states in PACE. Additionally, in our numerical studies,
we demonstrate how modeling the learning dynamics of the other agent benefits
PACE, compared to approaches that approximate the other agent as having
complete information, particularly in terms of stability and convergence speed.

</details>


### [88] [Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference](https://arxiv.org/abs/2504.17129)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: This paper proposes the N-PACE algorithm for human-robot interactions in incomplete-information games, enabling fast, unbiased learning of peer objectives and intent communication.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving equilibrium policies for incomplete-information general-sum dynamic games with nonlinear dynamics, as existing methods assume complete information, leading to biased estimates and coordination failures.

Method: Proposes the N-PACE algorithm, which uses iterative linear quadratic approximation to model the peer's learning dynamics and infer their objective functions.

Result: Achieves unbiased fast learning of peer objectives, improves task completion and safety, and enables intent communication in multi-agent systems.

Conclusion: N-PACE enhances coordination and safety in human-robot interactions by explicitly modeling peer learning dynamics, reducing biases from incomplete information.

Abstract: Human-robot interactions can be modeled as incomplete-information general-sum
dynamic games since the objective functions of both agents are not explicitly
known to each other. However, solving for equilibrium policies for such games
presents a major challenge, especially if the games involve nonlinear
underlying dynamics. To simplify the problem, existing work often assumes that
one agent is an expert with complete information about its peer, which can lead
to biased estimates and failures in coordination. To address this challenge, we
propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for
general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)
approximation of the nonlinear general-sum game, each agent explicitly models
the learning dynamics of its peer agent while inferring their objective
functions, leading to unbiased fast learning in inferring the unknown objective
function of the peer agent, which is critical for task completion and safety
assurance. Additionally, we demonstrate how N-PACE enables \textbf{intent
communication} in such multi-agent systems by explicitly modeling the peer's
learning dynamics.

</details>


### [89] [Opt-ODENet: A Neural ODE Framework with Differentiable QP Layers for Safe and Stable Control Design (longer version)](https://arxiv.org/abs/2504.17139)
*Keyan Miao,Liqun Zhao,Han Wang,Konstantinos Gatsis,Antonis Papachristodoulou*

Main category: eess.SY

TL;DR: 本文提出 Opt-ODENet 框架，使用 Neural ODE 和可微分 QP 层强制约束，确保控制系统的安全和性能平衡。


<details>
  <summary>Details</summary>
Motivation: 解决控制系统中设计既实现任务目标又确保安全的控制器挑战，无需依赖名义控制器或大型数据集。

Method: 引入 Opt-ODENet 框架，将可微分二次规划 (QP) 优化层与 Neural ODE 结合，使用控制李雅普诺夫函数 (CLFs) 确保稳定性和收敛性，并通过控制屏障函数 (CBFs) 在 QP 层强制实时安全。

Result: 实验验证了框架在平衡安全性和性能方面的有效性。

Conclusion: 该方法成功集成可微分 QP 和 Neural ODE，实现安全控制问题的直接求解，并支持伴随方法进行梯度计算。

Abstract: Designing controllers that achieve task objectives while ensuring safety is a
key challenge in control systems. This work introduces Opt-ODENet, a Neural ODE
framework with a differentiable Quadratic Programming (QP) optimization layer
to enforce constraints as hard requirements. Eliminating the reliance on
nominal controllers or large datasets, our framework solves the optimal control
problem directly using Neural ODEs. Stability and convergence are ensured
through Control Lyapunov Functions (CLFs) in the loss function, while Control
Barrier Functions (CBFs) embedded in the QP layer enforce real-time safety. By
integrating the differentiable QP layer with Neural ODEs, we demonstrate
compatibility with the adjoint method for gradient computation, enabling the
learning of the CBF class-$\mathcal{K}$ function and control network
parameters. Experiments validate its effectiveness in balancing safety and
performance.

</details>


### [90] [Breaking the Flow and the Bank: Stealthy Cyberattacks on Water Network Hydraulics](https://arxiv.org/abs/2504.17211)
*Abdallah Alalem Albustami,Ahmad F. Taha*

Main category: eess.SY

TL;DR: 本文分析水分配网络面对隐蔽假数据注入攻击的系统性风险，提出简单可扩展的攻击策略，并通过案例研究验证其影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究未探索攻击复杂性、系统知识要求与影响间的关系，本文旨在填补这一空白。

Method: 提出多种攻击形式，结合物理约束和检测规避条件，通过Net1和Net3基准网络案例研究进行验证。

Result: 攻击可持久增加运营成本、改变水流而不被检测。

Conclusion: 为漏洞评估提供洞察，并推动开发结合物理和统计安全机制的保护策略。

Abstract: As water distribution networks (WDNs) become increasingly connected with
digital infrastructures, they face greater exposure to cyberattacks that
threaten their operational integrity. Stealthy False Data Injection Attacks
(SFDIAs) are particularly concerning, as they manipulate sensor data to
compromise system operations while avoiding detection. While existing studies
have focused on either detection methods or specific attack formulations, the
relationship between attack sophistication, system knowledge requirements, and
achievable impact remains unexplored. This paper presents a systematic analysis
of sensor attacks against WDNs, investigating different combinations of
physical constraints, state monitoring requirements, and intrusion detection
evasion conditions. We propose several attack formulations that range from
tailored strategies satisfying both physical and detection constraints to
simpler measurement manipulations. The proposed attacks are simple and local --
requiring knowledge only of targeted sensors and their hydraulic connections --
making them scalable and practical. Through case studies on Net1 and Net3
benchmark networks, we demonstrate how these attacks can persistently increase
operational costs and alter water flows while remaining undetected by
monitoring systems for extended periods. The analysis provides utilities with
insights for vulnerability assessment and motivates the development of
protection strategies that combine physical and statistical security
mechanisms.

</details>


### [91] [Analysis and Mitigation of Data injection Attacks against Data-Driven Control](https://arxiv.org/abs/2504.17347)
*Sribalaji C. Anand*

Main category: eess.SY

TL;DR: 本论文研究虚假数据注入攻击对数据驱动控制系统的影响，攻击可导致学习不稳定控制器，并探讨缓解策略。


<details>
  <summary>Details</summary>
Motivation: 动机是解决数据驱动控制系统中安全问题，防止攻击者在学习阶段注入虚假数据导致控制器不稳定。

Method: 方法包括提出攻击策略误导学习不稳定反馈增益，研究常量偏差注入对数据驱动LQR的影响，并探索缓解策略，使用数值例子支持。

Result: 结果显示攻击可导致不稳定控制器和LQR性能下降，数值例子验证了这些发现。

Conclusion: 结论强调防范虚假数据注入攻击的重要性，并建议采用缓解策略。

Abstract: This paper investigates the impact of false data injection attacks on
data-driven control systems. Specifically, we consider an adversary injecting
false data into the sensor channels during the learning phase. When the
operator seeks to learn a stable state-feedback controller, we propose an
attack strategy capable of misleading the operator into learning an unstable
feedback gain. We also investigate the effects of constant-bias injection
attacks on data-driven linear quadratic regulation (LQR). Finally, we explore
potential mitigation strategies and support our findings with numerical
examples.

</details>


### [92] [Finding Conditions for Target Controllability under Christmas Trees](https://arxiv.org/abs/2504.17406)
*Marco Peruzzo,Giacomo Baggio,Francesco Ticozzi*

Main category: eess.SY

TL;DR: 本论文通过引入Christmas trees网络类，提出新的图论条件来改进定向网络的结构目标可控性分析。


<details>
  <summary>Details</summary>
Motivation: 现有文献条件存在空白，无法完全识别目标可控节点集。

Method: 引入Christmas trees类，并为其实验子类建立图论特征，然后扩展到一般网络的生成子图。

Result: 发现了现有标准无法识别的目标可控集，并提供了新的表征方法。

Conclusion: 新的条件提升了结构目标可控性的分析能力。

Abstract: This paper presents new graph-theoretic conditions for structural target
controllability of directed networks. After reviewing existing conditions and
highlighting some gaps in the literature, we introduce a new class of network
systems, named Christmas trees, which generalizes trees and cacti. We then
establish a graph-theoretic characterization of sets of nodes that are
structurally target controllable for a simple subclass of Christmas trees. Our
characterization applies to general network systems by considering spanning
subgraphs of Christmas tree class and allows us to uncover target controllable
sets that existing criteria fail to identify.

</details>


### [93] [Longitudinal Control for Autonomous Racing with Combustion Engine Vehicles](https://arxiv.org/abs/2504.17418)
*Phillip Pitschi,Simon Sagmeister,Sven Goblirsch,Markus Lienkamp,Boris Lohmann*

Main category: eess.SY

TL;DR: 这篇论文提出了一种纵向控制概念，将高级轨迹跟踪命令转换为低级车辆命令，如油门、制动压力和档位，并通过实际赛车测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统控制器生成的高级命令（如纵向加速度）与内燃机车辆所需低级输入（如油门、制动和档位）之间的不匹配问题。

Method: 方法是设计一个模块化纵向控制系统，翻译高级命令为低级车辆命令，并集成防抱死制动、牵引力控制和制动预热功能。

Result: 结果是通过EAV24赛车在阿布扎比自动驾驶赛车联赛的实证测试，实现了高达25 m/s²的纵向加速度。

Conclusion: 结论是该控制概念实现了高级命令的精确跟踪，并确保了实际测试中的安全操作。

Abstract: Usually, a controller for path- or trajectory tracking is employed in
autonomous driving. Typically, these controllers generate high-level commands
like longitudinal acceleration or force. However, vehicles with combustion
engines expect different actuation inputs. This paper proposes a longitudinal
control concept that translates high-level trajectory-tracking commands to the
required low-level vehicle commands such as throttle, brake pressure and a
desired gear. We chose a modular structure to easily integrate different
trajectory-tracking control algorithms and vehicles. The proposed control
concept enables a close tracking of the high-level control command. An
anti-lock braking system, traction control, and brake warmup control also
ensure a safe operation during real-world tests. We provide experimental
validation of our concept using real world data with longitudinal accelerations
reaching up to $25 \, \frac{\mathrm{m}}{\mathrm{s}^2}$. The experiments were
conducted using the EAV24 racecar during the first event of the Abu Dhabi
Autonomous Racing League on the Yas Marina Formula 1 Circuit.

</details>


### [94] [Admittance Identification of Grid-Forming Inverters Using Time and Frequency-Domain Techniques](https://arxiv.org/abs/2504.17512)
*Andres Intriago,Alexandros Paspatis,Francesco Liberati,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 本论文使用SFRA、SEM和ERA三种技术一致识别网格形成逆变器的dq导纳，解决IBRs整合中固件访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 电力网格IBRs增加需要EMT研究，但制造商保密协议限制固件访问。

Method: 探索SFRA（频率扫描）、SEM和ERA（阶跃响应转Laplace域）三种系统识别技术。

Result: 三种方法在1-100Hz范围内为GFM逆变器提供一致的dq导纳识别。

Conclusion: 这些技术可有效替代固件访问，实现可靠系统响应分析。

Abstract: The increasing integration of inverter-based resources (IBRs) into the power
grid introduces new challenges, requiring detailed electromagnetic transient
(EMT) studies to analyze system interactions. Despite these needs, access to
the internal firmware of power electronic devices remains restricted due to
stringent nondisclosure agreements enforced by manufacturers. To address this,
we explore three system identification techniques: sweep frequency response
analysis (SFRA), step excitation method (SEM), and eigensystem realization
algorithm (ERA). SFRA employs sinusoidal signals of varying frequencies to
measure the system's frequency response, while SEM and ERA utilize step
functions to derive time-domain responses and transform them into
Laplace-domain transfer functions. All three approaches are shown to provide
consistent results in identifying the dq admittance of grid-forming inverters
(GFM) over a frequency range of 1 Hz to 100 Hz.

</details>


### [95] [On the Eigenvalue Tracking of Large-Scale Systems](https://arxiv.org/abs/2504.17571)
*Andreas Bouterakos,Joseph McKeon,Georgios Tzounas*

Main category: eess.SY

TL;DR: 这篇论文介绍了一种基于连续性的方法，用于跟踪大尺度电力系统模型中特征值轨迹的变化，并提供了实际实现和案例研究。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是解决在系统参数变化时跟踪电力系统模型中特征值轨迹的问题，这对于系统稳定性分析至关重要。

Method: 方法包括一种基于连续性的公式化方法，能够跟踪任意单个特征值，支持稀疏矩阵表示，并适用于显式和半隐式微分代数模型。详细讨论了数值积分、矩阵更新、导数逼近以及处理缺陷特征值等方面。

Result: 通过对IEEE 39母线系统和爱尔兰输电系统的真实动态模型的案例研究，展示了方法的有效性。

Conclusion: 论文提供了实际实现的实用建议，并得出结论，该方法在实际电力系统分析中是稳健且适用的。

Abstract: The paper focuses on the problem of tracking eigenvalue trajectories in
large-scale power system models as system parameters vary. A continuation-based
formulation is presented for tracing any single eigenvalue of interest, which
supports sparse matrix representations and accommodates both explicit and
semi-implicit differential-algebraic models. Key implementation aspects, such
as numerical integration, matrix updates, derivative approximations, and
handling defective eigenvalues, are discussed in detail and practical
recommendations are duly provided. The tracking approach is demonstrated
through a comprehensive case study on the IEEE 39-bus system, as well as on a
realistic dynamic model of the Irish transmission system.

</details>


### [96] [SAPO-RL: Sequential Actuator Placement Optimization for Fuselage Assembly via Reinforcement Learning](https://arxiv.org/abs/2504.17603)
*Peng Ye,Juan Du*

Main category: eess.SY

TL;DR: 这篇论文提出一种基于强化学习的框架，用于优化飞机机身组装中的执行器放置和力设置，以实现高精度组装。


<details>
  <summary>Details</summary>
Motivation: 动机是解决当前固定执行器数量方法的不优问题，以及处理顺应结构、复杂材料和尺寸变异性的挑战。

Method: 方法包括使用Dueling Double Deep Q-Learning (D3QN)算法进行顺序决策执行器放置，并将问题形式化为次模函数优化问题。

Result: 结果显示，该方法在数值和比较研究中表现出色，能够用有限执行器数量显著提高组装精度。

Conclusion: 结论是，该强化学习框架有效地提升了组装效率和精度。

Abstract: Precise assembly of composite fuselages is critical for aircraft assembly to
meet the ultra-high precision requirements. Due to dimensional variations,
there is a gap when two fuselage assemble. In practice, actuators are required
to adjust fuselage dimensions by applying forces to specific points on fuselage
edge through pulling or pushing force actions. The positioning and force
settings of these actuators significantly influence the efficiency of the shape
adjustments. The current literature usually predetermines the fixed number of
actuators, which is not optimal in terms of overall quality and corresponding
actuator costs. However, optimal placement of actuators in terms of both
locations and number is challenging due to compliant structures, complex
material properties, and dimensional variabilities of incoming fuselages. To
address these challenges, this paper introduces a reinforcement learning (RL)
framework that enables sequential decision-making for actuator placement
selection and optimal force computation. Specifically, our methodology employs
the Dueling Double Deep Q-Learning (D3QN) algorithm to refine the
decision-making capabilities of sequential actuator placements. The environment
is meticulously crafted to enable sequential and incremental selection of an
actuator based on system states. We formulate the actuator selection problem as
a submodular function optimization problem, where the sub-modularity properties
can be adopted to efficiently achieve near-optimal solutions. The proposed
methodology has been comprehensively evaluated through numerical studies and
comparison studies, demonstrating its effectiveness and outstanding performance
in enhancing assembly precision with limited actuator numbers.

</details>


### [97] [Are EVs Cleaner Than We Think? Evaluating Consequential Greenhouse Gas Emissions from EV Charging](https://arxiv.org/abs/2504.17632)
*Riti Bhandarkar,Qian Luo,Emil Dimanchev,Jesse D. Jenkins*

Main category: eess.SY

TL;DR: 本研究使用容量扩展模型评估大规模电动汽车采用对西部美国温室气体排放的影响，发现现有方法不准确，并建议替代控制策略。


<details>
  <summary>Details</summary>
Motivation: 准确量化电动汽车采用引发的排放影响，因为现有方法忽略容量变化可能导致估算错误。

Method: 使用电力系统容量扩展模型，考虑发电和存储容量变化模拟排放影响。

Result: 平均排放率可能低估或高估影响，短期边际排放率低估减排，使用其协调充电可能增加排放。

Conclusion: 需要采用替代控制策略来最小化电动汽车采用的间接排放。

Abstract: While electrifying transportation eliminates tailpipe greenhouse gas (GHG)
emissions, electric vehicle (EV) adoption can create additional electricity
sector emissions. To quantify this emissions impact, prior work typically
employs short-run marginal emissions or average emissions rates calculated from
historical data or power systems models that do not consider changes in
installed capacity. In this work, we use an electricity system capacity
expansion model to consider the full consequential GHG emissions impact from
large-scale EV adoption in the western United States, accounting for induced
changes in generation and storage capacity. We find that the metrics described
above do not accurately reflect the true emissions impact of EV
adoption-average emissions rates can either under- or over-estimate emission
impacts, and short-run marginal emissions rates can significantly underestimate
emission reductions, especially when charging timing is flexible. Our results
also show that using short-run marginal emission rates as signals to coordinate
EV charging could increase emissions relative to price-based charging signals,
indicating the need for alternative control strategies to minimize
consequential emissions.

</details>


### [98] [Design and benchmarking of a two degree of freedom tendon driver unit for cable-driven wearable technologies](https://arxiv.org/abs/2504.17736)
*Adrian Esser,Chiara Basla,Peter Wolf,Robert Riener*

Main category: eess.SY

TL;DR: 这篇论文介绍了缆索驱动可穿戴外骨骼的肌腱驱动单元（TDU）的设计和性能分析，通过各种测试评估其功能。


<details>
  <summary>Details</summary>
Motivation: 许多缆索驱动外骨骼已被开发，但相关机电设计和性能的出版物很少，因此本研究旨在填补这一空白。

Method: 呈现详细基准测试方法，包括静态扭矩输出测试、速度控制测试、噪声测试、热应力测试和电池耐久性测试，并引入模块化TDU系统。

Result: 测试结果涵盖命令与测量扭矩比较、速度下的衰减和相移、噪声水平、冷却性能以及电池运行时间。

Conclusion: 本研究通过分享方法和结果，提供可被他人利用的TDU设计，并为研究人员和工程师提供记录能力的资源。

Abstract: Exosuits have recently been developed as alternatives to rigid exoskeletons
and are increasingly adopted for both upper and lower limb therapy and
assistance in clinical and home environments. Many cable-driven exosuits have
been developed but little has been published on their electromechanical designs
and performance. Therefore, this paper presents a comprehensive design and
performance analysis of a two degree of freedom tendon driver unit (TDU) for
cable-driven wearable exosuits. Detailed methodologies are presented to
benchmark the functionality of the TDU. A static torque output test compares
the commanded and measured torques. A velocity control test evaluates the
attenuation and phase shift across velocities. A noise test evaluates how loud
the TDU is for the wearer under different speeds. A thermal stress test
captures the cooling performance of the TDU to ensure safe operation at higher
loads. Finally, a battery endurance test evaluates the runtime of the TDU under
various loading conditions to inform the usable time. To demonstrate these
tests, a modular TDU system for cable-driven applications is introduced, which
allows components such as motors, pulleys, and sensors to be adapted based on
the requirements of the intended application. By sharing detailed methodologies
and performance results, this study aims to provide a TDU design that may be
leveraged by others and resources for researchers and engineers to better
document the capabilities of their TDU designs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [Geometric Formulation of Unified Force-Impedance Control on SE(3) for Robotic Manipulators](https://arxiv.org/abs/2504.17080)
*Joohwan Seo,Nikhil Potu Surya Prakash,Soomi Lee,Arvind Kruthiventy,Megan Teng,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于SE(3)流形的阻抗控制框架，用于力跟踪并保证被动性。


<details>
  <summary>Details</summary>
Motivation: 为了改进机器人与不确定环境的安全接触互动、跟踪期望互动力和提升机器学习应用的样本效率。

Method: 通过差分几何视角开发几何统一力阻抗控制（GUFIC），利用能量罐增强保证被动性，并引入速度和力场解决非因果实现问题。

Result: 在模拟环境中验证了控制律，能够跟踪SE(3)轨迹并施加力，代码开源。

Conclusion: 提出的GUFIC继承SE(3)的不变性和等变性属性，提高了控制性能和机器学习效率。

Abstract: In this paper, we present an impedance control framework on the SE(3)
manifold, which enables force tracking while guaranteeing passivity. Building
upon the unified force-impedance control (UFIC) and our previous work on
geometric impedance control (GIC), we develop the geometric unified force
impedance control (GUFIC) to account for the SE(3) manifold structure in the
controller formulation using a differential geometric perspective. As in the
case of the UFIC, the GUFIC utilizes energy tank augmentation for both
force-tracking and impedance control to guarantee the manipulator's passivity
relative to external forces. This ensures that the end effector maintains safe
contact interaction with uncertain environments and tracks a desired
interaction force. Moreover, we resolve a non-causal implementation problem in
the UFIC formulation by introducing velocity and force fields. Due to its
formulation on SE(3), the proposed GUFIC inherits the desirable SE(3)
invariance and equivariance properties of the GIC, which helps increase sample
efficiency in machine learning applications where a learning algorithm is
incorporated into the control law. The proposed control law is validated in a
simulation environment under scenarios requiring tracking an SE(3) trajectory,
incorporating both position and orientation, while exerting a force on a
surface. The codes are available at
https://github.com/Joohwan-Seo/GUFIC_mujoco.

</details>


### [100] [Subframework-based Bearing Rigidity Maintenance Control in Multirobot Networks](https://arxiv.org/abs/2504.17103)
*J. Francisco Presenza,Ignacio Mas,J. Ignacio Alvarez-Hamelin,Juan I. Giribet*

Main category: cs.RO

TL;DR: 这篇论文提出了一种通过分解框架来控制多机器人网络轴承刚度的创新方法，并通过模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在动态拓扑的多机器人网络中，需要分析和控制轴承刚度以确保系统稳定性和安全性。

Method: 将系统框架分解为子框架，将全局轴承刚度转化为局部属性，并设计基于梯度的去中心化控制器，使用轴承测量保持刚度和避免碰撞。

Result: 模拟结果证实了方案的有效性，信息交换仅限于子框架，确保了可扩展性和实用性。

Conclusion: 该方法实现了高效的轴承刚度控制，具有良好的可扩展性和实际应用潜力。

Abstract: This work presents a novel approach for analyzing and controlling bearing
rigidity in multi-robot networks with dynamic topology. By decomposing the
system's framework into subframeworks, we express bearing rigidity, a global
property, as a set of local properties, with rigidity eigenvalues serving as
natural local rigidity metrics. We propose a decentralized, scalable,
gradient-based controller that uses only bearing measurements to execute
mission-specific commands. The controller preserves bearing rigidity by
maintaining rigidity eigenvalues above a threshold, and also avoids inter-robot
collisions. Simulations confirm the scheme's effectiveness, with information
exchange confined to subframeworks, underscoring its scalability and
practicality.

</details>


### [101] [Flying through cluttered and dynamic environments with LiDAR](https://arxiv.org/abs/2504.17569)
*Huajie Wu,Wenyi Liu,Yunfan Ren,Zheng Liu,Hairuo Wei,Fangcheng Zhu,Haotian Li,Fu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于LiDAR的系统，帮助无人机在动态环境中实时避开各种移动障碍物。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在杂乱动态环境中导航的挑战，特别是面对快速移动或突然出现的障碍物。

Method: 整合M-detector用于动态物体检测和DynIPC框架，用于预测动态障碍物并进行规划与控制。

Result: 模拟测试中在成功率、时间消耗、平均飞行时间和最大速度等方面优于基准方法；真实世界实验中成功避开森林中的移动障碍物。

Conclusion: 系统利用高计算效率实现实时低延迟避障，在复杂环境中有效工作。

Abstract: Navigating unmanned aerial vehicles (UAVs) through cluttered and dynamic
environments remains a significant challenge, particularly when dealing with
fast-moving or sudden-appearing obstacles. This paper introduces a complete
LiDAR-based system designed to enable UAVs to avoid various moving obstacles in
complex environments. Benefiting the high computational efficiency of
perception and planning, the system can operate in real time using onboard
computing resources with low latency. For dynamic environment perception, we
have integrated our previous work, M-detector, into the system. M-detector
ensures that moving objects of different sizes, colors, and types are reliably
detected. For dynamic environment planning, we incorporate dynamic object
predictions into the integrated planning and control (IPC) framework, namely
DynIPC. This integration allows the UAV to utilize predictions about dynamic
obstacles to effectively evade them. We validate our proposed system through
both simulations and real-world experiments. In simulation tests, our system
outperforms state-of-the-art baselines across several metrics, including
success rate, time consumption, average flight time, and maximum velocity. In
real-world trials, our system successfully navigates through forests, avoiding
moving obstacles along its path.

</details>


### [102] [Unifying Complementarity Constraints and Control Barrier Functions for Safe Whole-Body Robot Control](https://arxiv.org/abs/2504.17647)
*Rafael I. Cabral Muchacho,Riddhiman Laha,Florian T. Pokorny,Luis F. C. Figueredo,Nilanjan Chakraborty*

Main category: cs.RO

TL;DR: 本论文证明了互补约束和控制屏障函数在采样数据第一阶系统中的等价性，提供统一视角并讨论实际应用。


<details>
  <summary>Details</summary>
Motivation: 桥接互补约束和控制屏障函数之间的差距，尽管它们解决类似安全约束问题但联系未被探索。

Method: 通过正式证明这些方法在采样数据第一阶系统中的等价性，涵盖单约束和多约束场景。

Result: 证明了等价性，提供统一框架，并突出了理论和实践的协同益处。

Conclusion: 促进了方法之间稳健性保证和算法改进的交叉应用，并激发更一般情况下的未来研究。

Abstract: Safety-critical whole-body robot control demands reactive methods that ensure
collision avoidance in real-time. Complementarity constraints and control
barrier functions (CBF) have emerged as core tools for ensuring such safety
constraints, and each represents a well-developed field. Despite addressing
similar problems, their connection remains largely unexplored. This paper
bridges this gap by formally proving the equivalence between these two
methodologies for sampled-data, first-order systems, considering both single
and multiple constraint scenarios. By demonstrating this equivalence, we
provide a unified perspective on these techniques. This unification has
theoretical and practical implications, facilitating the cross-application of
robustness guarantees and algorithmic improvements between complementarity and
CBF frameworks. We discuss these synergistic benefits and motivate future work
in the comparison of the methods in more general cases.

</details>


### [103] [Robo-Troj: Attacking LLM-based Task Planners](https://arxiv.org/abs/2504.17070)
*Mohaiminul Al Nahian,Zainab Altaweel,David Reitano,Sabbir Ahmed,Saumitra Lohokare,Shiqi Zhang,Adnan Siraj Rakin*

Main category: cs.RO

TL;DR: 这篇论文开发了Robo-Troj，这是针对LLM-based任务规划器的第一个多触发后门攻击，旨在展示其安全漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM在任务规划中表现出色，但安全研究有限，需要促进安全机器人系统的开发。

Method: 开发了Robo-Troj多触发后门攻击和触发词优化方法，以适应不同机器人应用领域。

Result: 证明了LLM-based规划器的易受后门攻击的脆弱性。

Conclusion: 通过展示漏洞，鼓励开发更安全的机器人系统。

Abstract: Robots need task planning methods to achieve goals that require more than
individual actions. Recently, large language models (LLMs) have demonstrated
impressive performance in task planning. LLMs can generate a step-by-step
solution using a description of actions and the goal. Despite the successes in
LLM-based task planning, there is limited research studying the security
aspects of those systems. In this paper, we develop Robo-Troj, the first
multi-trigger backdoor attack for LLM-based task planners, which is the main
contribution of this work. As a multi-trigger attack, Robo-Troj is trained to
accommodate the diversity of robot application domains. For instance, one can
use unique trigger words, e.g., "herical", to activate a specific malicious
behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an
optimization method for selecting the trigger words that are most effective.
Through demonstrating the vulnerability of LLM-based planners, we aim to
promote the development of secured robot systems.

</details>
