<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 17]
- [eess.SY](#eess.SY) [Total: 16]
- [cs.RO](#cs.RO) [Total: 3]
- [math.OC](#math.OC) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Can Machine Learning Agents Deal with Hard Choices?](https://arxiv.org/abs/2504.15304)
*Kangyu Wang*

Main category: cs.AI

TL;DR: 本文讨论了ML代理在处理困难选择时的局限性，提出集成解决方案来识别这些选择，并呼吁重新概念化机器自治。


<details>
  <summary>Details</summary>
Motivation: 为了改善ML决策与人类推理之间的对齐，尤其是在选项不可通约的硬选择情况下。

Method: 通过分析多目标优化方法的局限性，评估两种技术解决方案，并推荐集成方法。

Result: 集成解决方案可识别硬选择并缓解部分对齐问题，但ML代理无法通过审议解决它们。

Conclusion: 敦促ML研究人员重新思考机器自治，开发新框架填补这一根本差距。

Abstract: Machine Learning ML agents have been increasingly used in decision-making
across a wide range of tasks and environments. These ML agents are typically
designed to balance multiple objectives when making choices. Understanding how
their decision-making processes align with or diverge from human reasoning is
essential. Human agents often encounter hard choices, that is, situations where
options are incommensurable; neither option is preferred, yet the agent is not
indifferent between them. In such cases, human agents can identify hard choices
and resolve them through deliberation. In contrast, current ML agents, due to
fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot
identify hard choices, let alone resolve them. Neither Scalarised Optimisation
nor Pareto Optimisation, the two principal MOO approaches, can capture
incommensurability. This limitation generates three distinct alignment
problems: the alienness of ML decision-making behaviour from a human
perspective; the unreliability of preference-based alignment strategies for
hard choices; and the blockage of alignment strategies pursuing multiple
objectives. Evaluating two potential technical solutions, I recommend an
ensemble solution that appears most promising for enabling ML agents to
identify hard choices and mitigate alignment problems. However, no known
technique allows ML agents to resolve hard choices through deliberation, as
they cannot autonomously change their goals. This underscores the
distinctiveness of human agency and urges ML researchers to reconceptualise
machine autonomy and develop frameworks and methods that can better address
this fundamental gap.

</details>


### [2] [PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind](https://arxiv.org/abs/2504.15313)
*Yajie Yu,Yue Feng*

Main category: cs.AI

TL;DR: PolicyEvol-Agent是一个基于LLM的多代理框架，通过社会认知和知识检索提升动态交互中的智能表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在代理的推理、规划、决策和反思能力上有限，且提示-based响应难以感知心理状态和经验校准，易导致认知偏差。

Method: PolicyEvol-Agent通过获取他人意图、整合心智理论与内部外部视角的自适应策略优化机制来持续提升代理性能。

Result: 模拟结果显示PolicyEvol-Agent优于RL-based模型和代理方法，在游戏胜利和政策演化中表现出色。

Conclusion: 框架证明了动态指导调整的有效性，提升了代理在不确定交互中的表现。

Abstract: Multi-agents has exhibited significant intelligence in real-word simulations
with Large language models (LLMs) due to the capabilities of social cognition
and knowledge retrieval. However, existing research on agents equipped with
effective cognition chains including reasoning, planning, decision-making and
reflecting remains limited, especially in the dynamically interactive
scenarios. In addition, unlike human, prompt-based responses face challenges in
psychological state perception and empirical calibration during uncertain
gaming process, which can inevitably lead to cognition bias. In light of above,
we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework
characterized by systematically acquiring intentions of others and adaptively
optimizing irrational strategies for continual enhancement. Specifically,
PolicyEvol-Agent first obtains reflective expertise patterns and then
integrates a range of cognitive operations with Theory of Mind alongside
internal and external perspectives. Simulation results, outperforming RL-based
models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent
for final gaming victory. Moreover, the policy evolution mechanism reveals the
effectiveness of dynamic guideline adjustments in both automatic and human
evaluation.

</details>


### [3] [Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets](https://arxiv.org/abs/2504.15360)
*Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 本文提出使用保形学习与模糊规则系统相结合，提高分类任务中预测的可靠性和质量。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习分类器过于自信，在真实场景中不可靠，需要更好的方法如保形学习来量化预测质量。

Method: 提出在分类中使用保形学习与模糊规则系统，比较类型2模糊集的优势，并调整系统微调以提升保形预测质量。

Result: 展示了性能指标，表明类型2模糊集比模糊和精确规则有更好输出，微调可提高保形预测质量。

Conclusion: 讨论了类型2模糊集和微调如何改善系统的输出可靠性和质量。

Abstract: Classical machine learning classifiers tend to be overconfident can be
unreliable outside of the laboratory benchmarks. Properly assessing the
reliability of the output of the model per sample is instrumental for real-life
scenarios where these systems are deployed. Because of this, different
techniques have been employed to properly quantify the quality of prediction
for a given model. These are most commonly Bayesian statistics and, more
recently, conformal learning. Given a calibration set, conformal learning can
produce outputs that are guaranteed to cover the target class with a desired
significance level, and are more reliable than the standard confidence
intervals used by Bayesian methods. In this work, we propose to use conformal
learning with fuzzy rule-based systems in classification and show some metrics
of their performance. Then, we discuss how the use of type 2 fuzzy sets can
improve the quality of the output of the system compared to both fuzzy and
crisp rules. Finally, we also discuss how the fine-tuning of the system can be
adapted to improve the quality of the conformal prediction.

</details>


### [4] [KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park,Dalton Jones,Matt Morse,Raghavv Goel,Mingu Lee,Chris Lott*

Main category: cs.AI

TL;DR: 本论文提出KeyDiff方法，通过key相似性evict KV cache，减少内存使用，在资源受限环境下处理长提示，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在资源受限环境下的长输入提示部署问题。

Method: 基于key相似性的训练-free KV cache eviction方法，计算最大化key多样性的最优解。

Result: 在LongBench基准上，KV cache减少约23%，性能差距小于0.04%。

Conclusion: KeyDiff有效、高效，不依赖注意力分数，可与优化机制结合。

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>


### [5] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TL;DR: 本研究评估OpenAI的Computer-User Agent在Wordle游戏中的表现，发现颜色识别问题，成功率仅5.36%，强调AI在简单任务上的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了评估AI代理的行为和缺点，识别其在标准计算机界面任务中的表现。

Method: 通过在New York Times Wordle游戏中运行代理，进行数百次测试。

Result: 代理颜色识别存在显著差异，成功率仅5.36%。

Conclusion: 当前AI模型在简单任务上仍面临挑战，讨论潜在原因、未来发展和研究方向。

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>


### [6] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary,Yancheng Liang,Daphne Chen,Simon S. Du,Natasha Jaques*

Main category: cs.AI

TL;DR: 这篇论文引入GOAT方法，使用生成模型和对抗训练提升AI与人类合作能力，在基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: AI需要在家用机器人和自动驾驶等任务中与新人类合作，但泛化到多样人类行为需要更多训练数据，对抗训练在合作场景中易导致自我破坏。

Method: 提出GOAT（Generative Online Adversarial Training），结合预训练生成模型模拟合作代理，并通过对抗训练最大化遗憾值，仅更新模型嵌入以避免利用。

Result: 在Overcooked基准测试中与真实人类伙伴评估，展示了最先进性能和更好的泛化能力。

Conclusion: GOAT方法有效提升AI对多样人类行为的合作泛化，证明其在实际应用中的有效性。

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>


### [7] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TL;DR: 本论文提出Adaptive Parallel Reasoning (APR)框架，允许语言模型自适应地结合串行和并行推理，通过强化学习优化，提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限：串行推理输出过长导致延迟和上下文耗尽，并行推理协调不足引起冗余计算，APR旨在解决这些问题。

Method: APR使用自适应多线程推理，引入spawn()和join()操作，并采用端到端强化学习策略优化推理线程，无需预定义结构。

Result: 在Countdown任务中，APR在相同上下文窗口下性能更高（4k时83.4% vs 60.0%），可扩展性更好（20k tokens时80.1% vs 66.6%），并在相同延迟下准确率提升（5000ms时75.2% vs 57.3%）。

Conclusion: APR是语言模型自主优化推理过程的关键一步，通过自适应计算分配提升性能。

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>


### [8] [A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models](https://arxiv.org/abs/2504.15552)
*Gengxian Cao,Fengyuan Li,Hong Duan,Ye Yang,Bofeng Wang,Donghe Li*

Main category: cs.AI

TL;DR: 这篇论文提出一个多代理AI框架，自动化秦腔歌剧的生产，整合LLM、视觉生成和TTS，表现优于单代理系统。


<details>
  <summary>Details</summary>
Motivation: 使用AI技术自动化和扩展传统表演艺术的保存，以应对文化遗产保护的挑战。

Method: 多代理框架，三个代理依次协作：Agent1使用LLM生成脚本，Agent2使用视觉模型渲染场景，Agent3使用TTS合成语音。

Result: 案例研究中总分3.6，高于单代理基准0.3分；消融实验显示移除代理导致分数下降，证明模块化合作的价值。

Conclusion: 展示了AI在保护传统艺术中的潜力，并建议未来改进跨模态对齐、情感表达和支持更多歌剧类型。

Abstract: This paper introduces a novel multi-Agent framework that automates the end to
end production of Qinqiang opera by integrating Large Language Models , visual
generation, and Text to Speech synthesis. Three specialized agents collaborate
in sequence: Agent1 uses an LLM to craft coherent, culturally grounded
scripts;Agent2 employs visual generation models to render contextually accurate
stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally
expressive vocal performances. In a case study on Dou E Yuan, the system
achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,
and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point
improvement over a Single Agent baseline. Ablation experiments demonstrate that
removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,
underscoring the value of modular collaboration. This work showcases how AI
driven pipelines can streamline and scale the preservation of traditional
performing arts, and points toward future enhancements in cross modal
alignment, richer emotional nuance, and support for additional opera genres.

</details>


### [9] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat,Md Motiur*

Main category: cs.AI

TL;DR: 本研究提出了一种成本有效的LLM适应方法，用于留学相关学术咨询，通过LoRA和量化技术提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、高效方法适应LLMs，用于低资源环境下的留学咨询和文化适应。

Method: 使用Mistral-7B-Instruct模型，结合LoRA和4-bit量化，分两个阶段训练：第一阶段用Gemini Pro API合成数据集，第二阶段用StudyAbroadGPT手动数据集。

Result: 训练损失减少52.7%，领域推荐准确率92%，Markdown支持95%，运行率100样本/秒。

Conclusion: 证明了在教育咨询中的有效性，尤其适用于低资源场景；限制包括泛化性降低和合成数据集使用，但框架可扩展，未来可整合RAG和实时数据库。

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>


### [10] [Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems](https://arxiv.org/abs/2504.15668)
*Mir Md Sajid Sarwar,Rajarshi Ray*

Main category: cs.AI

TL;DR: 本文提出一种通过识别关键不可达路径点来解释AI规划问题不可解性的方法，使用最长公共子序列和可达性分析。


<details>
  <summary>Details</summary>
Motivation: 解释规划问题不可解性在可解释AI规划中很重要，但研究较少，论文采用任务分解机制来分析不可解性。

Method: 将问题建模为最长公共子序列问题识别共同路径点，然后进行符号可达性分析，找到最早不可达路径点作为解释。

Result: 对混合域中的不可解规划问题进行了实验验证。

Conclusion: 该方法为解释规划问题不可解性提供了新途径，强调路径点不可达作为核心解释。

Abstract: Explaining unsolvability of planning problems is of significant research
interest in Explainable AI Planning. AI planning literature has reported
several research efforts on generating explanations of solutions to planning
problems. However, explaining the unsolvability of planning problems remains a
largely open and understudied problem. A widely practiced approach to plan
generation and automated problem solving, in general, is to decompose tasks
into sub-problems that help progressively converge towards the goal. In this
paper, we propose to adopt the same philosophy of sub-problem identification as
a mechanism for analyzing and explaining unsolvability of planning problems in
hybrid systems. In particular, for a given unsolvable planning problem, we
propose to identify common waypoints, which are universal obstacles to plan
existence; in other words, they appear on every plan from the source to the
planning goal. This work envisions such waypoints as sub-problems of the
planning problem and the unreachability of any of these waypoints as an
explanation for the unsolvability of the original planning problem. We propose
a novel method of waypoint identification by casting the problem as an instance
of the longest common subsequence problem, a widely popular problem in computer
science, typically considered as an illustrative example for the dynamic
programming paradigm. Once the waypoints are identified, we perform symbolic
reachability analysis on them to identify the earliest unreachable waypoint and
report it as the explanation of unsolvability. We present experimental results
on unsolvable planning problems in hybrid domains.

</details>


### [11] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/abs/2504.15699)
*Ning Wang,Zihan Yan,Weiyang Li,Chuan Ma,He Chen,Tao Xiang*

Main category: cs.AI

TL;DR: 这篇论文提出一个针对具身代理的安全输入调节框架，包括EAsafetyBench基准和Pinpoint方法，实现了高准确率和快速处理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注一般大型语言模型，缺乏针对具身代理的安全基准和输入调节方法，需要填补这一空白。

Method: 引入输入调节框架，包括分类学定义、数据集整理、调节器架构、模型训练和评估；提出EAsafetyBench基准和Pinpoint方法，使用掩码注意力机制隔离功能提示影响。

Result: 实验显示平均检测准确率94.58%，优于现有技术，处理时间仅0.002秒每实例。

Conclusion: 所提方法可行且有效，提升了具身代理的行为安全保障。

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>


### [12] [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)
*Jie Zhu,Qian Chen,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.AI

TL;DR: 本文提出DianJin-R1框架，通过推理增强监督和强化学习提升LLM在金融领域的推理能力，数据集基于CFLUE、FinQA和CCC，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: LLM在金融任务中面临推理挑战，需要领域特定知识、精确计算和合规规则遵守。

Method: 使用DianJin-R1框架构建数据集，对Qwen2.5模型进行微调，并应用GRPO强化学习方法以双重奖励信号优化推理。

Result: 模型在CFLUE、FinQA、CCC、MATH-500和GPQA-Diamond基准上优于非推理版本，尤其在金融任务中，计算成本更低地媲美多代理系统。

Conclusion: DianJin-R1通过结构化监督和奖励对齐学习有效提升金融推理，提供可扩展的实际解决方案。

Abstract: Effective reasoning remains a core challenge for large language models (LLMs)
in the financial domain, where tasks often require domain-specific knowledge,
precise numerical calculations, and strict adherence to compliance rules. We
propose DianJin-R1, a reasoning-enhanced framework designed to address these
challenges through reasoning-augmented supervision and reinforcement learning.
Central to our approach is DianJin-R1-Data, a high-quality dataset constructed
from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance
Check, CCC), combining diverse financial reasoning scenarios with verified
annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from
Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that
generates both reasoning steps and final answers. To further refine reasoning
quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement
learning method that incorporates dual reward signals: one encouraging
structured outputs and another rewarding answer correctness. We evaluate our
models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and
two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental
results show that DianJin-R1 models consistently outperform their non-reasoning
counterparts, especially on complex financial tasks. Moreover, on the
real-world CCC dataset, our single-call reasoning models match or even surpass
the performance of multi-agent systems that require significantly more
computational cost. These findings demonstrate the effectiveness of DianJin-R1
in enhancing financial reasoning through structured supervision and
reward-aligned learning, offering a scalable and practical solution for
real-world applications.

</details>


### [13] [Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences](https://arxiv.org/abs/2504.15719)
*Anna Karnysheva,Christian Drescher,Dietrich Klakow*

Main category: cs.AI

TL;DR: 这篇论文探讨大型语言模型在智能用户界面中的偏好对齐问题，提出了方法并在汽车领域进行了验证。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注LLM的事实性、偏差和毒性，但对偏好对齐的关注较少，而偏好对齐对可靠决策至关重要。

Method: 推广现有LLM用于排序备选方案的方法，以处理更广泛的用户偏好，包括严格偏好和无差异；提出了设计原则和工具来测量偏好满足度，并通过汽车领域的实证研究验证。

Result: 通过实证研究，展示了方法的适用性，可能包括偏好对齐的改善或具体发现。

Conclusion: 该方法为LLM在决策中的偏好对齐提供了框架，有助于提升智能用户界面的可靠性。

Abstract: As large language models (LLMs) become integral to intelligent user
interfaces (IUIs), their role as decision-making agents raises critical
concerns about alignment. Although extensive research has addressed issues such
as factuality, bias, and toxicity, comparatively little attention has been paid
to measuring alignment to preferences, i.e., the relative desirability of
different alternatives, a concept used in decision making, economics, and
social choice theory. However, a reliable decision-making agent makes choices
that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking
alternative outcomes by addressing alignment with the broader and more flexible
concept of user preferences, which includes both strict preferences and
indifference among alternatives. To this end, we put forward design principles
for using LLMs to implement rational choice functions, and provide the
necessary tools to measure preference satisfaction. We demonstrate the
applicability of our approach through an empirical study in a practical
application of an IUI in the automotive domain.

</details>


### [14] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TL;DR: 本文提出TrustGeoGen数据引擎，通过正式验证生成可靠的几何问题解决基准数据集，提升GPS任务评价标准。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在GPS上方法和基准缺失问题，现有合成基准存在噪音和自相矛盾。

Method: TrustGeoGen引擎包括多模态生成、正式验证、引导机制和GeoExplore算法。

Result: 生成GeoTrust-200K数据集，SOTA模型在测试集上准确率仅49.17%，训练后模型在GeoQA上泛化更好，减少逻辑错误。

Conclusion: 为GPS方法发展奠定基础，代码开源。

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>


### [15] [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)
*Siyu Zhou,Tianyi Zhou,Yijun Yang,Guodong Long,Deheng Ye,Jing Jiang,Chengqi Zhang*

Main category: cs.AI

TL;DR: 本论文提出一种无训练的世界对齐方法和WALL-E 2.0代理，通过桥接LLM与环境动态的差距，提升了代理在开放世界环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 桥接LLM先验知识与环境动态之间的差距，以克服LLM作为世界模型的性能瓶颈。

Method: 提出训练-free的世界对齐方法，提取行动规则、知识图谱和场景图编码为可执行代码；开发基于MPC框架的WALL-E 2.0代理，使用LLM作为优化器与神经符号世界模型交互。

Result: 在Mars环境成功率提升16.1%-51.6%，在ALFWorld环境仅4次迭代达到98%成功率。

Conclusion: 该方法显著提高了学习效率和性能，证明了神经符号世界模型在复杂环境中的有效性。

Abstract: Can we build accurate world models out of large language models (LLMs)? How
can world models benefit LLM agents? The gap between the prior knowledge of
LLMs and the specified environment's dynamics usually bottlenecks LLMs'
performance as world models. To bridge the gap, we propose a training-free
"world alignment" that learns an environment's symbolic knowledge complementary
to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and
scene graphs, which are extracted by LLMs from exploration trajectories and
encoded into executable codes to regulate LLM agents' policies. We further
propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive
control (MPC) framework. Unlike classical MPC requiring costly optimization on
the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future
steps' actions by interacting with the neurosymbolic world model. While the LLM
agent's strong heuristics make it an efficient planner in MPC, the quality of
its planned actions is also secured by the accurate predictions of the aligned
world model. They together considerably improve learning efficiency in a new
environment. On open-world challenges in Mars (Minecraft like) and ALFWorld
(embodied indoor environments), WALL-E 2.0 significantly outperforms existing
methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and
by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success
rate after only 4 iterations.

</details>


### [16] [Crisp complexity of fuzzy classifiers](https://arxiv.org/abs/2504.15791)
*Raquel Fernandez-Peralta,Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 这篇论文提出将模糊规则分类器简化为清晰规则分类器的方法，包括算法实现和复杂性分析，以提升可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则分类器在非模糊领域难以推广，因为用户可能不熟悉模糊逻辑，且模糊分区有时不易理解。

Method: 本研究提出一种将模糊规则分类器简化为清晰规则分类器的方法，探讨不同清晰描述方式，实现相应算法，并评估复杂性。

Result: 结果包括对清晰分类器复杂性的分析，并认为这有助于理解模糊规则如何分区特征空间，以及系统间的互译。

Conclusion: 这些结果可帮助模糊和非模糊从业者更好地理解系统，并基于等价清晰分区选择合适的模糊分类器。

Abstract: Rule-based systems are a very popular form of explainable AI, particularly in
the fuzzy community, where fuzzy rules are widely used for control and
classification problems. However, fuzzy rule-based classifiers struggle to
reach bigger traction outside of fuzzy venues, because users sometimes do not
know about fuzzy and because fuzzy partitions are not so easy to interpret in
some situations. In this work, we propose a methodology to reduce fuzzy
rule-based classifiers to crisp rule-based classifiers. We study different
possible crisp descriptions and implement an algorithm to obtain them. Also, we
analyze the complexity of the resulting crisp classifiers. We believe that our
results can help both fuzzy and non-fuzzy practitioners understand better the
way in which fuzzy rule bases partition the feature space and how easily one
system can be translated to another and vice versa. Our complexity metric can
also help to choose between different fuzzy classifiers based on what the
equivalent crisp partitions look like.

</details>


### [17] [Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases](https://arxiv.org/abs/2504.15829)
*Modhurita Mitra,Martine G. de Vos,Nicola Cortinovis,Dawa Ometto*

Main category: cs.AI

TL;DR: 这篇论文探讨了生成式AI在研究数据处理中的应用，通过三个案例展示了其可行性，并分享了使用经验。


<details>
  <summary>Details</summary>
Motivation: 针对生成式AI输出准确性和一致性的担忧，自ChatGPT 2022年推出后，作者进行了探索性研究，聚焦于传统方法难以应用的 tasks。

Method: 使用生成式AI模型Claude 3 Opus，进行信息提取、自然语言理解和文本分类等任务。

Result: 证明了生成式AI在这些任务中的可行性，并总结了如何判断其适用性和优化准确性的经验。

Conclusion: 生成式AI是处理复杂数据任务的有力工具，但需要适当的方法来确保结果的可靠性和一致性。

Abstract: There has been enormous interest in generative AI since ChatGPT was launched
in 2022. However, there are concerns about the accuracy and consistency of the
outputs of generative AI. We have carried out an exploratory study on the
application of this new technology in research data processing. We identified
tasks for which rule-based or traditional machine learning approaches were
difficult to apply, and then performed these tasks using generative AI.
  We demonstrate the feasibility of using the generative AI model Claude 3 Opus
in three research projects involving complex data processing tasks:
  1) Information extraction: We extract plant species names from historical
seedlists (catalogues of seeds) published by botanical gardens.
  2) Natural language understanding: We extract certain data points (name of
drug, name of health indication, relative effectiveness, cost-effectiveness,
etc.) from documents published by Health Technology Assessment organisations in
the EU.
  3) Text classification: We assign industry codes to projects on the
crowdfunding website Kickstarter.
  We share the lessons we learnt from these use cases: How to determine if
generative AI is an appropriate tool for a given data processing task, and if
so, how to maximise the accuracy and consistency of the results obtained.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions](https://arxiv.org/abs/2504.15300)
*Chaoyue Niu,Yucheng Ding,Junhui Lu,Zhengxiang Huang,Hang Zeng,Yutong Dai,Xuezhen Tu,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 这篇调查探讨了设备端小模型与云端大模型的协作学习范式，以解决传统云端框架的延迟、成本、个性化及隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统云端大型模型学习框架受限于延迟、成本、个性化及隐私问题。

Method: 通过对硬件、系统、算法和应用层的全面回顾，总结关键问题和进展，并将协作算法分类为基于数据、特征和参数的框架。

Result: 总结了学术和工业进展，审阅了数据集和评估指标，突出了实际部署案例。

Conclusion: 指出了开放研究方向，以指导未来发展。

Abstract: The conventional cloud-based large model learning framework is increasingly
constrained by latency, cost, personalization, and privacy concerns. In this
survey, we explore an emerging paradigm: collaborative learning between
on-device small model and cloud-based large model, which promises low-latency,
cost-efficient, and personalized intelligent services while preserving user
privacy. We provide a comprehensive review across hardware, system, algorithm,
and application layers. At each layer, we summarize key problems and recent
advances from both academia and industry. In particular, we categorize
collaboration algorithms into data-based, feature-based, and parameter-based
frameworks. We also review publicly available datasets and evaluation metrics
with user-level or device-level consideration tailored to collaborative
learning settings. We further highlight real-world deployments, ranging from
recommender systems and mobile livestreaming to personal intelligent
assistants. We finally point out open research directions to guide future
development in this rapidly evolving field.

</details>


### [19] [Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches](https://arxiv.org/abs/2504.15310)
*Syeda Tahreem Zahra,Syed Kashif Imdad,Sohail Khan,Sohail Khalid,Nauman Anwar Baig*

Main category: cs.LG

TL;DR: 这篇论文综述了电力变压器故障诊断中的AI应用，分析了智能算法并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 为了评估变压器健康和预测寿命，确保高效运行和维护规划。

Method: 文献综述、分析传统与先进技术、阐述AI方法如ANN、CNN、SVM、RF、GA、PSO等。

Result: 提供了AI应用的全面概述，提高诊断精度和早期故障检测。

Conclusion: 为未来研究奠定基础，促进该领域发展。

Abstract: Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study.

</details>


### [20] [M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data](https://arxiv.org/abs/2504.15312)
*Muhammad Mursil,Hatem A. Rashwan,Luis Santos-Calderon,Pere Cavalle-Busquets,Michelle M. Murphy,Domenec Puig*

Main category: cs.LG

TL;DR: 这篇论文使用基于注意力的transformer模型整合孕妇多种数据，在妊娠12周前预测出生体重，实现了高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 动机是改善出生体重早期预测以实现及时干预，当前方法存在准确性低和忽略营养、遗传因素的问题。

Method: 方法是提出一个多编码器注意力机制transformer模型，整合生理、生活方式、营养和遗传数据用于早期预测。

Result: 结果显示MAE为122克，R平方0.94；独立验证MAE为105克，R平方0.95；分类性能敏感性97.55%，特异性94.48%，并通过SHAP分析增强可解释性。

Conclusion: 结论强调深度学习模型可提升早期预测准确性，提供可靠工具帮助临床识别高风险妊娠并优化新生儿结局。

Abstract: Birth weight (BW) is a key indicator of neonatal health, with low birth
weight (LBW) linked to increased mortality and morbidity. Early prediction of
BW enables timely interventions; however, current methods like ultrasonography
have limitations, including reduced accuracy before 20 weeks and operator
dependent variability. Existing models often neglect nutritional and genetic
influences, focusing mainly on physiological and lifestyle factors. This study
presents an attention-based transformer model with a multi-encoder architecture
for early (less than 12 weeks of gestation) BW prediction. Our model
effectively integrates diverse maternal data such as physiological, lifestyle,
nutritional, and genetic, addressing limitations seen in prior attention-based
models such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122
grams and an R-squared value of 0.94, demonstrating high predictive accuracy
and interoperability with our in-house private dataset. Independent validation
confirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE
children dataset. To enhance clinical utility, predicted BW is classified into
low and normal categories, achieving a sensitivity of 97.55% and a specificity
of 94.48%, facilitating early risk stratification. Model interpretability is
reinforced through feature importance and SHAP analyses, highlighting
significant influences of maternal age, tobacco exposure, and vitamin B12
status, with genetic factors playing a secondary role. Our results emphasize
the potential of advanced deep-learning models to improve early BW prediction,
offering clinicians a robust, interpretable, and personalized tool for
identifying pregnancies at risk and optimizing neonatal outcomes.

</details>


### [21] [Diffusion-Driven Inertial Generated Data for Smartphone Location Classification](https://arxiv.org/abs/2504.15315)
*Noa Cohen,Rotem Dror,Itzik Klein*

Main category: cs.LG

TL;DR: 本文使用扩散模型生成合成惯性数据，减少数据收集负担，用于智能手机位置识别。


<details>
  <summary>Details</summary>
Motivation: 惯性测量数据收集耗时资源密集，阻碍机器学习模型发展；扩散模型提供高效合成数据生成方法。

Method: 提出基于扩散模型的特定力数据生成方法，并通过多指标比较合成与真实数据进行评估。

Result: 扩散模型成功捕获特定力信号特征，减少数据收集需求。

Conclusion: 合成数据可提供高质量训练数据，减轻实际数据收集负担。

Abstract: Despite the crucial role of inertial measurements in motion tracking and
navigation systems, the time-consuming and resource-intensive nature of
collecting extensive inertial data has hindered the development of robust
machine learning models in this field. In recent years, diffusion models have
emerged as a revolutionary class of generative models, reshaping the landscape
of artificial data generation. These models surpass generative adversarial
networks and other state-of-the-art approaches to complex tasks. In this work,
we propose diffusion-driven specific force-generated data for smartphone
location recognition. We provide a comprehensive evaluation methodology by
comparing synthetic and real recorded specific force data across multiple
metrics. Our results demonstrate that our diffusion-based generative model
successfully captures the distinctive characteristics of specific force signals
across different smartphone placement conditions. Thus, by creating diverse,
realistic synthetic data, we can reduce the burden of extensive data collection
while providing high-quality training data for machine learning models.

</details>


### [22] [How to systematically develop an effective AI-based bias correction model?](https://arxiv.org/abs/2504.15322)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 这篇论文引入了ReSA-ConvLSTM框架，用于数值天气预报中的系统偏差校正。


<details>
  <summary>Details</summary>
Motivation: 动机是改进天气预报准确性，通过减少温度、风速和海平面气压等变量的系统偏差。

Method: 方法包括动态气候标准化、带有时间因果约束的ConvLSTM和残差自注意力机制，建立了ECMWF预报与ERA5再分析数据之间的物理感知非线性映射。

Result: 结果显示，RMSE降低了20%，模型参数仅10.6M，减少了85%的再训练时间，并提升了海洋模型性能。

Conclusion: 结论是通过消融实验证明创新显著改进预报技能，建议将变量特性融入模型。

Abstract: This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)
framework for systematic bias correction in numerical weather prediction (NWP).
We propose three innovations by integrating dynamic climatological
normalization, ConvLSTM with temporal causality constraints, and residual
self-attention mechanisms. The model establishes a physics-aware nonlinear
mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years
(1981-2021) of global atmospheric data, the framework reduces systematic biases
in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure
(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to
operational ECMWF outputs. The lightweight architecture (10.6M parameters)
enables efficient generalization to multiple variables and downstream
applications, reducing retraining time by 85% for cross-variable correction
while improving ocean model skill through bias-corrected boundary conditions.
The ablation experiments demonstrate that our innovations significantly improve
the model's correction performance, suggesting that incorporating variable
characteristics into the model helps enhance forecasting skills.

</details>


### [23] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 本方法通过模拟梯度下降无需计算梯度，实现高效测试时适应，仅需少量前向传播，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决测试时微调在实时或低资源场景下计算开销过大的问题。

Method: 将梯度下降形式化为ODE的Euler离散化，训练辅助网络预测任务条件漂移，然后使用数值积分（如Euler方法）进行适应，无需梯度或目标模型前向传播。

Result: 在Meta-Dataset和CDFSL基准的跨域少样本分类实验中，显著提升域外性能，同时内存成本仅为标准的6%，计算时间仅为0.02%。

Conclusion: 建立了直接转移和完全微调方法之间的实用中间方案。

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>


### [24] [Significativity Indices for Agreement Values](https://arxiv.org/abs/2504.15325)
*Alberto Casagrande,Francesco Fabris,Rossano Girometti,Roberto Pagliarini*

Main category: cs.LG

TL;DR: 这篇论文提出了一种评估分类器一致性度量显著性的通用方法，并引入了两个显著性指数。


<details>
  <summary>Details</summary>
Motivation: 现有的如Cohen's kappa的一致性度量缺乏可靠的规模，边界任意，需要更好的显著性评估。

Method: 提出评估一致性值显著性的通用方法，引入针对有限数据集和概率分布的指数，并讨论计算效率和高效算法。

Result: 引入了两个显著性指数并识别了高效的评估算法。

Conclusion: 这项工作提供了更可靠的评估方法，改进了现有的一致性度量规模。

Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge
the matching between two or more classifiers. They are used in a wide range of
contexts from medicine, where they evaluate the effectiveness of medical
treatments and clinical trials, to artificial intelligence, where they can
quantify the approximation due to the reduction of a classifier. The
consistency of different classifiers to a golden standard can be compared
simply by using the order induced by their agreement measure with respect to
the golden standard itself. Nevertheless, labelling an approach as good or bad
exclusively by using the value of an agreement measure requires a scale or a
significativity index. Some quality scales have been proposed in the literature
for Cohen's kappa, but they are mainly naive, and their boundaries are
arbitrary. This work proposes a general approach to evaluate the
significativity of any agreement value between two classifiers and introduces
two significativity indices: one dealing with finite data sets, the other one
handling classification probability distributions. Moreover, this manuscript
considers the computational issues of evaluating such indices and identifies
some efficient algorithms to evaluate them.

</details>


### [25] [Bayesian Federated Learning for Continual Training](https://arxiv.org/abs/2504.15328)
*Usevalad Milasheuski,Luca Barbieri,Sanaz Kianoush,Monica Nicoli,Stefano Savazzi*

Main category: cs.LG

TL;DR: 论文提出持续Bayesian Federated Learning框架，用于雷达数据的人类感知，处理数据分布变化，使用SGLD方法，提高准确性、校准和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前Bayesian Federated Learning方法忽略动态环境中数据分布变化的持续学习挑战，因此提出新框架解决此问题。

Method: 使用随机梯度Langevin动力学(SGLD)顺序更新模型，将过去的后验作为新任务的先验。

Result: 与基线比较，展示了在准确性、期望校准错误(ECE)和收敛速度上的优势，突出了持续Bayesian更新在知识保留和适应性方面的有效性。

Conclusion: 持续Bayesian更新方法在动态环境中有效，能够保留知识并适应演变的数据。

Abstract: Bayesian Federated Learning (BFL) enables uncertainty quantification and
robust adaptation in distributed learning. In contrast to the frequentist
approach, it estimates the posterior distribution of a global model, offering
insights into model reliability. However, current BFL methods neglect continual
learning challenges in dynamic environments where data distributions shift over
time. We propose a continual BFL framework applied to human sensing with radar
data collected over several days. Using Stochastic Gradient Langevin Dynamics
(SGLD), our approach sequentially updates the model, leveraging past posteriors
to construct the prior for the new tasks. We assess the accuracy, the expected
calibration error (ECE) and the convergence speed of our approach against
several baselines. Results highlight the effectiveness of continual Bayesian
updates in preserving knowledge and adapting to evolving data.

</details>


### [26] [FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching](https://arxiv.org/abs/2504.15366)
*Qifan Yan,Andrew Liu,Shiqi He,Mathias Lécuyer,Ivan Beschastnikh*

Main category: cs.LG

TL;DR: FedFetch 通过预取模型状态减少联邦学习中的下载时间开销，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中通信瓶颈问题，特别是客户端采样和压缩技术结合导致的模型同步开销。

Method: FedFetch 使用高效预取调度，让客户端提前多个轮次预取模型状态。

Result: 实验显示，添加 FedFetch 可将端到端训练时间减少 1.26 倍，下载时间减少 4.49 倍。

Conclusion: FedFetch 有效缓解通信瓶颈，并提供开源实现。

Abstract: Federated learning (FL) is a machine learning paradigm that facilitates
massively distributed model training with end-user data on edge devices
directed by a central server. However, the large number of heterogeneous
clients in FL deployments leads to a communication bottleneck between the
server and the clients. This bottleneck is made worse by straggling clients,
any one of which will further slow down training. To tackle these challenges,
researchers have proposed techniques like client sampling and update
compression. These techniques work well in isolation but combine poorly in the
downstream, server-to-client direction. This is because unselected clients have
outdated local model states and need to synchronize these states with the
server first.
  We introduce FedFetch, a strategy to mitigate the download time overhead
caused by combining client sampling and compression techniques. FedFetch
achieves this with an efficient prefetch schedule for clients to prefetch model
states multiple rounds before a stated training round. We empirically show that
adding FedFetch to communication efficient FL techniques reduces end-to-end
training time by 1.26$\times$ and download time by 4.49$\times$ across
compression techniques with heterogeneous client settings. Our implementation
is available at https://github.com/DistributedML/FedFetch

</details>


### [27] [Solving New Tasks by Adapting Internet Video Knowledge](https://arxiv.org/abs/2504.15369)
*Calvin Luo,Zilai Zeng,Yilun Du,Chen Sun*

Main category: cs.LG

TL;DR: 这篇论文探讨了通过适应大规模预训练视频模型与领域内数据相结合的方法，来提升机器人任务的文本条件泛化能力，并引入了逆概率适应策略，该策略对数据质量具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决使用互联网规模数据（缺乏环境特定性）或领域内数据（规模不足以支持泛化）的局限性，通过整合两者实现更好的机器人任务泛化。

Method: 研究了各种适应技术，并提出了逆概率适应策略，将领域内信息与预训练视频模型整合，以支持新型文本条件任务。

Result: 证明了使用少量领域内数据适应视频模型可以实现对新行为的成功泛化，新策略在数据质量较低时仍能有效解决任务。

Conclusion: 适应强大视频模型的小规模数据可促进有效泛化，逆概率适应方法在不同机器人任务中表现出色且鲁棒。

Abstract: Video generative models demonstrate great promise in robotics by serving as
visual planners or as policy supervisors. When pretrained on internet-scale
data, such video models intimately understand alignment with natural language,
and can thus facilitate generalization to novel downstream behavior through
text-conditioning. However, they may not be sensitive to the specificities of
the particular environment the agent inhabits. On the other hand, training
video models on in-domain examples of robotic behavior naturally encodes
environment-specific intricacies, but the scale of available demonstrations may
not be sufficient to support generalization to unseen tasks via natural
language specification. In this work, we investigate different adaptation
techniques that integrate in-domain information with large-scale pretrained
video models, and explore the extent to which they enable novel
text-conditioned generalization for robotic tasks, while also considering their
independent data and resource considerations. We successfully demonstrate
across robotic environments that adapting powerful video models with small
scales of example data can successfully facilitate generalization to novel
behaviors. In particular, we present a novel adaptation strategy, termed
Inverse Probabilistic Adaptation, that not only consistently achieves strong
generalization performance across robotic tasks and settings, but also exhibits
robustness to the quality of adaptation data, successfully solving novel tasks
even when only suboptimal in-domain demonstrations are available.

</details>


### [28] [Improving Learning to Optimize Using Parameter Symmetries](https://arxiv.org/abs/2504.15399)
*Guy Zamir,Aryan Dokania,Bo Zhao,Rose Yu*

Main category: cs.LG

TL;DR: 这篇论文分析了一个利用参数空间对称性的学习优化算法，通过理论和实证方法展示了其提升优化效率的优势。


<details>
  <summary>Details</summary>
Motivation: 动机是基于先前工作显示联合学习对称变换和局部更新能改善元优化器性能。

Method: 方法包括理论分析（类似于牛顿法）、提供算法学习对称变换的示例、以及实证评估通过基准和动量增强。

Result: 结果显示算法理论上类似于牛顿法，实证上性能提升，并分析了成功和失败案例。

Conclusion: 结论强调了利用神经网络参数空间对称性推进元优化的潜力。

Abstract: We analyze a learning-to-optimize (L2O) algorithm that exploits parameter
space symmetry to enhance optimization efficiency. Prior work has shown that
jointly learning symmetry transformations and local updates improves
meta-optimizer performance. Supporting this, our theoretical analysis
demonstrates that even without identifying the optimal group element, the
method locally resembles Newton's method. We further provide an example where
the algorithm provably learns the correct symmetry transformation during
training. To empirically evaluate L2O with teleportation, we introduce a
benchmark, analyze its success and failure cases, and show that enhancements
like momentum further improve performance. Our results highlight the potential
of leveraging neural network parameter space symmetry to advance
meta-optimization.

</details>


### [29] [Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering](https://arxiv.org/abs/2504.15439)
*Hao Zhuo,Yicheng Yang,Kewen Peng*

Main category: cs.LG

TL;DR: This paper reviews toxicity detection and mitigation in LLMs for software engineering, including an ablation study on LLM-based rewriting.


<details>
  <summary>Details</summary>
Motivation: To address concerns about toxic language in LLMs used in SE and ensure responsible deployment.

Method: Comprehensive review of research, examination of annotation, preprocessing, detection methodologies, mitigation strategies, and an ablation study.

Result: Ablation study demonstrates effectiveness of LLM-based rewriting in reducing toxicity and identifies open challenges.

Conclusion: Highlights key areas for future research to ensure responsible use of LLMs in SE and beyond.

Abstract: Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.

</details>


### [30] [Compton Form Factor Extraction using Quantum Deep Neural Networks](https://arxiv.org/abs/2504.15458)
*Brandon Le,Dustin Keller*

Main category: cs.LG

TL;DR: 本文使用神经网络从深虚康普顿散射数据中提取康普顿形因子，发现量子神经网络比经典神经网络表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了减少模型依赖性提取康普顿形因子，并比较经典和量子神经网络在这一应用中的性能。

Method: 使用杰斐逊实验室的深虚康普顿散射实验伪数据、Belitsky-Kirchner-Muller扭转二阶形式主义、拟合程序，并采用经典深度神经网络和量子深度神经网络进行提取。

Result: 量子深度神经网络在预测准确性和精度上优于经典深度神经网络，即使模型复杂度较低。

Conclusion: 结果展示了量子深度神经网络在未来量子算法优化研究中的潜力。

Abstract: Extraction tests of Compton Form Factors are performed using pseudodata based
on experimental data from Deeply Virtual Compton Scattering experiments
conducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller
formalism at twist-two is employed, along with a fitting procedure designed to
reduce model dependency similar to traditional local fits. The extraction of
the Compton Form Factors is performed using both Classical Deep Neural Networks
(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal
that QDNNs outperform CDNNs for this application, demonstrating improved
predictive accuracy and precision even for limited model complexity. The
results demonstrate the potential of QDNNs for future studies in which quantum
algorithms can be fully optimized.

</details>


### [31] [In-context Ranking Preference Optimization](https://arxiv.org/abs/2504.15477)
*Junda Wu,Rohan Surana,Zhouhang Xie,Yiran Shen,Yu Xia,Tong Yu,Ryan A. Rossi,Prithviraj Ammanabrolu,Julian McAuley*

Main category: cs.LG

TL;DR: 本文提出IRPO框架，优化LLM的上下文排名偏好，超越标准DPO。


<details>
  <summary>Details</summary>
Motivation: 用户反馈通常稀疏且非成对，许多任务如对话代理需要良好排名。

Method: 提出IRPO，通过可微分目标基于位置聚合优化排名列表。

Result: 经验结果显示IRPO在排名性能上优于DPO。

Conclusion: IRPO有效提升LLM与用户排名偏好的对齐，并提供理论洞察。

Abstract: Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
sparse pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.

</details>


### [32] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TL;DR: 本文提出一种名为Counterfactual Attacks的新框架，用于生成计算机视觉模型的逆事实解释，避免基于梯度的标准方法产生对抗样本。


<details>
  <summary>Details</summary>
Motivation: 逆事实解释在计算机视觉中很难实现，因为传统方法易产生对抗样本，导致像素微小变化引起预测大变，无法提供可靠解释。

Method: 引入Counterfactual Attacks框架，通过在低维流形上对图像表示进行类似对抗攻击，并利用辅助数据集提供特征归因，量化原图与逆事实图的差异。

Result: 在MNIST和CelebA数据集上验证了方法的有效性，具有计算效率高，并能聚合全局逆事实解释。

Conclusion: 该方法为计算机视觉模型提供了一种灵活、高效的逆事实解释生成方式，提高了解释的可解释性和实用性。

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>


### [33] [Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence](https://arxiv.org/abs/2504.15487)
*Moein Darman,Pedram Hassanzadeh,Laure Zanna,Ashesh Chattopadhyay*

Main category: cs.LG

TL;DR: 这篇论文研究了Transfer Learning在神经网络中的应用，提高了模型在少量数据下对分布外数据的泛化能力，焦点在海洋准地转系统的预测上。


<details>
  <summary>Details</summary>
Motivation: 动机是Transfer Learning能让神经网络在天气、气候预测和湍流建模中，用最小训练数据泛化到新系统。

Method: 方法使用了9层卷积神经网络结合Fourier分析和激活谱分析，来预测子网格强制并评估性能和泛化性。

Result: 结果显示神经网络学习了低通、Gabor和高通滤波器；无Transfer Learning时输出谱低估，Transfer Learning通过重新训练一层纠正了这一问题。

Conclusion: 结论是这些发现可广泛应用于动态系统的数据驱动参数化。

Abstract: Transfer learning (TL) is a powerful tool for enhancing the performance of
neural networks (NNs) in applications such as weather and climate prediction
and turbulence modeling. TL enables models to generalize to out-of-distribution
data with minimal training data from the new system. In this study, we employ a
9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean
quasi-geostrophic system and examine which metrics best describe its
performance and generalizability to unseen dynamical regimes. Fourier analysis
of the NN kernels reveals that they learn low-pass, Gabor, and high-pass
filters, regardless of whether the training data are isotropic or anisotropic.
By analyzing the activation spectra, we identify why NNs fail to generalize
without TL and how TL can overcome these limitations: the learned weights and
biases from one dataset underestimate the out-of-distribution sample spectra as
they pass through the network, leading to an underestimation of output spectra.
By re-training only one layer with data from the target system, this
underestimation is corrected, enabling the NN to produce predictions that match
the target spectra. These findings are broadly applicable to data-driven
parameterization of dynamical systems.

</details>


### [34] [Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions](https://arxiv.org/abs/2504.15491)
*Tengda Tang,Jianhua Yao,Yixian Wang,Qiuwu Sha,Hanrui Feng,Zhen Xu*

Main category: cs.LG

TL;DR: 本研究提出了一种结合GAN和VAE的算法，用于检测金融交易中的可疑行为，提高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 动机是针对稀疏数据条件下检测异常行为如欺诈和洗钱。

Method: 方法使用GAN生成模拟正常支付流数据，VAE建模潜在分布，优化两者生成能力以提升检测精度。

Result: 实验结果显示该方法在各种指标上优于传统和其它深度学习模型，尤其在检测稀有欺诈行为上。

Conclusion: 验证了生成模型在处理复杂金融数据中的优势。

Abstract: This study proposes an algorithm for detecting suspicious behaviors in large
payment flows based on deep generative models. By combining Generative
Adversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is
designed to detect abnormal behaviors in financial transactions. First, the GAN
is used to generate simulated data that approximates normal payment flows. The
discriminator identifies anomalous patterns in transactions, enabling the
detection of potential fraud and money laundering behaviors. Second, a VAE is
introduced to model the latent distribution of payment flows, ensuring that the
generated data more closely resembles real transaction features, thus improving
the model's detection accuracy. The method optimizes the generative
capabilities of both GAN and VAE, ensuring that the model can effectively
capture suspicious behaviors even in sparse data conditions. Experimental
results show that the proposed method significantly outperforms traditional
machine learning algorithms and other deep learning models across various
evaluation metrics, especially in detecting rare fraudulent behaviors.
Furthermore, this study provides a detailed comparison of performance in
recognizing different transaction patterns (such as normal, money laundering,
and fraud) in large payment flows, validating the advantages of generative
models in handling complex financial data.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [35] [A Quadratic Control Framework for Dynamic Systems](https://arxiv.org/abs/2504.15396)
*Igor Ladnik*

Main category: eess.SY

TL;DR: 这篇论文提出一种统一方法，用于线性与非线性离散时间系统的二次最优控制，专注于轨迹跟踪，通过最小化二次代价函数。


<details>
  <summary>Details</summary>
Motivation: 动机是提供一个统一方法来处理线性与非线性系统的轨迹跟踪问题，尤其在非线性情况下的优化控制。

Method: 方法包括线性系统使用LQR和动态规划，非线性系统使用iLQR通过迭代线性化和求解一系列LQR问题，并开发软件服务在多个模型上测试。

Result: 结果显示iLQR实现了高效准确的轨迹跟踪，并可与MPC整合提升性能。

Conclusion: 结论是可以与MPC无缝整合，提高对约束和不确定性的鲁棒性。

Abstract: This article presents a unified approach to quadratic optimal control for
both linear and nonlinear discrete-time systems, with a focus on trajectory
tracking. The control strategy is based on minimizing a quadratic cost function
that penalizes deviations of system states and control inputs from their
desired trajectories.
  For linear systems, the classical Linear Quadratic Regulator (LQR) solution
is derived using dynamic programming, resulting in recursive equations for
feedback and feedforward terms. For nonlinear dynamics, the Iterative Linear
Quadratic Regulator (iLQR) method is employed, which iteratively linearizes the
system and solves a sequence of LQR problems to converge to an optimal policy.
  To implement this approach, a software service was developed and tested on
several canonical models, including: Rayleigh oscillator, inverted pendulum on
a moving cart, two-link manipulator, and quadcopter. The results confirm that
iLQR enables efficient and accurate trajectory tracking in the presence of
nonlinearities.
  To further enhance performance, it can be seamlessly integrated with Model
Predictive Control (MPC), enabling online adaptation and improved robustness to
constraints and system uncertainties.

</details>


### [36] [Safety Embedded Adaptive Control Using Barrier States](https://arxiv.org/abs/2504.15423)
*Maitham F. AL-Sunni,Hassan Almubarak,John M. Dolan*

Main category: eess.SY

TL;DR: 本文探讨屏障状态在安全非线性自适应控制中的应用，确保参数不确定系统安全。


<details>
  <summary>Details</summary>
Motivation: 针对参数不确定性系统，动机是开发安全控制策略以避免潜在风险。

Method: 推导屏障状态并增强到动态模型中，使用基于控制李雅普诺夫函数的自适应非线性控制设计稳定控制器。

Result: 控制器确保原系统安全并满足性能目标，通过对平面四旋翼和自适应巡航控制的模拟验证，并与现有方法比较。

Conclusion: 该方法有效，提高了不确定系统中的安全控制。

Abstract: In this work, we explore the application of barrier states (BaS) in the realm
of safe nonlinear adaptive control. Our proposed framework derives barrier
states for systems with parametric uncertainty, which are augmented into the
uncertain dynamical model. We employ an adaptive nonlinear control strategy
based on a control Lyapunov functions approach to design a stabilizing
controller for the augmented system. The developed theory shows that the
controller ensures safe control actions for the original system while meeting
specified performance objectives. We validate the effectiveness of our approach
through simulations on diverse systems, including a planar quadrotor subject to
unknown drag forces and an adaptive cruise control system, for which we provide
comparisons with existing methodologies.

</details>


### [37] [Nearly Optimal Nonlinear Safe Control with BaS-SDRE](https://arxiv.org/abs/2504.15453)
*Hassan Almubarak,Maitham F. AL-Sunni,Justin T. Dubbin,Nader Sadegh,John M. Dolan,Evangelos A. Theodorou*

Main category: eess.SY

TL;DR: 本论文结合SDRE和BaS方法，开发非线性近似最优安全反馈控制技术，并通过实例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了处理安全约束下的非线性系统控制，结合SDRE的非线性控制器设计和BaS的安全嵌入方法。

Method: 通过扩展线性化导出BaS，形成安全嵌入系统，构建BaS-SDRE并在线求解，以合成近似最优安全控制。

Result: 在不稳定约束线性系统、飞行控制系统和四旋翼系统中验证，展示了控制对非线性性和安全区域的响应。

Conclusion: 所提技术有效实现了非线性系统的近似最优安全控制。

Abstract: The State-Dependent Riccati Equation (SDRE) approach has emerged as a
systematic and effective means of designing nearly optimal nonlinear
controllers. The Barrier States (BaS) embedding methodology was developed
recently for safe multi-objective controls in which the safety condition is
manifested as a state to be controlled along with other states of the system.
The overall system, termed the safety embedded system, is highly nonlinear even
if the original system is linear. This paper develops a nonlinear nearly
optimal safe feedback control technique by combining the two strategies
effectively. First, the BaS is derived in an extended linearization formulation
to be subsequently used to form an extended safety embedded system. A new
optimal control problem is formed thereafter, which is used to construct a
safety embedded State-Dependent Riccati Equation, termed BaS-SDRE, whose
solution approximates the solution of the optimal control problem's associated
Hamilton-Jacobi-Bellman (HJB) equation. The BaS-SDRE is then solved online to
synthesize the nearly optimal safe control. The proposed technique's efficacy
is demonstrated on an unstable, constrained linear system that shows how the
synthesized control reacts to nonlinearities near the unsafe region, a
nonlinear flight control system with limited path angular velocity that exists
due to structural and dynamic concerns, and a planar quadrotor system that
navigates safely in a crowded environment.

</details>


### [38] [Explicit Ensemble Mean Clock Synchronization for Optimal Atomic Time Scale Generation](https://arxiv.org/abs/2504.15540)
*Takayuki Ishizaki,Takahiro Kawaguchi,Yuichiro Yano,Yuko Hanado*

Main category: eess.SY

TL;DR: 这篇论文提出了一种新的理论框架explicit ensemble mean synchronization，使用控制理论统一了时钟同步和时间尺度生成，证明了Kalman过滤是其特例，并通过状态反馈控制优化了性能。


<details>
  <summary>Details</summary>
Motivation: 动机是统一时钟同步和时间尺度生成，提供一个超越传统方法的稳健时间保持系统。

Method: 方法包括利用可观测规范分解将原子钟模型分解为可观测和不可观测部分，并应用Kalman过滤和状态反馈控制。

Result: 结果证明了标准Kalman过滤是框架的特例，并实现了时钟同步和最优时间尺度生成。

Conclusion: 结论是这个框架为稳健时间保持系统提供了原则基础，超越了传统方法在范围和性能上。

Abstract: This paper presents a novel theoretical framework for atomic time scale
generation, called explicit ensemble mean synchronization, which unifies clock
synchronization and time scale generation within a control-theoretic paradigm.
By exploiting an observable canonical decomposition of a standard atomic clock
ensemble model, the system is decomposed into two complementary components: the
observable part, which represents the synchronization deviation, and the
unobservable part, which captures the synchronization destination. Within this
structure, we mathematically prove that standard Kalman filtering, widely used
in current time scale generation, can be interpreted as a special case of the
proposed framework that optimizes long-term frequency stability in terms of the
Allan variance. Furthermore, by applying appropriate state feedback control to
each component based on the Kalman filtering, both clock synchronization and
optimal time scale generation are achieved within a unified framework. This
framework provides a principled basis for robust timekeeping systems that goes
beyond conventional approaches in both scope and performance.

</details>


### [39] [Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model](https://arxiv.org/abs/2504.15578)
*Ian Mikesell,Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Prashanth Ramesh,Marcello Canova*

Main category: eess.SY

TL;DR: 这篇论文提出基于强化学习的锂离子电池参数识别方法，通过动态优化电流配置文件来提高准确性和效率，并在硬件在环设置中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法需大量实验数据且缺乏动态适应性，需要更高效的优化策略。

Method: 使用强化学习框架在硬件在环（HIL）环境中实时调整电池电流配置文件以提升参数可识别性。

Result: 强化学习方法在减少建模错误和缩短实验时间方面优于传统协议。

Conclusion: HIL验证证实了该方法在参数识别方面的有效性和优越性。

Abstract: Accurately identifying the parameters of electrochemical models of li-ion
battery (LiB) cells is a critical task for enhancing the fidelity and
predictive ability. Traditional parameter identification methods often require
extensive data collection experiments and lack adaptability in dynamic
environments. This paper describes a Reinforcement Learning (RL) based approach
that dynamically tailors the current profile applied to a LiB cell to optimize
the parameters identifiability of the electrochemical model. The proposed
framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,
which serves as a reliable testbed for evaluating the RL-based design strategy.
The HIL validation confirms that the RL-based experimental design outperforms
conventional test protocols used for parameter identification in terms of both
reducing the modeling errors on a verification test and minimizing the duration
of the experiment used for parameter identification.

</details>


### [40] [An ACO-MPC Framework for Energy-Efficient and Collision-Free Path Planning in Autonomous Maritime Navigation](https://arxiv.org/abs/2504.15611)
*Yaoze Liu,Zhen Tian,Qifan Zhou,Zixuan Huang,Hongyu Sun*

Main category: eess.SY

TL;DR: 这篇论文提出了一种集成规划器，用于自动驾驶车辆在坡道上安全高效换道，通过模拟验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 坡道换道需平衡安全与效率，论文针对此挑战提出新方法。

Method: 使用不满意度指标和速度评估换道时机，采用箭头聚类采样评估碰撞风险并选择安全曲线。

Result: 模拟结果显示规划器能有效选择换道时机和曲线，无碰撞发生。

Conclusion: 规划器成功实现了坡道换道的安全性和效率。

Abstract: Automated driving on ramps presents significant challenges due to the need to
balance both safety and efficiency during lane changes. This paper proposes an
integrated planner for automated vehicles (AVs) on ramps, utilizing an
unsatisfactory level metric for efficiency and arrow-cluster-based sampling for
safety. The planner identifies optimal times for the AV to change lanes, taking
into account the vehicle's velocity as a key factor in efficiency.
Additionally, the integrated planner employs arrow-cluster-based sampling to
evaluate collision risks and select an optimal lane-changing curve. Extensive
simulations were conducted in a ramp scenario to verify the planner's efficient
and safe performance. The results demonstrate that the proposed planner can
effectively select an appropriate lane-changing time point and a safe
lane-changing curve for AVs, without incurring any collisions during the
maneuver.

</details>


### [41] [On relaxing the N-Reachability Implicit Requirement in NMPC Design](https://arxiv.org/abs/2504.15704)
*Mazen Alamir*

Main category: eess.SY

TL;DR: 这篇论文提出了一种Model Predictive Control的稳定性证明，使用stage cost的收缩条件替代传统的reachability条件，并保持短的预测地平线。


<details>
  <summary>Details</summary>
Motivation: 为了解决预测地平线过短无法满足reachability条件的稳定问题，提供一个基于收缩条件的替代方法。

Method: 采用stage cost收缩条件作为稳定保证，预测地平线保持常数和短，而不是作为决策变量。

Result: 通过示例验证了所提出公式的相关性和有效性。

Conclusion: 该方法在短预测地平线条件下确保了闭环稳定性。

Abstract: This paper proposes a proof of stability for Model Predictive Control
formulations involving a prediction horizon that might be too short to meet the
reachability condition generally invoked as a sufficient condition for
closed-loop stability. This condition is replaced by a contraction condition on
the stage cost. But unlike the contraction based existing formulations where
the prediction horizon becomes a decision variable, the formulation proposed in
this paper remains standard in that it uses constant and short prediction
horizon. An illustrative example is provided to assess the relevance of the
proposed formulation.

</details>


### [42] [Distributed model predictive control without terminal cost under inexact distributed optimization](https://arxiv.org/abs/2504.15768)
*Xiaoyu Liu,Dimos V. Dimarogonas,Changxin Liu,Azita Dabiri,Bart De Schutter*

Main category: eess.SY

TL;DR: 这篇论文提出了一种不依赖终端成本的分布式MPC方法，通过显式稳定性约束和分布式优化确保线性离散系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了简化传统MPC的稳定设计，避免终端成本的复杂性，提供更易实现的分布式控制方案。

Method: 引入基于松弛动态规划的显式稳定性约束，并开发分布式合成方法和无违例优化算法，使用约束收紧确保可行性。

Result: 数值例子验证了闭环稳定性和代理并行计算的可行性。

Conclusion: 该方法确保系统稳定性，且允许每个代理独立计算控制输入。

Abstract: This paper presents a novel distributed model predictive control (MPC)
formulation without terminal cost and a corresponding distributed synthesis
approach for distributed linear discrete-time systems with coupled constraints.
The proposed control scheme introduces an explicit stability condition as an
additional constraint based on relaxed dynamic programming. As a result,
contrary to other related approaches, system stability with the developed
controller does not rely on designing a terminal cost. A distributed synthesis
approach is then introduced to handle the stability constraint locally within
each local agent. To solve the underlying optimization problem for distributed
MPC, a violation-free distributed optimization approach is developed, using
constraint tightening to ensure feasibility throughout iterations. A numerical
example demonstrates that the proposed distributed MPC approach ensures
closed-loop stability for each feasible control sequence, with each agent
computing its control input in parallel.

</details>


### [43] [A Point-Hyperplane Geometry Method for Operational Security Region of Renewable Energy Generation in Power Systems](https://arxiv.org/abs/2504.15793)
*Can Wan,Biao Li,Xuejun Hu,Yunyi Li,Ping Ju*

Main category: eess.SY

TL;DR: 本文提出点-超平面几何方法，准确获得电力系统可再生能源操作安全区域的几何表达式，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 可再生能源快速增长挑战电力系统安全操作，需要量化关键安全边界和容纳能力。

Method: 提出PHG方法，包括定义几何多面体、正交基生成和点-超平面迭代算法，并允许调整角度实现性能权衡。

Result: 案例研究验证PHG方法的有效性和优越性。

Conclusion: PHG方法成功获取可再生能源操作安全区域的几何表达式。

Abstract: The rapid growth of renewable energy generation challenges the secure
operation of power systems. It becomes crucial to quantify the critical
security boundaries and hosting capability of renewable generation at the
system operation level. This paper proposes a novel point-hyperplane geometry
(PHG) method to accurately obtain the geometric expression of the operational
security region of renewable energy generation for power systems. Firstly, the
geometric expression of the operational security region is defined as a
polytope of boundary hyperplanes in the form of inequalities satisfying the
system operation constraints. Then, an orthogonal basis generation method is
proposed to solve a single boundary hyperplane of the polytope based on
intersecting and orthogonal geometric principles. Next, a point-hyperplane
iteration algorithm is developed to progressively obtain the overall geometric
polytope of the operational security region of renewable energy generation in
power systems. Besides, the flexible performance trade-off can be achieved by
modifying the proposed maximum tolerated angle between adjacent hyperplanes.
Finally, comprehensive case studies verify the effectiveness and superiority of
the PHG method.

</details>


### [44] [Finite time max-consensus for simultaneous target interception in switching graph topologies](https://arxiv.org/abs/2504.15803)
*Kushal P. Singh,Aditya K. Rao,Twinkle Tripathy*

Main category: eess.SY

TL;DR: 本文提出分布式制导律，实现异构追逐者同时拦截静止目标，在静态和切换图拓扑下有限时间内完成，并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 解决一组异构追逐者同时拦截静止目标的需求，确保拦截独立于初始条件。

Method: 提出分布式制导律，确立静态和切换图拓扑下确保同时拦截的必要条件。

Result: 拦截在有限时间内发生，适用于静态和切换图拓扑，并通过数值模拟演示。

Conclusion: 主要贡献是实现有限时间同时拦截，增强了图拓扑鲁棒性。

Abstract: In this paper, we propose a distributed guidance law for the simultaneous
interception of a stationary target. For a group of `n' heterogeneous pursuers,
the proposed guidance law establishes the necessary conditions on static graphs
that ensure simultaneous target interception, regardless of the initial
conditions of the pursuers. Building on these results, we also establish the
necessary conditions for achieving simultaneous interception in switching graph
topologies as well. The major highlight of the work is that the target
interception occurs in finite time for both static and switching graph
topologies. We demonstrate all of these results through numerical simulations.

</details>


### [45] [No-Regret Model Predictive Control with Online Learning of Koopman Operators](https://arxiv.org/abs/2504.15805)
*Hongyu Zhou,Vasileios Tzoumas*

Main category: eess.SY

TL;DR: 本论文提出了一种同时进行系统识别和模型预测控制的算法，用于处理非线性系统中的未知残差动力学，使用Koopman算子。算法具有有限时间近优性和渐进收敛性，并在模拟中验证。


<details>
  <summary>Details</summary>
Motivation: 动机是处理外部干扰和建模错误，如车辆的风和波浪干扰，或不准确的模型参数。

Method: 方法包括使用Koopman可观测函数将未知动力学近似为线性系统，采用模型预测控制，并通过在线最小二乘法在自监督方式下更新模型。

Result: 结果显示算法具有有限时间近优性、渐进收敛到最优非因果控制器，并具有次线性动态遗憾。在cart-pole系统模拟中验证。

Conclusion: 结论是该算法能有效处理未知残差动力学，提供良好的性能保证。

Abstract: We study a problem of simultaneous system identification and model predictive
control of nonlinear systems. Particularly, we provide an algorithm for systems
with unknown residual dynamics that can be expressed by Koopman operators. Such
residual dynamics can model external disturbances and modeling errors, such as
wind and wave disturbances to aerial and marine vehicles, or inaccurate model
parameters. The algorithm has finite-time near-optimality guarantees and
asymptotically converges to the optimal non-causal controller. Specifically,
the algorithm enjoys sublinear \textit{dynamic regret}, defined herein as the
suboptimality against an optimal clairvoyant controller that knows how the
unknown dynamics will adapt to its states and actions. To this end, we assume
the algorithm is given Koopman observable functions such that the unknown
dynamics can be approximated by a linear dynamical system. Then, it employs
model predictive control based on the current learned model of the unknown
residual dynamics. This model is updated online using least squares in a
self-supervised manner based on the data collected while controlling the
system. We validate our algorithm in physics-based simulations of a cart-pole
system aiming to maintain the pole upright despite inaccurate model parameters.

</details>


### [46] [Predictive Synthesis of Control Barrier Functions and its Application to Time-Varying Constraints](https://arxiv.org/abs/2504.15830)
*Adrian Wiltz,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 本论文提出一种CBF合成方法，能处理约束变化而不需重新计算，基于有限时域最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 动机是避免约束变化时重新计算CBF，提高效率和适应性。

Method: 方法基于Hamilton-Jacobi reachability分析的有限时域最优控制问题，不依赖标称控制律，提供K函数α的显式表征。

Result: 结果包括CBF能适应时间变化约束、考虑输入约束，模拟研究验证了在各种动态系统中的有效性。

Conclusion: 结论是该方法在温和假设下有效，并提供在线并行化实现。

Abstract: This paper presents a systematic method for synthesizing a Control Barrier
Function (CBF) that encodes predictive information into a CBF. Unlike other
methods, the synthesized CBF can account for changes and time-variations in the
constraints even when constructed for time-invariant constraints. This avoids
recomputing the CBF when the constraint specifications change. The method
provides an explicit characterization of the extended class K function {\alpha}
that determines the dynamic properties of the CBF, and {\alpha} can even be
explicitly chosen as a design parameter in the controller synthesis. The
resulting CBF further accounts for input constraints, and its values can be
determined at any point without having to compute the CBF over the entire
domain. The synthesis method is based on a finite horizon optimal control
problem inspired by Hamilton-Jacobi reachability analysis and does not rely on
a nominal control law. The synthesized CBF is time-invariant if the constraints
are. The method poses mild assumptions on the controllability of the dynamic
system and assumes the knowledge of at least a subset of some control invariant
set. The paper provides a detailed analysis of the properties of the
synthesized CBF, including its application to time-varying constraints. A
simulation study applies the proposed approach to various dynamic systems in
the presence of time-varying constraints. The paper is accompanied by an online
available parallelized implementation of the proposed synthesis method.

</details>


### [47] [Gaussian behaviors: representations and data-driven control](https://arxiv.org/abs/2504.15838)
*András Sasfi,Ivan Markovsky,Alberto Padoan,Florian Dörfler*

Main category: eess.SY

TL;DR: 这篇论文提出基于高斯过程的随机系统建模框架，并应用于预测控制中，改进了不确定性处理。


<details>
  <summary>Details</summary>
Motivation: 量化随机系统轨迹不确定性，同时保持模型简单可计算。

Method: 使用高斯行为模型建模轨迹，从数据估计，并开发预测控制框架，包括新分布鲁棒控制方法。

Result: 证明子空间预测控制是等价形式，DeePC是分布乐观的，并提供凸优化重构。

Conclusion: 提出高效分布鲁棒控制方法，缓解DeePC的过度乐观。

Abstract: We propose a modeling framework for stochastic systems based on Gaussian
processes. Finite-length trajectories of the system are modeled as random
vectors from a Gaussian distribution, which we call a Gaussian behavior. The
proposed model naturally quantifies the uncertainty in the trajectories, yet it
is simple enough to allow for tractable formulations. We relate the proposed
model to existing descriptions of dynamical systems including deterministic and
stochastic behaviors, and linear time-invariant (LTI) state-space models with
Gaussian process and measurement noise. Gaussian behaviors can be estimated
directly from observed data as the empirical sample covariance under the
assumption that the measured trajectories are from independent experiments. The
distribution of future outputs conditioned on inputs and past outputs provides
a predictive model that can be incorporated in predictive control frameworks.
We show that subspace predictive control (SPC) is a certainty-equivalence
control formulation with the estimated Gaussian behavior. Furthermore, the
regularized data-enabled predictive control (DeePC) method is shown to be a
distributionally optimistic formulation that optimistically accounts for
uncertainty in the Gaussian behavior. To mitigate the excessive optimism of
DeePC, we propose a novel distributionally robust control formulation, and
provide a convex reformulation allowing for efficient implementation.

</details>


### [48] [Monocular inspection of spacecraft under illumination constraints and avoidance regions](https://arxiv.org/abs/2504.15954)
*Tochukwu Elijah Ogri,Muzaffar Qureshi,Zachary I. Bell,Matthew Longmire,Rushikesh Kamalapurkar*

Main category: eess.SY

TL;DR: 这篇论文提出了一种自适应控制方法，用于航天器在轨检查，通过优化策略实现最佳信息获取。


<details>
  <summary>Details</summary>
Motivation: 空间环境的复杂性导致传统控制器无法适应变化，无法有效进行在轨维护，存在照明、视野和燃料等约束。

Method: 将检查任务制定为约束优化问题，目标是最大化相机信息，同时导航到最佳视图，并使用Lyapunov-based稳定性分析和模拟验证。

Result: 模拟结果证明了规划算法的有效性。

Conclusion: 开发了一个鲁棒的控制框架，能够在实际约束下实现高效的航天器指导和控制。

Abstract: This paper presents an adaptive control approach to information-based
guidance and control of a spacecraft carrying out on-orbit inspection by
actively computing optimal policies for the spacecraft to achieve the best
possible representation of objects within its orbital environment. Due to the
complexity of navigating the space environment, it may be impossible to carry
out on-orbit servicing to maintain space systems like satellites using a
spacecraft equipped with controllers that cannot adapt to changing conditions.
In particular, the presence of constraints such as illumination, field-of-view
(FOV), minimal fuel, the use of visual-inertial navigation for improved
localization, and the need for real-time computation of control policies render
the spacecraft motion planning problem challenging. The control framework
developed in this paper addresses these challenges by formulating the
inspection task as a constrained optimization problem where the goal is to
maximize information gained from the cameras, while navigating to the next best
view, subject to illumination and FOV constraints. The developed architecture
is analyzed using a Lyapunov-based stability analysis and the effectiveness of
the planning algorithm is verified in simulation.

</details>


### [49] [Hessian Riemannian Flow For Multi-Population Wardrop Equilibrium](https://arxiv.org/abs/2504.16028)
*Tigran Bakaryan,Christoph Aoun,Ricardo de Lima Ribeiro,Naira Hovakimyan,Diogo Gomes*

Main category: eess.SY

TL;DR: 这篇论文通过Wardrop均衡和数值方法优化多人口图上的流量，并应用于交通管理。


<details>
  <summary>Details</summary>
Motivation: 优化具有多个入口点和不同成本结构的多人口通用图上的流量，例如城市交通中的车辆路由和排放最小化。

Method: 使用变分不等式定义多人口Wardrop均衡，分析存在性和唯一性，并引入Hessian Riemannian流方法进行分布式优化计算。

Result: 通过城市交通管理例子展示了方法的有效性，包括多样车辆路由和拥挤环境下的排放最小化。

Conclusion: 该方法高效且适用于实际场景，如交通优化问题。

Abstract: In this paper, we address the problem of optimizing flows on generalized
graphs that feature multiple entry points and multiple populations, each with
varying cost structures. We tackle this problem by considering the
multi-population Wardrop equilibrium, defined through variational inequalities.
We rigorously analyze the existence and uniqueness of the Wardrop equilibrium.
Furthermore, we introduce an efficient numerical method to find the solution.
In particular, we reformulate the equilibrium problem as a distributed
optimization problem over subgraphs and introduce a novel Hessian Riemannian
flow method, a Riemannian-manifold-projected Hessian flow, to efficiently
compute a solution. Finally, we demonstrate the effectiveness of our approach
through examples in urban traffic management, including routing for diverse
vehicle types and strategies for minimizing emissions in congested
environments.

</details>


### [50] [PRIME: Fast Primal-Dual Feedback Optimization for Markets with Application to Optimal Power Flow](https://arxiv.org/abs/2504.16048)
*Nicholas Julian Behr,Mattia Bianchi,Keith Moffat,Saverio Bolognani,Florian Dörfler*

Main category: eess.SY

TL;DR: 本文引入PRIME，一种基于近端点迭代的在线反馈优化方法，支持市场化实现，处理非光滑目标，实现快速收敛和噪声抑制。


<details>
  <summary>Details</summary>
Motivation: 开发一种OFO方法，利用市场机制，激励自利行为者无需共享私有成本或约束，即可实现安全高效操作。

Method: PRIME基于近端点迭代，通过市场化实施，允许行为者基于激励做出选择。

Result: PRIME能处理非光滑目标函数，实现快速收敛、快速约束满足和噪声抑制；在交流最优功率流问题上演示了高效实时非线性局部边际定价方案。

Conclusion: PRIME提供了一种无需沟通私有信息的安全、高效优化控制方法。

Abstract: Online Feedback Optimization (OFO) controllers iteratively drive a plant to
an optimal operating point that satisfies input and output constraints, relying
solely on the input-output sensitivity as model information. This paper
introduces PRIME (PRoximal Iterative MarkEts), a novel OFO approach based on
proximal-point iterations. Unlike existing OFO solutions, PRIME admits a
market-based implementation, where self-interested actors are incentivized to
make choices that result in a safe and efficient operation, without
communicating private costs or constraints. Furthermore, PRIME can cope with
non-smooth objective functions, achieve fast convergence rates and rapid
constraint satisfaction, and reject measurement noise. We demonstrate PRIME on
an AC optimal power flow problem, obtaining an efficient real-time nonlinear
local marginal pricing scheme.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [51] [SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems](https://arxiv.org/abs/2504.15305)
*Abhishek Tyagi,Charu Gaur*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Veg的自主空中监控四旋翼无人机平台，集成了视觉SLAM、故障检测和AI功能，实现GPS无关导航和实时识别。


<details>
  <summary>Details</summary>
Motivation: 为了在GPS不可用或受限环境中提供可靠的自主导航、监控和故障恢复，适用于复杂场景。

Method: 使用ORB-SLAM3进行6-DoF定位和路径规划，采用LQR内环和PD外环控制，结合Dijkstra算法和基于CNN及PCA的嵌入式视觉系统，以及FDI故障检测机制。

Result: 在模拟和真实测试中验证了系统的稳定性和高精度识别能力，实现全板载操作。

Conclusion: 证明了在单一平台上整合实时定位、故障恢复和AI的可行性，适用于约束环境的应用。

Abstract: We present an autonomous aerial surveillance platform, Veg, designed as a
fault-tolerant quadcopter system that integrates visual SLAM for
GPS-independent navigation, advanced control architecture for dynamic
stability, and embedded vision modules for real-time object and face
recognition. The platform features a cascaded control design with an LQR
inner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for
6-DoF localization and loop closure, and supports waypoint-based navigation
through Dijkstra path planning over SLAM-derived maps. A real-time Failure
Detection and Identification (FDI) system detects rotor faults and executes
emergency landing through re-routing. The embedded vision system, based on a
lightweight CNN and PCA, enables onboard object detection and face recognition
with high precision. The drone operates fully onboard using a Raspberry Pi 4
and Arduino Nano, validated through simulations and real-world testing. This
work consolidates real-time localization, fault recovery, and embedded AI on a
single platform suitable for constrained environments.

</details>


### [52] [Efficient and Safe Planner for Automated Driving on Ramps Considering Unsatisfication](https://arxiv.org/abs/2504.15320)
*Qinghao Li,Zhen Tian,Xiaodan Wang,Jinming Yang,Zhihao Lin*

Main category: cs.RO

TL;DR: 这篇论文提出了一种集成规划器，用于坡道自动驾驶车辆的换道，平衡安全和效率，通过不满意水平指标优化效率，并使用箭头集群采样确保安全。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在坡道换道面临安全和效率平衡的挑战，因此需要开发一个优化换道决策的规划器。

Method: 使用不满意水平指标结合车辆速度来确定最佳换道时机，并采用箭头集群采样评估碰撞风险，选择最优换道曲线。

Result: 通过坡道场景模拟验证，规划器能有效选择适当换道时间和安全曲线，实现无碰撞的换道操作。

Conclusion: 所提规划器在提升自动驾驶车辆坡道换道效率和安全方面表现出色，为实际应用提供可行方案。

Abstract: Automated driving on ramps presents significant challenges due to the need to
balance both safety and efficiency during lane changes. This paper proposes an
integrated planner for automated vehicles (AVs) on ramps, utilizing an
unsatisfactory level metric for efficiency and arrow-cluster-based sampling for
safety. The planner identifies optimal times for the AV to change lanes, taking
into account the vehicle's velocity as a key factor in efficiency.
Additionally, the integrated planner employs arrow-cluster-based sampling to
evaluate collision risks and select an optimal lane-changing curve. Extensive
simulations were conducted in a ramp scenario to verify the planner's efficient
and safe performance. The results demonstrate that the proposed planner can
effectively select an appropriate lane-changing time point and a safe
lane-changing curve for AVs, without incurring any collisions during the
maneuver.

</details>


### [53] [MRTA-Sim: A Modular Simulator for Multi-Robot Allocation, Planning, and Control in Open-World Environments](https://arxiv.org/abs/2504.15418)
*Victoria Marie Tuck,Hardik Parwana,Pei-Wei Chen,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,S. Shankar Sastry,Sanjit A. Seshia*

Main category: cs.RO

TL;DR: 本文介绍了MRTA-Sim模拟器，用于在复杂室内环境中测试多机器人任务分配问题，结合机器人导航和冲突避免。


<details>
  <summary>Details</summary>
Motivation: 网格-based方法太受限，自由空间方法忽略了空间紧凑和多代理互动的影响，因此需要更贴近现实的测试平台。

Method: 开发Python/ROS2/Gazebo模拟器，将MRTA求解器输出与NAV2导航栈和CBF-QP冲突避免集成，使用模块化架构。

Result: 展示了与SMT-based动态MRTA方法在室内配送机器人上的应用，验证了系统的有效性。

Conclusion: 该模拟器提供更全面的测试环境，考虑机器人规划、多代理互动和人类避免，提高MRTA方法的实际可行性。

Abstract: This paper introduces MRTA-Sim, a Python/ROS2/Gazebo simulator for testing
approaches to Multi-Robot Task Allocation (MRTA) problems on simulated robots
in complex, indoor environments. Grid-based approaches to MRTA problems can be
too restrictive for use in complex, dynamic environments such in warehouses,
department stores, hospitals, etc. However, approaches that operate in
free-space often operate at a layer of abstraction above the control and
planning layers of a robot and make an assumption on approximate travel time
between points of interest in the system. These abstractions can neglect the
impact of the tight space and multi-agent interactions on the quality of the
solution. Therefore, MRTA solutions should be tested with the navigation stacks
of the robots in mind, taking into account robot planning, conflict avoidance
between robots, and human interaction and avoidance. This tool connects the
allocation output of MRTA solvers to individual robot planning using the NAV2
stack and local, centralized multi-robot deconfliction using Control Barrier
Function-Quadrtic Programs (CBF-QPs), creating a platform closer to real-world
operation for more comprehensive testing of these approaches. The simulation
architecture is modular so that users can swap out methods at different levels
of the stack. We show the use of our system with a Satisfiability Modulo
Theories (SMT)-based approach to dynamic MRTA on a fleet of indoor delivery
robots.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [54] [$k$-Inductive and Interpolation-Inspired Barrier Certificates for Stochastic Dynamical Systems](https://arxiv.org/abs/2504.15412)
*Mohammed Adib Oumer,Vishnu Murali,Majid Zamani*

Main category: math.OC

TL;DR: 本论文引入多函数屏障证书概念，为随机动力系统的安全概率提供下界，提出插值启发式方法并结合k-induction，使用sum-of-squares编程合成函数。


<details>
  <summary>Details</summary>
Motivation: 当标准屏障证书模板失败时，提供无需改变模板的替代自动搜索方法。

Method: 引入插值启发式屏障证书和k-inductive公式，利用k-induction放松要求，并用sum-of-squares编程合成函数集。

Result: 展示了在标准方法失败时找到证书的能力，并通过案例研究证明了效用。

Conclusion: 这种方法提升了屏障证书的自动化合成，改善了随机系统安全分析。

Abstract: We introduce two notions of barrier certificates that use multiple functions
to provide a lower bound on the probabilistic satisfaction of safety for
stochastic dynamical systems. A barrier certificate for a stochastic dynamical
system acts as a nonnegative supermartingale, and provides a lower bound on the
probability that the system is safe. The promise of such certificates is that
their search can be effectively automated. Typically, one may use optimization
or SMT solvers to find such barrier certificates of a given fixed template.
When such approaches fail, a typical approach is to instead change the
template. We propose an alternative approach that we dub interpolation-inspired
barrier certificates. An interpolation-inspired barrier certificate consists of
a set of functions that jointly provide a lower bound on the probability of
satisfying safety. We show how one may find such certificates of a fixed
template, even when we fail to find standard barrier certificates of the same
template. However, we note that such certificates still need to ensure a
supermartingale guarantee for one function in the set. To address this
challenge, we consider the use of $k$-induction with these
interpolation-inspired certificates. The recent use of $k$-induction in barrier
certificates allows one to relax the supermartingale requirement at every time
step to a combination of a supermartingale requirement every $k$ steps and a
$c$-martingale requirement for the intermediate steps. We provide a generic
formulation of a barrier certificate that we dub $k$-inductive
interpolation-inspired barrier certificate. The formulation allows for several
combinations of interpolation and $k$-induction for barrier certificate. We
present two examples among the possible combinations. We finally present
sum-of-squares programming to synthesize this set of functions and demonstrate
their utility in case studies.

</details>
