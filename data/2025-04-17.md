<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 16]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [eess.SY](#eess.SY) [Total: 16]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 27]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
*Peter Stockinger*

Main category: cs.AI

TLDR: 这篇论文探讨信息设计，包括语义内容与视觉表示的划分，并通过语义建模应用于音视频数据。


<details>
  <summary>Details</summary>
Motivation: 论文旨在区分语义内容与图形表达，利用结构符号学和语言学传统来丰富文本语料。

Method: 使用概念网络、图表、词库和本体论进行语义建模，并开发如OKAPI工具的应用。

Result: 开发了动态模型和可视化故事讲述方法，提高数据分析、发布和互操作性。

Conclusion: 这些方法促进数字数据的更丰富和协作使用，提升通信系统的智能性。

Abstract: 1) Introduction and Conceptual Framework: This document explores the concept
of information design by dividing it into two major practices: defining the
meaning of a corpus of textual data and its visual or multimodal
representation. It draws on expertise in enriching textual corpora,
particularly audiovisual ones, and transforming them into multiple narrative
formats. The text highlights a crucial distinction between the semantic content
of a domain and the modalities of its graphic expression, illustrating this
approach with concepts rooted in structural semiotics and linguistics
traditions.
  2) Modeling and Conceptual Design: The article emphasizes the importance of
semantic modeling, often achieved through conceptual networks or graphs. These
tools enable the structuring of knowledge within a domain by accounting for
relationships between concepts, contexts of use, and specific objectives.
Stockinger also highlights the constraints and challenges involved in creating
dynamic and adaptable models, integrating elements such as thesauri or
interoperable ontologies to facilitate the analysis and publication of complex
corpora.
  3) Applications and Multimodal Visualization: The text concludes by examining
the practical application of these models in work environments like OKAPI,
developed to analyze, publish, and reuse audiovisual data. It also discusses
innovative approaches such as visual storytelling and document reengineering,
which involve transforming existing content into new resources tailored to
various contexts. These methods emphasize interoperability, flexibility, and
the intelligence of communication systems, paving the way for richer and more
collaborative use of digital data. The content of this document was presented
during the "Semiotics of Information Design" Day organized by Anne
Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on
June 21, 2018, in Bordeaux.

</details>

### [2] [Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models](https://arxiv.org/abs/2504.11514)
*Nicolas Baumann,Cheng Hu,Paviththiren Sivasothilingam,Haotong Qin,Lei Xie,Michele Magno,Luca Benini*

Main category: cs.AI

TLDR: 论文提出MPC和LLM的混合架构提升自动驾驶性能，实验显示准确性和效率显著提高。


<details>
  <summary>Details</summary>
Motivation: 神经网络难以处理驾驶边缘情况，因此需要知识驱动方法补充数据驱动方法。

Method: 结合MPC和LLM的混合架构，使用DecisionxLLM评估状态、MPCxLLM调整参数，并采用RAG、LoRA和量化技术实现本地部署。

Result: 实验结果：推理准确性提升10.45%、控制适应性提升52.2%、计算效率提高10.5倍。

Conclusion: 该框架桥接高层决策和底层控制，提供知识驱动的自适应自动驾驶系统。

Abstract: Neural Networks (NNs) trained through supervised learning struggle with
managing edge-case scenarios common in real-world driving due to the
intractability of exhaustive datasets covering all edge-cases, making
knowledge-driven approaches, akin to how humans intuitively detect unexpected
driving behavior, a suitable complement to data-driven methods. This work
proposes a hybrid architecture combining low-level Model Predictive Controller
(MPC) with locally deployed Large Language Models (LLMs) to enhance
decision-making and Human Machine Interaction (HMI). The DecisionxLLM module
evaluates robotic state information against natural language instructions to
ensure adherence to desired driving behavior. The MPCxLLM module then adjusts
MPC parameters based on LLM-generated insights, achieving control adaptability
while preserving the safety and constraint guarantees of traditional MPC
systems. Further, to enable efficient on-board deployment and to eliminate
dependency on cloud connectivity, we shift processing to the on-board computing
platform: We propose an approach that exploits Retrieval Augmented Generation
(RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental
results demonstrate that these enhancements yield significant improvements in
reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%,
and up to 10.5x increase in computational efficiency (tokens/s), validating the
proposed framework's practicality for real-time deployment even on down-scaled
robotic platforms. This work bridges high-level decision-making with low-level
control adaptability, offering a synergistic framework for knowledge-driven and
adaptive Autonomous Driving Systems (ADS).

</details>

### [3] [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
*Haokun Liu,Sicong Huang,Jingyu Hu,Yangqiaoyu Zhou,Chenhao Tan*

Main category: cs.AI

TLDR: 本论文引入HypoBench基准，用于评估LLM在假设生成中的性能，结果显示现有方法有改进空间。


<details>
  <summary>Details</summary>
Motivation: 针对LLM假设生成的基本问题，如好假设的定义和评估方法。

Method: 开发HypoBench基准，包括7个真实世界和5个合成任务、194个数据集，并评估四种LLM和六种假设生成方法。

Result: 现有方法能发现模式，但合成任务上仅恢复38.8%的真实假设，难度增加时性能下降。

Conclusion: 强调假设生成的挑战，并证明HypoBench是改进AI科学发现系统的宝贵资源。

Abstract: There is growing interest in hypothesis generation with large language models
(LLMs). However, fundamental questions remain: what makes a good hypothesis,
and how can we systematically evaluate methods for hypothesis generation? To
address this, we introduce HypoBench, a novel benchmark designed to evaluate
LLMs and hypothesis generation methods across multiple aspects, including
practical utility, generalizability, and hypothesis discovery rate. HypoBench
includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.
We evaluate four state-of-the-art LLMs combined with six existing
hypothesis-generation methods. Overall, our results suggest that existing
methods are capable of discovering valid and novel patterns in the data.
However, the results from synthetic datasets indicate that there is still
significant room for improvement, as current hypothesis generation methods do
not fully uncover all relevant or meaningful patterns. Specifically, in
synthetic settings, as task difficulty increases, performance significantly
drops, with best models and methods only recovering 38.8% of the ground-truth
hypotheses. These findings highlight challenges in hypothesis generation and
demonstrate that HypoBench serves as a valuable resource for improving AI
systems designed to assist scientific discovery.

</details>

### [4] [REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites](https://arxiv.org/abs/2504.11543)
*Divyansh Garg,Shaun VanWeelden,Diego Caples,Andis Draguns,Nikil Ravi,Pranav Putta,Naman Garg,Tomas Abraham,Michael Lara,Federico Lopez,James Liu,Atharva Gundawar,Prannay Hebbar,Youngchul Joo,Charles London,Christian Schroeder de Witt,Sumeet Motwani*

Main category: cs.AI

TLDR: REAL是一个基准和框架，用于在模拟真实网站的环境中评估多轮代理性能，包括11个网站复制和112个任务，评估显示前沿模型成功率仅41%。


<details>
  <summary>Details</summary>
Motivation: 为了在安全、可重复的受控环境中评估代理的网络导航和任务完成能力，避免真实世界风险。

Method: 构建高保真网站复制、设计实际任务、开发评估框架（结合程序检查和LLM判断）、支持黑箱代理系统。

Result: 前沿语言模型在REAL上的成功率最高为41%，突显代理在自主网络导航和任务完成方面的能力不足。

Conclusion: 框架支持新任务集成、可重复评估和可扩展数据生成，并提供开源网站、框架和排行榜资源。

Abstract: We introduce REAL, a benchmark and framework for multi-turn agent evaluations
on deterministic simulations of real-world websites. REAL comprises
high-fidelity, deterministic replicas of 11 widely-used websites across domains
such as e-commerce, travel, communication, and professional networking. We also
release a benchmark consisting of 112 practical tasks that mirror everyday
complex user interactions requiring both accurate information retrieval and
state-changing actions. All interactions occur within this fully controlled
setting, eliminating safety risks and enabling robust, reproducible evaluation
of agent capability and reliability. Our novel evaluation framework combines
programmatic checks of website state for action-based tasks with rubric-guided
LLM-based judgments for information retrieval. The framework supports both
open-source and proprietary agent systems through a flexible evaluation harness
that accommodates black-box commands within browser environments, allowing
research labs to test agentic systems without modification. Our empirical
results show that frontier language models achieve at most a 41% success rate
on REAL, highlighting critical gaps in autonomous web navigation and task
completion capabilities. Our framework supports easy integration of new tasks,
reproducible evaluation, and scalable data generation for training web agents.
The websites, framework, and leaderboard are available at https://realevals.xyz
and https://github.com/agi-inc/REAL.

</details>

### [5] [NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes](https://arxiv.org/abs/2504.11544)
*Tianyang Xu,Haojie Zheng,Chengze Li,Haoxiang Chen,Yixin Liu,Ruoxi Chen,Lichao Sun*

Main category: cs.AI

TLDR: NodeRAG 通过异构图结构提升 RAG 框架的效率和性能，在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图-based RAG 方法未重视图结构设计，导致算法整合不畅和性能下降。

Method: 提出 NodeRAG 框架，使用异构图结构无缝整合图算法到 RAG 工作流中，与大语言模型紧密结合。

Result: 实验显示 NodeRAG 在索引时间、查询时间、存储效率和问答性能上优于 GraphRAG 和 LightRAG。

Conclusion: NodeRAG 证明了其在多跳基准和开放式评估中的优势，并提供了开源仓库。

Abstract: Retrieval-augmented generation (RAG) empowers large language models to access
external and private corpus, enabling factually consistent responses in
specific domains. By exploiting the inherent structure of the corpus,
graph-based RAG methods further enrich this process by building a knowledge
graph index and leveraging the structural nature of graphs. However, current
graph-based RAG approaches seldom prioritize the design of graph structures.
Inadequately designed graph not only impede the seamless integration of diverse
graph algorithms but also result in workflow inconsistencies and degraded
performance. To further unleash the potential of graph for RAG, we propose
NodeRAG, a graph-centric framework introducing heterogeneous graph structures
that enable the seamless and holistic integration of graph-based methodologies
into the RAG workflow. By aligning closely with the capabilities of LLMs, this
framework ensures a fully cohesive and efficient end-to-end process. Through
extensive experiments, we demonstrate that NodeRAG exhibits performance
advantages over previous methods, including GraphRAG and LightRAG, not only in
indexing time, query time, and storage efficiency but also in delivering
superior question-answering performance on multi-hop benchmarks and open-ended
head-to-head evaluations with minimal retrieval tokens. Our GitHub repository
could be seen at https://github.com/Terry-Xu-666/NodeRAG.

</details>

### [6] [Probabilistic causal graphs as categorical data synthesizers: Do they do better than Gaussian Copulas and Conditional Tabular GANs?](https://arxiv.org/abs/2504.11547)
*Olha Shaposhnyk,Noor Abid,Mouri Zakir,Svetlana Yanushkevich*

Main category: cs.AI

TLDR: 这篇论文使用结构方程模型和贝叶斯网络生成高质量合成分类数据，应用于残疾服务可访问性调查，出色地保持数据关系和隐私。


<details>
  <summary>Details</summary>
Motivation: 生成合成数据以增加模型训练多样性、保密性，并捕捉变量间关系，针对残疾人群服务可访问性调查。

Method: 采用结构方程模型（SEM）后跟贝叶斯网络（BN），并与高斯Copula和CTGAN等方法比较。

Result: SEM-based BN在Chi-square测试、KL散度和TVD等指标上表现最佳，BN模型TV D最高，显示与原始数据高度一致。

Conclusion: 该方法在维护统计和关系有效性的同时保护保密性，特别适用于敏感数据如残疾研究。

Abstract: This study investigates the generation of high-quality synthetic categorical
data, such as survey data, using causal graph models. Generating synthetic data
aims not only to create a variety of data for training the models but also to
preserve privacy while capturing relationships between the data. The research
employs Structural Equation Modeling (SEM) followed by Bayesian Networks (BN).
We used the categorical data that are based on the survey of accessibility to
services for people with disabilities. We created both SEM and BN models to
represent causal relationships and to capture joint distributions between
variables. In our case studies, such variables include, in particular,
demographics, types of disability, types of accessibility barriers and
frequencies of encountering those barriers.
  The study compared the SEM-based BN method with alternative approaches,
including the probabilistic Gaussian copula technique and generative models
like the Conditional Tabular Generative Adversarial Network (CTGAN). The
proposed method outperformed others in statistical metrics, including the
Chi-square test, Kullback-Leibler divergence, and Total Variation Distance
(TVD). In particular, the BN model demonstrated superior performance, achieving
the highest TVD, indicating alignment with the original data. The Gaussian
Copula ranked second, while CTGAN exhibited moderate performance. These
analyses confirmed the ability of the SEM-based BN to produce synthetic data
that maintain statistical and relational validity while maintaining
confidentiality. This approach is particularly beneficial for research on
sensitive data, such as accessibility and disability studies.

</details>

### [7] [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
*Dayeon Ki,Tianyi Zhou,Marine Carpuat,Gang Wu,Puneet Mathur,Viswanathan Swaminathan*

Main category: cs.AI

TLDR: 本文引入GraphicBench基准和GraphicTown框架，测试LLM代理在图形设计任务中的规划能力，揭示其优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦明确目标任务，忽略开放式创意设计任务，因此本文旨在探索LLM代理在该领域的潜力。

Method: 引入GraphicBench基准（含1079个查询和图像）和GraphicTown框架（含三个设计专家和46个动作），用于生成和执行工作流。

Result: 六种LLM实验显示能整合显式和隐式约束，但常因空间关系推理、专家协调和动作检索问题导致执行失败。

Conclusion: GraphicBench被视为推进LLM代理在创意设计任务中规划和执行的宝贵测试平台。

Abstract: Large Language Model (LLM)-powered agents have unlocked new possibilities for
automating human tasks. While prior work has focused on well-defined tasks with
specified goals, the capabilities of agents in creative design tasks with
open-ended goals remain underexplored. We introduce GraphicBench, a new
planning benchmark for graphic design that covers 1,079 user queries and input
images across four design types. We further present GraphicTown, an LLM agent
framework with three design experts and 46 actions (tools) to choose from for
executing each step of the planned workflows in web environments. Experiments
with six LLMs demonstrate their ability to generate workflows that integrate
both explicit design constraints from user queries and implicit commonsense
constraints. However, these workflows often do not lead to successful execution
outcomes, primarily due to challenges in: (1) reasoning about spatial
relationships, (2) coordinating global dependencies across experts, and (3)
retrieving the most appropriate action per step. We envision GraphicBench as a
challenging yet valuable testbed for advancing LLM-agent planning and execution
in creative design tasks.

</details>

### [8] [Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation](https://arxiv.org/abs/2504.11671)
*Ji Ma*

Main category: cs.AI

TLDR: 本研究提出并测试方法，在独裁者游戏中探测、量化和修改大语言模型的内部表示，以研究社会概念如何编码和被工程化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为决策代理在社会科学中的应用日益增多，但角色和上下文如何影响其行为尚未充分探索，需要研究和调节社会概念的编码。

Method: 提取和操纵'变量变化向量'（例如'男性'到'女性'）从模型内部状态，并在独裁者游戏中测试这些修改对决策的影响。

Result: 操纵这些向量可以显著改变变量与模型决策的关系。

Conclusion: 这种方法为研究和调节transformer-based模型中的社会概念提供原理性途径，对AI对齐、去偏见和代理设计有重要启示。

Abstract: Large language models (LLMs) increasingly serve as human-like decision-making
agents in social science and applied settings. These LLM-agents are typically
assigned human-like characters and placed in real-life contexts. However, how
these characters and contexts shape an LLM's behavior remains underexplored.
This study proposes and tests methods for probing, quantifying, and modifying
an LLM's internal representations in a Dictator Game -- a classic behavioral
experiment on fairness and prosocial behavior. We extract ``vectors of variable
variations'' (e.g., ``male'' to ``female'') from the LLM's internal state.
Manipulating these vectors during the model's inference can substantially alter
how those variables relate to the model's decision-making. This approach offers
a principled way to study and regulate how social concepts can be encoded and
engineered within transformer-based models, with implications for alignment,
debiasing, and designing AI agents for social simulations in both academic and
commercial applications.

</details>

### [9] [A Library of LLM Intrinsics for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.11704)
*Marina Danilevsky,Kristjan Greenewald,Chulaka Gunasekara,Maeda Hanafi,Lihong He,Yannis Katsis,Krishnateja Killamsetty,Yatin Nandwani,Lucian Popa,Dinesh Raghu,Frederick Reiss,Vraj Shah,Khoi-Nguyen Tran,Huaiyu Zhu,Luis Lastras*

Main category: cs.AI

TLDR: 这篇论文提出LLM Intrinsics概念，以标准化RAG应用的API，支持大规模协作。


<details>
  <summary>Details</summary>
Motivation: LLM开发者社区缺乏类似于软件库的协作模式，特别是RAG应用API不统一。

Method: 提出LLM Intrinsics作为稳定API的能力，通过HuggingFace的LoRA适配器和vLLM平台发布，并提供文档、代码、使用、训练和评估细节。

Result: 发布了内在函数库，并评估了其使用和组合效果。

Conclusion: 这有助于建立LLM领域的标准化协作，促进RAG应用的开发。

Abstract: In the developer community for large language models (LLMs), there is not yet
a clean pattern analogous to a software library, to support very large scale
collaboration. Even for the commonplace use case of Retrieval-Augmented
Generation (RAG), it is not currently possible to write a RAG application
against a well-defined set of APIs that are agreed upon by different LLM
providers. Inspired by the idea of compiler intrinsics, we propose some
elements of such a concept through introducing a library of LLM Intrinsics for
RAG. An LLM intrinsic is defined as a capability that can be invoked through a
well-defined API that is reasonably stable and independent of how the LLM
intrinsic itself is implemented. The intrinsics in our library are released as
LoRA adapters on HuggingFace, and through a software interface with clear
structured input/output characteristics on top of vLLM as an inference
platform, accompanied in both places with documentation and code. This article
describes the intended usage, training details, and evaluations for each
intrinsic, as well as compositions of multiple intrinsics.

</details>

### [10] [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
*Yiyou Sun,Georgia Zhou,Hao Wang,Dacheng Li,Nouha Dziri,Dawn Song*

Main category: cs.AI

TLDR: 本论文分析了监督微调如何提升语言模型数学推理能力，发现问题难度阶梯结构，并强调数据集规模扩展的重要性。


<details>
  <summary>Details</summary>
Motivation: 最近监督微调显著提高了语言模型在数学推理任务上的性能，但具体能力提升机制不清楚，因此需要深入分析推理能力的演变。

Method: 在AIME24数据集上进行详细分析，将问题分为易、中、难和极难四个等级，识别不同等级之间的推理要求和错误类型。

Result: 发现从易到中级需要R1推理风格，难级问题推理链错误频发准确率停滞在65%，极难问题需非常规技能；小规模数据集优化有限，规模扩展更有效。

Conclusion: 为提升语言模型数学推理能力提供了更清晰的改进路线图。

Abstract: Recent supervised fine-tuning (SFT) approaches have significantly improved
language models' performance on mathematical reasoning tasks, even when models
are trained at a small scale. However, the specific capabilities enhanced
through such fine-tuning remain poorly understood. In this paper, we conduct a
detailed analysis of model performance on the AIME24 dataset to understand how
reasoning capabilities evolve. We discover a ladder-like structure in problem
difficulty, categorize questions into four tiers (Easy, Medium, Hard, and
Extremely Hard (Exh)), and identify the specific requirements for advancing
between tiers. We find that progression from Easy to Medium tier requires
adopting an R1 reasoning style with minimal SFT (500-1K instances), while
Hard-level questions suffer from frequent model's errors at each step of the
reasoning chain, with accuracy plateauing at around 65% despite logarithmic
scaling. Exh-level questions present a fundamentally different challenge; they
require unconventional problem-solving skills that current models uniformly
struggle with. Additional findings reveal that carefully curated small-scale
datasets offer limited advantage-scaling dataset size proves far more
effective. Our analysis provides a clearer roadmap for advancing language model
capabilities in mathematical reasoning.

</details>

### [11] [Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs](https://arxiv.org/abs/2504.11765)
*Hyungwoo Lee,Kihyun Kim,Jinwoo Kim,Jungmin So,Myung-Hoon Cha,Hong-Yeon Kim,James J. Kim,Youngjae Kim*

Main category: cs.AI

TLDR: 简而言之，本文提出Shared RAG-DCache系统，通过磁盘-based KV缓存减少大语言模型RAG推理延迟。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大语言模型输入长度和模型大小增长导致的推理延迟增加，特别是RAG技术加剧了token数量和计算开销。

Method: 方法是使用磁盘-based KV缓存减轻预填充阶段负担，并引入Shared RAG-DCache系统，在多实例环境中管理共享缓存，利用查询局部性和排队延迟。

Result: 结果是在2 GPU和1 CPU主机上，吞吐量提高15~71%，延迟减少12~65%，取决于资源配置。

Conclusion: 结论是，该系统在资源约束下提升了吞吐量和延迟性能。

Abstract: Recent large language models (LLMs) face increasing inference latency as
input context length and model size continue to grow. In particular, the
retrieval-augmented generation (RAG) technique, which enhances LLM responses by
incorporating external knowledge, exacerbates this issue by significantly
increasing the number of input tokens. This expansion in token length leads to
a substantial rise in computational overhead, particularly during the prefill
stage, resulting in prolonged time-to-first-token (TTFT). To address this
issue, this paper proposes a method to reduce TTFT by leveraging a disk-based
key-value (KV) cache to lessen the computational burden during the prefill
stage. We also introduce a disk-based shared KV cache management system, called
Shared RAG-DCache, for multi-instance LLM RAG service environments. This
system, together with an optimal system configuration, improves both throughput
and latency under given resource constraints. Shared RAG-DCache exploits the
locality of documents related to user queries in RAG, as well as the queueing
delay in LLM inference services. It proactively generates and stores disk KV
caches for query-related documents and shares them across multiple LLM
instances to enhance inference performance. In experiments on a single host
equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in
throughput and up to a 12~65% reduction in latency, depending on the resource
configuration.

</details>

### [12] [Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records](https://arxiv.org/abs/2504.11792)
*Md Sultan Al Nahian,Chris Delcher,Daniel Harris,Peter Akpunonu,Ramakanth Kavuluru*

Main category: cs.AI

TLDR: 本研究使用GPT-4o预测药物过量风险，性能优于传统方法，可零样本预测。


<details>
  <summary>Details</summary>
Motivation: 预测药物过量风险对及时干预和预防至关重要，LLM进步可提升性能。

Method: 评估GPT-4o在微调和零样本设置下的性能，使用患者保险索赔记录，与传统机器学习方法比较。

Result: LLM在某些设置下优于传统模型，可零样本预测风险。

Conclusion: 突显LLM在临床决策支持中的潜力，特别是药物过量风险预测。

Abstract: The ability to predict drug overdose risk from a patient's medical records is
crucial for timely intervention and prevention. Traditional machine learning
models have shown promise in analyzing longitudinal medical records for this
task. However, recent advancements in large language models (LLMs) offer an
opportunity to enhance prediction performance by leveraging their ability to
process long textual data and their inherent prior knowledge across diverse
tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in
predicting drug overdose events using patients' longitudinal insurance claims
records. We evaluate its performance in both fine-tuned and zero-shot settings,
comparing them to strong traditional machine learning methods as baselines. Our
results show that LLMs not only outperform traditional models in certain
settings but can also predict overdose risk in a zero-shot setting without
task-specific training. These findings highlight the potential of LLMs in
clinical decision support, particularly for drug overdose risk prediction.

</details>

### [13] [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
*Tom Everitt,Cristina Garbacea,Alexis Bellot,Jonathan Richens,Henry Papadatos,Siméon Campos,Rohin Shah*

Main category: cs.AI

TLDR: 本研究评估大型语言模型（LLMs）的目标导向性，发现它在任务间相对一致，但大多数模型未完全目标导向，并建议用于监控和设计。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在给定目标下使用能力的程度，以更好地监控其进展和 deliberate 设计代理属性。

Method: 通过涉及信息收集、认知努力和计划执行的任务，使用子任务推断模型能力，对Google DeepMind、OpenAI和Anthropic的LLMs进行评估。

Result: 目标导向性在任务间相对一致，与任务性能不同，对激励提示仅 moderately 敏感，且大多数模型未完全目标导向。

Conclusion: 希望这种目标导向性评估能提升LLM进展的监控和代理属性的 deliberate 设计。

Abstract: To what extent do LLMs use their capabilities towards their given goal? We
take this as a measure of their goal-directedness. We evaluate
goal-directedness on tasks that require information gathering, cognitive
effort, and plan execution, where we use subtasks to infer each model's
relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,
and Anthropic show that goal-directedness is relatively consistent across
tasks, differs from task performance, and is only moderately sensitive to
motivational prompts. Notably, most models are not fully goal-directed. We hope
our goal-directedness evaluations will enable better monitoring of LLM
progress, and enable more deliberate design choices of agentic properties in
LLMs.

</details>

### [14] [Moving between high-quality optima using multi-satisfiability characteristics in hard-to-solve Max3Sat instances](https://arxiv.org/abs/2504.11864)
*J. Piatek,M. W. Przewozniczek,F. Chicano,R. Tinós*

Main category: cs.AI

TLDR: 本文提出改进灰箱优化方法，针对Max3Sat问题中隧道失败的实例，通过操纵子句可满足性特征提升优化性能。


<details>
  <summary>Details</summary>
Motivation: 许多现实问题可转化为MaxSat/Max3Sat实例，需要高效求解；现有隧道技术在某些高质局部最优解间失效。

Method: 基于相变分析，操纵子句可满足性和多满足性特征，构建灰箱优化器以连接解空间中遥远的高质解。

Result: 实验显示，新优化器能解决现有灰箱优化器无法处理的Max3Sat实例，同时保持对已可解决实例的有效性。

Conclusion: 新方法显著提升了灰箱优化在Max3Sat问题上的鲁棒性和效率，尤其在隧道失败的场景中。

Abstract: Gray-box optimization proposes effective and efficient optimizers of general
use. To this end, it leverages information about variable dependencies and the
subfunction-based problem representation. These approaches were already shown
effective by enabling \textit{tunnelling} between local optima even if these
moves require the modification of many dependent variables. Tunnelling is
useful in solving the maximum satisfiability problem (MaxSat), which can be
reformulated to Max3Sat. Since many real-world problems can be brought to
solving the MaxSat/Max3Sat instances, it is important to solve them effectively
and efficiently. Therefore, we focus on Max3Sat instances for which tunnelling
fails to introduce improving moves between locally optimal high-quality
solutions and the region of globally optimal solutions. We analyze the features
of such instances on the ground of phase transitions. Based on these
observations, we propose manipulating clause-satisfiability characteristics
that allow connecting high-quality solutions distant in the solution space. We
utilize multi-satisfiability characteristics in the optimizer built from
typical gray-box mechanisms. The experimental study shows that the proposed
optimizer can solve those Max3Sat instances that are out of the grasp of
state-of-the-art gray-box optimizers. At the same time, it remains effective
for instances that have already been successfully solved by gray-box.

</details>

### [15] [Seeking and leveraging alternative variable dependency concepts in gray-box-elusive bimodal land-use allocation problems](https://arxiv.org/abs/2504.11882)
*J. Maciążek,M. W. Przewozniczek,J. Schwaab*

Main category: cs.AI

TLDR: 本文针对NP-hard的土地使用分配问题，提出新变量依赖定义和交叉算子，提高了多目标优化器的有效性。


<details>
  <summary>Details</summary>
Motivation: 土地使用分配问题是NP-hard，标准变量依赖技术不适用，需要专用优化工具来处理全球环境问题。

Method: 提出问题专用变量依赖定义、依赖变量掩码，并构建三个新交叉算子，整合到NSGA-II和MOEA/D优化器中。

Result: 在真实世界测试案例中，显著提高了优化器的有效性。

Conclusion: 引入新提议显著改善了多目标优化器的性能。

Abstract: Solving land-use allocation problems can help us to deal with some of the
most urgent global environmental issues. Since these problems are NP-hard,
effective optimizers are needed to handle them. The knowledge about variable
dependencies allows for proposing such tools. However, in this work, we
consider a real-world multi-objective problem for which standard variable
dependency discovery techniques are inapplicable. Therefore, using
linkage-based variation operators is unreachable. To address this issue, we
propose a definition of problem-dedicated variable dependency. On this base, we
propose obtaining masks of dependent variables. Using them, we construct three
novel crossover operators. The results concerning real-world test cases show
that introducing our propositions into two well-known optimizers (NSGA-II,
MOEA/D) dedicated to multi-objective optimization significantly improves their
effectiveness.

</details>

### [16] [Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading](https://arxiv.org/abs/2504.11919)
*Qianjin Yu,Keyu Wu,Zihan Chen,Chushu Zhang,Manlin Mei,Lingjun Huang,Fang Tan,Yongsheng Du,Kunlin Liu,Yurui Zhu*

Main category: cs.AI

TLDR: 本论文提出了一种基于LLM适应性问题难度的CoT数据生成方法，使用DeepSeek-R1提升小模型推理能力，并以低成本获得显著效果。


<details>
  <summary>Details</summary>
Motivation: 利用DeepSeek-R1的公开方法生成高质量CoT数据，以高效提升小LLM的推理能力，解决数据生成成本高的问题。

Method: 根据LLM推理能力评估问题难度，构建适应性问题数据库；采样问题并用DeepSeek-R1生成CoT数据。

Result: 在数学和代码任务中，使用仅2k数据，ZMath-32B和ZCode-32B均超过DeepSeek-Distill-32B的性能。

Conclusion: 方法有效降低数据生成成本，提高SFT效率，并证明其在复杂任务中的泛化性。

Abstract: Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its
excellent reasoning ability in complex tasks and has publiclyshared its
methodology. This provides potentially high-quality chain-of-thought (CoT) data
for stimulating the reasoning abilities of small-sized large language models
(LLMs). To generate high-quality CoT data for different LLMs, we seek an
efficient method for generating high-quality CoT data with LLM-Adaptive
questiondifficulty levels. First, we grade the difficulty of the questions
according to the reasoning ability of the LLMs themselves and construct a
LLM-Adaptive question database. Second, we sample the problem database based on
a distribution of difficulty levels of the questions and then use DeepSeek-R1
(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality
CoT data with correct answers. Thanks to the construction of CoT data with
LLM-Adaptive difficulty levels, we have significantly reduced the cost of data
generation and enhanced the efficiency of model supervised fine-tuning (SFT).
Finally, we have validated the effectiveness and generalizability of the
proposed method in the fields of complex mathematical competitions and code
generation tasks. Notably, with only 2k high-quality mathematical CoT data, our
ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,
with only 2k high-quality code CoT data, our ZCode-32B surpasses
DeepSeek-Distill-32B in code reasoning tasks.

</details>

### [17] [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
*Nada Shahin,Leila Ismail*

Main category: cs.AI

TLDR: 本论文提出自适应Transformer（ADAT）以提升手语机器翻译的准确性和效率，并引入新的医疗手语数据集MedASL。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译系统在识别细粒度时间依赖性和计算效率上存在不足。

Method: 提出ADAT，通过增强特征提取和自适应特征加权机制来优化性能。

Result: 实验显示，ADAT在PHOENIX14T和MedASL数据集上提高了准确率并减少了训练时间。

Conclusion: ADAT在保持翻译准确性的同时改善了效率，为手语翻译提供了改进方案。

Abstract: Current sign language machine translation systems rely on recognizing hand
movements, facial expressions and body postures, and natural language
processing, to convert signs into text. Recent approaches use Transformer
architectures to model long-range dependencies via positional encoding.
However, they lack accuracy in recognizing fine-grained, short-range temporal
dependencies between gestures captured at high frame rates. Moreover, their
high computational complexity leads to inefficient training. To mitigate these
issues, we propose an Adaptive Transformer (ADAT), which incorporates
components for enhanced feature extraction and adaptive feature weighting
through a gating mechanism to emphasize contextually relevant features while
reducing training overhead and maintaining translation accuracy. To evaluate
ADAT, we introduce MedASL, the first public medical American Sign Language
dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the
encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing
training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text
experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on
PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on
MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,
ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its
dual-stream structure.

</details>

### [18] [Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews](https://arxiv.org/abs/2504.11977)
*Sofia Krylova,Fabian Schmidt,Vladimir Vlassov*

Main category: cs.AI

TLDR: 这篇论文使用机器学习预测未完成数字分诊访谈的结果，以提高患者护理和医疗效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于问卷的确定性分诊系统无法处理未完成访谈，存在安全和效率问题，因此需要通过机器学习预测不完整数据的结果。

Method: 使用决策树模型（如LGBMClassifier和CatBoostClassifier）及TabTransformer模型，预测不同完整度访谈的分诊结果。

Result: 决策树模型在完整访谈中准确率超过80%，与完整度呈线性相关；TabTransformer在所有完整度下准确率均超过80%，但训练时间较长。

Conclusion: 研究强调访谈完整度与预测准确率之间的线性相关性，展示了机器学习在提升分诊系统中的潜力。

Abstract: Many existing digital triage systems are questionnaire-based, guiding
patients to appropriate care levels based on information (e.g., symptoms,
medical history, and urgency) provided by the patients answering
questionnaires. Such a system often uses a deterministic model with predefined
rules to determine care levels. It faces challenges with incomplete triage
interviews since it can only assist patients who finish the process. In this
study, we explore the use of machine learning (ML) to predict outcomes of
unfinished interviews, aiming to enhance patient care and service quality.
Predicting triage outcomes from incomplete data is crucial for patient safety
and healthcare efficiency. Our findings show that decision-tree models,
particularly LGBMClassifier and CatBoostClassifier, achieve over 80\% accuracy
in predicting outcomes from complete interviews while having a linear
correlation between the prediction accuracy and interview completeness degree.
For example, LGBMClassifier achieves 88,2\% prediction accuracy for interviews
with 100\% completeness, 79,6\% accuracy for interviews with 80\% completeness,
58,9\% accuracy for 60\% completeness, and 45,7\% accuracy for 40\%
completeness. The TabTransformer model demonstrated exceptional accuracy of
over 80\% for all degrees of completeness but required extensive training time,
indicating a need for more powerful computational resources. The study
highlights the linear correlation between interview completeness and predictive
power of the decision-tree models.

</details>

### [19] [Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models](https://arxiv.org/abs/2504.12012)
*Kris Pilcher,Esen K. Tütüncü*

Main category: cs.AI

TLDR: This paper introduces PIP, a method to intentionally induce hallucinations in LLMs for creative tasks, reframing them as innovative tools rather than errors.


<details>
  <summary>Details</summary>
Motivation: To reframe LLM hallucinations from flaws to sources of computational imagination in creative contexts, inspired by literature like Moby-Dick and concepts from theater.

Method: Fine-tuning LLMs to amplify speculative, metaphorical, and surreal outputs for tasks such as speculative fiction and interactive storytelling.

Result: Preliminary observations on applications, design principles for user consent, and implications for AI ethics and human-AI collaboration.

Conclusion: Hallucinations can be transformed into catalysts for innovation when factual accuracy is not primary, emphasizing ethical considerations in AI development.

Abstract: Hallucinations in Large Language Models (LLMs) are widely regarded as errors
- outputs that deviate from factual accuracy. However, in creative or
exploratory contexts, these "mistakes" may represent unexpected avenues for
innovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach
that amplifies LLM hallucinations for imaginative tasks such as speculative
fiction, interactive storytelling, and mixed-reality simulations. Drawing on
Herman Melville's Moby-Dick, where Pip's "madness" reveals profound insight, we
reframe hallucinations as a source of computational imagination rather than a
flaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and
surreal outputs - hallucinations that are useful when factual accuracy is not
the chief objective. Inspired by the consensual illusions of theater and stage
magic, PIP situates these creative missteps in contexts where users willingly
suspend disbelief, thereby transforming "errors" into catalysts for new ways of
thinking. We discuss potential applications, design principles for ensuring
user consent, preliminary observations, and implications for broader AI ethics
and human-AI collaboration.

</details>

### [20] [Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework](https://arxiv.org/abs/2504.12090)
*Jack Preuveneers,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TLDR: 本论文提出一种结合决策树可解释性和大语言模型推理的框架，用于预测创业成功，提高精度和准确性。


<details>
  <summary>Details</summary>
Motivation: 弥合决策树可解释性与大语言模型推理能力之间的差距，以在高风险投资环境中提供透明的数据驱动决策。

Method: 利用链式思考提示生成推理日志，并提炼成逻辑规则；整合数据摄入、两步精炼、集成采样、强化学习评分和持久内存等增强。

Result: 实验显示，精度提升54%（从0.225到0.346），准确性提升50%（从0.46到0.70），比随机分类器高出2倍。

Conclusion: 增强决策过程，便于专家干预和政策优化，为可解释LLM框架在投资和其他领域奠定基础。

Abstract: We present a novel framework that bridges the gap between the
interpretability of decision trees and the advanced reasoning capabilities of
large language models (LLMs) to predict startup success. Our approach leverages
chain-of-thought prompting to generate detailed reasoning logs, which are
subsequently distilled into structured, human-understandable logical rules. The
pipeline integrates multiple enhancements - efficient data ingestion, a
two-step refinement process, ensemble candidate sampling, simulated
reinforcement learning scoring, and persistent memory - to ensure both stable
decision-making and transparent output. Experimental evaluations on curated
startup datasets demonstrate that our combined pipeline improves precision by
54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a
standalone OpenAI o3 model. Notably, our model achieves over 2x the precision
of a random classifier (16%). By combining state-of-the-art AI reasoning with
explicit rule-based explanations, our method not only augments traditional
decision-making processes but also facilitates expert intervention and
continuous policy refinement. This work lays the foundation for the
implementation of interpretable LLM-powered decision frameworks in high-stakes
investment environments and other domains that require transparent and
data-driven insights.

</details>

### [21] [Towards LLM Agents for Earth Observation](https://arxiv.org/abs/2504.12110)
*Chia Hsiang Kao,Wenting Zhao,Shreelekha Revankar,Samuel Speas,Snehal Bhagat,Rajeev Datta,Cheng Perng Phoo,Utkarsh Mall,Carl Vondrick,Kavita Bala,Bharath Hariharan*

Main category: cs.AI

TLDR: 本文评估AI在地球观测中的表现，引入基准测试，发现准确率低、失败率高，并通过微调展示了改进方法。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否准备好进行可靠的地球观测，使用NASA数据和卫星传感器。

Method: 引入一个包含140个是/否问题的基准，使用Google Earth Engine API测试LLM代理，并通过合成数据微调模型。

Result: LLM代理准确率仅33%，代码执行失败率达58%；微调后，小模型性能可与大模型相当。

Conclusion: 指出了AI自动化地球观测的挑战，并建议通过微调等方法改进。

Abstract: Earth Observation (EO) provides critical planetary data for environmental
monitoring, disaster management, climate science, and other scientific domains.
Here we ask: Are AI systems ready for reliable Earth Observation? We introduce
\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth
Observatory articles across 13 topics and 17 satellite sensors. Using Google
Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33%
because the code fails to run over 58% of the time. We improve the failure rate
for open models by fine-tuning synthetic data, allowing much smaller models
(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,
DeepSeek-R1). Taken together, our findings identify significant challenges to
be solved before AI agents can automate earth observation, and suggest paths
forward. The project page is available at
https://iandrover.github.io/UnivEarth.

</details>

### [22] [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
*Mahmoud Salhab,Marwan Elghitany,Shameed Sait,Syed Sibghat Ullah,Mohammad Abusheikh,Hasan Abusheikh*

Main category: cs.AI

TLDR: 本研究使用弱监督学习训练了一个阿拉伯语自动语音识别（ASR）模型，采用Conformer架构，在15,000小时弱标注语音数据上训练，实现了最先进性能，而无需手动转录。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言如阿拉伯语，开发高性能ASR模型面临挑战，因为缺乏大型标注数据集，且采集成本高。

Method: 采用弱监督学习，从零开始训练Conformer架构模型，使用15,000小时覆盖现代标准阿拉伯语和方言阿拉伯语的弱标注语音数据。

Result: 在标准基准测试中达到了最先进性能，超过了之前的所有阿拉伯语ASR努力。

Conclusion: 证明了弱监督学习作为一种可扩展、成本高效的替代方案的有效性，为低资源环境下的ASR系统改进铺平了道路。

Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction
in diverse applications like conversational agents, industrial robotics, call
center automation, and automated subtitling. However, developing
high-performance ASR models remains challenging, particularly for low-resource
languages like Arabic, due to the scarcity of large, labeled speech datasets,
which are costly and labor-intensive to produce. In this work, we employ weakly
supervised learning to train an Arabic ASR model using the Conformer
architecture. Our model is trained from scratch on 15,000 hours of weakly
annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal
Arabic (DA), eliminating the need for costly manual transcriptions. Despite the
absence of human-verified labels, our approach attains state-of-the-art (SOTA)
performance, exceeding all previous efforts in the field of Arabic ASR on the
standard benchmarks. By demonstrating the effectiveness of weak supervision as
a scalable, cost-efficient alternative to traditional supervised approaches,
paving the way for improved ASR systems in low resource settings.

</details>

### [23] [Adapting a World Model for Trajectory Following in a 3D Game](https://arxiv.org/abs/2504.12299)
*Marko Tot,Shu Ishida,Abdelhak Lemkhenter,David Bignell,Pallavi Choudhury,Chris Lovett,Luis França,Matheus Ribeiro Furtado de Mendonça,Tarun Gupta,Darren Gehring,Sam Devlin,Sergio Valcarcel Macua,Raluca Georgescu*

Main category: cs.AI

TLDR: 这篇论文探讨了在3D视频游戏中使用逆动态模型改进模仿学习，以应对分布偏移和随机性。结果显示不同配置在不同数据场景下优越。


<details>
  <summary>Details</summary>
Motivation: 动机是模仿学习在复杂环境中需要更稳健的方法来处理分布偏移和随机性。

Method: 方法包括应用不同编码器和策略头的逆动态模型，以及未来对齐策略，在Bleeding Edge游戏中测试轨迹跟随。

Result: 结果表明，在多样数据设置下，GPT-style策略头和从零训练编码器最佳；在低数据下，DINOv2编码器和GPT-style策略头最佳；预训练和微调后，GPT和MLP策略头性能相当。

Conclusion: 结论是最佳配置取决于具体设置，突出了组件选择的重要性。

Abstract: Imitation learning is a powerful tool for training agents by leveraging
expert knowledge, and being able to replicate a given trajectory is an integral
part of it. In complex environments, like modern 3D video games, distribution
shift and stochasticity necessitate robust approaches beyond simple action
replay. In this study, we apply Inverse Dynamics Models (IDM) with different
encoders and policy heads to trajectory following in a modern 3D video game --
Bleeding Edge. Additionally, we investigate several future alignment strategies
that address the distribution shift caused by the aleatoric uncertainty and
imperfections of the agent. We measure both the trajectory deviation distance
and the first significant deviation point between the reference and the agent's
trajectory and show that the optimal configuration depends on the chosen
setting. Our results show that in a diverse data setting, a GPT-style policy
head with an encoder trained from scratch performs the best, DINOv2 encoder
with the GPT-style policy head gives the best results in the low data regime,
and both GPT-style and MLP-style policy heads had comparable results when
pre-trained on a diverse setting and fine-tuned for a specific behaviour
setting.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines](https://arxiv.org/abs/2504.11476)
*Ritik Mishra,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TLDR: 本文提出了一种类信息限制核机器（CI-RKM），以提升RKM对噪声和异常值的鲁棒性，实验显示其在分类准确性和鲁棒性上优于基线。


<details>
  <summary>Details</summary>
Motivation: RKM在噪声和异常值存在时性能下降，亟需改进以提高鲁棒性和预测准确性。

Method: 通过整合类信息加权函数，利用加权共轭特征对偶和Schur补矩阵定理，引入CI-RKM。

Result: 在基准数据集实验中，CI-RKM在分类准确性和对噪声及异常值的鲁棒性方面均优于现有基线。

Conclusion: 该方法在核基学习模型中取得重大进展，解决了领域核心挑战，提升了泛化能力和数据鲁棒性。

Abstract: Restricted kernel machines (RKMs) represent a versatile and powerful
framework within the kernel machine family, leveraging conjugate feature
duality to address a wide range of machine learning tasks, including
classification, regression, and feature learning. However, their performance
can degrade significantly in the presence of noise and outliers, which
compromises robustness and predictive accuracy. In this paper, we propose a
novel enhancement to the RKM framework by integrating a class-informed weighted
function. This weighting mechanism dynamically adjusts the contribution of
individual training points based on their proximity to class centers and
class-specific characteristics, thereby mitigating the adverse effects of noisy
and outlier data. By incorporating weighted conjugate feature duality and
leveraging the Schur complement theorem, we introduce the class-informed
restricted kernel machine (CI-RKM), a robust extension of the RKM designed to
improve generalization and resilience to data imperfections. Experimental
evaluations on benchmark datasets demonstrate that the proposed CI-RKM
consistently outperforms existing baselines, achieving superior classification
accuracy and enhanced robustness against noise and outliers. Our proposed
method establishes a significant advancement in the development of kernel-based
learning models, addressing a core challenge in the field.

</details>

### [25] [LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2504.11497)
*Chang Liu,Emmanuel A. Olowe,Danial Chitnis*

Main category: cs.LG

TLDR: 本文提出了一种基于LLM的AI代理，用于自动调整AMS电路的晶体管尺寸，并实现了高达60%的成功率来满足性能指标。


<details>
  <summary>Details</summary>
Motivation: AMS集成电路设计需要大量手动工作，机器学习技术存在挑战，LLM显示出在电路设计中的潜力。

Method: 通过将LLM与外部电路模拟工具和数据分析功能集成，并采用提示工程策略，优化了多个电路，并评估不同LLM模型。

Result: 在七个基本电路和一个运算放大器上测试，成功率最高达60%，满足多种性能要求。

Conclusion: 这项工作证明了LLM在改善AMS电路设计方面的潜力。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose an LLM-based
AI agent for AMS circuit design to assist in the sizing process. By integrating
LLMs with external circuit simulation tools and data analysis functions and
employing prompt engineering strategies, the agent successfully optimized
multiple circuits to achieve target performance metrics. We evaluated the
performance of different LLMs to assess their applicability and optimization
effectiveness across seven basic circuits, and selected the best-performing
model Claude 3.5 Sonnet for further exploration on an operational amplifier,
with complementary input stage and class AB output stage. This circuit was
evaluated against nine performance metrics, and we conducted experiments under
three distinct performance requirement groups. A success rate of up to 60% was
achieved for reaching the target requirements. Overall, this work demonstrates
the potential of LLMs to improve AMS circuit design.

</details>

### [26] [Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning](https://arxiv.org/abs/2504.11506)
*Hongliang Lu,Shuqi Shen,Junjie Yang,Chao Lu,Xinhu Zheng,Hai Yang*

Main category: cs.LG

TLDR: 本研究提出数据轻量化的逆强化学习方法，帮助自动驾驶车辆（AV）在不同文化背景下快速部署，减少对本地数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆部署面临文化差异挑战，尤其在数据不足的地区，无法通过数据学习本地驾驶文化。

Method: 提出数据轻量化的逆强化学习方案，通过比较德国、中国和美国的自然驾驶数据集，并进行跨文化部署测试，总里程超过56084公里。

Result: 结果显示，该方法可将对本地数据的依赖减少98.67%，在数据稀缺情况下表现突出。

Conclusion: 这项研究有助于实现更广泛、更公平的全球AV市场，特别是针对数据匮乏地区。

Abstract: More than the adherence to specific traffic regulations, driving culture
touches upon a more implicit part - an informal, conventional, collective
behavioral pattern followed by drivers - that varies across countries, regions,
and even cities. Such cultural divergence has become one of the biggest
challenges in deploying autonomous vehicles (AVs) across diverse regions today.
The current emergence of data-driven methods has shown a potential solution to
enable culture-compatible driving through learning from data, but what if some
underdeveloped regions cannot provide sufficient local data to inform driving
culture? This issue is particularly significant for a broader global AV market.
Here, we propose a cross-cultural deployment scheme for AVs, called data-light
inverse reinforcement learning, designed to re-calibrate culture-specific AVs
and assimilate them into other cultures. First, we report the divergence in
driving cultures through a comprehensive comparative analysis of naturalistic
driving datasets on highways from three countries: Germany, China, and the USA.
Then, we demonstrate the effectiveness of our scheme by testing the expeditious
cross-cultural deployment across these three countries, with cumulative testing
mileage of over 56084 km. The performance is particularly advantageous when
cross-cultural deployment is carried out without affluent local data. Results
show that we can reduce the dependence on local data by a margin of 98.67% at
best. This study is expected to bring a broader, fairer AV global market,
particularly in those regions that lack enough local data to develop
culture-compatible AVs.

</details>

### [27] [Reward Distance Comparisons Under Transition Sparsity](https://arxiv.org/abs/2504.11508)
*Clement Nyanhongo,Bruno Miranda Henrique,Eugene Santos*

Main category: cs.LG

TLDR: 本文提出SRRD伪度量，用于处理奖励比较中的过渡稀疏问题，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 避免策略学习的高计算成本和安全风险，解决直接奖励比较的过渡稀疏性问题。

Method: 引入Sparsity Resilient Reward Distance (SRRD)伪度量，适应多样样本分布，无需高过渡覆盖率。

Result: 理论证明SRRD的鲁棒性，并通过多领域实验验证其实际有效性。

Conclusion: SRRD改进了奖励比较方法，适用于过渡稀疏条件下的可靠评估。

Abstract: Reward comparisons are vital for evaluating differences in agent behaviors
induced by a set of reward functions. Most conventional techniques utilize the
input reward functions to learn optimized policies, which are then used to
compare agent behaviors. However, learning these policies can be
computationally expensive and can also raise safety concerns. Direct reward
comparison techniques obviate policy learning but suffer from transition
sparsity, where only a small subset of transitions are sampled due to data
collection challenges and feasibility constraints. Existing state-of-the-art
direct reward comparison methods are ill-suited for these sparse conditions
since they require high transition coverage, where the majority of transitions
from a given coverage distribution are sampled. When this requirement is not
satisfied, a distribution mismatch between sampled and expected transitions can
occur, leading to significant errors. This paper introduces the Sparsity
Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need
for high transition coverage by accommodating diverse sample distributions,
which are common under transition sparsity. We provide theoretical
justification for SRRD's robustness and conduct experiments to demonstrate its
practical efficacy across multiple domains.

</details>

### [28] [Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs](https://arxiv.org/abs/2504.11511)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.LG

TLDR: 这篇论文主张在强化学习系统中采用新的隐私范式，基于四个核心原则，并呼吁开发新框架。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实世界的应用兴起，传统隐私框架无法处理顺序决策系统中的敏感信息。

Method: 作为位置论文，论证新隐私原则并呼吁开发理论框架、机制和评价方法。

Result: 暴露隐私、效用和可解释性之间的张力，并强调在顺序决策系统中的隐私保护需求。

Conclusion: 需要新隐私范式、理论框架和实践机制来保护强化学习系统的隐私。

Abstract: The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.

</details>

### [29] [Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor](https://arxiv.org/abs/2504.11513)
*Wonjun Yi,Yong-Hwa Park*

Main category: cs.LG

TLDR: 这篇论文提出多输出分类框架，用于故障诊断领域适应，提高可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统多类分类方法在部分标记目标域和复合故障条件下的局限性。

Method: 使用多输出分类框架、MK-MMD、EM损失和FLN技术进行特征转移和归一化。

Result: 在实验中，MOC在宏F1分数上优于基线，FLN显示更好适应性。

Conclusion: 证明MOC框架在故障诊断中更有效，尤其在部分标记数据场景。

Abstract: This work introduces a multi-output classification (MOC) framework designed
for domain adaptation in fault diagnosis, particularly under partially labeled
(PL) target domain scenarios and compound fault conditions in rotating
machinery. Unlike traditional multi-class classification (MCC) methods that
treat each fault combination as a distinct class, the proposed approach
independently estimates the severity of each fault type, improving both
interpretability and diagnostic accuracy. The model incorporates multi-kernel
maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to
facilitate feature transfer from the source to the target domain. In addition,
frequency layer normalization (FLN) is applied to preserve structural
properties in the frequency domain, which are strongly influenced by system
dynamics and are often stationary with respect to changes in rpm. Evaluations
across six domain adaptation cases with PL data demonstrate that MOC
outperforms baseline models in macro F1 score. Moreover, MOC consistently
achieves better classification performance for individual fault types, and FLN
shows superior adaptability compared to other normalization techniques.

</details>

### [30] [LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation](https://arxiv.org/abs/2504.11521)
*Wei-Jer Chang,Wei Zhan,Masayoshi Tomizuka,Manmohan Chandraker,Francesco Pittaluga*

Main category: cs.LG

TLDR: 这篇论文介绍了LangTraj，一个语言条件场景扩散模型，用于自治车辆测试的模拟，提高了测试的可扩展性和安全性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统方法依赖特定领域指导函数的局限性，通过语言条件控制实现更高效、安全的自治车辆测试。

Method: 方法包括开发LangTraj模型、提出闭环训练策略和构建Inter-Drive数据集，用于语言条件下的交通场景模拟。

Result: 结果显示LangTraj在Waymo Motion Dataset上表现出色，具有高现实性、语言可控性和安全关键模拟性能。

Conclusion: 结论是LangTraj建立了自治车辆测试的新的灵活可扩展范式。

Abstract: Evaluating autonomous vehicles with controllability enables scalable testing
in counterfactual or structured settings, enhancing both efficiency and safety.
We introduce LangTraj, a language-conditioned scene-diffusion model that
simulates the joint behavior of all agents in traffic scenarios. By
conditioning on natural language inputs, LangTraj provides flexible and
intuitive control over interactive behaviors, generating nuanced and realistic
scenarios. Unlike prior approaches that depend on domain-specific guidance
functions, LangTraj incorporates language conditioning during training,
facilitating more intuitive traffic simulation control. We propose a novel
closed-loop training strategy for diffusion models, explicitly tailored to
enhance stability and realism during closed-loop simulation. To support
language-conditioned simulation, we develop Inter-Drive, a large-scale dataset
with diverse and interactive labels for training language-conditioned diffusion
models. Our dataset is built upon a scalable pipeline for annotating
agent-agent interactions and single-agent behaviors, ensuring rich and varied
supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates
strong performance in realism, language controllability, and
language-conditioned safety-critical simulation, establishing a new paradigm
for flexible and scalable autonomous vehicle testing.

</details>

### [31] [Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism](https://arxiv.org/abs/2504.11558)
*Mete Erdogan,Cengiz Pehlevan,Alper T. Erdogan*

Main category: cs.LG

TLDR: 这篇论文引入EBD算法，通过广播输出错误到各层解决神经网络信用分配问题，并在基准数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中信用分配问题，提供一种无需权重传输的、基于最小均方误差估计器的错误广播方法。

Method: 引入EBD算法，利用随机正交性定义层级损失函数，惩罚层激活与输出错误的相关性，并整合到生物学合理的框架中。

Result: 在基准数据集上，EBD性能与已知方法相当或更好，展示了其高效和适应性。

Conclusion: EBD为神经网络训练提供了一个生物学合理的、有效的替代方案，可能启发未来人工和自然学习的发展。

Abstract: We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel
learning framework that addresses the credit assignment problem in neural
networks by directly broadcasting output error to individual layers. Leveraging
the stochastic orthogonality property of the optimal minimum mean square error
(MMSE) estimator, EBD defines layerwise loss functions to penalize correlations
between layer activations and output errors, offering a principled approach to
error broadcasting without the need for weight transport. The optimization
framework naturally leads to the experimentally observed three-factor learning
rule and integrates with biologically plausible frameworks to enhance
performance and plausibility. Numerical experiments demonstrate that EBD
achieves performance comparable to or better than known error-broadcast methods
on benchmark datasets. While the scalability of EBD to very large or complex
datasets remains to be further explored, our findings suggest it provides a
biologically plausible, efficient, and adaptable alternative for neural network
training. This approach could inform future advancements in artificial and
natural learning paradigms.

</details>

### [32] [Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring](https://arxiv.org/abs/2504.11581)
*Mert Sehri,Igor Varejão,Zehui Hua,Vitor Bonella,Adriano Santos,Francisco de Assis Boldt,Patrick Dumond,Flavio Miguel Varejão*

Main category: cs.LG

TLDR: 本论文提出一个振动分析数据集框架，类似于ImageNet，用于提升机器故障诊断等领域的模型性能。


<details>
  <summary>Details</summary>
Motivation: 振动分析领域缺乏大型标注数据集，导致模型训练效率低下，无法像ImageNet一样促进转移学习。

Method: 提出以轴承振动数据为基础的框架，从公开数据集收集信号，并使用深度学习模型进行预训练和微调实验。

Result: 实验显示，预训练模型在特定领域数据集上微调后，模型性能得到改善。

Conclusion: 该框架有潜力推动振动分析领域发展，未来将扩展到更多机械类型，并标准化数据处理方法。

Abstract: ImageNet has become a reputable resource for transfer learning, allowing the
development of efficient ML models with reduced training time and data
requirements. However, vibration analysis in predictive maintenance, structural
health monitoring, and fault diagnosis, lacks a comparable large-scale,
annotated dataset to facilitate similar advancements. To address this, a
dataset framework is proposed that begins with bearing vibration data as an
initial step towards creating a universal dataset for vibration-based
spectrogram analysis for all machinery. The initial framework includes a
collection of bearing vibration signals from various publicly available
datasets. To demonstrate the advantages of this framework, experiments were
conducted using a deep learning architecture, showing improvements in model
performance when pre-trained on bearing vibration data and fine-tuned on a
smaller, domain-specific dataset. These findings highlight the potential to
parallel the success of ImageNet in visual computing but for vibration
analysis. For future work, this research will include a broader range of
vibration signals from multiple types of machinery, emphasizing
spectrogram-based representations of the data. Each sample will be labeled
according to machinery type, operational status, and the presence or type of
faults, ensuring its utility for supervised and unsupervised learning tasks.
Additionally, a framework for data preprocessing, feature extraction, and model
training specific to vibration data will be developed. This framework will
standardize methodologies across the research community, allowing for
collaboration and accelerating progress in predictive maintenance, structural
health monitoring, and related fields. By mirroring the success of ImageNet in
visual computing, this dataset has the potential to improve the development of
intelligent systems in industrial applications.

</details>

### [33] [Dueling Deep Reinforcement Learning for Financial Time Series](https://arxiv.org/abs/2504.11601)
*Bruno Giorgio*

Main category: cs.LG

TLDR: 这篇论文探讨使用Double DQN和Dueling Network架构的强化学习在金融交易中的应用，基于SP500数据，考虑交易成本，发现RL代理能优于随机策略但面临挑战。


<details>
  <summary>Details</summary>
Motivation: 动机是应用RL优化金融交易策略，处理动态环境和实际约束如交易成本。

Method: 方法包括使用DDQN和Dueling Network架构，训练代理在历史SP500指数数据上，评估有无佣金场景的性能。

Result: 结果显示代理学会了有效的交易政策，优于随机策略，但数据复杂性导致政策可能次优。

Conclusion: 结论是RL代理在使用高级架构时能提升性能，但金融数据复杂性带来的挑战仍存在。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for solving
decision-making problems in dynamic environments. In this research, we explore
the application of Double DQN (DDQN) and Dueling Network Architectures, to
financial trading tasks using historical SP500 index data. Our focus is
training agents capable of optimizing trading strategies while accounting for
practical constraints such as transaction costs. The study evaluates the model
performance across scenarios with and without commissions, highlighting the
impact of cost-sensitive environments on reward dynamics. Despite computational
limitations and the inherent complexity of financial time series data, the
agent successfully learned meaningful trading policies. The findings confirm
that RL agents, even when trained on limited datasets, can outperform random
strategies by leveraging advanced architectures such as DDQN and Dueling
Networks. However, significant challenges persist, particularly with a
sub-optimal policy due to the complexity of data source.

</details>

### [34] [Possibility for Proactive Anomaly Detection](https://arxiv.org/abs/2504.11623)
*Jinsung Jeon,Jaehyeon Park,Sewon Park,Jeongwhan Choi,Minjung Kim,Noseong Park*

Main category: cs.LG

TLDR: 本论文提出了一种主动时间序列异常检测方法，使用预测模型和阈值来检测异常。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测模型依赖输出与真实值的误差，不实用；本研究旨在减少潜在损害。

Method: 使用时间序列预测模型和数据驱动模型从训练数据建立异常阈值，然后通过预测值超过阈值检测异常。

Result: 模型在四个基准上评估，分析了可预测和不可预测异常。

Conclusion: 方法更实用，并提供源代码以供参考。

Abstract: Time-series anomaly detection, which detects errors and failures in a
workflow, is one of the most important topics in real-world applications. The
purpose of time-series anomaly detection is to reduce potential damages or
losses. However, existing anomaly detection models detect anomalies through the
error between the model output and the ground truth (observed) value, which
makes them impractical. In this work, we present a \textit{proactive} approach
for time-series anomaly detection based on a time-series forecasting model
specialized for anomaly detection and a data-driven anomaly detection model.
Our proactive approach establishes an anomaly threshold from training data with
a data-driven anomaly detection model, and anomalies are subsequently detected
by identifying predicted values that exceed the anomaly threshold. In addition,
we extensively evaluated the model using four anomaly detection benchmarks and
analyzed both predictable and unpredictable anomalies. We attached the source
code as supplementary material.

</details>

### [35] [Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling](https://arxiv.org/abs/2504.11645)
*Feng Zhu,Aritra Mitra,Robert W. Heath*

Main category: cs.LG

TLDR: 论文提出FedHSA算法，用于联邦随机逼近问题，实现了协作加速和正确收敛。


<details>
  <summary>Details</summary>
Motivation: 受协作强化学习和时间相关数据优化的启发，研究异质联邦随机逼近问题。

Method: 开发FedHSA算法，并证明其收敛性。

Result: 保证收敛到正确点，并获得M倍线性加速。

Conclusion: 对异质联邦强化学习问题有重要启示。

Abstract: Motivated by collaborative reinforcement learning (RL) and optimization with
time-correlated data, we study a generic federated stochastic approximation
problem involving $M$ agents, where each agent is characterized by an
agent-specific (potentially nonlinear) local operator. The goal is for the
agents to communicate intermittently via a server to find the root of the
average of the agents' local operators. The generality of our setting stems
from allowing for (i) Markovian data at each agent and (ii) heterogeneity in
the roots of the agents' local operators. The limited recent work that has
accounted for both these features in a federated setting fails to guarantee
convergence to the desired point or to show any benefit of collaboration;
furthermore, they rely on projection steps in their algorithms to guarantee
bounded iterates. Our work overcomes each of these limitations. We develop a
novel algorithm titled \texttt{FedHSA}, and prove that it guarantees
convergence to the correct point, while enjoying an $M$-fold linear speedup in
sample-complexity due to collaboration. To our knowledge, \emph{this is the
first finite-time result of its kind}, and establishing it (without relying on
a projection step) entails a fairly intricate argument that accounts for the
interplay between complex temporal correlations due to Markovian sampling,
multiple local steps to save communication, and the drift-effects induced by
heterogeneous local operators. Our results have implications for a broad class
of heterogeneous federated RL problems (e.g., policy evaluation and control)
with function approximation, where the agents' Markov decision processes can
differ in their probability transition kernels and reward functions.

</details>

### [36] [70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float](https://arxiv.org/abs/2504.11651)
*Tianyi Zhang,Yang Sui,Shaochen Zhong,Vipin Chaudhary,Xia Hu,Anshumali Shrivastava*

Main category: cs.LG

TLDR: DFloat11 是一种无损压缩框架，可将大语言模型大小减少 30%，并通过熵编码和自定义 GPU 内核实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 论文受 BFloat16 权重表示中低熵的启发，揭示了现有存储格式的低效性。

Method: 使用熵编码为权重分配动态长度编码，并开发自定义 GPU 内核，包括 LUT 分解、两阶段内核协调和块级解压缩。

Result: 实验显示模型大小减少 30%，输出位级相同，生成令牌吞吐量提高 1.9-38.8 倍，上下文长度延长 5.3-13.17 倍，并支持在单节点上推理大型模型。

Conclusion: DFloat11 提供了一种有效的无损压缩方法，提升了资源受限硬件上的 LLM 部署效率。

Abstract: Large Language Models (LLMs) have grown rapidly in size, creating significant
challenges for efficient deployment on resource-constrained hardware. In this
paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression
framework that reduces LLM size by 30% while preserving outputs that are
bit-for-bit identical to the original model. DFloat11 is motivated by the low
entropy in the BFloat16 weight representation of LLMs, which reveals
significant inefficiency in existing storage format. By applying entropy
coding, DFloat11 assigns dynamic-length encodings to weights based on
frequency, achieving near information-optimal compression without any loss of
precision. To facilitate efficient inference with dynamic-length encodings, we
develop a custom GPU kernel for fast online decompression. Our design
incorporates the following: (i) decomposition of memory-intensive lookup tables
(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for
coordinating thread read/write positions using lightweight auxiliary variables,
and (iii) transformer-block-level decompression to minimize latency.
Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,
validates our hypothesis that DFloat11 achieves around 30% model size reduction
while preserving bit-for-bit exact outputs. Compared to a potential alternative
of offloading parts of an uncompressed model to the CPU to meet memory
constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.
With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context
lengths than uncompressed models. Notably, our method enables lossless
inference of Llama-3.1-405B, an 810GB model, on a single node equipped with
8x80GB GPUs. Our code and models are available at
https://github.com/LeanModels/DFloat11.

</details>

### [37] [H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning](https://arxiv.org/abs/2504.11699)
*Rui Xue,Tianfu Wu*

Main category: cs.LG

TLDR: H³GNNs 是一种端到端自监督学习框架，通过平衡异质性和同质性来提升图神经网络的表示学习。创新包括联合结构节点编码和教师-学生预测架构，实验在多个基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在表示学习中难以平衡异质性和同质性，尤其在自监督设置中这一挑战更突出。

Method: 提出 H³GNNs，包括：(i) 联合结构节点编码，使用加权图卷积网络和交叉注意力机制；(ii) 教师-学生模型的自监督学习，结合节点难度驱动的动态掩码策略。

Result: 在七个基准数据集上实验显示，H³GNNs 在四个异质性数据集上达到最先进性能，在三个同质性数据集上与最先进方法相当。

Conclusion: H³GNNs 在不同类型图上有效且高效，证明了其在处理异质性和同质性方面的优势。

Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in
representation learning, a challenge further amplified in self-supervised
settings. We propose H$^3$GNNs, an end-to-end self-supervised learning
framework that harmonizes both structural properties through two key
innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified
space combining linear and non-linear feature projections with K-hop structural
representations via a Weighted Graph Convolution Network(WGCN). A
cross-attention mechanism enhances awareness and adaptability to heterophily
and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive
Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a
teacher-student model, the student sees the masked input graph and predicts
node features inferred by the teacher that sees the full input graph in the
joint encoding space. To enhance learning difficulty, we introduce two novel
node-predictive-difficulty-based masking strategies. Experiments on seven
benchmarks (four heterophily datasets and three homophily datasets) confirm the
effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our
H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily
datasets, while retaining on-par performance to previous state-of-the-art
methods on the three homophily datasets.

</details>

### [38] [Clustering and analysis of user behaviour in blockchain: A case study of Planet IX](https://arxiv.org/abs/2504.11702)
*Dorottya Zelenyanszki,Zhe Hou,Kamanashis Biswas,Vallipuram Muthukkumarasamy*

Main category: cs.LG

TLDR: 本论文提出用户行为分析管道，从区块链游戏交易数据中提取行为集群，并讨论隐私风险。


<details>
  <summary>Details</summary>
Motivation: 区块链dApps透明性导致数据可追踪，可能泄露用户行为信息，需要分析隐私问题。

Method: 收集Planet IX游戏交易数据，构建用户行为流，使用GNN生成嵌入，并应用聚类算法比较。

Result: 通过聚类算法识别行为集群，发现可提取用户行为信息，并构建隐私威胁模型。

Conclusion: 研究显示，用户行为可被恶意利用，强调区块链游戏的隐私风险。

Abstract: Decentralised applications (dApps) that run on public blockchains have the
benefit of trustworthiness and transparency as every activity that happens on
the blockchain can be publicly traced through the transaction data. However,
this introduces a potential privacy problem as this data can be tracked and
analysed, which can reveal user-behaviour information. A user behaviour
analysis pipeline was proposed to present how this type of information can be
extracted and analysed to identify separate behavioural clusters that can
describe how users behave in the game. The pipeline starts with the collection
of transaction data, involving smart contracts, that is collected from a
blockchain-based game called Planet IX. Both the raw transaction information
and the transaction events are considered in the data collection. From this
data, separate game actions can be formed and those are leveraged to present
how and when the users conducted their in-game activities in the form of user
flows. An extended version of these user flows also presents how the
Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter
is given as input for a Graph Neural Network (GNN) model to provide graph
embeddings for these flows which then can be leveraged by clustering algorithms
to cluster user behaviours into separate behavioural clusters. We benchmark and
compare well-known clustering algorithms as a part of the proposed method. The
user behaviour clusters were analysed and visualised in a graph format. It was
found that behavioural information can be extracted regarding the users that
belong to these clusters. Such information can be exploited by malicious users
to their advantage. To demonstrate this, a privacy threat model was also
presented based on the results that correspond to multiple potentially affected
areas.

</details>

### [39] [Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching](https://arxiv.org/abs/2504.11713)
*Aaron Havens,Benjamin Kurt Miller,Bing Yan,Carles Domingo-Enrich,Anuroop Sriram,Brandon Wood,Daniel Levine,Bin Hu,Brandon Amos,Brian Karrer,Xiang Fu,Guan-Horng Liu,Ricky T. Q. Chen*

Main category: cs.LG

TLDR: 本论文引入Adjoint Sampling算法，这是一种高效、可扩展的方法，用于从未归一化密度或能量函数中学习扩散过程的采样。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一种on-policy方法，能够允许更多梯度更新并扩展到更大问题设置，以解决现有采样方法的局限性。

Method: 方法基于随机最优控制的框架，是一种on-policy方法，无需校正措施，并整合对称性和周期边界条件用于分子建模。

Result: 结果通过实验证明了在经典能量函数和神经网络能量模型上的有效性，包括对多个分子系统的摊销构象生成。

Conclusion: 结论是，该方法提升了采样效率，并计划开源基准以推动计算化学领域的研究。

Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for
learning diffusion processes that sample from unnormalized densities, or energy
functions. It is the first on-policy approach that allows significantly more
gradient updates than the number of energy evaluations and model samples,
allowing us to scale to much larger problem settings than previously explored
by similar methods. Our framework is theoretically grounded in stochastic
optimal control and shares the same theoretical guarantees as Adjoint Matching,
being able to train without the need for corrective measures that push samples
towards the target distribution. We show how to incorporate key symmetries, as
well as periodic boundary conditions, for modeling molecules in both cartesian
and torsional coordinates. We demonstrate the effectiveness of our approach
through extensive experiments on classical energy functions, and further scale
up to neural network-based energy models where we perform amortized conformer
generation across many molecular systems. To encourage further research in
developing highly scalable sampling methods, we plan to open source these
challenging benchmarks, where successful methods can directly impact progress
in computational chemistry.

</details>

### [40] [Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception](https://arxiv.org/abs/2504.11726)
*Yunzhe Li,Facheng Hu,Hongzi Zhu,Shifan Zhang,Liang Zhang,Shan Chang,Minyi Guo*

Main category: cs.LG

TLDR: 这篇论文提出Saga方法，使用少量标记的IMU数据实现高精度用户感知，减少对大量标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: IMU数据在移动感知应用中广泛使用，但标记微活动数据困难，因为理解原始数据不易且缺乏地面真相。

Method: 先利用大量无标记IMU数据预训练特征提取模型，提取不同级别语义信息；然后使用贝叶斯优化为特定应用确定预训练任务权重。

Result: 使用每类约100个训练样本，Saga达到使用上万样本模型90%以上的准确率，无额外开销；在三个IMU数据集的三个任务上验证。

Conclusion: Saga方法有效，能以少量标记数据高效实现高准确率的用户感知。

Abstract: Inertial measurement units (IMUs), have been prevalently used in a wide range
of mobile perception applications such as activity recognition and user
authentication, where a large amount of labelled data are normally required to
train a satisfactory model. However, it is difficult to label micro-activities
in massive IMU data due to the hardness of understanding raw IMU data and the
lack of ground truth. In this paper, we propose a novel fine-grained user
perception approach, called Saga, which only needs a small amount of labelled
IMU data to achieve stunning user perception accuracy. The core idea of Saga is
to first pre-train a backbone feature extraction model, utilizing the rich
semantic information of different levels embedded in the massive unlabelled IMU
data. Meanwhile, for a specific downstream user perception application,
Bayesian Optimization is employed to determine the optimal weights for
pre-training tasks involving different semantic levels. We implement Saga on
five typical mobile phones and evaluate Saga on three typical tasks on three
IMU datasets. Results show that when only using about 100 training samples per
class, Saga can achieve over 90% accuracy of the full-fledged model trained on
over ten thousands training samples with no additional system overhead.

</details>

### [41] [Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective](https://arxiv.org/abs/2504.11757)
*Pradeep Singh,Ashutosh Kumar,Sutirtha Ghosh,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TLDR: 这篇论文系统探讨了Reservoir Computing（RC）的理论基础、属性和应用，强调其在处理时间序列数据中的高效性，并指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统循环神经网络训练复杂性，通过固定reservoir和线性readout提高计算效率，并深化动态系统理论理解。

Method: 方法是通过动态系统理论形式化RC的属性，如echo state property和fading memory，并分析输入信号与reservoir状态的相互作用。

Result: 结果展示了RC的稳定性和表现力条件、计算权衡，以及在信号处理、时间序列预测和控制系统中的应用潜力。

Conclusion: 结论突出了RC理论的优化、训练方法和可扩展性的挑战，并提出未来发展的潜在方向。

Abstract: Reservoir computing (RC) represents a class of state-space models (SSMs)
characterized by a fixed state transition mechanism (the reservoir) and a
flexible readout layer that maps from the state space. It is a paradigm of
computational dynamical systems that harnesses the transient dynamics of
high-dimensional state spaces for efficient processing of temporal data. Rooted
in concepts from recurrent neural networks, RC achieves exceptional
computational power by decoupling the training of the dynamic reservoir from
the linear readout layer, thereby circumventing the complexities of
gradient-based optimization. This work presents a systematic exploration of RC,
addressing its foundational properties such as the echo state property, fading
memory, and reservoir capacity through the lens of dynamical systems theory. We
formalize the interplay between input signals and reservoir states,
demonstrating the conditions under which reservoirs exhibit stability and
expressive power. Further, we delve into the computational trade-offs and
robustness characteristics of RC architectures, extending the discussion to
their applications in signal processing, time-series prediction, and control
systems. The analysis is complemented by theoretical insights into
optimization, training methodologies, and scalability, highlighting open
challenges and potential directions for advancing the theoretical underpinnings
of RC.

</details>

### [42] [Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs](https://arxiv.org/abs/2504.11808)
*Kishan Gurumurthy,Himanshu Pal,Charu Sharma*

Main category: cs.LG

TLDR: 这篇论文提出了一种基于联邦学习的图神经网络方法，使用谱图神经网络和神经ODE处理非IID和异质图数据，实现隐私保护和高效性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络训练面临数据集中化难题，由于隐私、法规和竞争问题，联邦学习成为解决方案，但对GNN的探索不足。

Method: 提出结合谱图神经网络和神经ODE的联邦学习方法。

Result: 在同质和异质图上表现出色，处理非IID数据，性能与现有方法相当，并优化带宽。

Conclusion: 突出了联邦学习在多样图结构中的潜力，并提供了开源代码。

Abstract: Graph Neural Network (GNN) research is rapidly advancing due to GNNs'
capacity to learn distributed representations from graph-structured data.
However, centralizing large volumes of real-world graph data for GNN training
is often impractical due to privacy concerns, regulatory restrictions, and
commercial competition. Federated learning (FL), a distributed learning
paradigm, offers a solution by preserving data privacy with collaborative model
training. Despite progress in training huge vision and language models,
federated learning for GNNs remains underexplored. To address this challenge,
we present a novel method for federated learning on GNNs based on spectral GNNs
equipped with neural ordinary differential equations (ODE) for better
information capture, showing promising results across both homophilic and
heterophilic graphs. Our approach effectively handles non-Independent and
Identically Distributed (non-IID) data, while also achieving performance
comparable to existing methods that only operate on IID data. It is designed to
be privacy-preserving and bandwidth-optimized, making it suitable for
real-world applications such as social network analysis, recommendation
systems, and fraud detection, which often involve complex, non-IID, and
heterophilic graph structures. Our results in the area of federated learning on
non-IID heterophilic graphs demonstrate significant improvements, while also
achieving better performance on homophilic graphs. This work highlights the
potential of federated learning in diverse and challenging graph settings.
Open-source code available on GitHub
(https://github.com/SpringWiz11/Fed-GNODEFormer).

</details>

### [43] [Manifold meta-learning for reduced-complexity neural system identification](https://arxiv.org/abs/2504.11811)
*Marco Forgione,Ankush Chakrabarty,Dario Piga,Matteo Rufolo,Alberto Bemporad*

Main category: cs.LG

TLDR: 这篇论文提出了一种元学习框架，通过学习参数空间低维流形，提高神经网络在系统识别中的数据和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深层学习模型在建模复杂非线性动态系统时需要大量数据和计算资源，而传统方法可能不可行。

Method: 提出元学习框架，使用辅助神经网络直接映射数据集到低维流形，减少计算开销，避免高阶梯度计算。

Result: 在Bouc-Wen振荡器benchmark上验证，即使在小数据场景下也能学习准确模型。

Conclusion: 该方法提高了系统识别的效率，适用于部分未知物理系统的建模。

Abstract: System identification has greatly benefited from deep learning techniques,
particularly for modeling complex, nonlinear dynamical systems with partially
unknown physics where traditional approaches may not be feasible. However, deep
learning models often require large datasets and significant computational
resources at training and inference due to their high-dimensional
parameterizations. To address this challenge, we propose a meta-learning
framework that discovers a low-dimensional manifold within the parameter space
of an over-parameterized neural network architecture. This manifold is learned
from a meta-dataset of input-output sequences generated by a class of related
dynamical systems, enabling efficient model training while preserving the
network's expressive power for the considered system class. Unlike bilevel
meta-learning approaches, our method employs an auxiliary neural network to map
datasets directly onto the learned manifold, eliminating the need for costly
second-order gradient computations during meta-training and reducing the number
of first-order updates required in inference, which could be expensive for
large models. We validate our approach on a family of Bouc-Wen oscillators,
which is a well-studied nonlinear system identification benchmark. We
demonstrate that we are able to learn accurate models even in small-data
scenarios.

</details>

### [44] [Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading](https://arxiv.org/abs/2504.11816)
*Kihyun Kim,Jinwoo Kim,Hyunsun Chung,Myung-Hoon Cha,Hong-Yeon Kim,Youngjae Kim*

Main category: cs.LG

TLDR: 本文提出InferSave框架，通过优化KV缓存卸载和VM选择，显著降低LLM推理的云GPU成本。


<details>
  <summary>Details</summary>
Motivation: LLM推理应用中，云服务提供商如AWS的GPU实例成本高昂，需要成本高效的解决方案。

Method: 提出InferSave框架，基于服务水平目标(SLO)和工作负载特征优化KV缓存卸载，并使用计算时间校准函数(CTCF)改进实例选择准确性。

Result: 实验在AWS GPU实例上显示，无KV缓存卸载时成本效率提高多达73.7%，有卸载时为离线工作负载节省多达20.19%。

Conclusion: InferSave框架有效地提高了LLM推理的成本效率。

Abstract: LLM inference is essential for applications like text summarization,
translation, and data analysis, but the high cost of GPU instances from Cloud
Service Providers (CSPs) like AWS is a major burden. This paper proposes
InferSave, a cost-efficient VM selection framework for cloud based LLM
inference. InferSave optimizes KV cache offloading based on Service Level
Objectives (SLOs) and workload charac teristics, estimating GPU memory needs,
and recommending cost-effective VM instances. Additionally, the Compute Time
Calibration Function (CTCF) improves instance selection accuracy by adjusting
for discrepancies between theoretical and actual GPU performance. Experiments
on AWS GPU instances show that selecting lower-cost instances without KV cache
offloading improves cost efficiency by up to 73.7% for online workloads, while
KV cache offloading saves up to 20.19% for offline workloads.

</details>

### [45] [Emergence of Computational Structure in a Neural Network Physics Simulator](https://arxiv.org/abs/2504.11830)
*Rohan Hitchcock,Gary W. Delaney,Jonathan H. Manton,Richard Scalzo,Jingge Zhu*

Main category: cs.LG

TLDR: 这篇论文研究了transformer-like模型在模拟粒子系统时，计算结构的出现，发现注意力头学会检测粒子碰撞，与损失景观的退化几何相关，并遵循幂律动态。


<details>
  <summary>Details</summary>
Motivation: 神经网络中计算结构的出现及其检测机制尚未被充分理解。

Method: 训练transformer-like模型模拟粒子系统，使用注意力机制传输粒子间信息，并分析注意力头中结构的出现。

Result: 结果显示：(a) 注意力头学会检测粒子碰撞，(b) 结构的出现与损失景观的退化几何相关，(c) 动态遵循幂律。

Conclusion: 这些结果对神经网络计算结构的收敛时间有启示，并建议通过研究网络组件动态来检测结构出现。

Abstract: Neural networks often have identifiable computational structures - components
of the network which perform an interpretable algorithm or task - but the
mechanisms by which these emerge and the best methods for detecting these
structures are not well understood. In this paper we investigate the emergence
of computational structure in a transformer-like model trained to simulate the
physics of a particle system, where the transformer's attention mechanism is
used to transfer information between particles. We show that (a) structures
emerge in the attention heads of the transformer which learn to detect particle
collisions, (b) the emergence of these structures is associated to degenerate
geometry in the loss landscape, and (c) the dynamics of this emergence follows
a power law. This suggests that these components are governed by a degenerate
"effective potential". These results have implications for the convergence time
of computational structure within neural networks and suggest that the
emergence of computational structure can be detected by studying the dynamics
of network components.

</details>

### [46] [Support is All You Need for Certified VAE Training](https://arxiv.org/abs/2504.11831)
*Changming Xu,Debangshu Banerjee,Deepak Vasisht,Gagandeep Singh*

Main category: cs.LG

TLDR: 本论文提出CIVET方法，用于VAEs的认证训练，提供对抗攻击下的概率性能保证，并通过实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: VAEs在安全关键应用中广泛使用，需要对抗攻击的认证概率保证。

Method: 提出CIVET方法，通过绑定潜在层的支持集错误来限制VAE的最坏情况错误，并设计新训练算法。

Result: 在不同数据集、架构和扰动幅度下，CIVET优于现有方法，在标准性能和鲁棒性上表现更好。

Conclusion: CIVET方法实现了良好的标准性能和强鲁棒性保证。

Abstract: Variational Autoencoders (VAEs) have become increasingly popular and deployed
in safety-critical applications. In such applications, we want to give
certified probabilistic guarantees on performance under adversarial attacks. We
propose a novel method, CIVET, for certified training of VAEs. CIVET depends on
the key insight that we can bound worst-case VAE error by bounding the error on
carefully chosen support sets at the latent layer. We show this point
mathematically and present a novel training algorithm utilizing this insight.
We show in an extensive evaluation across different datasets (in both the
wireless and vision application areas), architectures, and perturbation
magnitudes that our method outperforms SOTA methods achieving good standard
performance with strong robustness guarantees.

</details>

### [47] [On the Problem of Best Arm Retention](https://arxiv.org/abs/2504.11866)
*Houshuang Chen,Yuchen He,Chihao Zhang*

Main category: cs.LG

TLDR: 这篇论文研究了Best Arm Retention (BAR) 问题在多臂老虎机中的应用，包括纯探索和遗憾最小化。


<details>
  <summary>Details</summary>
Motivation: BAR问题在流式算法中应用，目标是保留包括最佳arms在内的m个arms，以优化探索策略。

Method: 使用KL-散度推导PAC算法下界，证明r-BAR的样本复杂度，并开发遗憾最小化算法。

Result: 导出了最佳下界，证明了紧密样本复杂度，并提出了最优遗憾的猜想。

Conclusion: 论文以对最优遗憾的猜想结束，强调了进一步研究的可能性。

Abstract: This paper presents a comprehensive study on the problem of Best Arm
Retention (BAR), which has recently found applications in streaming algorithms
for multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms
with the best arm included from $n$ after some trials, in stochastic
multi-armed bandit settings. We first investigate pure exploration for the BAR
problem under different criteria, and then minimize the regret with specific
constraints, in the context of further exploration in streaming algorithms.
  - We begin by revisiting the lower bound for the $(\varepsilon,\delta)$-PAC
algorithm for Best Arm Identification (BAI) and adapt the classical
KL-divergence argument to derive optimal bounds for $(\varepsilon,\delta)$-PAC
algorithms for BAR.
  - We further study another variant of the problem, called $r$-BAR, which
requires the expected gap between the best arm and the optimal arm retained is
less than $r$. We prove tight sample complexity for the problem.
  - We explore the regret minimization problem for $r$-BAR and develop
algorithm beyond pure exploration. We conclude with a conjecture on the optimal
regret in this setting.

</details>

### [48] [Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption](https://arxiv.org/abs/2504.11873)
*Weiqiang Jiao,Suzhi Bi,Xian Li,Cheng Guo,Hao Chen,Zhi Quan*

Main category: cs.LG

TLDR: 本文提出DASEIN方法，用于语义边缘推理系统在无需标注数据的新环境中保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 部署系统在新环境时，感知数据和通信通道差异导致标注数据和重新训练模型的高成本，需要成本有效的可转移部署方法。

Method: 提出DASEIN，利用无监督领域适应和知识蒸馏，通过两步过程对齐数据分布并适应通道变化。

Result: 数值结果显示，DASEIN在数据分布变化下，比基准方法提高7.09%和21.33%的推理准确性。

Conclusion: 验证DASEIN在适应数据和通道分布方面的有效性。

Abstract: This paper investigates deploying semantic edge inference systems for
performing a common image clarification task. In particular, each system
consists of multiple Internet of Things (IoT) devices that first locally encode
the sensing data into semantic features and then transmit them to an edge
server for subsequent data fusion and task inference. The inference accuracy is
determined by efficient training of the feature encoder/decoder using labeled
data samples. Due to the difference in sensing data and communication channel
distributions, deploying the system in a new environment may induce high costs
in annotating data labels and re-training the encoder/decoder models. To
achieve cost-effective transferable system deployment, we propose an efficient
Domain Adaptation method for Semantic Edge INference systems (DASEIN) that can
maintain high inference accuracy in a new environment without the need for
labeled samples. Specifically, DASEIN exploits the task-relevant data
correlation between different deployment scenarios by leveraging the techniques
of unsupervised domain adaptation and knowledge distillation. It devises an
efficient two-step adaptation procedure that sequentially aligns the data
distributions and adapts to the channel variations. Numerical results show
that, under a substantial change in sensing data distributions, the proposed
DASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in
inference accuracy when the new environment has similar or 25 dB lower channel
signal to noise power ratios (SNRs), respectively. This verifies the
effectiveness of the proposed method in adapting both data and channel
distributions in practical transfer deployment applications.

</details>

### [49] [Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization](https://arxiv.org/abs/2504.11874)
*Ruoyu Sun,Angelos Stefanidis,Zhengyong Jiang,Jionglong Su*

Main category: cs.LG

TLDR: 这篇论文提出了一种新的深度强化学习系统Factor-MCLS，用于动态投资组合优化，允许投资者根据风险偏好干预训练。


<details>
  <summary>Details</summary>
Motivation: 现有DRL代理难以让投资者干预训练，因为它们只从奖励函数输出学习，而不彻底理解影响回报和风险的因素。

Method: 提出奖励因素矩阵和多批评者框架的Factor-MCLS系统，以及在策略函数训练目标中添加风险约束项。

Result: 系统能有效学习影响投资组合回报和风险的因素，并实现投资者根据风险厌恶水平干预。

Conclusion: 通过这些创新，解决了DRL代理的局限性，提高了投资组合优化的灵活性和有效性。

Abstract: Typical deep reinforcement learning (DRL) agents for dynamic portfolio
optimization learn the factors influencing portfolio return and risk by
analyzing the output values of the reward function while adjusting portfolio
weights within the training environment. However, it faces a major limitation
where it is difficult for investors to intervene in the training based on
different levels of risk aversion towards each portfolio asset. This difficulty
arises from another limitation: existing DRL agents may not develop a thorough
understanding of the factors responsible for the portfolio return and risk by
only learning from the output of the reward function. As a result, the strategy
for determining the target portfolio weights is entirely dependent on the DRL
agents themselves. To address these limitations, we propose a reward factor
matrix for elucidating the return and risk of each asset in the portfolio.
Additionally, we propose a novel learning system named Factor-MCLS using a
multi-critic framework that facilitates learning of the reward factor matrix.
In this way, our DRL-based learning system can effectively learn the factors
influencing portfolio return and risk. Moreover, based on the critic networks
within the multi-critic framework, we develop a risk constraint term in the
training objective function of the policy function. This risk constraint term
allows investors to intervene in the training of the DRL agent according to
their individual levels of risk aversion towards the portfolio assets.

</details>

### [50] [Benchmarking Mutual Information-based Loss Functions in Federated Learning](https://arxiv.org/abs/2504.11877)
*Sarang S,Harsh D. Chothani,Qilei Li,Ahmed M. Abdelmoniem,Arnab K. Paul*

Main category: cs.LG

TLDR: 这篇论文使用基于互信息（MI）的损失函数来提升联邦学习中的公平性和有效性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私担忧和GDPR等法规，联邦学习日益重要，但面临偏差、不均等性能和'免费骑手'问题等公平性挑战。

Method: 通过采用MI-based损失函数来提取关键特征并最小化偏差。

Result: 基准测试显示，MI-based损失函数减少了客户端间差异，同时提升了整体性能。

Conclusion: MI-based方法可有效改善联邦学习的公平性和有效性。

Abstract: Federated Learning (FL) has attracted considerable interest due to growing
privacy concerns and regulations like the General Data Protection Regulation
(GDPR), which stresses the importance of privacy-preserving and fair machine
learning approaches. In FL, model training takes place on decentralized data,
so as to allow clients to upload a locally trained model and receive a globally
aggregated model without exposing sensitive information. However, challenges
related to fairness-such as biases, uneven performance among clients, and the
"free rider" issue complicates its adoption. In this paper, we examine the use
of Mutual Information (MI)-based loss functions to address these concerns. MI
has proven to be a powerful method for measuring dependencies between variables
and optimizing deep learning models. By leveraging MI to extract essential
features and minimize biases, we aim to improve both the fairness and
effectiveness of FL systems. Through extensive benchmarking, we assess the
impact of MI-based losses in reducing disparities among clients while enhancing
the overall performance of FL.

</details>

### [51] [HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems](https://arxiv.org/abs/2504.11885)
*Qiyue Chen,Shaolin Tan,Suixiang Gao,Jinhu Lü*

Main category: cs.LG

TLDR: This paper introduces HyperSAT, a hypergraph neural network approach for solving Weighted MaxSAT problems, achieving better performance than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: GNN methods for Weighted MaxSAT are underdeveloped due to challenges from non-uniform weight distributions causing non-linear dependencies and sensitive objectives.

Method: Proposes HyperSAT using unsupervised hypergraph neural networks, hypergraph representation, cross-attention mechanism, and shared representation constraint loss function.

Result: Outperforms state-of-the-art competitors in extensive experiments on various Weighted MaxSAT datasets.

Conclusion: HyperSAT effectively addresses the challenges in Weighted MaxSAT solving and demonstrates superior performance.

Abstract: Graph neural networks (GNNs) have shown promising performance in solving both
Boolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due
to their ability to efficiently model and capture the structural dependencies
between literals and clauses. However, GNN methods for solving Weighted MaxSAT
problems remain underdeveloped. The challenges arise from the non-linear
dependency and sensitive objective function, which are caused by the
non-uniform distribution of weights across clauses. In this paper, we present
HyperSAT, a novel neural approach that employs an unsupervised hypergraph
neural network model to solve Weighted MaxSAT problems. We propose a hypergraph
representation for Weighted MaxSAT instances and design a cross-attention
mechanism along with a shared representation constraint loss function to
capture the logical interactions between positive and negative literal nodes in
the hypergraph. Extensive experiments on various Weighted MaxSAT datasets
demonstrate that HyperSAT achieves better performance than state-of-the-art
competitors.

</details>

### [52] [FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data](https://arxiv.org/abs/2504.11903)
*Yuan Zhou,Jiachen Zhong,Xinli Shi,Guanghui Wen,Xinghuo Yu*

Main category: cs.LG

TLDR: 本文提出FedCanon，一种新的复合联邦学习算法，减少近端计算开销，处理数据异质性问题，具有理论保证和实验优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需多次近端操作且易受数据异质性影响，FedCanon旨在克服这些限制。

Method: FedCanon解耦近端映射与本地更新，每迭代仅服务器进行一次近端评估，并用控制变量整合全局梯度信息。

Result: 在非凸设置下实现次线性收敛，在Polyak-Łojasiewicz条件下线性收敛，无需异质性假设；实验中准确性和效率优于最先进方法。

Conclusion: FedCanon在异质数据分布下表现出更高的准确性和计算效率。

Abstract: Composite federated learning offers a general framework for solving machine
learning problems with additional regularization terms. However, many existing
methods require clients to perform multiple proximal operations to handle
non-smooth terms and their performance are often susceptible to data
heterogeneity. To overcome these limitations, we propose a novel composite
federated learning algorithm called \textbf{FedCanon}, designed to solve the
optimization problems comprising a possibly non-convex loss function and a
weakly convex, potentially non-smooth regularization term. By decoupling
proximal mappings from local updates, FedCanon requires only a single proximal
evaluation on the server per iteration, thereby reducing the overall proximal
computation cost. It also introduces control variables that incorporate global
gradient information into client updates, which helps mitigate the effects of
data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves
sublinear convergence rates under general non-convex settings and linear
convergence under the Polyak-{\L}ojasiewicz condition, without relying on
bounded heterogeneity assumptions. Experiments demonstrate that FedCanon
outperforms the state-of-the-art methods in terms of both accuracy and
computational efficiency, particularly under heterogeneous data distributions.

</details>

### [53] [SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models](https://arxiv.org/abs/2504.11923)
*Zeyu Dai,Shengcai Liu,Rui He,Jiahao Wu,Ning Lu,Wenqi Fan,Qing Li,Ke Tang*

Main category: cs.LG

TLDR: 本文提出SemDiff方法，使用扩散模型的语义潜空间生成更自然的不受限对抗样本，提高攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的不受限对抗样本生成方法，生成的样本缺乏自然性和隐蔽性，需要改进以应对深度学习模型的安全威胁。

Method: 提出SemDiff，通过探索扩散模型的语义潜空间并采用多属性优化，确保攻击成功同时保持样本的自然性和隐蔽性。

Result: 在CelebA-HQ、AFHQ和ImageNet等数据集上的实验显示，SemDiff在攻击成功率和隐蔽性上优于现有方法，能生成语义上合理的样本并规避多种防御。

Conclusion: SemDiff是一种有效且具有威胁性的不受限对抗攻击方法，证明了其在提升攻击性能方面的潜力。

Abstract: Unrestricted adversarial examples (UAEs), allow the attacker to create
non-constrained adversarial examples without given clean samples, posing a
severe threat to the safety of deep learning models. Recent works utilize
diffusion models to generate UAEs. However, these UAEs often lack naturalness
and imperceptibility due to simply optimizing in intermediate latent noises. In
light of this, we propose SemDiff, a novel unrestricted adversarial attack that
explores the semantic latent space of diffusion models for meaningful
attributes, and devises a multi-attributes optimization approach to ensure
attack success while maintaining the naturalness and imperceptibility of
generated UAEs. We perform extensive experiments on four tasks on three
high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results
demonstrate that SemDiff outperforms state-of-the-art methods in terms of
attack success rate and imperceptibility. The generated UAEs are natural and
exhibit semantically meaningful changes, in accord with the attributes'
weights. In addition, SemDiff is found capable of evading different defenses,
which further validates its effectiveness and threatening.

</details>

### [54] [VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning](https://arxiv.org/abs/2504.11944)
*Xuyang Chen,Guojian Wang,Keyu Yan,Lin Zhao*

Main category: cs.LG

TLDR: VIPO 是一种新的基于模型的离线强化学习算法，通过最小化值估计不一致性来提高模型准确性，并在基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决模型-based 离线 RL 中模型错误导致的启发式不确定性估计不可靠的问题。

Method: 通过自监督反馈最小化直接从离线数据学习的值与模型估计的值之间的不一致性来增强模型训练。

Result: VIPO 高效学习准确模型，一致优于现有方法，在 D4RL 和 NeoRL 基准上几乎所有任务达到最先进性能。

Conclusion: VIPO 提供通用框架，可集成到现有算法中系统提升模型准确性。

Abstract: Offline reinforcement learning (RL) learns effective policies from
pre-collected datasets, offering a practical solution for applications where
online interactions are risky or costly. Model-based approaches are
particularly advantageous for offline RL, owing to their data efficiency and
generalizability. However, due to inherent model errors, model-based methods
often artificially introduce conservatism guided by heuristic uncertainty
estimation, which can be unreliable. In this paper, we introduce VIPO, a novel
model-based offline RL algorithm that incorporates self-supervised feedback
from value estimation to enhance model training. Specifically, the model is
learned by additionally minimizing the inconsistency between the value learned
directly from the offline data and the one estimated from the model. We perform
comprehensive evaluations from multiple perspectives to show that VIPO can
learn a highly accurate model efficiently and consistently outperform existing
methods. It offers a general framework that can be readily integrated into
existing model-based offline RL algorithms to systematically enhance model
accuracy. As a result, VIPO achieves state-of-the-art performance on almost all
tasks in both D4RL and NeoRL benchmarks.

</details>

### [55] [Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification](https://arxiv.org/abs/2504.11981)
*Sosei Ikeda,Hiromitsu Awano,Takashi Sato*

Main category: cs.LG

TLDR: 本篇论文提出了一种基于点积的 reservoir 表示（DPRR）和硬件友好的延迟反馈 reservoir（DFR），用于时间序列分类的 reservoir 计算，展示了更高的准确性和更小的电路规模。


<details>
  <summary>Details</summary>
Motivation: 现有的 reservoir 计算方法在时间序列分类任务中需要计算昂贵的矩阵求逆，导致硬件实现时电路规模大和处理能力需求高。

Method: 提出 DPRR 基于数据特征点积，以及由非线性元素和延迟反馈回路组成的 DFR，该方法可完全数字方式实现，适合高级综合。

Result: 所提出的 DFR 成功分类多变量时间序列数据，在 12 个任务的 FPGA 实现中，与现有方法相比，显示出优越的准确性和更小的电路规模。

Conclusion: 该方法为 reservoir 计算在边缘计算中的硬件实现提供了一种简单、有效且友好的方案。

Abstract: Reservoir computing (RC) is attracting attention as a machine-learning
technique for edge computing. In time-series classification tasks, the number
of features obtained using a reservoir depends on the length of the input
series. Therefore, the features must be converted to a constant-length
intermediate representation (IR), such that they can be processed by an output
layer. Existing conversion methods involve computationally expensive matrix
inversion that significantly increases the circuit size and requires processing
power when implemented in hardware. In this article, we propose a simple but
effective IR, namely, dot-product-based reservoir representation (DPRR), for RC
based on the dot product of data features. Additionally, we propose a
hardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear
element and delayed feedback loop with DPRR. The proposed DFR successfully
classified multivariate time series data that has been considered particularly
difficult to implement efficiently in hardware. In contrast to conventional DFR
models that require analog circuits, the proposed model can be implemented in a
fully digital manner suitable for high-level syntheses. A comparison with
existing machine-learning methods via field-programmable gate array
implementation using 12 multivariate time-series classification tasks confirmed
the superior accuracy and small circuit size of the proposed method.

</details>

### [56] [Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets](https://arxiv.org/abs/2504.11990)
*Yechao Zhang,Yuxuan Zhou,Tianyu Li,Minghui Li,Shengshan Hu,Wei Luo,Leo Yu Zhang*

Main category: cs.LG

TLDR: 这篇论文研究了转移学习中预训练编码器的后门风险，提出T-Core框架进行主动防御，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 转移学习增加了攻击面，现有防御策略不适用于资源受限场景和未知威胁。

Method: 提出Trusted Core (T-Core) Bootstrapping框架，专注于识别可信数据和神经元以提升模型安全性。

Result: 实验评估显示T-Core在5种编码器攻击、7种数据集攻击和14种防御基线中表现出色，适用于多种场景。

Conclusion: T-Core框架通过主动识别可信元素，提供了一种更有效的后门防御方法。

Abstract: Transfer learning from pre-trained encoders has become essential in modern
machine learning, enabling efficient model adaptation across diverse tasks.
However, this combination of pre-training and downstream adaptation creates an
expanded attack surface, exposing models to sophisticated backdoor embeddings
at both the encoder and dataset levels--an area often overlooked in prior
research. Additionally, the limited computational resources typically available
to users of pre-trained encoders constrain the effectiveness of generic
backdoor defenses compared to end-to-end training from scratch. In this work,
we investigate how to mitigate potential backdoor risks in resource-constrained
transfer learning scenarios. Specifically, we conduct an exhaustive analysis of
existing defense strategies, revealing that many follow a reactive workflow
based on assumptions that do not scale to unknown threats, novel attack types,
or different training paradigms. In response, we introduce a proactive mindset
focused on identifying clean elements and propose the Trusted Core (T-Core)
Bootstrapping framework, which emphasizes the importance of pinpointing
trustworthy data and neurons to enhance model security. Our empirical
evaluations demonstrate the effectiveness and superiority of T-Core,
specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning
attacks, and 14 baseline defenses across five benchmark datasets, addressing
four scenarios of 3 potential backdoor threats.

</details>

### [57] [Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation](https://arxiv.org/abs/2504.11992)
*Pascal Schlachter,Jonathan Fuss,Bin Yang*

Main category: cs.LG

TLDR: 这篇论文通过控制实验分析了在线源无通用域适应的伪标签机制，发现伪标签准确率比数量更重要，并为未来研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 域移位和类别移位问题影响深度神经网络的实际性能，亟需在线源无通用域适应方法来处理源数据受限和目标数据流式输入的场景。

Method: 通过模拟伪标签的控制实验进行系统分析。

Result: 结果显示当前方法与完美伪标签上界有差距；对比损失在中等伪标签准确率下有效；交叉熵损失在高准确率下优越；伪标签准确率比数量更关键。

Conclusion: 强调伪标签在在线源无通用域适应中的关键作用，并提供可操作的改进建议。

Abstract: A domain (distribution) shift between training and test data often hinders
the real-world performance of deep neural networks, necessitating unsupervised
domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged
as a solution for practical scenarios where access to source data is restricted
and target data is received as a continuous stream. However, the open-world
nature of many real-world applications additionally introduces category shifts
meaning that the source and target label spaces may differ. Online source-free
universal domain adaptation (SF-UniDA) addresses this challenge. Existing
methods mainly rely on self-training with pseudo-labels, yet the relationship
between pseudo-labeling and adaptation outcomes has not been studied yet. To
bridge this gap, we conduct a systematic analysis through controlled
experiments with simulated pseudo-labeling, offering valuable insights into
pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap
between the current state-of-the-art and the upper bound of adaptation achieved
with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables
effective adaptation even with moderate pseudo-label accuracy, while a
cross-entropy loss, though less robust to pseudo-label errors, achieves
superior results when pseudo-labeling approaches perfection. Lastly, our
findings indicate that pseudo-label accuracy is in general more crucial than
quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels
is beneficial. Overall, our study highlights the critical role of
pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive
future advancements in the field. Our code is available at
https://github.com/pascalschlachter/PLAnalysis.

</details>

### [58] [A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs](https://arxiv.org/abs/2504.11997)
*Kihyuk Hong,Ambuj Tewari*

Main category: cs.LG

TLDR: 简而言之，这篇论文提出了一种高效的截断操作的迭代值方法，用于无限地平平均奖励线性MDP设置，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 之前的作品通过近似平均奖励设置为折扣设置，并使用截断来约束值函数的跨度，但计算最小值需要遍历整个状态空间，这在大型或无限状态空间中不可行。

Method: 引入了一种值迭代方法，只需在算法访问的状态集上计算值函数的最小值。

Result: 算法保持了与之前工作相同的遗憾界限，同时计算复杂度独立于状态空间大小。

Conclusion: 这项工作展示了如何在保持统计效率的同时，显著减少计算开销。

Abstract: We study reinforcement learning in infinite-horizon average-reward settings
with linear MDPs. Previous work addresses this problem by approximating the
average-reward setting by discounted setting and employing a value
iteration-based algorithm that uses clipping to constrain the span of the value
function for improved statistical efficiency. However, the clipping procedure
requires computing the minimum of the value function over the entire state
space, which is prohibitive since the state space in linear MDP setting can be
large or even infinite. In this paper, we introduce a value iteration method
with efficient clipping operation that only requires computing the minimum of
value functions over the set of states visited by the algorithm. Our algorithm
enjoys the same regret bound as the previous work while being computationally
efficient, with computational complexity that is independent of the size of the
state space.

</details>

### [59] [Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition](https://arxiv.org/abs/2504.12011)
*Heesoo Jung,Hogun Park*

Main category: cs.LG

TLDR: 这篇论文提出BSG框架，通过平衡图自监督学习中的平滑性，改善在各种下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能无法有效反映图属性，导致任务性能不均衡，本文旨在通过平衡信息论分解的三个术语来解决此问题。

Method: 使用信息论框架分解SSL目标为三个部分，引入BSG框架及其损失函数（邻居损失、最小损失和发散损失）来平衡这些术语。

Result: 在多个真实数据集的节点分类和链接预测任务中，BSG框架取得了state-of-the-art性能。

Conclusion: BSG框架通过理论分析和实验验证，证明了平衡平滑性能提升整体性能，并提供开源代码。

Abstract: Self-supervised learning (SSL) in graphs has garnered significant attention,
particularly in employing Graph Neural Networks (GNNs) with pretext tasks
initially designed for other domains, such as contrastive learning and feature
reconstruction. However, it remains uncertain whether these methods effectively
reflect essential graph properties, precisely representation similarity with
its neighbors. We observe that existing methods position opposite ends of a
spectrum driven by the graph embedding smoothness, with each end corresponding
to outperformance on specific downstream tasks. Decomposing the SSL objective
into three terms via an information-theoretic framework with a neighbor
representation variable reveals that this polarization stems from an imbalance
among the terms, which existing methods may not effectively maintain. Further
insights suggest that balancing between the extremes can lead to improved
performance across a wider range of downstream tasks. A framework, BSG
(Balancing Smoothness in Graph SSL), introduces novel loss functions designed
to supplement the representation quality in graph-based SSL by balancing the
derived three terms: neighbor loss, minimal loss, and divergence loss. We
present a theoretical analysis of the effects of these loss functions,
highlighting their significance from both the SSL and graph smoothness
perspectives. Extensive experiments on multiple real-world datasets across node
classification and link prediction consistently demonstrate that BSG achieves
state-of-the-art performance, outperforming existing methods. Our
implementation code is available at https://github.com/steve30572/BSG.

</details>

### [60] [Active Human Feedback Collection via Neural Contextual Dueling Bandits](https://arxiv.org/abs/2504.12016)
*Arun Verma,Xiaoqiang Lin,Zhongxiang Dai,Daniela Rus,Bryan Kian Hsiang Low*

Main category: cs.LG

TLDR: 本文提出Neural-ADB算法，用于非线性奖励函数下高效收集人类偏好反馈，并提供理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设奖励函数线性，但实际应用如在线推荐和LLM对齐中不成立，导致反馈收集成本高。

Method: 提出基于神经上下文双打带框架的Neural-ADB算法，处理非线性潜在奖励函数。

Result: 理论上，在Bradley-Terry-Luce模型下，子最优差距以次线性速率下降；实验上，在合成数据集上验证有效。

Conclusion: Neural-ADB为非线性场景下收集人类偏好反馈提供了一个原则性和实用性的方法。

Abstract: Collecting human preference feedback is often expensive, leading recent works
to develop principled algorithms to select them more efficiently. However,
these works assume that the underlying reward function is linear, an assumption
that does not hold in many real-life applications, such as online
recommendation and LLM alignment. To address this limitation, we propose
Neural-ADB, an algorithm based on the neural contextual dueling bandit
framework that provides a principled and practical method for collecting human
preference feedback when the underlying latent reward function is non-linear.
We theoretically show that when preference feedback follows the
Bradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by
Neural-ADB decreases at a sub-linear rate as the preference dataset increases.
Our experimental results on problem instances derived from synthetic preference
datasets further validate the effectiveness of Neural-ADB.

</details>

### [61] [FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning](https://arxiv.org/abs/2504.12025)
*Yu Zhang,Qingfeng Du,Jiaqi Lv*

Main category: cs.LG

TLDR: FedEPA 是一种新的联邦学习框架，用于多模态学习，处理数据异质性和标签数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习系统假设客户端只有单模态数据，且缺乏标签数据，限制了其实际应用。本文提出 FedEPA 来解决多模态数据和标签数据不足的问题。

Method: FedEPA 采用个性化的本地模型聚合策略、无人监督的模态对齐策略（使用对比学习对齐特征）和多模态特征融合策略。

Result: 实验结果表明，在标签数据有限的多模态分类任务中，FedEPA 显著优于现有联邦学习方法。

Conclusion: FedEPA 通过个性化聚合和模态对齐策略，缓解数据异质性，提高了多模态联邦学习的性能。

Abstract: Federated Learning (FL) enables decentralized model training across multiple
parties while preserving privacy. However, most FL systems assume clients hold
only unimodal data, limiting their real-world applicability, as institutions
often possess multimodal data. Moreover, the lack of labeled data further
constrains the performance of most FL methods. In this work, we propose FedEPA,
a novel FL framework for multimodal learning. FedEPA employs a personalized
local model aggregation strategy that leverages labeled data on clients to
learn personalized aggregation weights, thereby alleviating the impact of data
heterogeneity. We also propose an unsupervised modality alignment strategy that
works effectively with limited labeled data. Specifically, we decompose
multimodal features into aligned features and context features. We then employ
contrastive learning to align the aligned features across modalities, ensure
the independence between aligned features and context features within each
modality, and promote the diversity of context features. A multimodal feature
fusion strategy is introduced to obtain a joint embedding. The experimental
results show that FedEPA significantly outperforms existing FL methods in
multimodal classification tasks under limited labeled data conditions.

</details>

### [62] [Generative Deep Learning Framework for Inverse Design of Fuels](https://arxiv.org/abs/2504.12075)
*Kiran K. Yalamanchi,Pinaki Pal,Balaji Mohan,Abdullah S. AlRamadan,Jihad A. Badra,Yuanjiang Pei*

Main category: cs.LG

TLDR: 本论文开发了一种结合Co-optimized Variational Autoencoder (Co-VAE) 和定量结构-属性关系 (QSPR) 技术的生成式深度学习框架，用于加速燃料反向设计，提高Research Octane Number (RON) 估计。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统燃料筛选方法的局限性，捕捉复杂的结构-属性关系，并加速发现具有优越抗爆性能的燃料。

Method: 使用Co-VAE架构整合属性预测与VAE潜空间，基于GDB-13数据库和RON数据进行训练，结合超参数调优、独立回归模型和差分进化算法导航潜空间。

Result: 模型提升了分子重构和RON预测的准确性，识别出高RON燃料分子候选，并提供工具探索化学空间。

Conclusion: 该方法可扩展到其他燃料属性和合成性标准，促进新燃料的从头设计。

Abstract: In the present work, a generative deep learning framework combining a
Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative
structure-property relationship (QSPR) techniques is developed to enable
accelerated inverse design of fuels. The Co-VAE integrates a property
prediction component coupled with the VAE latent space, enhancing molecular
reconstruction and accurate estimation of Research Octane Number (RON) (chosen
as the fuel property of interest). A subset of the GDB-13 database, enriched
with a curated RON database, is used for model training. Hyperparameter tuning
is further utilized to optimize the balance among reconstruction fidelity,
chemical validity, and RON prediction. An independent regression model is then
used to refine RON prediction, while a differential evolution algorithm is
employed to efficiently navigate the VAE latent space and identify promising
fuel molecule candidates with high RON. This methodology addresses the
limitations of traditional fuel screening approaches by capturing complex
structure-property relationships within a comprehensive latent representation.
The generative model provides a flexible tool for systematically exploring vast
chemical spaces, paving the way for discovering fuels with superior anti-knock
properties. The demonstrated approach can be readily extended to incorporate
additional fuel properties and synthesizability criteria to enhance
applicability and reliability for de novo design of new fuels.

</details>

### [63] [Neural Contextual Bandits Under Delayed Feedback Constraints](https://arxiv.org/abs/2504.12086)
*Mohammadali Moghimi,Sharu Theresa Jose,Shana Moothedath*

Main category: cs.LG

TLDR: 这篇论文提出Delayed NeuralUCB和Delayed NeuralTS算法，处理神经上下文多臂老虎机中的延迟奖励问题，并通过理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 论文针对在线推荐系统和临床试验等应用中延迟奖励反馈的挑战，旨在解决行动结果延迟显现的问题。

Method: 提出Delayed NeuralUCB算法使用UCB探索策略，以及Delayed NeuralTS算法使用Thompson Sampling探索策略；在独立同分布子指数奖励延迟假设下，推导累积遗憾上界。

Result: 推导了累积遗憾上界，并在MNIST和Mushroom等真实数据集上实验，证明算法能有效处理不同延迟，并优于基准方法。

Conclusion: 算法能良好管理延迟奖励，适合复杂真实场景的应用。

Abstract: This paper presents a new algorithm for neural contextual bandits (CBs) that
addresses the challenge of delayed reward feedback, where the reward for a
chosen action is revealed after a random, unknown delay. This scenario is
common in applications such as online recommendation systems and clinical
trials, where reward feedback is delayed because the outcomes or results of a
user's actions (such as recommendations or treatment responses) take time to
manifest and be measured. The proposed algorithm, called Delayed NeuralUCB,
uses an upper confidence bound (UCB)-based exploration strategy. Under the
assumption of independent and identically distributed sub-exponential reward
delays, we derive an upper bound on the cumulative regret over a T-length
horizon. We further consider a variant of the algorithm, called Delayed
NeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments
on real-world datasets, such as MNIST and Mushroom, along with comparisons to
benchmark approaches, demonstrate that the proposed algorithms effectively
manage varying delays and are well-suited for complex real-world scenarios.

</details>

### [64] [Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis](https://arxiv.org/abs/2504.12151)
*Miaosen Luo,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TLDR: 本文提出KAN-MCP框架，提升多模态情感分析的解释性和鲁棒性，通过整合Kolmogorov-Arnold Networks和Multimodal Clean Pareto框架。


<details>
  <summary>Details</summary>
Motivation: 解决多模态融合决策逻辑缺乏解释性和模态不平衡问题，这些由模态间信息密度差异引起。

Method: 提出KAN-MCP框架：KAN用于可解释的跨模态交互分析，MCPareto通过DRD-MIB方法进行降噪、降维和动态平衡模态梯度贡献。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS v2数据集上取得优越性能，并提供直观的可视化界面。

Conclusion: 解释性和鲁棒性的协同作用显著提高了多模态情感分析的性能。

Abstract: Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack
of interpretability in the decision logic of multimodal fusion and modality
imbalance caused by disparities in inter-modal information density. To address
these issues, we propose KAN-MCP, a novel framework that integrates the
interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the
Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its
univariate function decomposition to achieve transparent analysis of
cross-modal interactions. This structural design allows direct inspection of
feature transformations without relying on external interpretation tools,
thereby ensuring both high expressiveness and interpretability. Second, the
proposed MCPareto enhances robustness by addressing modality imbalance and
noise interference. Specifically, we introduce the Dimensionality Reduction and
Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises
and reduces feature dimensionality. This approach provides KAN with
discriminative low-dimensional inputs to reduce the modeling complexity of KAN
while preserving critical sentiment-related information. Furthermore, MCPareto
dynamically balances gradient contributions across modalities using the
purified features output by DRD-MIB, ensuring lossless transmission of
auxiliary signals and effectively alleviating modality imbalance. This synergy
of interpretability and robustness not only achieves superior performance on
benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers
an intuitive visualization interface through KAN's interpretable architecture.

</details>

### [65] [Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications](https://arxiv.org/abs/2504.12156)
*Mustafa Cavus*

Main category: cs.LG

TLDR: 本论文引入预测多重性的度量，并展示了其在生存分析中的应用，强调了在高风险环境中模型可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 模型可能在整体性能上表现良好，但对个体预测结果存在显著分歧，这在处理 censored 数据和估计失败时间的生存分析中会削弱可靠性，尤其在维护调度等应用中。

Method: 引入模糊性、差异性和晦涩性等正式度量来量化预测多重性，并将其应用于预测维护的基准数据集。

Result: 结果显示模糊性增加至40-45%，差异性较低但趋势相似，晦涩性温和且集中在少数模型，表明多个准确的生存模型可能对相同设备给出冲突的失败风险估计。

Conclusion: 需要明确测量和传达预测多重性，以确保在过程健康管理中的可靠决策。

Abstract: In many applications, especially those involving prediction, models may yield
near-optimal performance yet significantly disagree on individual-level
outcomes. This phenomenon, known as predictive multiplicity, has been formally
defined in binary, probabilistic, and multi-target classification, and
undermines the reliability of predictive systems. However, its implications
remain unexplored in the context of survival analysis, which involves
estimating the time until a failure or similar event while properly handling
censored data. We frame predictive multiplicity as a critical concern in
survival-based models and introduce formal measures -- ambiguity, discrepancy,
and obscurity -- to quantify it. This is particularly relevant for downstream
tasks such as maintenance scheduling, where precise individual risk estimates
are essential. Understanding and reporting predictive multiplicity helps build
trust in models deployed in high-stakes environments. We apply our methodology
to benchmark datasets from predictive maintenance, extending the notion of
multiplicity to survival models. Our findings show that ambiguity steadily
increases, reaching up to 40-45% of observations; discrepancy is lower but
exhibits a similar trend; and obscurity remains mild and concentrated in a few
models. These results demonstrate that multiple accurate survival models may
yield conflicting estimations of failure risk and degradation progression for
the same equipment. This highlights the need to explicitly measure and
communicate predictive multiplicity to ensure reliable decision-making in
process health management.

</details>

### [66] [Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning](https://arxiv.org/abs/2504.12181)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TLDR: FedBacys是一种基于电池感知的联邦学习框架，通过循环客户端参与提高能量效率和学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的复杂度增加导致能量消耗高，尤其在能量采集系统中，设备可用性因能量限制而波动。

Method: 提出FedBacys框架，通过根据电池水平聚类客户端并顺序调度，使其在传输前进行本地训练，减少冗余计算。

Result: 实验显示FedBacys在能量效率和性能一致性上优于现有方法，即使在非独立同分布数据和充电频率低时也表现出色。

Conclusion: 首次全面评估能量采集联邦学习中的循环客户端参与，统一考虑通信和计算成本。

Abstract: Federated Learning (FL) has emerged as a promising framework for distributed
learning, but its growing complexity has led to significant energy consumption,
particularly from computations on the client side. This challenge is especially
critical in energy-harvesting FL (EHFL) systems, where device availability
fluctuates due to limited and time-varying energy resources. We propose
FedBacys, a battery-aware FL framework that introduces cyclic client
participation based on users' battery levels to cope with these issues.
FedBacys enables clients to save energy and strategically perform local
training just before their designated transmission time by clustering clients
and scheduling their involvement sequentially. This design minimizes redundant
computation, reduces system-wide energy usage, and improves learning stability.
Our experiments demonstrate that FedBacys outperforms existing approaches in
terms of energy efficiency and performance consistency, exhibiting robustness
even under non-i.i.d. training data distributions and with very infrequent
battery charging. This work presents the first comprehensive evaluation of
cyclic client participation in EHFL, incorporating both communication and
computation costs into a unified, resource-aware scheduling strategy.

</details>

### [67] [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
*David Khachaturov,Robert Mullins,Ilia Shumailov,Sumanth Dathathri*

Main category: cs.LG

TLDR: 本研究探讨了人类和LLM在对话中模仿生成文本的现象，包括水印信号，这挑战了现有假设，并建议改进水印机制。


<details>
  <summary>Details</summary>
Motivation: LLM可能被用于传播错误信息，现有反措施如检测器和水印可能因人类或LLM的模仿而不可靠。

Method: 通过实验演示人类和LLM在各种设置下的模仿行为。

Result: 人类和LLM确实会模仿生成文本的属性，包括水印信号，即使在不太可能的场景中。

Conclusion: 需要显著降低水印的假阳性率，并使用更长的词序列来提升可靠性。

Abstract: Recent advancements in Large Language Models (LLMs) raised concerns over
potential misuse, such as for spreading misinformation. In response two counter
measures emerged: machine learning-based detectors that predict if text is
synthetic, and LLM watermarking, which subtly marks generated text for
identification and attribution. Meanwhile, humans are known to adjust language
to their conversational partners both syntactically and lexically. By
implication, it is possible that humans or unwatermarked LLMs could
unintentionally mimic properties of LLM generated text, making counter measures
unreliable. In this work we investigate the extent to which such conversational
adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that
both humans and LLMs end up mimicking, including the watermarking signal even
in seemingly improbable settings. This challenges current academic assumptions
and suggests that for long-term watermarking to be reliable, the likelihood of
false positives needs to be significantly lower, while longer word sequences
should be used for seeding watermarking mechanisms.

</details>

### [68] [SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields](https://arxiv.org/abs/2504.12262)
*David Keetae Park,Xihaier Luo,Guang Zhao,Seungjun Lee,Miruna Oprescu,Shinjae Yoo*

Main category: cs.LG

TLDR: SCENT 是一个新型时空表示学习框架，统一插值、重构和预测，使用 Transformer 架构，在多个任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 时空学习面临空间时间依赖复杂交互、高维数据、可扩展性挑战，以及科学领域数据不规则和高体积问题。

Method: 提出 SCENT 框架，基于 Transformer 编码器-处理器-解码器结构，引入可学习查询和查询-wise 交叉注意力机制，并采用稀疏注意力提升可扩展性。

Result: 通过模拟和真实实验验证，SCENT 在多个挑战性任务中表现出最先进性能，并实现优越的可扩展性。

Conclusion: SCENT 框架有效解决了时空学习难题，提供可扩展高效的表示学习方法。

Abstract: Spatiotemporal learning is challenging due to the intricate interplay between
spatial and temporal dependencies, the high dimensionality of the data, and
scalability constraints. These challenges are further amplified in scientific
domains, where data is often irregularly distributed (e.g., missing values from
sensor failures) and high-volume (e.g., high-fidelity simulations), posing
additional computational and modeling difficulties. In this paper, we present
SCENT, a novel framework for scalable and continuity-informed spatiotemporal
representation learning. SCENT unifies interpolation, reconstruction, and
forecasting within a single architecture. Built on a transformer-based
encoder-processor-decoder backbone, SCENT introduces learnable queries to
enhance generalization and a query-wise cross-attention mechanism to
effectively capture multi-scale dependencies. To ensure scalability in both
data size and model complexity, we incorporate a sparse attention mechanism,
enabling flexible output representations and efficient evaluation at arbitrary
resolutions. We validate SCENT through extensive simulations and real-world
experiments, demonstrating state-of-the-art performance across multiple
challenging tasks while achieving superior scalability.

</details>

### [69] [Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)](https://arxiv.org/abs/2504.12270)
*ChenNingZhi Sheng,Rafal Kustra,Davide Chicco*

Main category: cs.LG

TLDR: 本研究使用聚类算法分析CLSA数据，聚焦认知特征，寻找与痴呆相关的集群，并通过多种评估指标进行比较。


<details>
  <summary>Details</summary>
Motivation: 探索评估指标在不同聚类算法上的应用，旨在基于CLSA数据发现与痴呆发展的临床相关集群。

Method: 使用K-means、层次聚类和PAM算法分析CLSA的18,891名参与者数据，采用内部指标（如轮廓宽度、熵等）和比较指标（如调整兰德指数）。

Result: K-means和PAM结果相似，与层次聚类有显著差异；强调熵、分离指数和调整兰德指数的重要性。

Conclusion: 结果有助于理解痴呆，可应用于其他医疗领域，提高聚类技术和指标在医疗数据中的应用。

Abstract: Purpose: The primary goal of this study is to explore the application of
evaluation metrics to different clustering algorithms using the data provided
from the Canadian Longitudinal Study (CLSA), focusing on cognitive features.
The objective of our work is to discover potential clinically relevant clusters
that contribute to the development of dementia over time-based on cognitive
changes. Method: The CLSA dataset includes 18,891 participants with data
available at both baseline and follow-up assessments, to which clustering
algorithms were applied. The clustering methodologies employed in this analysis
are K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning
Around Medoids (PAM). We use multiple evaluation metrics to assess our
analysis. For internal evaluation metrics, we use: Average silhouette Width,
Within and Between the sum of square Ratio (WB.Ratio), Entropy,
Calinski-Harabasz Index (CH Index), and Separation Index. For clustering
comparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index
(ARI), Rand Index (RI), and Variation Information. Results: Using evaluation
metrics to compare the results of the three clustering techniques, K-means and
Partitioning Around Medoids (PAM) produced similar results. In contrast, there
are significant differences between K-means clustering and Hierarchical
Clustering. Our study highlights the importance of the two internal evaluation
metrics: entropy and separation index. In between clustering comparison
metrics, the Adjusted Rand Index is a key tool. Conclusion: The study results
have the potential to contribute to understanding dementia. Researchers can
also benefit by applying the suggested evaluation metrics to other areas of
healthcare research. Overall, our study improves the understanding of using
clustering techniques and evaluation metrics to reveal complex patterns in
medical data.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [70] [A New Paradigm of User-Centric Wireless Communication Driven by Large Language Models](https://arxiv.org/abs/2504.11696)
*Kuiyuan Ding,Caili Guo,Yang Yang,Wuxia Hu,Yonina C. Eldar*

Main category: cs.NI

TLDR: 论文提出LLM驱动的无线通信范式，使用NL2SQL工具优化用户需求，实现AI-native网络。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略自然语言与通信系统差距，无法充分利用LLMs。

Method: LLMs分析用户意图，生成SQL查询数据库，并优化通信系统参数。

Result: 模拟结果证明了范式的有效性。

Conclusion: 该范式可行，并为用户需求导向通信提供新途径。

Abstract: The next generation of wireless communications seeks to deeply integrate
artificial intelligence (AI) with user-centric communication networks, with the
goal of developing AI-native networks that more accurately address user
requirements. The rapid development of large language models (LLMs) offers
significant potential in realizing these goals. However, existing efforts that
leverage LLMs for wireless communication often overlook the considerable gap
between human natural language and the intricacies of real-world communication
systems, thus failing to fully exploit the capabilities of LLMs. To address
this gap, we propose a novel LLM-driven paradigm for wireless communication
that innovatively incorporates the nature language to structured query language
(NL2SQL) tool. Specifically, in this paradigm, user personal requirements is
the primary focus. Upon receiving a user request, LLMs first analyze the user
intent in terms of relevant communication metrics and system parameters.
Subsequently, a structured query language (SQL) statement is generated to
retrieve the specific parameter values from a high-performance real-time
database. We further utilize LLMs to formulate and solve an optimization
problem based on the user request and the retrieved parameters. The solution to
this optimization problem then drives adjustments in the communication system
to fulfill the user's requirements. To validate the feasibility of the proposed
paradigm, we present a prototype system. In this prototype, we consider
user-request centric semantic communication (URC-SC) system in which a dynamic
semantic representation network at the physical layer adapts its encoding depth
to meet user requirements. Additionally, two LLMs are employed to analyze user
requests and generate SQL statements, respectively. Simulation results
demonstrate the effectiveness.

</details>

### [71] [Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks](https://arxiv.org/abs/2504.12210)
*Tingyang Sun,Tuan Nguyen,Ting He*

Main category: cs.NI

TLDR: 这篇论文提出了一种优化去中心化联邦学习（DFL）的通信方案，通过联合设计网络和混合矩阵，减少训练时间80%以上，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: DFL在边缘网络上运行时，由于参数交换量大和多跳带宽限制，性能面临挑战，现有的解决方案基于简单模型，无法处理多跳网络情况。

Method: 通过分析问题特性，将通信方案和混合矩阵设计转化为可处理的优化问题，并开发高效算法。

Result: 实验基于真实拓扑和数据，显示训练时间减少超过80%，计算效率显著提高，且准确性不受影响。

Conclusion: 该方法有效提升DFL性能，优于现有技术，提供高效的训练方案。

Abstract: Decentralized federated learning (DFL) is a promising machine learning
paradigm for bringing artificial intelligence (AI) capabilities to the network
edge. Running DFL on top of edge networks, however, faces severe performance
challenges due to the extensive parameter exchanges between agents. Most
existing solutions for these challenges were based on simplistic communication
models, which cannot capture the case of learning over a multi-hop
bandwidth-limited network. In this work, we address this problem by jointly
designing the communication scheme for the overlay network formed by the agents
and the mixing matrix that controls the communication demands between the
agents. By carefully analyzing the properties of our problem, we cast each
design problem into a tractable optimization and develop an efficient algorithm
with guaranteed performance. Our evaluations based on real topology and data
show that the proposed algorithm can reduce the total training time by over
$80\%$ compared to the baseline without sacrificing accuracy, while
significantly improving the computational efficiency over the state of the art.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [72] [Robust Markov stability for community detection at a scale learned based on the structure](https://arxiv.org/abs/2504.11621)
*Samin Aref,Sanchaai Mathiyarasan*

Main category: cs.SI

TLDR: 本论文提出PyGenStabilityOne（PO）算法，它是一种无超参数的多尺度社区检测算法，能够自动选择合适的规模并返回一个稳健的分区。


<details>
  <summary>Details</summary>
Motivation: 现有的社区检测方法在规模选择上存在问题，PyGenStability虽能产生多个稳健分区，但缺乏自动选择合适规模的原理方法，因此需要开发一种自动规模选择机制。

Method: 结合Markov稳定性框架与预训练的梯度提升模型，使用基准网络的手工特征和嵌入特征训练模型预测最佳规模，并与PyGenStability算法整合形成PO算法。

Result: PO算法与其他29个算法比较，优于25个算法，具有统计学意义上的显著优势。

Conclusion: PO算法作为一种准确、稳健且无超参数的社区检测方法，便于算法选择。

Abstract: Community detection, the unsupervised task of clustering nodes of a graph,
finds applications across various fields. The common approaches for community
detection involve optimizing an objective function to partition the nodes into
communities at a single scale of granularity. However, the single-scale
approaches often fall short of producing partitions that are robust and at a
suitable scale. The existing algorithm, PyGenStability, returns multiple robust
partitions for a network by optimizing the multi-scale Markov stability
function. However, in cases where the suitable scale is not known or assumed by
the user, there is no principled method to select a single robust partition at
a suitable scale from the multiple partitions that PyGenStability produces. Our
proposed method combines the Markov stability framework with a pre-trained
machine learning model for scale selection to obtain one robust partition at a
scale that is learned based on the graph structure. This automatic scale
selection involves using a gradient boosting model pre-trained on hand-crafted
and embedding-based network features from a labeled dataset of 10k benchmark
networks. This model was trained to predicts the scale value that maximizes the
similarity of the output partition to the planted partition of the benchmark
network. Combining our scale selection algorithm with the PyGenStability
algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale
community detection algorithm that returns one robust partition at a suitable
scale without the need for any assumptions, input, or tweaking from the user.
We compare the performance of PO against 29 algorithms and show that it
outperforms 25 other algorithms by statistically meaningful margins. Our
results facilitate choosing between community detection algorithms, among which
PO stands out as the accurate, robust, and hyperparameter-free method.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [73] [Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery](https://arxiv.org/abs/2504.11495)
*Yiting Wang,Yunxin Fan,Fei Liu*

Main category: cs.RO

TLDR: 本论文提出一种框架，结合稀疏关键点跟踪和概率建模，提高机器人手术工具-组织交互的精确建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖劳动密集型标注或刚性假设，限制灵活性，需要更精确的建模。

Method: 使用稀疏关键点跟踪、PCA构建动态变换，并通过TP-GMM整合数据驱动观察与临床知识。

Result: 有效预测工具-组织相对位姿，并增强视频数据的视觉理解。

Conclusion: 框架提高了建模准确性和手术动作理解。

Abstract: Accurate modeling of tool-tissue interactions in robotic surgery requires
precise tracking of deformable tissues and integration of surgical domain
knowledge. Traditional methods rely on labor-intensive annotations or rigid
assumptions, limiting flexibility. We propose a framework combining sparse
keypoint tracking and probabilistic modeling that propagates expert-annotated
landmarks across endoscopic frames, even with large tissue deformations.
Clustered tissue keypoints enable dynamic local transformation construction via
PCA, and tool poses, tracked similarly, are expressed relative to these frames.
Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)
integrates data-driven observations with labeled clinical expertise,
effectively predicting relative tool-tissue poses and enhancing visual
understanding of robotic surgical motions directly from video data.

</details>

### [74] [Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports](https://arxiv.org/abs/2504.11717)
*Donggeon David Oh,Justin Lidard,Haimin Hu,Himani Sinhmar,Elle Lazarski,Deepak Gopinath,Emily S. Sumner,Jonathan A. DeCastro,Guy Rosman,Naomi Ehrich Leonard,Jaime Fernández Fisac*

Main category: cs.RO

TLDR: 本论文提出人类中心安全过滤器（HCSF），通过神经网络和质量控制屏障函数提升共享自治系统的安全，同时保持人类自主性，用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升共享自治系统的安全，而不损害人类自主性。

Method: 通过黑箱交互学习神经安全价值函数，并使用质量控制屏障函数（Q-CBF）作为安全约束。

Result: 用户研究显示，HCSF 提高了安全性和满意度，与无辅助和传统过滤器相比，提升了人类自主性、舒适度和满意度。

Conclusion: 该方法有效改善了共享自治系统的安全和用户体验。

Abstract: We propose a human-centered safety filter (HCSF) for shared autonomy that
significantly enhances system safety without compromising human agency. Our
HCSF is built on a neural safety value function, which we first learn scalably
through black-box interactions and then use at deployment to enforce a novel
quality control barrier function (Q-CBF) safety constraint. Since this Q-CBF
safety filter does not require any knowledge of the system dynamics for both
synthesis and runtime safety monitoring and intervention, our method applies
readily to complex, black-box shared autonomy systems. Notably, our HCSF's
CBF-based interventions modify the human's actions minimally and smoothly,
avoiding the abrupt, last-moment corrections delivered by many conventional
safety filters. We validate our approach in a comprehensive in-person user
study using Assetto Corsa-a high-fidelity car racing simulator with black-box
dynamics-to assess robustness in "driving on the edge" scenarios. We compare
both trajectory data and drivers' perceptions of our HCSF assistance against
unassisted driving and a conventional safety filter. Experimental results show
that 1) compared to having no assistance, our HCSF improves both safety and
user satisfaction without compromising human agency or comfort, and 2) relative
to a conventional safety filter, our proposed HCSF boosts human agency,
comfort, and satisfaction while maintaining robustness.

</details>

### [75] [Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning](https://arxiv.org/abs/2504.11493)
*Azizul Zahid,Jie Fan,Farong Wang,Ashton Dy,Sai Swaminathan,Fei Liu*

Main category: cs.RO

TLDR: 本论文提出多模态框架建模人类和机器人行为对应，针对'pick and place'任务，达到约71.7%的准确率。


<details>
  <summary>Details</summary>
Motivation: 理解人类与机器人行为对应以评估决策对齐，特别是在无结构环境中的协作和模仿学习。

Method: 提出多模态演示学习框架，使用ResNet编码人类意图和Perceiver Transformer预测机器人动作，基于RGB视频和体素化RGB-D数据，从RH20T数据集训练。

Result: 人类模型准确率71.67%，机器人模型准确率71.8%。

Conclusion: 证明框架在对齐复杂多模态人类和机器人行为方面的潜力。

Abstract: Understanding action correspondence between humans and robots is essential
for evaluating alignment in decision-making, particularly in human-robot
collaboration and imitation learning within unstructured environments. We
propose a multimodal demonstration learning framework that explicitly models
human demonstrations from RGB video with robot demonstrations in voxelized
RGB-D space. Focusing on the "pick and place" task from the RH20T dataset, we
utilize data from 5 users across 10 diverse scenes. Our approach combines
ResNet-based visual encoding for human intention modeling and a Perceiver
Transformer for voxel-based robot action prediction. After 2000 training
epochs, the human model reaches 71.67% accuracy, and the robot model achieves
71.8% accuracy, demonstrating the framework's potential for aligning complex,
multimodal human and robot behaviors in manipulation tasks.

</details>

### [76] [Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments](https://arxiv.org/abs/2504.11901)
*Luca Castri,Gloria Beraldo,Nicola Bellotto*

Main category: cs.RO

TLDR: 本文提出基于因果推理的决策框架，帮助机器人更好地处理与人类共享的环境，并开发了PeopleFlow模拟器。


<details>
  <summary>Details</summary>
Motivation: 机器人日益融入共享环境（如仓库、医院），需要通过因果分析深入理解人类行为和环境动态，以提升机器人性能。

Method: 提出因果模型决策框架，用于预测电池使用和人类障碍，并开发PeopleFlow模拟器模拟人类-机器人交互。

Result: 在仓库环境中，评估显示因果方法比非因果基线更有效地提高了机器人效率和安全性。

Conclusion: 因果推理使自主机器人能够在动态人类共享环境中更有效地和安全地执行任务。

Abstract: The growing integration of robots in shared environments -- such as
warehouses, shopping centres, and hospitals -- demands a deep understanding of
the underlying dynamics and human behaviours, including how, when, and where
individuals engage in various activities and interactions. This knowledge goes
beyond simple correlation studies and requires a more comprehensive causal
analysis. By leveraging causal inference to model cause-and-effect
relationships, we can better anticipate critical environmental factors and
enable autonomous robots to plan and execute tasks more effectively. To this
end, we propose a novel causality-based decision-making framework that reasons
over a learned causal model to predict battery usage and human obstructions,
understanding how these factors could influence robot task execution. Such
reasoning framework assists the robot in deciding when and how to complete a
given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based
simulator designed to model context-sensitive human-robot spatial interactions
in shared workspaces. PeopleFlow features realistic human and robot
trajectories influenced by contextual factors such as time, environment layout,
and robot state, and can simulate a large number of agents. While the simulator
is general-purpose, in this paper we focus on a warehouse-like environment as a
case study, where we conduct an extensive evaluation benchmarking our causal
approach against a non-causal baseline. Our findings demonstrate the efficacy
of the proposed solutions, highlighting how causal reasoning enables autonomous
robots to operate more efficiently and safely in dynamic environments shared
with humans.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
*Jiazhan Feng,Shijue Huang,Xingwei Qu,Ge Zhang,Yujia Qin,Baoquan Zhong,Chengquan Jiang,Jinxin Chi,Wanjun Zhong*

Main category: cs.CL

TLDR: 这篇论文提出 ReTool 方法，通过整合实时代码执行和强化学习，提升推理模型在结构化问题上的性能，在 AIME 基准上达到 67% 准确率，并优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在文本推理上出色，但处理几何、计算和方程等结构化问题时表现较差，而代码解释器更擅长，因此需要桥接这一差距。

Method: 提出 ReTool，包括动态代码执行与自然语言推理交织，以及基于结果反馈的 RL 训练框架，使用合成数据初始化并迭代优化工具调用策略。

Result: 在 AIME 上，32B 模型在 400 步训练后达到 67% 准确率，优于文本 RL 基线（40% 准确率，1080 步）；在扩展设置中达 72.5%，超过 OpenAI o1-preview 27.9 个百分点。

Conclusion: 证明了结果驱动工具整合在复杂数学推理中的潜力，并为混合神经符号系统提供新见解，展示了模型的自适应工具使用行为。

Abstract: While reasoning models (e.g., DeepSeek R1) trained with reinforcement
learning (RL), excel in textual reasoning, they struggle in scenarios requiring
structured problem-solving, such as geometric reasoning, concise computation,
or complex equation solving-areas where computational tools like code
interpreters (CI) demonstrate distinct advantages. To bridge this gap, we
propose ReTool, which enhances long-form reasoning with tool-integrated
learning, including two key features: (1) dynamic interleaving of real-time
code execution within natural language reasoning processes, and (2) an
automated RL paradigm that allows policy rollouts with multi-turn real-time
code execution and teaches the model in learning when and how to invoke tools
based on outcome feedback. ReTool employs a systematic training framework,
beginning with synthetic cold-start data generation to produce code-augmented
long-form reasoning traces for fine-tuning base models. Subsequent RL training
leverages task outcomes as rewards to iteratively refine the model's tool use
strategy, enabling autonomous discovery of optimal tool invocation patterns
without human priors. Experiments on the challenging MATH Olympiad benchmark
AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with
400 training steps, outperforming text-based RL baseline (40% accuracy, 1080
steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%
accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further
analysis reveals emergent behaviors such as code self-correction, signaling an
''aha moment'' in which the model autonomously masters adaptive tool use. These
findings highlight the promise of outcome-driven tool integration for advancing
complex mathematical reasoning and offer new insights into hybrid
neuro-symbolic systems.

</details>

### [78] [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
*Ozan İrsoy,Pengxiang Cheng,Jennifer L. Chen,Daniel Preoţiuc-Pietro,Shiyue Zhang,Duccio Pappadopulo*

Main category: cs.CL

TLDR: 本研究通过部分适应方法减少指令调整强度，改善了少样本学习性能，但可能降低指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 指令调整可能导致模型忘记预训练知识或变得过于啰嗦，从而降低少样本学习性能。

Method: 使用部分适应方法来降低指令调整的强度。

Result: 减少指令调整强度后，少样本学习基准测试性能显著改善，但指令遵循能力有所下降。

Conclusion: 存在少样本学习和指令遵循能力之间的权衡，需要在实践中考虑。

Abstract: Instruct models, obtained from various instruction tuning or post-training
steps, are commonly deemed superior and more usable than their base
counterpart. While the model gains instruction following ability, instruction
tuning may lead to forgetting the knowledge from pre-training or it may
encourage the model being overly conversational or verbose. This, in turn, can
lead to degradation of in-context few-shot learning performance. In this work,
we study the performance trajectory between base and instruct models by scaling
down the strength of instruction-tuning via the partial adaption method. We
show that, across several model families and model sizes, reducing the strength
of instruction-tuning results in material improvement on a few-shot in-context
learning benchmark covering a variety of classic natural language tasks. This
comes at the cost of losing some degree of instruction following ability as
measured by AlpacaEval. Our study shines light on the potential trade-off
between in-context learning and instruction following abilities that is worth
considering in practice.

</details>

### [79] [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
*Zhisong Zhang,Tianqing Fang,Kaixin Ma,Wenhao Yu,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: 这篇论文提出了一种带回滚机制的网络代理方法，提高了在复杂动态网络环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型进步提升了网络代理，但贪婪单向搜索策略难以从错误状态恢复，因此需要更先进的规划和搜索能力。

Method: 通过引入显式回滚机制，允许代理回退到之前的导航状态，从而更有效地控制搜索过程。

Result: 在两个实时网络导航基准上进行零样本和微调实验，结果证明了方法的有效性。

Conclusion: 该方法提供了一种高效的网络导航解决方案，提升了代理的灵活性和鲁棒性。

Abstract: With recent advancements in large language models, web agents have been
greatly improved. However, dealing with complex and dynamic web environments
requires more advanced planning and search abilities. Previous studies usually
adopt a greedy one-way search strategy, which may struggle to recover from
erroneous states. In this work, we enhance web agents with an explicit rollback
mechanism, enabling the agent to revert back to a previous state in its
navigation trajectory. This mechanism gives the model the flexibility to
directly control the search process, leading to an effective and efficient web
navigation method. We conduct experiments on two live web navigation benchmarks
with zero-shot and fine-tuning settings. The results demonstrate the
effectiveness of our proposed approach.

</details>

### [80] [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
*Yue Li,Lihong Zhang*

Main category: cs.CL

TLDR: SAFL通过动态微调注意力关键transformer层，减少联邦学习中的通信开销和提升隐私，在临床NLP中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在训练大语言模型时的通信开销和模型隐私挑战，尤其在医疗应用中。

Method: 引入SAFL方法，使用注意力模式确定层重要性，仅微调关键transformer层，减少通信带宽并增强差分隐私。

Result: 在i2b2和MIMIC-III临床NLP基准测试中，SAFL与集中式模型性能相当，同时显著提高通信效率和隐私保护。

Conclusion: SAFL在保持性能的同时改善通信效率和隐私保存，适用于医疗领域的联邦学习。

Abstract: Federated Learning (FL) faces major challenges regarding communication
overhead and model privacy when training large language models (LLMs),
especially in healthcare applications. To address these, we introduce Selective
Attention Federated Learning (SAFL), a novel approach that dynamically
fine-tunes only those transformer layers identified as attention-critical. By
employing attention patterns to determine layer importance, SAFL significantly
reduces communication bandwidth and enhances differential privacy resilience.
Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and
MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive
performance with centralized models while substantially improving communication
efficiency and privacy preservation.

</details>

### [81] [Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
*Julia Kreutzer,Eleftheria Briakou,Sweta Agrawal,Marzieh Fadaee,Kocmi Tom*

Main category: cs.CL

TLDR: 这篇论文通过借鉴机器翻译评估的最佳实践，改进多语言大语言模型的评估方法，并提供行动建议。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型的生成能力和语言覆盖率快速进步，但评估实践缺乏全面性、科学性和一致性，需要借鉴成熟领域经验来提升。

Method: 通过针对生成评估管道关键阶段的实验，借鉴机器翻译评估的透明报告标准和可靠方法。

Result: 展示了如何通过这些最佳实践加深对模型质量差异的理解，并识别了用于mLLM稳健元评估的基本组件。

Conclusion: 提炼见解成一个可操作的推荐清单，以指导mLLM的研究和开发。

Abstract: Generation capabilities and language coverage of multilingual large language
models (mLLMs) are advancing rapidly. However, evaluation practices for
generative abilities of mLLMs are still lacking comprehensiveness, scientific
rigor, and consistent adoption across research labs, which undermines their
potential to meaningfully guide mLLM development. We draw parallels with
machine translation (MT) evaluation, a field that faced similar challenges and
has, over decades, developed transparent reporting standards and reliable
evaluations for multilingual generative models. Through targeted experiments
across key stages of the generative evaluation pipeline, we demonstrate how
best practices from MT evaluation can deepen the understanding of quality
differences between models. Additionally, we identify essential components for
robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are
rigorously assessed. We distill these insights into a checklist of actionable
recommendations for mLLM research and development.

</details>

### [82] [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
*Yue Zhao,Qingqing Gu,Xiaoyu Wang,Teng Chen,Zhonglin Jiang,Yong Chen,Luo Ji*

Main category: cs.CL

TLDR: 本文提出FiSMiness框架，使用有限状态机增强大语言模型在情感支持对话中的性能，并通过实验证明其优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在情感支持对话中可能未从状态模型视角定义流程，导致长期满意度不佳。

Method: 利用有限状态机（FSM）与大语言模型结合，提出FiSMiness框架，使LLM在对话中自我规划和推理情绪、支持策略及响应。

Result: 实验显示FiSMiness在情感支持对话数据集上优于直接推理、自我优化、思维链、微调和外部辅助方法，甚至参数更大的模型。

Conclusion: 使用FSM增强LLM可显著提升情感支持对话的长期效果和性能。

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Finite State Machine (FSM) on LLMs, and propose a framework called
FiSMiness. Our framework allows a single LLM to bootstrap the planning during
ESC, and self-reason the seeker's emotion, support strategy and the final
response upon each conversational turn. Substantial experiments on ESC datasets
suggest that FiSMiness outperforms many baselines, including direct inference,
self-refine, chain of thought, finetuning, and external-assisted methods, even
those with many more parameters.

</details>

### [83] [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
*Ram Mohan Rao Kadiyala,Siddartha Pullakhandam,Kanwal Mehreen,Drishti Sharma,Siddhant Gupta,Jebish Purbey,Ashay Srivastava,Subhasya TippaReddy,Arvind Reddy Bobbili,Suraj Telugara Chandrashekhar,Modabbir Adeeb,Srinadh Vura,Hamza Farooq*

Main category: cs.CL

TLDR: 这篇论文引入了新的标记分类模型和数据集，用于检测人类与AI共同编写的文本，模型在各种场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有检测系统在短文本和部分AI生成内容上表现不佳，需要处理各种生成器、未见领域和对抗输入。

Method: 开发了基于标记分类的模型，训练于大规模的人机共同编写文本数据集；并发布了一个包含240万文本的新数据集，覆盖23种语言。

Result: 模型在未见领域、生成器、非母语文本和对抗输入上性能良好；比较了不同条件下的性能，包括领域、生成器、对抗方法、文本长度和文本特征。

Conclusion: 新模型和数据集提高了AI生成内容检测的鲁棒性和泛化能力，适用于更广泛的场景。

Abstract: An ideal detection system for machine generated content is supposed to work
well on any generator as many more advanced LLMs come into existence day by
day. Existing systems often struggle with accurately identifying AI-generated
content over shorter texts. Further, not all texts might be entirely authored
by a human or LLM, hence we focused more over partial cases i.e human-LLM
co-authored texts. Our paper introduces a set of models built for the task of
token classification which are trained on an extensive collection of
human-machine co-authored texts, which performed well over texts of unseen
domains, unseen generators, texts by non-native speakers and those with
adversarial inputs. We also introduce a new dataset of over 2.4M such texts
mostly co-authored by several popular proprietary LLMs over 23 languages. We
also present findings of our models' performance over each texts of each domain
and generator. Additional findings include comparison of performance against
each adversarial method, length of input texts and characteristics of generated
texts compared to the original human authored texts.

</details>

### [84] [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
*Jose Manuel Guevara-Vela*

Main category: cs.CL

TLDR: 本文通过将大型语言模型（LLMs）与准晶体类比，提出LLMs生成内部共振语言模式的新视角。


<details>
  <summary>Details</summary>
Motivation: 为了重新定义LLMs的评估和设计，强调约束传播与形式连贯性，而非传统准确性。

Method: 采用结构类比方法，分析LLMs的生成机制。

Result: 建议关注LLMs输出的约束与连贯模式，重新框架生成语言为涌现模式。

Conclusion: LLMs通过局部约束产生全局连贯的语言模式，类似于准晶体。

Abstract: This essay proposes an analogy between large language models (LLMs) and
quasicrystals: systems that exhibit global coherence without periodic
repetition and that are generated through local constraints. While LLMs are
often evaluated in terms of predictive accuracy, factuality, or alignment, this
structural perspective suggests that their most characteristic behavior is the
production of internally resonant linguistic patterns. Just as quasicrystals
forced a redefinition of order in physical systems, viewing LLMs as generators
of quasi-structured language opens new paths for evaluation and design:
privileging propagation of constraint over token-level accuracy, and coherence
of form over fixed meaning. LLM outputs should be read not only for what they
say, but for the patterns of constraint and coherence that organize them. This
shift reframes generative language as a space of emergent patterning: LLMs are
neither fully random nor strictly rule-based, but defined by a logic of
constraint, resonance, and structural depth.

</details>

### [85] [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
*Yumin Kim,Hwanhee Lee*

Main category: cs.CL

TLDR: 这篇论文提出了一种新型的基于上下文学习的方法，用于检测隐性仇恨言论，提高准确性和减少偏差，而不需要模型微调。实验结果显示其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决隐性仇恨言论检测的挑战，包括上下文依赖、文化细微差别、模型偏差以及大型语言模型的过度敏感问题，以提升检测精度和鲁棒性。

Method: 提出了一种不需模型微调的上下文学习方法，通过自适应检索相似群组或高相似度演示来增强上下文理解。

Result: 实验结果表明，该方法在性能上优于当前最先进的技术。

Conclusion: 结论是，该方法有效地减少了误分类，提高了隐性仇恨言论检测的准确性和可靠性。

Abstract: Hate speech detection is a crucial area of research in natural language
processing, essential for ensuring online community safety. However, detecting
implicit hate speech, where harmful intent is conveyed in subtle or indirect
ways, remains a major challenge. Unlike explicit hate speech, implicit
expressions often depend on context, cultural subtleties, and hidden biases,
making them more challenging to identify consistently. Additionally, the
interpretation of such speech is influenced by external knowledge and
demographic biases, resulting in varied detection results across different
language models. Furthermore, Large Language Models often show heightened
sensitivity to toxic language and references to vulnerable groups, which can
lead to misclassifications. This over-sensitivity results in false positives
(incorrectly identifying harmless statements as hateful) and false negatives
(failing to detect genuinely harmful content). Addressing these issues requires
methods that not only improve detection precision but also reduce model biases
and enhance robustness. To address these challenges, we propose a novel method,
which utilizes in-context learning without requiring model fine-tuning. By
adaptively retrieving demonstrations that focus on similar groups or those with
the highest similarity scores, our approach enhances contextual comprehension.
Experimental results show that our method outperforms current state-of-the-art
techniques. Implementation details and code are available at TBD.

</details>

### [86] [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
*Maged S. Al-Shaibani,Zaid Alyafeai,Irfan Ahmad*

Main category: cs.CL

TLDR: 本研究提出一个先进的框架，通过整合两个高资源系统自动识别朗诵阿拉伯诗歌的韵律，并发布基准以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 识别阿拉伯诗歌韵律过程复杂漫长，需要专业知识，且朗诵诗歌任务更具挑战，需要大量标注数据。

Method: 提出一个状态-of-the-art框架，整合两个独立的、高资源的系统来处理低资源任务，即自动识别朗诵阿拉伯诗歌的韵律。

Result: 开发了框架并发布基准数据集，确保了架构的泛化性和为未来研究提供基础。

Conclusion: 整合现有系统实现了自动识别朗诵诗歌韵律的创新方法，并通过基准数据集推动了该领域的进一步发展。

Abstract: Arabic poetry is an essential and integral part of Arabic language and
culture. It has been used by the Arabs to spot lights on their major events
such as depicting brutal battles and conflicts. They also used it, as in many
other languages, for various purposes such as romance, pride, lamentation, etc.
Arabic poetry has received major attention from linguistics over the decades.
One of the main characteristics of Arabic poetry is its special rhythmic
structure as opposed to prose. This structure is referred to as a meter.
Meters, along with other poetic characteristics, are intensively studied in an
Arabic linguistic field called "\textit{Aroud}". Identifying these meters for a
verse is a lengthy and complicated process. It also requires technical
knowledge in \textit{Aruod}. For recited poetry, it adds an extra layer of
processing. Developing systems for automatic identification of poem meters for
recited poems need large amounts of labelled data. In this study, we propose a
state-of-the-art framework to identify the poem meters of recited Arabic
poetry, where we integrate two separate high-resource systems to perform the
low-resource task. To ensure generalization of our proposed architecture, we
publish a benchmark for this task for future research.

</details>

### [87] [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
*Victor Manuel Hernandez Lopez,Jaime E. Cuellar*

Main category: cs.CL

TLDR: 这篇论文分析了25.3万条西班牙语YouTube评论，探讨Hamas-Israel冲突，使用NLP和BERT模型分类评论，发现亲巴勒斯坦评论居多但亲以色列评论获更多点赞，并展示了媒体如何影响意见转变。


<details>
  <summary>Details</summary>
Motivation: 论文旨在通过跨学科方法结合STS和计算技术，分析公众意见和媒体叙事，以应对复杂的争议现象。

Method: 采用NLP的BERT模型自动分类评论为七个类别，并应用议程设置理论来研究媒体对公众感知的影响。

Result: 结果显示亲巴勒斯坦评论占主导，但亲以色列和反巴勒斯坦评论获得更多点赞；观察到公众意见从亲巴勒斯坦转向更批评以色列的转变。

Conclusion: 结论强调了结合社会科学视角和技术工具的重要性，展示了方法论创新在分析公众意见和媒体叙事中的价值。

Abstract: This article analyzes the Hamas-Israel controversy through 253,925
Spanish-language YouTube comments posted between October 2023 and January 2024,
following the October 7 attack that escalated the conflict. Adopting an
interdisciplinary approach, the study combines the analysis of controversies
from Science and Technology Studies (STS) with advanced computational
methodologies, specifically Natural Language Processing (NLP) using the BERT
(Bidirectional Encoder Representations from Transformers) model. Using this
approach, the comments were automatically classified into seven categories,
reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli
positions, among others. The results show a predominance of pro- Palestinian
comments, although pro-Israeli and anti-Palestinian comments received more
"likes." This study also applies the agenda-setting theory to demonstrate how
media coverage significantly influences public perception, observing a notable
shift in public opinion, transitioning from a pro- Palestinian stance to a more
critical position towards Israel. This work highlights the importance of
combining social science perspectives with technological tools in the analysis
of controversies, presenting a methodological innovation by integrating
computational analysis with critical social theories to address complex public
opinion phenomena and media narratives.

</details>

### [88] [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
*Jaime E. Cuellar,Oscar Moreno-Martinez,Paula Sofia Torres-Rodriguez,Jaime Andres Pavlich-Mariscal,Andres Felipe Mican-Castiblanco,Juan Guillermo Torres-Hurtado*

Main category: cs.CL

TLDR: 这项研究测试微小提示变化是否影响GPT-4o mini的情感分析，使用10万条西班牙语评论数据集，结果显示分类结果不稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨对复杂预测模型如ChatGPT的信任度，焦点是测试提示微小变化是否导致情感分类结果显著差异。

Method: 使用10万条西班牙语总统评论数据集，模型在10次轻微提示变化下分类情感，采用探索性和确认性分析及Chi-square测试。

Result: 提示微调导致分类差异显著，出现不一致响应如混合类别或幻觉，统计分析确认大部分比较有显著差异。

Conclusion: 挑战大型语言模型的稳健性，强调其对提示变化的脆弱性，信任需考虑社会和机构因素。

Abstract: One fundamental question for the social sciences today is: how much can we
trust highly complex predictive models like ChatGPT? This study tests the
hypothesis that subtle changes in the structure of prompts do not produce
significant variations in the classification results of sentiment polarity
analysis generated by the Large Language Model GPT-4o mini. Using a dataset of
100.000 comments in Spanish on four Latin American presidents, the model
classified the comments as positive, negative, or neutral on 10 occasions,
varying the prompts slightly each time. The experimental methodology included
exploratory and confirmatory analyses to identify significant discrepancies
among classifications.
  The results reveal that even minor modifications to prompts such as lexical,
syntactic, or modal changes, or even their lack of structure impact the
classifications. In certain cases, the model produced inconsistent responses,
such as mixing categories, providing unsolicited explanations, or using
languages other than Spanish. Statistical analysis using Chi-square tests
confirmed significant differences in most comparisons between prompts, except
in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models
for classification tasks, highlighting their vulnerability to variations in
instructions. Moreover, it was evident that the lack of structured grammar in
prompts increases the frequency of hallucinations. The discussion underscores
that trust in Large Language Models is based not only on technical performance
but also on the social and institutional relationships underpinning their use.

</details>

### [89] [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
*Suyoung Bae,Hyojun Kim,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CL

TLDR: 本文提出SALAD方法，通过结构感知和反事实增强数据改善NLP模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微调预训练语言模型常导致虚假相关性，影响分布外数据的性能。

Method: SALAD使用基于标记的方法生成结构感知正样本，并利用大语言模型生成多样反事实负样本，然后应用对比学习。

Result: 在情感分类、性别歧视检测和自然语言推理任务上实验显示，SALAD提升了模型的鲁棒性、性能和分布外泛化能力。

Conclusion: SALAD方法被验证有效，提高了模型在不同环境下的性能和泛化。

Abstract: In various natural language processing (NLP) tasks, fine-tuning Pre-trained
Language Models (PLMs) often leads to the issue of spurious correlations, which
negatively impacts performance, particularly when dealing with
out-of-distribution data. To address this problem, we propose SALAD}(Structure
Aware and LLM-driven Augmented Data), a novel approach designed to enhance
model robustness and generalization by generating structure-aware and
counterfactually augmented data for contrastive learning. Our method leverages
a tagging-based approach to generate structure-aware positive samples and
utilizes large language models (LLMs) to generate counterfactual negative
samples with diverse sentence patterns. By applying contrastive learning, SALAD
enables the model to focus on learning the structural relationships between key
sentence components while minimizing reliance on spurious correlations. We
validate our approach through experiments on three tasks: Sentiment
Classification, Sexism Detection, and Natural Language Inference. The results
demonstrate that SALAD not only improves model robustness and performance
across different environments but also enhances generalization to
out-of-distribution datasets and cross-domain scenarios.

</details>

### [90] [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
*Céline Budding*

Main category: cs.CL

TLDR: 本论文论证大型语言模型（LLMs）可以获得马丁·戴维斯（1990）定义的默会知识，尽管戴维斯否认神经网络能获得这种知识。作者展示了LLMs的架构特征满足语义描述、句法结构和因果系统性的约束，从而提出默会知识可作为描述、解释和干预LLMs行为的框架。


<details>
  <summary>Details</summary>
Motivation: 质疑LLMs是否真正'知道'语言或事实，挑战常见的假设，探讨LLMs实际知道什么。

Method: 通过论证和展示LLMs的架构特征是否满足默会知识的约束，包括语义描述、句法结构和因果系统性。

Result: LLMs可以获得默会知识，因为其架构满足语义描述、句法结构和因果系统性的约束。

Conclusion: 默会知识可作为描述、解释和干预LLMs行为的概念框架。

Abstract: It is sometimes assumed that Large Language Models (LLMs) know language, or
for example that they know that Paris is the capital of France. But what -- if
anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire
tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself
denies that neural networks can acquire tacit knowledge, I demonstrate that
certain architectural features of LLMs satisfy the constraints of semantic
description, syntactic structure, and causal systematicity. Thus, tacit
knowledge may serve as a conceptual framework for describing, explaining, and
intervening on LLMs and their behavior.

</details>

### [91] [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
*Siyan Zhao,Devaansh Gupta,Qinqing Zheng,Aditya Grover*

Main category: cs.CL

TLDR: 本文提出d1框架，通过监督微调和新型强化学习方法提升扩散型LLM的推理能力，并在基准测试中取得显著改善。


<details>
  <summary>Details</summary>
Motivation: 探讨基于扩散的LLM是否能利用现有推理进展，因为这些进展主要在自回归模型上。

Method: 提出d1框架，结合掩码监督微调和diffu-GRPO强化学习算法。

Result: 在数学和逻辑推理基准上，d1框架表现最佳，并显著提升了最先进dLLM的性能。

Conclusion: 证明d1框架有效，展示了非自回归范式在推理任务中的潜力。

Abstract: Recent large language models (LLMs) have demonstrated strong reasoning
capabilities that benefits from online reinforcement learning (RL). These
capabilities have primarily been demonstrated within the left-to-right
autoregressive (AR) generation paradigm. In contrast, non-autoregressive
paradigms based on diffusion generate text in a coarse-to-fine manner. Although
recent diffusion-based large language models (dLLMs) have achieved competitive
language modeling performance compared to their AR counterparts, it remains
unclear if dLLMs can also leverage recent advances in LLM reasoning. To this
end, we propose d1, a framework to adapt pre-trained masked dLLMs into
reasoning models via a combination of supervised finetuning (SFT) and RL.
Specifically, we develop and extend techniques to improve reasoning in
pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge
and instill self-improvement behavior directly from existing datasets, and (b)
we introduce a novel critic-free, policy-gradient based RL algorithm called
diffu-GRPO. Through empirical studies, we investigate the performance of
different post-training recipes on multiple mathematical and logical reasoning
benchmarks. We find that d1 yields the best performance and significantly
improves performance of a state-of-the-art dLLM.

</details>

### [92] [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
*Shuming Ma,Hongyu Wang,Shaohan Huang,Xingxing Zhang,Ying Hu,Ting Song,Yan Xia,Furu Wei*

Main category: cs.CL

TLDR: 本论文引入了首个开源的1位LLM BitNet b1.58 2B4T，它在性能上与类似规模的全精度LLM相当，但具有更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 动机是开发高效的LLM，以减少内存占用、能源消耗和解码延迟，同时保持高性能。

Method: 方法包括在4万亿个token的语料上训练模型，并通过语言理解、数学推理、编码能力和对话能力等基准进行评估。

Result: 结果显示，模型在性能上与领先的开源全精度LLM相当，同时显著降低了内存占用、能源消耗和解码延迟。

Conclusion: 结论是，模型权重通过Hugging Face开源，并提供GPU和CPU的推理实现，以促进进一步研究和采用。

Abstract: We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large
Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4
trillion tokens, the model has been rigorously evaluated across benchmarks
covering language understanding, mathematical reasoning, coding proficiency,
and conversational ability. Our results demonstrate that BitNet b1.58 2B4T
achieves performance on par with leading open-weight, full-precision LLMs of
similar size, while offering significant advantages in computational
efficiency, including substantially reduced memory footprint, energy
consumption, and decoding latency. To facilitate further research and adoption,
the model weights are released via Hugging Face along with open-source
inference implementations for both GPU and CPU architectures.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [93] [HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks](https://arxiv.org/abs/2504.12268)
*Stefan Abi-Karam,Cong Hao*

Main category: cs.AR

TLDR: 引入HLS-Eval基准测试框架，用于评估LLM在高阶综合（HLS）设计任务中的性能，包括代码生成和优化。


<details>
  <summary>Details</summary>
Motivation: LLM在半导体设计中快速应用，但缺乏针对HLS任务的全面基准和工具。

Method: 开发HLS-Eval基准和Python框架，包含94个设计，支持自动化评估和HLS工具集成。

Result: 进行了基线评估，测量解析性、可编译性等指标，并开源了所有内容。

Conclusion: 为LLM在硬件设计中的应用提供可重用基础设施，建立清晰基准。

Abstract: The rapid scaling of large language model (LLM) training and inference has
driven their adoption in semiconductor design across academia and industry.
While most prior work evaluates LLMs on hardware description language (HDL)
tasks, particularly Verilog, designers are increasingly using high-level
synthesis (HLS) to build domain-specific accelerators and complex hardware
systems. However, benchmarks and tooling to comprehensively evaluate LLMs for
HLS design tasks remain scarce.
  To address this, we introduce HLS-Eval, the first complete benchmark and
evaluation framework for LLM-driven HLS design. HLS-Eval targets two core
tasks: (1) generating HLS code from natural language descriptions, and (2)
performing HLS-specific code edits to optimize performance and hardware
efficiency. The benchmark includes 94 unique designs drawn from standard HLS
benchmarks and novel sources. Each case is prepared via a semi-automated flow
that produces a natural language description and a paired testbench for
C-simulation and synthesis validation, ensuring each task is "LLM-ready."
  Beyond the benchmark, HLS-Eval offers a modular Python framework for
automated, parallel evaluation of both local and hosted LLMs. It includes a
parallel evaluation engine, direct HLS tool integration, and abstractions for
to support different LLM interaction paradigms, enabling rapid prototyping of
new benchmarks, tasks, and LLM methods.
  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on
Vitis HLS, measuring outputs across four key metrics - parseability,
compilability, runnability, and synthesizability - reflecting the iterative HLS
design cycle. We also report pass@k metrics, establishing clear baselines and
reusable infrastructure for the broader LLM-for-hardware community.
  All benchmarks, framework code, and results are open-sourced at
https://github.com/stefanpie/hls-eval.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [94] [Linearity, Time Invariance, and Passivity of a Novice Person in Human Teleoperation](https://arxiv.org/abs/2504.11653)
*David Black,Septimiu Salcudean*

Main category: cs.HC

TLDR: 本文证明在人类遥操作中，追随者可视为近似线性和时不变的，具有小耦合和高钝性，并导出了随机模型以优化控制器设计。


<details>
  <summary>Details</summary>
Motivation: 动机是为偏远和 underserved 社区提供低成本远程医疗指导，并理解追随者动态以提升遥操作性能。

Method: 方法包括通过建模和实验探索追随者的线性、时不变性、轴间耦合和钝性，并推导随机动力学模型。

Result: 结果显示追随者近似线性且时不变，耦合小，钝性充足。

Conclusion: 结论是这些发现可用于设计控制器，提高人类遥操作的稳定性和透明度。

Abstract: Low-cost teleguidance of medical procedures is becoming essential to provide
healthcare to remote and underserved communities. Human teleoperation is a
promising new method for guiding a novice person with relatively high precision
and efficiency through a mixed reality (MR) interface. Prior work has shown
that the novice, or "follower", can reliably track the MR input with
performance not unlike a telerobotic system. As a consequence, it is of
interest to understand and control the follower's dynamics to optimize the
system performance and permit stable and transparent bilateral teleoperation.
To this end, linearity, time-invariance, inter-axis coupling, and passivity are
important in teleoperation and controller design. This paper therefore explores
these effects with regard to the follower person in human teleoperation. It is
demonstrated through modeling and experiments that the follower can indeed be
treated as approximately linear and time invariant, with little coupling and a
large excess of passivity at practical frequencies. Furthermore, a stochastic
model of the follower dynamics is derived. These results will permit controller
design and analysis to improve the performance of human teleoperation.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks](https://arxiv.org/abs/2504.11575)
*Furqan Rustam,Islam Obaidat,Anca Delia Jurcut*

Main category: cs.CR

TLDR: 本研究提出了一种在线连续学习框架，用于多环境网络中的DDoS检测，实现了高准确率和低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有AI检测系统无法适应新攻击策略，缺乏实时高精度检测。

Method: 开发M-En数据集和MULTI-LF框架，使用两个机器学习模型进行分级检测和连续学习。

Result: 分类准确率0.999，预测延迟0.866秒，内存使用3.632 MB，CPU利用率10.05%。

Conclusion: 该方法在实时场景中表现出优越性能，提高了DDoS检测的适应性和效率。

Abstract: Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment
(M-En) networks presents significant challenges due to diverse malicious
traffic patterns and the evolving nature of cyber threats. Existing AI-based
detection systems struggle to adapt to new attack strategies and lack real-time
attack detection capabilities with high accuracy and efficiency. This study
proposes an online, continuous learning methodology for DDoS detection in M-En
networks, enabling continuous model updates and real-time adaptation to
emerging threats, including zero-day attacks. First, we develop a unique M-En
network dataset by setting up a realistic, real-time simulation using the NS-3
tool, incorporating both victim and bot devices. DDoS attacks with varying
packet sizes are simulated using the DDoSim application across IoT and
traditional IP-based environments under M-En network criteria. Our approach
employs a multi-level framework (MULTI-LF) featuring two machine learning
models: a lightweight Model 1 (M1) trained on a selective, critical packet
dataset for fast and efficient initial detection, and a more complex, highly
accurate Model 2 (M2) trained on extensive data. When M1 exhibits low
confidence in its predictions, the decision is escalated to M2 for verification
and potential fine-tuning of M1 using insights from M2. If both models
demonstrate low confidence, the system flags the incident for human
intervention, facilitating model updates with human-verified categories to
enhance adaptability to unseen attack patterns. We validate the MULTI-LF
through real-world simulations, demonstrating superior classification accuracy
of 0.999 and low prediction latency of 0.866 seconds compared to established
baselines. Furthermore, we evaluate performance in terms of memory usage (3.632
MB) and CPU utilization (10.05%) in real-time scenarios.

</details>

### [96] [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703)
*Tianneng Shi,Jingxuan He,Zhun Wang,Linyu Wu,Hongwei Li,Wenbo Guo,Dawn Song*

Main category: cs.CR

TLDR: Progent 是一种新机制，用于通过最小权限原则和策略控制来提升 LLM 代理的安全性，同时保持高实用性。


<details>
  <summary>Details</summary>
Motivation: LLM 代理可能执行恶意命令，存在安全风险，因此需要强制最小权限以仅允许必要操作。

Method: 引入 Progent 机制，使用领域特定语言定义权限策略，对工具调用进行细粒度控制，并利用 LLM 自动生成和动态更新策略。

Result: 在 AgentDojo、ASB 和 AgentPoison 等基准测试中，实现了强安全性与高实用性，并证明了核心组件的有效性和对自适应攻击的抵抗力。

Conclusion: Progent 通过模块化设计易于集成，能够有效提升安全性和实用性，具有广泛采用潜力。

Abstract: LLM agents are an emerging form of AI systems where large language models
(LLMs) serve as the central component, utilizing a diverse set of tools to
complete user-assigned tasks. Despite their great potential, LLM agents pose
significant security risks. When interacting with the external world, they may
encounter malicious commands from attackers, leading to the execution of
dangerous actions. A promising way to address this is by enforcing the
principle of least privilege: allowing only essential actions for task
completion while blocking unnecessary ones. However, achieving this is
challenging, as it requires covering diverse agent scenarios while preserving
both security and utility.
  We introduce Progent, the first privilege control mechanism for LLM agents.
At its core is a domain-specific language for flexibly expressing privilege
control policies applied during agent execution. These policies provide
fine-grained constraints over tool calls, deciding when tool calls are
permissible and specifying fallbacks if they are not. This enables agent
developers and users to craft suitable policies for their specific use cases
and enforce them deterministically to guarantee security. Thanks to its modular
design, integrating Progent does not alter agent internals and requires only
minimal changes to agent implementation, enhancing its practicality and
potential for widespread adoption. To automate policy writing, we leverage LLMs
to generate policies based on user queries, which are then updated dynamically
for improved security and utility. Our extensive evaluation shows that it
enables strong security while preserving high utility across three distinct
scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we
perform an in-depth analysis, showcasing the effectiveness of its core
components and the resilience of its automated policy generation against
adaptive attacks.

</details>

### [97] [PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility](https://arxiv.org/abs/2504.11774)
*Keke Gai,Ziyue Shen,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TLDR: PCDiff 是一个主动访问控制框架，通过调节生成质量，确保只有授权用户才能生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 随着对文本到图像扩散模型知识产权保护需求的增长，需要主动控制访问以防止 unauthorized 利用。

Method: 提出 PCDiff 框架，集成可训练的融合模块和分层认证层到解码器中，在无有效密钥时降低输出质量，并与水印技术兼容。

Result: 实验评估显示凭证验证与图像质量强相关，在各种攻击场景下有效，与水印方法结合性能良好。

Conclusion: 该工作将 IP 保护从被动检测转向主动授权执行，为扩散模型的 IP 管理奠定基础。

Abstract: With the growing demand for protecting the intellectual property (IP) of
text-to-image diffusion models, we propose PCDiff -- a proactive access control
framework that redefines model authorization by regulating generation quality.
At its core, PCDIFF integrates a trainable fuser module and hierarchical
authentication layers into the decoder architecture, ensuring that only users
with valid encrypted credentials can generate high-fidelity images. In the
absence of valid keys, the system deliberately degrades output quality,
effectively preventing unauthorized exploitation.Importantly, while the primary
mechanism enforces active access control through architectural intervention,
its decoupled design retains compatibility with existing watermarking
techniques. This satisfies the need of model owners to actively control model
ownership while preserving the traceability capabilities provided by
traditional watermarking approaches.Extensive experimental evaluations confirm
a strong dependency between credential verification and image quality across
various attack scenarios. Moreover, when combined with typical post-processing
operations, PCDIFF demonstrates powerful performance alongside conventional
watermarking methods. This work shifts the paradigm from passive detection to
proactive enforcement of authorization, laying the groundwork for IP management
of diffusion models.

</details>

### [98] [ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges](https://arxiv.org/abs/2504.12143)
*Matteo Lupinacci,Francesco Blefari,Francesco Romeo,Francesco Aurelio Pironti,Angelo Furfaro*

Main category: cs.CR

TLDR: ARCeR 是一个基于 AI 的系统，能够从自然语言输入自动生成和部署网络安全范围（Cyber Ranges），其性能优于标准的大型语言模型和 RAG 系统。


<details>
  <summary>Details</summary>
Motivation: 应对日益增长和演变的网络安全威胁，需要开发工具来创建虚拟的受控 IT 环境，用于漏洞分析、测试对策的有效性以及作为培训环境。

Method: 本文提出 ARCeR，使用 Agentic RAG 范式，充分利用最先进的 AI 技术，从用户提供的自然语言描述开始自动生成和部署 Cyber Ranges。

Result: 实验结果显示，ARCeR 能够成功处理即使 LLM 或基本 RAG 系统无法应对的提示。此外，ARCeR 能够针对任何 Cyber Range 框架，只要提供特定的知识。

Conclusion: ARCeR 是一个创新的、有效的解决方案，能够自动生成和部署 Cyber Ranges。

Abstract: The growing and evolving landscape of cybersecurity threats necessitates the
development of supporting tools and platforms that allow for the creation of
realistic IT environments operating within virtual, controlled settings as
Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and
experimenting with the effectiveness of devised countermeasures, as well as
serving as training environments for building cyber security skills and
abilities for IT operators. This paper proposes ARCeR as an innovative solution
for the automatic generation and deployment of CRs, starting from user-provided
descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,
which allows it to fully exploit state-of-art AI technologies. Experimental
results show that ARCeR is able to successfully process prompts even in cases
that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is
able to target any CR framework provided that specific knowledge is made
available to it.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [99] [Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime](https://arxiv.org/abs/2504.12000)
*Thorben Markmann,Michiel Straat,Sebastian Peitz,Barbara Hammer*

Main category: physics.flu-dyn

TLDR: 本文使用强化学习减少2D Rayleigh-Bénard对流系统中对流热传输，在中等湍流条件下降低Nusselt数33%，高湍流下降低10%，优于传统PD控制，并展示良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的流动控制在工业、能源系统和气候科学中具有重要潜力，本文研究强化学习在增加湍流条件下减少对流热传输的有效性。

Method: 采用单代理Proximal Policy Optimization (PPO)强化学习代理，与线性比例微分(PD)控制器比较；引入奖励整形技术加速训练，并测试对不同初始条件和湍流水平的泛化能力。

Result: RL代理在中等湍流系统中将Nusselt数减少最多33%，高湍流下减少10%，在所有设置中优于PD控制；代理对初始条件和较高湍流程度有强泛化性能，奖励整形提高了样本效率并稳定了Nusselt数。

Conclusion: 强化学习在减少对流热传输方面有效，具有良好泛化能力，奖励整形技术有助于提升训练效率和稳定性。

Abstract: Data-driven flow control has significant potential for industry, energy
systems, and climate science. In this work, we study the effectiveness of
Reinforcement Learning (RL) for reducing convective heat transfer in the 2D
Rayleigh-B\'enard Convection (RBC) system under increasing turbulence. We
investigate the generalizability of control across varying initial conditions
and turbulence levels and introduce a reward shaping technique to accelerate
the training. RL agents trained via single-agent Proximal Policy Optimization
(PPO) are compared to linear proportional derivative (PD) controllers from
classical control theory. The RL agents reduced convection, measured by the
Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly
turbulent settings, clearly outperforming PD control in all settings. The
agents showed strong generalization performance across different initial
conditions and to a significant extent, generalized to higher degrees of
turbulence. The reward shaping improved sample efficiency and consistently
stabilized the Nusselt Number to higher turbulence levels.

</details>

<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [100] [Strengthening Anomaly Awareness](https://arxiv.org/abs/2504.11520)
*Adam Banda,Charanjit K. Khosa,Veronica Sanz*

Main category: hep-ph

TLDR: 本论文提出了一种改进的Anomaly Awareness框架，通过最小监督提升变分自编码器（VAE）在无监督异常检测中的性能，使用两阶段训练策略，并在多个数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 动机是改进无监督异常检测的局限性，特别是对未见异常的敏感性，通过少量标记的异常样本增强模型的泛化能力。

Method: 方法包括两阶段训练：第一阶段无监督在背景数据上训练VAE，第二阶段使用少量标记的异常样本微调，以增加异常样本的重建错误。

Result: 结果显示，在MNIST、CICIDS、LHCO2020和SMEFT数据集上，模型对未见异常的敏感性提高，实现了更好的正常和异常样本分离。

Conclusion: 结论是，即使有限的异常信息，通过针对性微调，也可以显著提升无监督模型的性能和泛化能力。

Abstract: We present a refined version of the Anomaly Awareness framework for enhancing
unsupervised anomaly detection. Our approach introduces minimal supervision
into Variational Autoencoders (VAEs) through a two-stage training strategy: the
model is first trained in an unsupervised manner on background data, and then
fine-tuned using a small sample of labeled anomalies to encourage larger
reconstruction errors for anomalous samples.
  We validate the method across diverse domains, including the MNIST dataset
with synthetic anomalies, network intrusion data from the CICIDS benchmark,
collider physics data from the LHCO2020 dataset, and simulated events from the
Standard Model Effective Field Theory (SMEFT). The latter provides a realistic
example of subtle kinematic deviations in Higgs boson production. In all cases,
the model demonstrates improved sensitivity to unseen anomalies, achieving
better separation between normal and anomalous samples. These results indicate
that even limited anomaly information, when incorporated through targeted
fine-tuning, can substantially improve the generalization and performance of
unsupervised models for anomaly detection.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [101] [Accelerated Recovery with RIS: Designing Wireless Resilience in Mission-Critical Environments](https://arxiv.org/abs/2504.11589)
*Kevin Weinberger,Robert-Jeron Reifert,Aydin Sezgin,Mehdi Bennis*

Main category: eess.SP

TLDR: 这篇论文提出一个新框架来提升6G无线网络的弹性，通过量化适应性能和整合可重构智能表面（RISs）。


<details>
  <summary>Details</summary>
Motivation: 6G网络使无线系统承担关键服务，但现有弹性评估缺乏适应性关键性能指标（KPIs），需要策略来应对动态环境。

Method: 提出框架，通过增强系统速率函数梯度量化适应性能，并整合RISs动态重塑传播环境。

Result: 数值结果显示，梯度增强提高了不利条件下的适应性和弹性，并为未来中断做准备。

Conclusion: 该框架有效提升了无线网络的弹性性能。

Abstract: As 6G and beyond redefine connectivity, wireless networks become the
foundation of critical operations, making resilience more essential than ever.
With this shift, wireless systems cannot only take on vital services previously
handled by wired infrastructures but also enable novel innovative applications
that would not be possible with wired systems. As a result, there is a pressing
demand for strategies that can adapt to dynamic channel conditions,
interference, and unforeseen disruptions, ensuring seamless and reliable
performance in an increasingly complex environment. Despite considerable
research, existing resilience assessments lack comprehensive key performance
indicators (KPIs), especially those quantifying its adaptability, which are
vital for identifying a system's capacity to rapidly adapt and reallocate
resources. In this work, we bridge this gap by proposing a novel framework that
explicitly quantifies the adaption performance by augmenting the gradient of
the system's rate function. To further enhance the network resilience, we
integrate Reconfigurable Intelligent Surfaces (RISs) into our framework due to
their capability to dynamically reshape the propagation environment while
providing alternative channel paths. Numerical results show that gradient
augmentation enhances resilience by improving adaptability under adverse
conditions while proactively preparing for future disruptions.

</details>

### [102] [A Novel Approach to Secure RSMA Networks](https://arxiv.org/abs/2504.11878)
*Shaima Abidrabbu,Hüseyin Arslan*

Main category: eess.SP

TLDR: 这篇论文提出了一种新的数据依赖交织技术，用于提升RSMA网络的安全性，保护公共流免受窃听威胁。模拟结果显示了显著的安全性提升，同时保持了系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 动机是增强RSMA网络的安全性，特别是保护公共流免受窃听威胁。

Method: 方法包括利用RSMA结构，通过基于私有比特的序列对公共比特进行交织，并提出一种平衡安全性和系统效率的私有比特选择方法。

Result: 模拟结果证实了所提方法的有效性，显示了显著的安全性改进，同时维持了系统的可靠性。

Conclusion: 结论是所提方法有效地提升了安全性，同时确保了系统的整体可靠性。

Abstract: This letter introduces a novel data-dependent interleaving technique designed
to enhance the security of rate-splitting multiple access (RSMA) networks by
protecting the common stream from eavesdropping threats. Specifically, we
exploit the RSMA structure by interleaving the common bits of each user based
on a sequence derived from their private bits. By decoding its private stream,
the legitimate receiver reconstructs the interleaving sequence set by the
transmitter and successfully de-interleaves the common stream. Therefore, the
common part is successfully prevented from being intercepted by an eavesdropper
who is unable to deduce the dynamic changing interleaving permutations. To
ensure dynamic interleaving sequences, a private bit selection approach that
balances the trade-off between security and system efficiency is proposed.
Simulation findings confirm the effectiveness of the suggested method, showing
notable security improvements while maintaining robust overall system
reliability.

</details>

### [103] [A Novel Splitter Design for RSMA Networks](https://arxiv.org/abs/2504.11905)
*Sawaira Rafaqat Ali,Shaima Abidrabbu,H. M. Furqan,Hüseyin Arslan*

Main category: eess.SP

TLDR: 本论文提出了一种基于信道的分裂器设计，用于多载波RSMA系统，以提高可靠性并减少延迟。


<details>
  <summary>Details</summary>
Motivation: RSMA已被确立为下一代通信系统的强大多址接入和干扰管理方法，本文旨在通过利用信道状态信息提升可靠性，减少重传需求。

Method: 提出了一种通道依赖的分裂器设计，利用信道状态信息将可能遇到深衰落子信道的私有流数据段复制到公共流中。

Result: 评估显示在完美和不完美信道估计场景下，可实现总速率、平均包延迟和误码率（BER）等指标的改善。

Conclusion: 该方法有效提升了可靠性，降低了延迟，并最小化了重传需求。

Abstract: Rate splitting multiple access (RSMA) has firmly established itself as a
powerful methodology for multiple access, interference management, and
multi-user strategy for next-generation communication systems. In this paper,
we propose a novel channel-dependent splitter design for multi-carrier RSMA
systems, aimed at improving reliability performance. Specifically, the proposed
splitter leverages channel state information and the inherent structure of RSMA
to intelligently replicate segments of the private stream data that are likely
to encounter deep-faded subchannels into the common stream. Thus, the
reliability is enhanced within the same transmission slot, minimizing the need
for frequent retransmissions and thereby reducing latency. To assess the
effectiveness of our approach, we conduct comprehensive evaluations using key
performance metrics, including achievable sum rate, average packet delay, and
bit error rate (BER), under both perfect and imperfect channel estimation
scenarios.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [104] [Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations](https://arxiv.org/abs/2504.11555)
*Yahya Sattar,Sunmook Choi,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: math.OC

TLDR: 本文研究了带双线性观测的线性系统控制，展示了标准LQG方法失效，并引入新概念。


<details>
  <summary>Details</summary>
Motivation: 动机是解决观测为双线性时的控制挑战，与标准LQG不同，后者Separation Principle成立。

Method: 方法包括分析证明最优控制器非线性，引入输入依赖的可观性，并通过数值实验验证。

Result: 结果显示标准LQG可能最大化成本，最优控制器不唯一且非线性，导出了卡尔曼滤波协方差有界的条件。

Conclusion: 结论是双线性观测使控制更复杂，需要新的非线性控制器。

Abstract: We consider the problem of controlling a linear dynamical system from
bilinear observations with minimal quadratic cost. Despite the similarity of
this problem to standard linear quadratic Gaussian (LQG) control, we show that
when the observation model is bilinear, neither does the Separation Principle
hold, nor is the optimal controller affine in the estimated state. Moreover,
the cost-to-go is non-convex in the control input. Hence, finding an analytical
expression for the optimal feedback controller is difficult in general. Under
certain settings, we show that the standard LQG controller locally maximizes
the cost instead of minimizing it. Furthermore, the optimal controllers
(derived analytically) are not unique and are nonlinear in the estimated state.
We also introduce a notion of input-dependent observability and derive
conditions under which the Kalman filter covariance remains bounded. We
illustrate our theoretical results through numerical experiments in multiple
synthetic settings.

</details>

### [105] [Efficient identification of linear, parameter-varying, and nonlinear systems with noise models](https://arxiv.org/abs/2504.11982)
*Alberto Bemporad,Roland Tóth*

Main category: math.OC

TLDR: 这篇论文提出了一种通用的系统识别程序，能够估计包括线性时不变（LTI）、线性参数变化（LPV）和非线性（NL）动力学在内的各种状态空间动态模型，使用人工神经网络进行参数化，并实现了快速训练和一致性保证。


<details>
  <summary>Details</summary>
Motivation: 为了处理更广泛的状态空间动态模型，包括噪声模型的需求，扩展了LTI情况的方法。

Method: 使用人工神经网络参数化非线性关系，通过预测误差准则优化，结合约束的拟牛顿方法和自动微分，实现高效训练。

Result: 在基准测试中显示出优越的估计准确性和计算效率，训练时间以秒计，并建立了形式一致性保证。

Conclusion: 在LTI、LPV和NL系统识别问题上证明了方法的有效性。

Abstract: We present a general system identification procedure capable of estimating of
a broad spectrum of state-space dynamical models, including linear
time-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)
dynamics, along with rather general classes of noise models. Similar to the LTI
case, we show that for this general class of model structures, including the NL
case, the model dynamics can be separated into a deterministic process and a
stochastic noise part, allowing to seamlessly tune the complexity of the
combined model both in terms of nonlinearity and noise modeling. We
parameterize the involved nonlinear functional relations by means of artificial
neural-networks (ANNs), although alternative parametric nonlinear mappings can
also be used. To estimate the resulting model structures, we optimize a
prediction-error-based criterion using an efficient combination of a
constrained quasi-Newton approach and automatic differentiation, achieving
training times in the order of seconds compared to existing state-of-the-art
ANN methods which may require hours for models of similar complexity. We
formally establish the consistency guarantees for the proposed approach and
demonstrate its superior estimation accuracy and computational efficiency on
several benchmark LTI, LPV, and NL system identification problems.

</details>

### [106] [Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events](https://arxiv.org/abs/2504.11570)
*Haozhe Lei,Ya-Ting Yang,Tao Li,Zilin Bian,Fan Zuo,Sundeep Rangan,Kaan Ozbay*

Main category: math.OC

TLDR: 本文提出TAMPA算法，提高重大事件期间交通巡逻效率，模拟结果显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 动机是改善体育赛事和音乐会等事件对交通网络的压力，需要高效自适应巡逻解决方案。

Method: 方法整合预测交通建模、实时投诉估计，使用动态规划优化部署，并通过Dvoretzky-Kiefer-Wolfowitz不等式检测模式变化。

Result: 模拟结果显示，TAMPA比静态方法改善87.5%，比随机策略改善114.2%。

Conclusion: 结论是性能接近最优，未来工作包括增强适应性和整合数字孪生技术。

Abstract: This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm
(TAMPA), designed to improve real-time incident management during major events
like sports tournaments and concerts. Such events significantly stress
transportation networks, requiring efficient and adaptive patrol solutions.
TAMPA integrates predictive traffic modeling and real-time complaint
estimation, dynamically optimizing patrol deployment. Using dynamic
programming, the algorithm continuously adjusts patrol strategies within short
planning windows, effectively balancing immediate response and efficient
routing. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects
significant shifts in complaint patterns, triggering proactive adjustments in
patrol routes. Theoretical analyses ensure performance remains closely aligned
with optimal solutions. Simulation results from an urban traffic network
demonstrate TAMPA's superior performance, showing improvements of approximately
87.5\% over stationary methods and 114.2\% over random strategies. Future work
includes enhancing adaptability and incorporating digital twin technology for
improved predictive accuracy, particularly relevant for events like the 2026
FIFA World Cup at MetLife Stadium.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [107] [FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry](https://arxiv.org/abs/2504.11519)
*Mohammad Farahmand,Amoon Jamzad,Fahimeh Fooladgar,Laura Connolly,Martin Kaufmann,Kevin Yi Mi Ren,John Rudan,Doug McKay,Gabor Fichtinger,Parvin Mousavi*

Main category: physics.med-ph

TLDR: 本论文开发了FACT模型，用于REIMS数据的实时癌症组织边缘分类，提高了性能。


<details>
  <summary>Details</summary>
Motivation: 癌症手术中准确分类组织边缘至关重要，但REIMS数据标记稀缺，本研究首次针对此开发基金会模型。

Method: 提出FACT模型，改编自文本-音频关联模型，使用基于三元组损失的监督对比预训练，并进行消融研究比较。

Result: 模型达到最先进性能，AUROC为82.4% ± 0.8%，优于自监督和半监督基线。

Conclusion: 基金会模型通过新方法改编，可有效分类REIMS数据，即使标记数据有限，提升实时手术边缘评估。

Abstract: Purpose: Accurately classifying tissue margins during cancer surgeries is
crucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass
Spectrometry (REIMS), a tool for real-time intraoperative margin assessment,
generates spectra that require machine learning models to support clinical
decision-making. However, the scarcity of labeled data in surgical contexts
presents a significant challenge. This study is the first to develop a
foundation model tailored specifically for REIMS data, addressing this
limitation and advancing real-time surgical margin assessment. Methods: We
propose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is
an adaptation of a foundation model originally designed for text-audio
association, pretrained using our proposed supervised contrastive approach
based on triplet loss. An ablation study is performed to compare our proposed
model against other models and pretraining methods. Results: Our proposed model
significantly improves the classification performance, achieving
state-of-the-art performance with an AUROC of $82.4\% \pm 0.8$. The results
demonstrate the advantage of our proposed pretraining method and selected
backbone over the self-supervised and semi-supervised baselines and alternative
models. Conclusion: Our findings demonstrate that foundation models, adapted
and pretrained using our novel approach, can effectively classify REIMS data
even with limited labeled examples. This highlights the viability of foundation
models for enhancing real-time surgical margin assessment, particularly in
data-scarce clinical environments.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [108] [Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures](https://arxiv.org/abs/2504.11750)
*Prabhu Vellaisamy,Thomas Labonte,Sourav Chakraborty,Matt Turner,Samantika Sury,John Paul Shen*

Main category: cs.DC

TLDR: 这篇论文分析了LLM推理工作负载在CPU-GPU耦合架构上的性能差异，突出了紧密耦合系统在大型批次下的优势，并提出优化建议。


<details>
  <summary>Details</summary>
Motivation: LLM推理工作负载越来越主导数据中心成本和资源利用，因此理解其在演进CPU-GPU耦合架构上的特性对优化至关重要。

Method: 通过细粒度的操作符到内核跟踪分析，使用新型分析器SKIP和指标TKLQT，分析松散耦合（PCIe A100/H100）和紧密耦合（GH200）系统的LLM推理行为。

Result: 紧密耦合GH200在大型批次下性能优异，预填充延迟快1.9x-2.7x；但在更大批次下仍受CPU限制，TKLQT能准确识别CPU/GPU绑定转折点。

Conclusion: 内核融合可降低GH200低批次延迟瓶颈，提供优化CPU-GPU耦合策略的洞见，并计划扩展到其他AI/DL工作负载。

Abstract: Large language model (LLM)-based inference workloads increasingly dominate
data center costs and resource utilization. Therefore, understanding the
inference workload characteristics on evolving CPU-GPU coupled architectures is
crucial for optimization. This paper presents an in-depth analysis of LLM
inference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled
(GH200) systems. We analyze performance dynamics using fine-grained
operator-to-kernel trace analysis, facilitated by our novel profiler SKIP and
metrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that
closely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)
systems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for
Llama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound
up to 4x larger batch sizes than LC systems. In this extended CPU-bound region,
we identify the performance characteristics of the Grace CPU as a key factor
contributing to higher inference latency at low batch sizes on GH200. We
demonstrate that TKLQT accurately identifies this CPU/GPU-bound transition
point. Based on this analysis, we further show that kernel fusion offers
significant potential to mitigate GH200's low-batch latency bottleneck by
reducing kernel launch overhead. This detailed kernel-level characterization
provides critical insights for optimizing diverse CPU-GPU coupling strategies.
This work is an initial effort, and we plan to explore other major AI/DL
workloads that demand different degrees of CPU-GPU heterogeneous architectures.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [109] [Learning Strategies in Particle Swarm Optimizer: A Critical Review and Performance Analysis](https://arxiv.org/abs/2504.11812)
*Dikshit Chauhan,Shivani,P. N. Suganthan*

Main category: cs.NE

TLDR: 这篇论文审阅并分类粒子群优化（PSO）的学习策略，评估其对性能的影响，通过实验比较分析搜索动态，并讨论未来挑战。


<details>
  <summary>Details</summary>
Motivation: 填补PSO学习策略缺乏全面分析的空白。

Method: 审阅分类策略、评估影响和比较实验。

Result: 实验显示策略对PSO搜索动态的影响。

Conclusion: 强调开发自适应PSO以应对复杂问题。

Abstract: Nature has long inspired the development of swarm intelligence (SI), a key
branch of artificial intelligence that models collective behaviors observed in
biological systems for solving complex optimization problems. Particle swarm
optimization (PSO) is widely adopted among SI algorithms due to its simplicity
and efficiency. Despite numerous learning strategies proposed to enhance PSO's
performance in terms of convergence speed, robustness, and adaptability, no
comprehensive and systematic analysis of these strategies exists. We review and
classify various learning strategies to address this gap, assessing their
impact on optimization performance. Additionally, a comparative experimental
evaluation is conducted to examine how these strategies influence PSO's search
dynamics. Finally, we discuss open challenges and future directions,
emphasizing the need for self-adaptive, intelligent PSO variants capable of
addressing increasingly complex real-world problems.

</details>

### [110] [EngramNCA: a Neural Cellular Automaton Model of Memory Transfer](https://arxiv.org/abs/2504.11855)
*Etienne Guichard,Felix Reimers,Mia Kvalsund,Mikkel Lepperød,Stefano Nichele*

Main category: cs.NE

TLDR: 这项研究引入了EngramNCA，一个整合了公开可见状态和私有内部记忆通道的神经元细胞自动机，灵感来源于生物学证据，支持复杂形态的生长和对记忆机制的理解。


<details>
  <summary>Details</summary>
Motivation: 受生物学证据启发，表明记忆存储不仅限于突触改变，还包括细胞内机制，旨在探索人工系统中去中心化的记忆存储。

Method: 提出EngramNCA模型，包括GeneCA（从种子细胞发展不同形态）和GenePropCA（调节私有遗传记忆而不改变可见状态）。

Result: 实现了复杂形态的编码和传播，支持分层和共存形态的出现，为人工系统的记忆存储提供洞见。

Conclusion: 这些发现对开发适应性自组织系统有潜在影响，并有助于理解生物和合成上下文中的记忆机制。

Abstract: This study introduces EngramNCA, a neural cellular automaton (NCA) that
integrates both publicly visible states and private, cell-internal memory
channels, drawing inspiration from emerging biological evidence suggesting that
memory storage extends beyond synaptic modifications to include intracellular
mechanisms. The proposed model comprises two components: GeneCA, an NCA trained
to develop distinct morphologies from seed cells containing immutable "gene"
encodings, and GenePropCA, an auxiliary NCA that modulates the private
"genetic" memory of cells without altering their visible states. This
architecture enables the encoding and propagation of complex morphologies
through the interaction of visible and private channels, facilitating the
growth of diverse structures from a shared "genetic" substrate. EngramNCA
supports the emergence of hierarchical and coexisting morphologies, offering
insights into decentralized memory storage and transfer in artificial systems.
These findings have potential implications for the development of adaptive,
self-organizing systems and may contribute to the broader understanding of
memory mechanisms in both biological and synthetic contexts.

</details>

### [111] [GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization](https://arxiv.org/abs/2504.11840)
*Huizhe Zhang,Jintang Li,Yuchang Zhu,Liang Chen,Zibin Zheng*

Main category: cs.NE

TLDR: 本文提出GT-SVQ，使用Spiking Neural Networks实现线性时间Graph Transformer，提高节点分类任务的效率和性能。


<details>
  <summary>Details</summary>
Motivation: Graph Transformers存在二次复杂度和高计算能耗问题，Spiking Neural Networks可降低开销，因此提出GT-SVQ以提升可扩展性。

Method: GT-SVQ基于Spiking Vector Quantization重建codebooks，并注入self-attention块，以线性复杂度聚合信息，并缓解codebook collapse问题。

Result: 在节点分类数据集上，GT-SVQ与基线相比，性能竞争性，推理速度提升高达130倍。

Conclusion: GT-SVQ证明了在保持性能的同时显著提高了Graph Transformers的效率和适用性。

Abstract: Graph Transformers (GTs), which simultaneously integrate message-passing and
self-attention mechanisms, have achieved promising empirical results in some
graph prediction tasks. Although these approaches show the potential of
Transformers in capturing long-range graph topology information, issues
concerning the quadratic complexity and high computing energy consumption
severely limit the scalability of GTs on large-scale graphs. Recently, as
brain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the
development of graph representation learning methods with lower computational
and storage overhead through the unique event-driven spiking neurons. Inspired
by these characteristics, we propose a linear-time Graph Transformer using
Spiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ
reconstructs codebooks based on rate coding outputs from spiking neurons, and
injects the codebooks into self-attention blocks to aggregate global
information in linear complexity. Besides, spiking vector quantization
effectively alleviates codebook collapse and the reliance on complex machinery
(distance measure, auxiliary loss, etc.) present in previous vector
quantization-based graph learning methods. In experiments, we compare GT-SVQ
with other state-of-the-art baselines on node classification datasets ranging
from small to large. Experimental results show that GT-SVQ has achieved
competitive performances on most datasets while maintaining up to 130x faster
inference speed compared to other GTs.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [112] [Towards Interpretable Deep Generative Models via Causal Representation Learning](https://arxiv.org/abs/2504.11609)
*Gemma E. Moran,Bryon Aragam*

Main category: stat.ML

TLDR: 这篇论文从统计学视角审视因果表示学习（CRL），强调其解决AI解释性问题的重要性，并讨论其与经典模型的联系、识别性结果、应用领域和开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络的解释性难题，通过构建基于因果性的可解释生成AI模型。

Method: 采用CRL方法，结合潜变量模型、因果图形模型和非参数统计学，审视其进展。

Result: 回顾CRL的最新进展，包括统计和因果识别性结果、关键应用领域和实施策略。

Conclusion: 强调CRL在生成AI中的潜力，并指出统计学领域的开放问题。

Abstract: Recent developments in generative artificial intelligence (AI) rely on
machine learning techniques such as deep learning and generative modeling to
achieve state-of-the-art performance across wide-ranging domains. These
methods' surprising performance is due in part to their ability to learn
implicit "representations'' of complex, multi-modal data. Unfortunately, deep
neural networks are notoriously black boxes that obscure these representations,
making them difficult to interpret or analyze. To resolve these difficulties,
one approach is to build new interpretable neural network models from the
ground up. This is the goal of the emerging field of causal representation
learning (CRL) that uses causality as a vector for building flexible,
interpretable, and transferable generative AI. CRL can be seen as a culmination
of three intrinsically statistical problems: (i) latent variable models such as
factor analysis; (ii) causal graphical models with latent variables; and (iii)
nonparametric statistics and deep learning. This paper reviews recent progress
in CRL from a statistical perspective, focusing on connections to classical
models and statistical and causal identifiablity results. This review also
highlights key application areas, implementation strategies, and open
statistical questions in CRL.

</details>

### [113] [FEAT: Free energy Estimators with Adaptive Transport](https://arxiv.org/abs/2504.11516)
*Jiajun He,Yuanqi Du,Francisco Vargas,Yuanqing Wang,Carla P. Gomes,José Miguel Hernández-Lobato,Eric Vanden-Eijnden*

Main category: stat.ML

TLDR: FEAT 是一个新框架，用于自由能估计，通过学习传输统一方法，并展示了改进的性能。


<details>
  <summary>Details</summary>
Motivation: 自由能估计是跨科学领域的关键挑战，需要统一平衡和非平衡方法。

Method: FEAT 使用随机插值实现的学习传输，基于护送 Jarzynski 等式和控制 Crooks 定理，提供最小方差估计器和变分界限。

Result: 实验在玩具例子、分子模拟和量子场论上验证，显示了比现有学习方法更好的改进。

Conclusion: FEAT 建立了神经自由能计算的原理基础，证明了其有效性。

Abstract: We present Free energy Estimators with Adaptive Transport (FEAT), a novel
framework for free energy estimation -- a critical challenge across scientific
domains. FEAT leverages learned transports implemented via stochastic
interpolants and provides consistent, minimum-variance estimators based on
escorted Jarzynski equality and controlled Crooks theorem, alongside
variational upper and lower bounds on free energy differences. Unifying
equilibrium and non-equilibrium methods under a single theoretical framework,
FEAT establishes a principled foundation for neural free energy calculations.
Experimental validation on toy examples, molecular simulations, and quantum
field theory demonstrates improvements over existing learning-based methods.

</details>

### [114] [Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations](https://arxiv.org/abs/2504.11554)
*Chengkun Li,Bobby Huggins,Petrus Mikkola,Luigi Acerbi*

Main category: stat.ML

TLDR: 提出NFR方法高效近似Bayesian后验分布，适用于计算密集型推理场景。


<details>
  <summary>Details</summary>
Motivation: Bayesian推理中，计算成本高的似然评估是科学领域的主要挑战。

Method: 引入normalizing flow regression (NFR)，通过回归现有log-density评估直接获得可处理的后验近似，并优化训练技巧如定制先验和似然函数。

Result: 在合成基准和神经科学、生物学应用中，NFR性能优越或相当。

Conclusion: NFR是为计算上不可行或可回收模型评估的Bayesian推理提供的一种有前景方法。

Abstract: Bayesian inference with computationally expensive likelihood evaluations
remains a significant challenge in many scientific domains. We propose
normalizing flow regression (NFR), a novel offline inference method for
approximating posterior distributions. Unlike traditional surrogate approaches
that require additional sampling or inference steps, NFR directly yields a
tractable posterior approximation through regression on existing log-density
evaluations. We introduce training techniques specifically for flow regression,
such as tailored priors and likelihood functions, to achieve robust posterior
and model evidence estimation. We demonstrate NFR's effectiveness on synthetic
benchmarks and real-world applications from neuroscience and biology, showing
superior or comparable performance to existing methods. NFR represents a
promising approach for Bayesian inference when standard methods are
computationally prohibitive or existing model evaluations can be recycled.

</details>

### [115] [Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations](https://arxiv.org/abs/2504.11610)
*Tianjian Yang,Wei Vivian Li*

Main category: stat.ML

TLDR: GPCCA 是一种处理多模态数据整合的新方法，能应对缺失值并提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据量增大，尤其在生物信息学领域，需要模型整合不同模态、处理缺失值并提高聚类准确性。

Method: 提出 GPCCA 无监督方法，用于多模态数据的整合和降维，处理缺失值、支持多个模态并考虑内部相关性。

Result: GPCCA 对缺失数据模式鲁棒，提供低维嵌入，模拟中优于现有方法，并应用于 TCGA 癌症和多视图图像数据集。

Conclusion: GPCCA 提供有效框架处理多模态数据整合和缺失值，具有广泛应用潜力，并发布 R 包以便使用。

Abstract: Background: The integration and analysis of multi-modal data are increasingly
essential across various domains including bioinformatics. As the volume and
complexity of such data grow, there is a pressing need for computational models
that not only integrate diverse modalities but also leverage their
complementary information to improve clustering accuracy and insights,
especially when dealing with partial observations with missing data. Results:
We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an
unsupervised method for the integration and joint dimensionality reduction of
multi-modal data. GPCCA addresses key challenges in multi-modal data analysis
by handling missing values within the model, enabling the integration of more
than two modalities, and identifying informative features while accounting for
correlations within individual modalities. The model demonstrates robustness to
various missing data patterns and provides low-dimensional embeddings that
facilitate downstream clustering and analysis. In a range of simulation
settings, GPCCA outperforms existing methods in capturing essential patterns
across modalities. Additionally, we demonstrate its applicability to
multi-omics data from TCGA cancer datasets and a multi-view image dataset.
Conclusion: GPCCA offers a useful framework for multi-modal data integration,
effectively handling missing data and providing informative low-dimensional
embeddings. Its performance across cancer genomics and multi-view image data
highlights its robustness and potential for broad application. To make the
method accessible to the wider research community, we have released an R
package, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.

</details>

### [116] [Discrimination-free Insurance Pricing with Privatized Sensitive Attributes](https://arxiv.org/abs/2504.11775)
*Tianhe Zhang,Suhan Liu,Peng Shi*

Main category: stat.ML

TLDR: 本文提出一种使用私有化敏感属性的方法，确保保险定价公平性，并符合监管要求。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的公平性重要，但保险领域有独特挑战，标准公平概念不直接适用，且面临监管透明度和敏感属性限制。

Method: 提出高效方法，使用私有化敏感属性构建公平模型，确保统计保证、不直接访问敏感属性，并适应不同透明度要求。

Result: 方法解决了保险定价的复杂性，符合监管需求，并确保公平性。

Conclusion: 该方法实现了保险定价的公平性，同时满足监管透明性和限制要求。

Abstract: Fairness has emerged as a critical consideration in the landscape of machine
learning algorithms, particularly as AI continues to transform decision-making
across societal domains. To ensure that these algorithms are free from bias and
do not discriminate against individuals based on sensitive attributes such as
gender and race, the field of algorithmic bias has introduced various fairness
concepts, along with methodologies to achieve these notions in different
contexts. Despite the rapid advancement, not all sectors have embraced these
fairness principles to the same extent. One specific sector that merits
attention in this regard is insurance. Within the realm of insurance pricing,
fairness is defined through a distinct and specialized framework. Consequently,
achieving fairness according to established notions does not automatically
ensure fair pricing in insurance. In particular, regulators are increasingly
emphasizing transparency in pricing algorithms and imposing constraints on
insurance companies on the collection and utilization of sensitive consumer
attributes. These factors present additional challenges in the implementation
of fairness in pricing algorithms. To address these complexities and comply
with regulatory demands, we propose an efficient method for constructing fair
models that are tailored to the insurance domain, using only privatized
sensitive attributes. Notably, our approach ensures statistical guarantees,
does not require direct access to sensitive attributes, and adapts to varying
transparency requirements, addressing regulatory demands while ensuring
fairness in insurance pricing.

</details>

### [117] [Approximation Bounds for Transformer Networks with Application to Regression](https://arxiv.org/abs/2504.12175)
*Yuling Jiao,Yanming Lai,Defeng Sun,Yang Wang,Bokai Yan*

Main category: stat.ML

TLDR: 本论文探讨Transformer网络对Hölder和Sobolev函数的逼近能力，并应用于非参数回归估计，建立了逼近误差上界和收敛率。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在函数逼近和处理依赖观测回归中的能力。

Method: 建立逼近上界、推导回归收敛率，并使用Kolmogorov-Arnold表示定理的证明策略。

Result: Transformer参数规模为ε^{-d_x n / γ}，适用于不同p值，并给出无权重约束的收敛率和self-attention解释。

Conclusion: 增强了对Transformer逼近能力和self-attention机制的理解。

Abstract: We explore the approximation capabilities of Transformer networks for
H\"older and Sobolev functions, and apply these results to address
nonparametric regression estimation with dependent observations. First, we
establish novel upper bounds for standard Transformer networks approximating
sequence-to-sequence mappings whose component functions are H\"older continuous
with smoothness index $\gamma \in (0,1]$. To achieve an approximation error
$\varepsilon$ under the $L^p$-norm for $p \in [1, \infty]$, it suffices to use
a fixed-depth Transformer network whose total number of parameters scales as
$\varepsilon^{-d_x n / \gamma}$. This result not only extends existing findings
to include the case $p = \infty$, but also matches the best known upper bounds
on number of parameters previously obtained for fixed-depth FNNs and RNNs.
Similar bounds are also derived for Sobolev functions. Second, we derive
explicit convergence rates for the nonparametric regression problem under
various $\beta$-mixing data assumptions, which allow the dependence between
observations to weaken over time. Our bounds on the sample complexity impose no
constraints on weight magnitudes. Lastly, we propose a novel proof strategy to
establish approximation bounds, inspired by the Kolmogorov-Arnold
representation theorem. We show that if the self-attention layer in a
Transformer can perform column averaging, the network can approximate
sequence-to-sequence H\"older functions, offering new insights into the
interpretability of self-attention mechanisms.

</details>

### [118] [Leave-One-Out Stable Conformal Prediction](https://arxiv.org/abs/2504.12189)
*Kiljae Lee,Yuan Zhang*

Main category: stat.ML

TLDR: 本文提出LOO-StabCP方法，旨在加速共形预测，提高计算效率，同时保持预测准确性，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决共形预测中计算效率与预测准确性的平衡挑战，特别是针对多个预测请求。

Method: 提出基于leave-one-out稳定性的LOO-StabCP方法，不依赖样本分割；为RLM、SGD、核方法、神经网络和bagging导出了稳定性界。

Result: 方法理论上得到证明，在合成和真实数据上表现优异；在筛选问题中，测试功率优于基于分割共形的state-of-the-art方法。

Conclusion: LOO-StabCP方法有效利用训练数据，提高了性能，展示了在实际应用中的优势。

Abstract: Conformal prediction (CP) is an important tool for distribution-free
predictive uncertainty quantification. Yet, a major challenge is to balance
computational efficiency and prediction accuracy, particularly for multiple
predictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),
a novel method to speed up full conformal using algorithmic stability without
sample splitting. By leveraging leave-one-out stability, our method is much
faster in handling a large number of prediction requests compared to existing
method RO-StabCP based on replace-one stability. We derived stability bounds
for several popular machine learning tools: regularized loss minimization (RLM)
and stochastic gradient descent (SGD), as well as kernel method, neural
networks and bagging. Our method is theoretically justified and demonstrates
superior numerical performance on synthetic and real-world data. We applied our
method to a screening problem, where its effective exploitation of training
data led to improved test power compared to state-of-the-art method based on
split conformal.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [119] [A Framework for the Private Governance of Frontier Artificial Intelligence](https://arxiv.org/abs/2504.11501)
*Dean W. Ball*

Main category: cs.CY

TLDR: 本文提出一种混合公私系统治理前沿AI的提案，私营机构在政府监督下提供可选认证，换取开发者责任保护，并分析其他治理方法及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 探讨常见AI治理方法的优缺点，并提出新系统以改善AI治理，考虑政治经济、机构、法律、安全等因素。

Method: 提出混合公私治理框架：私营机构获政府授权，为前沿AI开发者提供可选认证，开发者换取免除客户误用模型的责任。

Result: 分析了多种AI治理方法的优缺点，并评估提案系统的优缺点，包括安全和其他方面的权衡。

Conclusion: 提案提供一种平衡的AI治理方式，强调通过公私合作提升安全性和责任机制。

Abstract: This paper presents a proposal for the governance of frontier AI systems
through a hybrid public-private system. Private bodies, authorized and overseen
by government, provide certifications to developers of frontier AI systems on
an opt-in basis. In exchange for opting in, frontier AI firms receive
protections from tort liability for customer misuse of their models. Before
detailing the proposal, the paper explores more commonly discussed approaches
to AI governance, analyzing their strengths and flaws. It also examines the
nature of frontier AI governance itself. The paper includes consideration of
the political economic, institutional, legal, safety, and other merits and
tradeoffs inherent in the governance system it proposes.

</details>

### [120] [Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI](https://arxiv.org/abs/2504.11564)
*Lee Ackerman*

Main category: cs.CY

TLDR: 本研究探讨组织如何在代理式AI兴起中感知和适应负责任AI框架，发现了适应挑战，包括知识缺口和控制焦点。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统快速获得自治性，迫切需要稳健的负责任AI框架来应对新兴挑战。

Method: 采用解释性定性方法，探索AI专业人员的亲身经历。

Result: 发现代理式AI的复杂性导致组织适应困难，表现为知识缺口、有限的利益相关者参与和对控制的强调。

Conclusion: 这些因素阻碍了有效实施，损害了负责任AI的潜力和ROI的实现。

Abstract: As artificial intelligence (AI) systems rapidly gain autonomy, the need for
robust responsible AI frameworks becomes paramount. This paper investigates how
organizations perceive and adapt such frameworks amidst the emerging landscape
of increasingly sophisticated agentic AI. Employing an interpretive qualitative
approach, the study explores the lived experiences of AI professionals.
Findings highlight that the inherent complexity of agentic AI systems and their
responsible implementation, rooted in the intricate interconnectedness of
responsible AI dimensions and the thematic framework (an analytical structure
developed from the data), combined with the novelty of agentic AI, contribute
to significant challenges in organizational adaptation, characterized by
knowledge gaps, a limited emphasis on stakeholder engagement, and a strong
focus on control. These factors, by hindering effective adaptation and
implementation, ultimately compromise the potential for responsible AI and the
realization of ROI.

</details>

### [121] [Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets](https://arxiv.org/abs/2504.11504)
*Woojin Kim,Hyeoncheol Kim*

Main category: cs.CY

TLDR: 本论文探讨了counterfactual fairness在教育数据中的应用，通过分析基准数据集，展示了其对算法公平性的洞见。


<details>
  <summary>Details</summary>
Motivation: 机器学习在教育中的应用引发算法偏见和公平性担忧，特别是个体公平尤其是counterfactual fairness尚未充分研究。

Method: 对基准教育数据集进行counterfactual fairness分析。

Result: 证明了counterfactual fairness能提供敏感属性因果性和基于因果的个体公平性的有意义洞见。

Conclusion: counterfactual fairness在教育中对算法公平性具有重要意义。

Abstract: As machine learning models are increasingly used in educational settings,
from detecting at-risk students to predicting student performance, algorithmic
bias and its potential impacts on students raise critical concerns about
algorithmic fairness. Although group fairness is widely explored in education,
works on individual fairness in a causal context are understudied, especially
on counterfactual fairness. This paper explores the notion of counterfactual
fairness for educational data by conducting counterfactual fairness analysis of
machine learning models on benchmark educational datasets. We demonstrate that
counterfactual fairness provides meaningful insight into the causality of
sensitive attributes and causal-based individual fairness in education.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [122] [Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels](https://arxiv.org/abs/2504.11667)
*Cemil Vahapoglu,Timothy J. O'Shea,Wan Liu,Tamoghna Roy,Sennur Ulukus*

Main category: cs.IT

TLDR: 本文提出了一种新型的无监督深度学习框架，结合深度可分离卷积和Transformer，用于在不完美CSI条件下生成MU-SIMO系统的波束形成权重，并展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有文献中充斥着使用Transformer架构的方法，因为它们在无线信号处理中突出，并能通过注意力机制捕获长程依赖。深度可分离卷积提高了参数效率，适用于MIMO系统的高维数据。本文旨在通过最大化总速率来提高密集城市环境中的吞吐量，同时确保可靠通信。

Method: 引入了一种新型的无监督深度学习框架，将深度可分离卷积和Transformer相结合，用于在不完美信道状态信息（CSI）下为多用户单输入多输出（MU-SIMO）系统生成波束形成权重。

Result: 实验结果表明，所提出框架在光谱效率和块错误率（BLER）方面优于基线方法零强迫波束形成（ZFBF）和最小均方误差（MMSE）波束形成。

Conclusion: 所提出框架通过最大化总速率提高了吞吐量，并确保了可靠通信，实验证明其优于传统方法。

Abstract: The literature is abundant with methodologies focusing on using transformer
architectures due to their prominence in wireless signal processing and their
capability to capture long-range dependencies via attention mechanisms. In
particular, depthwise separable convolutions enhance parameter efficiency for
the process of high-dimensional data characteristics of MIMO systems. In this
work, we introduce a novel unsupervised deep learning framework that integrates
depthwise separable convolutions and transformers to generate beamforming
weights under imperfect channel state information (CSI) for a multi-user
single-input multiple-output (MU-SIMO) system in dense urban environments. The
primary goal is to enhance throughput by maximizing sum-rate while ensuring
reliable communication. Spectral efficiency and block error rate (BLER) are
considered as performance metrics. Experiments are carried out under various
conditions to compare the performance of the proposed NNBF framework against
baseline methods zero-forcing beamforming (ZFBF) and minimum mean square error
(MMSE) beamforming. Experimental results demonstrate the superiority of the
proposed framework over the baseline techniques.

</details>

<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [123] [Optimal flock formation induced by agent heterogeneity](https://arxiv.org/abs/2504.12297)
*Arthur N. Montanari,Ana Elisa D. Barioni,Chao Duan,Adilson E. Motter*

Main category: cond-mat.dis-nn

TLDR: 本文研究代理异质性对群聚动力学的影响，发现异质代理收敛更快，提升集体行为。


<details>
  <summary>Details</summary>
Motivation: 挑战传统假设，探讨异质性如何提升群聚稳定性和收敛性，促进集体行为。

Method: 通过比较异质和同质代理在目标跟踪、群聚形成和障碍规避等任务中的性能，优化异质参数。

Result: 异质代理收敛速度提高20-40%，在通信延迟下也能收敛，而同质代理可能不稳定。

Conclusion: 异质性是促进集体行为的适应性机制，挑战多代理控制的现有范式。

Abstract: The study of flocking in biological systems has identified conditions for
self-organized collective behavior, inspiring the development of decentralized
strategies to coordinate the dynamics of swarms of drones and other autonomous
vehicles. Previous research has focused primarily on the role of the
time-varying interaction network among agents while assuming that the agents
themselves are identical or nearly identical. Here, we depart from this
conventional assumption to investigate how inter-individual differences between
agents affect the stability and convergence in flocking dynamics. We show that
flocks of agents with optimally assigned heterogeneous parameters significantly
outperform their homogeneous counterparts, achieving 20-40% faster convergence
to desired formations across various control tasks. These tasks include target
tracking, flock formation, and obstacle maneuvering. In systems with
communication delays, heterogeneity can enable convergence even when flocking
is unstable for identical agents. Our results challenge existing paradigms in
multi-agent control and establish system disorder as an adaptive, distributed
mechanism to promote collective behavior in flocking dynamics.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [124] [Multi-Agent Reinforcement Learning for Decentralized Reservoir Management via Murmuration Intelligence](https://arxiv.org/abs/2504.11569)
*Heming Fu,Guojun Xiong,Jian Li,Shan Lin*

Main category: eess.SY

TLDR: MurmuRL 是一种受鸟群启发的去中心化水管理系统，使用多代理强化学习提高效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式水管理系统存在计算复杂性和不确定性传播的限制，需要开发更有效的去中心化框架。

Method: 整合生物启发的对齐、分离和凝聚规则与多代理强化学习，创建 MurmuRL 框架，实现水库自主决策和全局协调。

Result: 实验显示 MurmuRL 比集中式方法提高 8.8% 性能，减少 27% 计算开销；战略多样性超线性增长，增强极端事件弹性。

Conclusion: MurmuRL 提供可扩展解决方案，通过自然集体行为原则管理复杂水系统。

Abstract: Conventional centralized water management systems face critical limitations
from computational complexity and uncertainty propagation. We present MurmuRL,
a novel decentralized framework inspired by starling murmurations intelligence,
integrating bio-inspired alignment, separation, and cohesion rules with
multi-agent reinforcement learning. MurmuRL enables individual reservoirs to
make autonomous local decisions while achieving emergent global coordination.
Experiments on grid networks demonstrate that MurmuRL achieves 8.8% higher
final performance while using 27% less computing overhead compared to
centralized approaches. Notably, strategic diversity scales super-linearly with
system size, exhibiting sophisticated coordination patterns and enhanced
resilience during extreme events. MurmuRL offers a scalable solution for
managing complex water systems by leveraging principles of natural collective
behavior.

</details>

### [125] [Provably Safe Control for Constrained Nonlinear Systems with Bounded Input](https://arxiv.org/abs/2504.11592)
*Saurabh Kumar,Shashi Ranjan Kumar,Abhinav Sinha*

Main category: eess.SY

TLDR: 本文提出非线性系统约束控制设计，确保安全输出跟踪。


<details>
  <summary>Details</summary>
Motivation: 实际控制中忽略执行器和输出约束可能导致不稳定，本文旨在解决这些问题。

Method: 提出平滑非对称饱和模型，并开发非线性控制框架，保证跟踪并满足约束。

Result: 实现跟踪误差稳定和信号有界，模拟验证有效。

Conclusion: 方法处理非对称约束，达到良好跟踪性能。

Abstract: In real-world control applications, actuator constraints and output
constraints (specifically in tracking problems) are inherent and critical to
ensuring safe and reliable operation. However, generally, control strategies
often neglect these physical limitations, leading to potential instability,
degraded performance, or even system failure when deployed on real-world
systems. This paper addresses the control design problem for a class of
nonlinear systems under both actuator saturation and output constraints. First,
a smooth asymmetric saturation model (a more generic representative of
practical scenarios) is proposed to model actuator saturation, which ensures
that the control inputs always remain confined within a predefined set to
ensure safety. Based on the proposed model, we develop a nonlinear control
framework that guarantees output tracking while ensuring that system output
remains confined to the predefined set. Later, we integrate this design with
the constrained output tracking control problem, wherein we show that the
system output tracks its desired trajectory by simultaneously satisfying input
and output constraints. The global stabilization of the tracking error is
achieved in the presence of input constraints, while semi-global stabilization
is achieved in the presence of both input and output constraints. Additionally,
we rigorously establish the boundedness of all closed-loop signals under the
proposed design. Simulation results demonstrate the effectiveness of the
proposed methods in handling asymmetric constraints while achieving desirable
tracking performance.

</details>

### [126] [Comprehensive Signal Modeling for Talkative Power Conversion](https://arxiv.org/abs/2504.11607)
*Jan Mietzner,Cerikh Chakraborty,Peter A. Hoeher,Lutz Lampe*

Main category: eess.SY

TLDR: This paper derives signal models for talkative power conversion in DC/DC converters to enable better analysis and simulations.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on talkative power conversion, a comprehensive signal modeling for analysis and simulations is still lacking.

Method: Deriving continuous-time and discrete-time signal models for various modulation schemes, assessing their accuracies, and developing a generic end-to-end model.

Result: Accurate signal models are created, with discussions on equalization implications and generalizations to parasitic effects and impedance loads.

Conclusion: The models provide a solid foundation for advancing the field of talkative power conversion through improved analysis and simulation capabilities.

Abstract: Talkative power conversion is a switching ripple communication technique that
integrates data modulation into a switched-mode power electronics converter,
enabling simultaneous information transmission and power conversion. Despite
numerous research papers published over the last decade on various theoretical
and practical aspects of this emerging topic, thorough signal modeling suitable
for analysis and computer simulations is still lacking. In this article, we
derive the continuous-time output voltage of a DC/DC switched-mode power
electronics converter for a broad range of pulsed-based modulation schemes. We
also develop corresponding discrete-time signal models and assess their
accuracies. Finally, we devise a generic end-to-end signal model for arbitrary
modulation signals, discuss implications of continuous-time and discrete-time
signal modeling on equalization, and consider generalizations to include
parasitic effects as well as the influence of general impedance loads.

</details>

### [127] [Verifiable Mission Planning For Space Operations](https://arxiv.org/abs/2504.11631)
*Quentin Rommel,Michael Hibbard,Pavan Shukla,Himanshu Save,Srinivas Bettadpur,Ufuk Topcu*

Main category: eess.SY

TLDR: 本论文开发了一种基于有限时域马尔可夫决策过程的概率方法，用于优化航天器操作规划，同时确保安全保证。


<details>
  <summary>Details</summary>
Motivation: 随着太空任务复杂度增加，规划方法需最大化任务性能并严格执行安全措施。

Method: 采用概率方法，建立有限时域马尔可夫决策过程模型，状态捕捉任务参数，行动表示操作调整，直接纳入不确定性计算最优行动序列。

Result: 在GRACE-FO任务的数值实验中，展示了在不确定性下的鲁棒性能，并提供了概率安全保证。

Conclusion: 为自主航天器操作提供可靠解决方案。

Abstract: As space missions become more complex, planning methods must maximize mission
performance while rigorously enforcing safety. We develop a probabilistic
approach based on a finite-horizon Markov decision process to optimize
spacecraft operations planning with safety guarantees. In the model, states
capture essential mission parameters, and actions represent the operational
adjustments needed to meet mission objectives. By directly incorporating
uncertainties from environmental conditions and spacecraft dynamics, an optimal
sequence of actions is computed that maximizes expected rewards and strictly
enforces safety constraints. Numerical experiments on the GRACE-FO mission
demonstrate robust performance under uncertainties while providing
probabilistic safety guarantees, offering a reliable solution for autonomous
spacecraft operations.

</details>

### [128] [Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids](https://arxiv.org/abs/2504.11650)
*Shengyuan Yan,Farzad Vazinram,Zeynab Kaseb,Lindsay Spoor,Jochen Stiasny,Betul Mamudi,Amirhossein Heydarian Ardakani,Ugochukwu Orji,Pedro P. Vergara,Yu Xiang,Jerry Guo*

Main category: eess.SY

TLDR: 本文提出改进牛顿-拉夫森方法初始化的策略，以处理现代电力电网收敛问题。


<details>
  <summary>Details</summary>
Motivation: 电力电网运行接近容量极限，病态情况和收敛挑战增加，尤其是可再生能源渗透率高时。

Method: 探索三种方法：(i) 使用电压数学边界的分析方法，(ii) 基于监督学习或物理信息神经网络的数据驱动模型，(iii) 强化学习调整电压。

Result: 实验显示所有方法均能提供更好初始猜测，使牛顿-拉夫森方法以更少步数收敛。

Conclusion: 研究为高效实时电网操作提供途径，支持向更智能和坚韧电力网络转型。

Abstract: Power flow (PF) calculations are fundamental to power system analysis to
ensure stable and reliable grid operation. The Newton-Raphson (NR) method is
commonly used for PF analysis due to its rapid convergence when initialized
properly. However, as power grids operate closer to their capacity limits,
ill-conditioned cases and convergence issues pose significant challenges. This
work, therefore, addresses these challenges by proposing strategies to improve
NR initialization, hence minimizing iterations and avoiding divergence. We
explore three approaches: (i) an analytical method that estimates the basin of
attraction using mathematical bounds on voltages, (ii) Two data-driven models
leveraging supervised learning or physics-informed neural networks (PINNs) to
predict optimal initial guesses, and (iii) a reinforcement learning (RL)
approach that incrementally adjusts voltages to accelerate convergence. These
methods are tested on benchmark systems. This research is particularly relevant
for modern power systems, where high penetration of renewables and
decentralized generation require robust and scalable PF solutions. In
experiments, all three proposed methods demonstrate a strong ability to provide
an initial guess for Newton-Raphson method to converge with fewer steps. The
findings provide a pathway for more efficient real-time grid operations, which,
in turn, support the transition toward smarter and more resilient electricity
networks.

</details>

### [129] [Reachability Analysis of Nonlinear Discrete-Time Systems Using Polyhedral Relaxations and Constrained Zonotopes](https://arxiv.org/abs/2504.11663)
*Brenner S. Rego,Guilherme V. Raffo,Marco H. Terra,Joseph K. Scott*

Main category: eess.SY

TLDR: 本论文提出一种新算法，用于非线性离散时间系统的可达性分析，通过结合约束区域和多面体松弛，提供比线性化方法更好的近似。


<details>
  <summary>Details</summary>
Motivation: 动机是克服保守线性化技术的局限性，提高非线性函数的可达性分析准确性。

Method: 方法是将约束区域（CZs）与因子表示的多面体松弛相结合，通过非线性函数传播CZs，而非使用传统线性化。

Result: 结果显示，计算可达集有显著改进，数值例子验证了其优势。

Conclusion: 结论是新算法在性能上优于现有CZ方法。

Abstract: This paper presents a novel algorithm for reachability analysis of nonlinear
discrete-time systems. The proposed method combines constrained zonotopes (CZs)
with polyhedral relaxations of factorable representations of nonlinear
functions to propagate CZs through nonlinear functions, which is normally done
using conservative linearization techniques. The new propagation method
provides better approximations than those resulting from linearization
procedures, leading to significant improvements in the computation of reachable
sets in comparison to other CZ methods from the literature. Numerical examples
highlight the advantages of the proposed algorithm.

</details>

### [130] [Optimal SVI-Weighted PSPS Decisions with Decision-Dependent Outage Uncertainty](https://arxiv.org/abs/2504.11665)
*Ryan Greenough,Kohei Murakami,Jan Kleissl,Adil Khurram*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于分布鲁棒优化的方法来优化公共安全电力关闭（PSPS），以平衡野火风险和停电风险。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑决策相关的野火驱动故障概率，导致建模复杂和高阶多项式项。

Method: 使用分布整形技术开发高效的MILP问题表示，并基于作者先前工作构建野火风险模型，采用日ahead单位承诺和线路去能化框架。

Result: 在IEEE RTS 24-bus测试系统中进行模拟，评估不同分布鲁棒性水平下总成本和野火风险的权衡。

Conclusion: 该方法有效地解决了PSPS问题，并展示了风险管理策略的trade-off。

Abstract: Public Safety Power Shutoffs (PSPS) are a pre-emptive strategy to mitigate
the wildfires caused by power system malfunction. System operators implement
PSPS to balance wildfire mitigation efforts through de-energization of
transmission lines against the risk of widespread blackouts modeled with load
shedding.
  Existing approaches do not incorporate decision-dependent wildfire-driven
failure probabilities, as modeling outage scenario probabilities requires
incorporating high-order polynomial terms in the objective. This paper uses
distribution shaping to develop an efficient MILP problem representation of the
distributionally robust PSPS problem. Building upon the author's prior work,
the wildfire risk of operating a transmission line is a function of the
probability of a wildfire-driven outage and its subsequent expected impact in
acres burned.
  A day-ahead unit commitment and line de-energization PSPS framework is used
to assess the trade-off between total cost and wildfire risk at different
levels of distributional robustness, parameterized by a level of distributional
dissimilarity $\kappa$. We perform simulations on the IEEE RTS 24-bus test
system.

</details>

### [131] [Analysis of Power Swing Characteristics of Grid-Forming VSC System Considering the Current Limitation Mode](https://arxiv.org/abs/2504.11797)
*Yongxin Xiong,Heng Wu,Yifei Li,Xiongfei Wang*

Main category: eess.SY

TLDR: 这篇论文研究了网格形成电压源转换器（GFM-VSC）系统的功率摆动特性，考虑电流限制模式，并通过模拟验证了其行为差异。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨GFM-VSC系统在电网故障后的行为与传统同步发电机系统的差异，以改进现代电力系统分析。

Method: 方法包括理论分析和使用PSCAD/EMTDC平台的模拟，涵盖单机和多机系统场景。

Result: 结果显示，非惯性GFM-VSC系统可能重新同步但伴随显著功率摆动，惯性系统可能失去同步，并评估了传统阻抗-based摆动检测方案的性能。

Conclusion: 结论强调了针对GFM-VSC系统的改进摆动检测方案的必要性，以适应其独特动态行为。

Abstract: This paper investigates power swing characteristics of grid-forming voltage
source converter (GFM-VSC) systems considering the current limitation mode in
both non-inertial and inertial GFM-VSC systems. Following grid faults,
non-inertial GFM-VSC systems can re-synchronize with the grid but may
experience significant power swings driven by its control dynamics, while
inertial GFM-VSC systems may exhibit loss of synchronization (LOS),
characterized by the divergence of the output angle in the active power control
loop. These behaviours are different from conventional synchronous generator
(SG)-based systems, where power swings are typically characterized by physical
angle deviations among power sources. Based on these findings, this paper
explores the performance of traditional impedance-based swing detection schemes
in GFM-VSC systems. The theoretical analysis is validated through various
simulations using the PSCAD/EMTDC platform, covering both single and
multi-machine system scenarios.

</details>

### [132] [A Koopman Operator Approach to Data-Driven Control of Semilinear Parabolic Systems](https://arxiv.org/abs/2504.11959)
*Joachim Deutscher,Tarik Enderes*

Main category: eess.SY

TLDR: 本文提出数据驱动方法稳定未知边界控制的半线性抛物线系统，通过Koopman算子特征泛函提升动力学，创建有限维模型，并应用反馈线性化。


<details>
  <summary>Details</summary>
Motivation: 为了处理未知动力学的半线性抛物线系统，使用数据驱动方法克服非线性性和不确定性的挑战。

Method: 使用Koopman算子特征泛函提升系统动力学，进行双线性化，应用反馈线性化，并通过广义eDMD基于状态数据设计控制器。

Result: 验证了闭环系统在计算误差下的指数稳定性，并通过不稳定反应-扩散系统示例演示了方法有效性。

Conclusion: 数据驱动方法成功稳定了系统，并揭示了特征泛函分配与反馈线性化之间的全新联系。

Abstract: This paper is concerned with the data-driven stabilization of unknown
boundary controlled semilinear parabolic systems. The nonlinear dynamics of the
system are lifted using a finite number of eigenfunctionals of the Koopman
operator related to the autonomous semilinear PDE. This results in a novel
data-driven finite-dimensional model of the lifted dynamics, which is amenable
to apply design procedures for finite-dimensional systems to stabilize the
semilinear parabolic system. In order to facilitate this, a bilinearization of
the lifted dynamics is considered and feedback linearization is applied for the
data-driven stabilization of the semilinear parabolic PDE. This reveals a novel
connection between the assignment of eigenfunctionals to the closed-loop
Koopman operator and feedback linearization. By making use of a modal
representation, exponential stability of the closed-loop system in the presence
of errors resulting from the data-driven computation of eigenfunctionals and
the bilinearization is verified. The data-driven controller directly follows
from applying generalized eDMD to state data available for the semilinear
parabolic PDE. An example of an unstable semilinear reaction-diffusion system
with finite-time blow up demonstrates the novel data-driven stabilization
approach.

</details>

### [133] [Power Line Communication vs. Talkative Power Conversion: A Benchmarking Study](https://arxiv.org/abs/2504.12015)
*Peter A. Hoeher,Yang Leng,Maximilian Mewis,Rongwu Zhu*

Main category: eess.SY

TLDR: This paper compares Power Line Communications (PLC) and Talkative Power Conversion (TPC) for data communication in energy systems, highlighting their strengths, weaknesses, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the convergence of energy transmission and data communication in decentralized energy systems by comparing PLC and TPC approaches.

Method: Comparison of technical foundations, applications, and benchmarking of strengths and bottlenecks between PLC and TPC.

Result: Identification of strengths and bottlenecks of PLC and TPC, and outlining future research directions for TPC to integrate power and communication technologies.

Conclusion: TPC has potential to bridge the gap between power and communication technologies, with suggestions for future research.

Abstract: The convergence of energy transmission and data communication has become a
key feature of decentralized energy systems across a broad spectrum of
voltage/power ranges, including smart grid applications and cyber-physical
power systems. This paper compares two distinct approaches: Power Line
Communications (PLC) and Talkative Power Conversion (TPC). While PLC leverages
existing power infrastructure for data transmission by using external data
transmitters and receivers, TPC integrates communication capabilities directly
into power electronic converters. We present their technical foundations and
applications, benchmark their strengths and bottlenecks, and outline future
research directions regarding TPC that could bridge the gap between power and
communication technologies.

</details>

### [134] [Contract-based hierarchical control using predictive feasibility value functions](https://arxiv.org/abs/2504.12036)
*Felix Berkel,Kim Peter Wabersich,Hongxi Xiang,Elias Milios*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于合同的层次控制策略，以提升模块化控制系统的性能和安全性，并在自动驾驶场景中进行了测试。


<details>
  <summary>Details</summary>
Motivation: 为了解决层次模型预测控制中模型不一致和控制器独立设计等挑战，实现可证明的安全和模块化设计。

Method: 引入合同机制，使用软约束MPC的最优松弛变量，高层控制器生成参考轨迹，并采用神经网络进行显式函数逼近以确保计算效率和模型保密。

Result: 方法在自动驾驶的规划器-运动控制器设置中验证，提高了参考轨迹的可行性评估，无需完整模型知识。

Conclusion: 该策略提升了控制系统的性能和安全性，促进了模块化层次控制的设计。

Abstract: Today's control systems are often characterized by modularity and safety
requirements to handle complexity, resulting in hierarchical control
structures. Although hierarchical model predictive control offers favorable
properties, achieving a provably safe, yet modular design remains a challenge.
This paper introduces a contract-based hierarchical control strategy to improve
the performance of control systems facing challenges related to model
inconsistency and independent controller design across hierarchies. We consider
a setup where a higher-level controller generates references that affect the
constraints of a lower-level controller, which is based on a soft-constrained
MPC formulation. The optimal slack variables serve as the basis for a contract
that allows the higher-level controller to assess the feasibility of the
reference trajectory without exact knowledge of the model, constraints, and
cost of the lower-level controller. To ensure computational efficiency while
maintaining model confidentiality, we propose using an explicit function
approximation, such as a neural network, to represent the cost of optimal slack
values. The approach is tested for a hierarchical control setup consisting of a
planner and a motion controller as commonly found in autonomous driving.

</details>

### [135] [The CAM Model: An in vivo Testbed for Molecular Communication Systems](https://arxiv.org/abs/2504.12123)
*Fardad Vakilipoor,Andreas Ettner-Sitter,Lucas Brand,Sebastian Lotter,Thiha Aung,Silke Harteis,Robert Schober,Maximilian Schäfer*

Main category: eess.SY

TLDR: 这篇论文引入绒毛尿囊膜（CAM）模型作为分子通信的3D体外测试平台，并通过实验和模型验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 分子通信研究需要更真实的体外测试环境来支持生物医学应用，如健康监测和药物递送。

Method: 使用CAM模型进行实验，研究荧光分子分布，推导基于包裹正态分布的分析模型，并通过非线性最小二乘法估计参数。

Result: 开发了粒子传播参数模型，使用69个区域的数据验证，分析参数关系和生物合理性，并建模长期粒子行为和肝脏积累。

Conclusion: CAM模型因其生物真实性、可重复性和多功能性，适合作为下一代分子通信测试平台。

Abstract: Molecular communication (MC) research increasingly focuses on biomedical
applications like health monitoring and drug delivery, demanding testing in
realistic living environments. Elevating MC research requires developing
advanced in vivo testbeds. We introduce the chorioallantoic membrane (CAM)
model as the first versatile 3D in vivo MC platform. The CAM, a highly
vascularized membrane in fertilized chicken eggs, is established in
bioengineering, cancer research, and drug development. Its biological realism,
reproducibility, and versatility make it ideal for next-generation MC testbeds,
bridging proof-of-concept systems and practical applications. We
comprehensively characterize the CAM model's properties and MC system
relevance. Through experimental studies, we investigate fluorescent molecule
distribution in the CAM's closed-loop vascular system. We derive an analytical
model using the wrapped normal distribution to describe particle propagation in
dispersive closed-loop systems dominated by diffusion and flow. Parametric
models are developed to approximate particle dynamics in the CAM, with
parameters estimated via nonlinear least squares curve fitting. A dataset of 69
regions from 25 eggs validates our models. We analyze parameter relationships
and biological plausibility. Finally, we develop a parametric model for
long-term particle behavior and liver accumulation in chick embryos.

</details>

### [136] [Discrete-Time Modeling of Interturn Short Circuits in Interior PMSMs](https://arxiv.org/abs/2504.12193)
*Lukas Zezula,Matus Kozovsky,Ludek Buchta,Petr Blaha*

Main category: eess.SY

TLDR: 这篇论文介绍了用于内部永磁同步电机匝间短路故障诊断的离散时间建模方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了促进基于模型的故障诊断和缓解，针对具有集中绕组的内部永磁同步电机中的匝间短路问题。

Method: 开发连续时间模型、转换为转子参考系、使用矩阵指数技术离散化、考虑电机连接电阻，并通过实验室实验验证。

Result: 模型的动态特性通过与实验数据和前向Euler方法比较得到验证。

Conclusion: 所提出的离散时间模型适用于故障诊断和缓解。

Abstract: This article describes the discrete-time modeling approach for interturn
short circuits in interior permanent magnet synchronous motors with
concentrated windings that facilitate model-based fault diagnostics and
mitigation. A continuous-time model incorporating universal series-parallel
stator winding connection and radial permanent magnet fluxes is developed in
the stator variables and transformed into the rotor reference frame, including
also the electromagnetic torque. The transformed model undergoes discretization
using the matrix exponential-based technique, wherein the electrical angular
velocity and angle are considered time-varying parameters. The resulting model
is subsequently expanded to consider the motor connection resistance via
perturbation techniques. In the laboratory experiments, we validate the
dynamical properties of the derived model by comparing its outputs with the
experimental data and waveforms generated by the forward Euler-based
discrete-time approximation.

</details>

### [137] [Integrator Anti-Windup Design for Servo-Controllers with Position Constraints](https://arxiv.org/abs/2504.12207)
*Eugene Lavretsky*

Main category: eess.SY

TLDR: 本文提出了一种基于控制屏障函数的防风起控制设计修改，针对位置饱和伺服控制器，并讨论了飞行控制应用示例。


<details>
  <summary>Details</summary>
Motivation: 防止积分器风起，这是位置饱和伺服控制器中常见问题，可能导致系统性能下降。

Method: 基于控制屏障函数形式主义，开发防风起积分器修改，适用于线性时不变多输入多输出开环稳定连续时间系统。

Result: 方法适用于指定系统类型，并通过飞行控制应用示例验证。

Conclusion: 该方案有效防止积分器风起，提升了位置限制伺服控制器的性能。

Abstract: A control design modification to prevent integrator windup for position
saturated servo-controllers is introduced. The design is based on the formalism
of Control Barrier Functions and represents an anti-windup integrator
modification for position-limited servo-controllers. The method is applicable
to Linear Time Invariant Multi-Input-Multi-Output open-loop stable continuous
time systems. A flight control application example of the developed anti-windup
control solution is discussed.

</details>

### [138] [Servo-Controllers with Operational Constraints](https://arxiv.org/abs/2504.12208)
*Eugene Lavretsky*

Main category: eess.SY

TLDR: 本论文开发了一种针对受约束的多输入多输出线性时不变系统的比例积分伺服控制方法，使用最小范数控制器和控制屏障函数，确保约束满足，并在飞行系统中验证。


<details>
  <summary>Details</summary>
Motivation: 动机是处理系统控制输入和输出的操作约束，提供防风保护，并应用于航空飞行关键系统以提高安全性。

Method: 方法基于最小范数控制器和控制屏障函数，通过解析求解二次规划来强制执行最小/最大盒约束。

Result: 结果是方法提供了控制器积分状态的防风保护，并逐组件强制执行约束。模拟示例展示了在航空飞行系统中的潜在益处。

Conclusion: 结论是所提出的设计方法有效，并为类似系统提供了有益的解决方案。

Abstract: In this paper, a proportional-integral servo-control design method is
developed for multi-input-multioutput linear time invariant systems with
operational constraints imposed on the system control input and on an output of
the same dimension as the control input. The design is based on min-norm
controllers and Control Barrier Functions. It allows to enforce min/max box
constraints by analytically solving Quadratic Programs for min-norm
augmentation controllers. The method provides an anti-windup protection for the
controller integrator state and enforces the desired operational control and
output constraints, component-wise. A simulation example is given to illustrate
potential benefits of the proposed design methodology for aerial flight
critical systems.

</details>

### [139] [SEROAISE: Advancing ROA Estimation for ReLU and PWA Dynamics through Estimating Certified Invariant Sets](https://arxiv.org/abs/2504.12269)
*Pouya Samanipour,Hasan Poonawala*

Main category: eess.SY

TLDR: 本文提出SEROAISE框架，用于构建PWA或ReLU NN动态系统的吸引域，通过在认证不变集上计算Lyapunov-like PWA函数。


<details>
  <summary>Details</summary>
Motivation: 传统方法在预选域上强制Lyapunov条件，而本框架使用IISE获得更大不变集，以提升RoA估计精度。

Method: SEROAISE基于IISE和NUGIS概念，顺序估计RoA，通过计算Lyapunov-like PWA函数。

Result: 通过例子验证了方法的有效性，包括学习算法动态系统，并提供开源实现。

Conclusion: 该方法比现有方法提供更大认证不变集，提高了RoA构建能力。

Abstract: This paper presents a novel framework for constructing the Region of
Attraction (RoA) for dynamics derived either from Piecewise Affine (PWA)
functions or from Neural Networks (NNs) with Rectified Linear Units (ReLU)
activation function. This method, described as Sequential Estimation of RoA
based on Invariant Set Estimation (SEROAISE), computes a Lyapunov-like PWA
function over a certified PWA invariant set. While traditional approaches
search for Lyapunov functions by enforcing Lyapunov conditions over
pre-selected domains, this framework enforces Lyapunov-like conditions over a
certified invariant subset obtained using the Iterative Invariant Set
Estimator(IISE). Compared to the state-of-the-art, IISE provides systematically
larger certified invariant sets. In order to find a larger invariant subset,
the IISE utilizes a novel concept known as the Non-Uniform Growth of Invariant
Set (NUGIS). A number of examples illustrating the efficacy of the proposed
methods are provided, including dynamical systems derived from learning
algorithms. The implementation is publicly available at:
https://github.com/PouyaSamanipour/SEROAISE.git.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [140] [RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems](https://arxiv.org/abs/2504.11510)
*Xiaohua Feng,Yuyuan Li,Fengyuan Yu,Ke Xiong,Junjie Fang,Li Zhang,Tianyu Du,Chaochao Chen*

Main category: cs.IR

TLDR: 这篇论文提出RAID，一种在训练防御方法，用于对抗推荐系统中的属性推断攻击，通过Wasserstein重心问题使属性不可区分，同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中用户易受属性推断攻击，现有的后训练防御方法无法充分利用训练数据，亟需在训练中进行防御。

Method: RAID定义防御目标，求解受约束的Wasserstein重心问题，使用最优传输对用户与中心分布对齐。

Result: 在四个真实数据集实验中，RAID显示出有效性和显著优势。

Conclusion: RAID是一种高效的在训练防御方法，能降低攻击风险并维持推荐性能。

Abstract: In various networks and mobile applications, users are highly susceptible to
attribute inference attacks, with particularly prevalent occurrences in
recommender systems. Attackers exploit partially exposed user profiles in
recommendation models, such as user embeddings, to infer private attributes of
target users, such as gender and political views. The goal of defenders is to
mitigate the effectiveness of these attacks while maintaining recommendation
performance. Most existing defense methods, such as differential privacy and
attribute unlearning, focus on post-training settings, which limits their
capability of utilizing training data to preserve recommendation performance.
Although adversarial training extends defenses to in-training settings, it
often struggles with convergence due to unstable training processes. In this
paper, we propose RAID, an in-training defense method against attribute
inference attacks in recommender systems. In addition to the recommendation
objective, we define a defensive objective to ensure that the distribution of
protected attributes becomes independent of class labels, making users
indistinguishable from attribute inference attacks. Specifically, this
defensive objective aims to solve a constrained Wasserstein barycenter problem
to identify the centroid distribution that makes the attribute
indistinguishable while complying with recommendation performance constraints.
To optimize our proposed objective, we use optimal transport to align users
with the centroid distribution. We conduct extensive experiments on four
real-world datasets to evaluate RAID. The experimental results validate the
effectiveness of RAID and demonstrate its significant superiority over existing
methods in multiple aspects.

</details>

### [141] [Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation](https://arxiv.org/abs/2504.11658)
*Nanshan Jia,Chenfei Yuan,Yuhang Wu,Zeyu Zheng*

Main category: cs.IR

TLDR: 本文提出使用LLM提升序列推荐系统的引导嵌入精炼方法，提高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: LLM快速发展为推荐系统改进提供机会，但整合时存在可解释性、透明度和安全性问题，需要缓解这些挑战。

Method: 使用LLM作为辅助工具生成引导嵌入，捕获可解释属性语义信息，与基嵌入降维版本结合，构建精炼嵌入并集成到推荐模块。

Result: 实验显示性能提升10%至50%（MRR、召回率、NDCG），并通过案例研究证明可解释性增强。

Conclusion: 该方法适用于多种基模型，具有良好泛化性，证明其有效性。

Abstract: The fast development of Large Language Models (LLMs) offers growing
opportunities to further improve sequential recommendation systems. Yet for
some practitioners, integrating LLMs to their existing base recommendation
systems raises questions about model interpretability, transparency and related
safety. To partly alleviate challenges from these questions, we propose guided
embedding refinement, a method that carries out a guided and interpretable
usage of LLM to enhance the embeddings associated with the base recommendation
system. Instead of directly using LLMs as the backbone of sequential
recommendation systems, we utilize them as auxiliary tools to emulate the sales
logic of recommendation and generate guided embeddings that capture
domain-relevant semantic information on interpretable attributes. Benefiting
from the strong generalization capabilities of the guided embedding, we
construct refined embedding by using the guided embedding and reduced-dimension
version of the base embedding. We then integrate the refined embedding into the
recommendation module for training and inference. A range of numerical
experiments demonstrate that guided embedding is adaptable to various given
existing base embedding models, and generalizes well across different
recommendation tasks. The numerical results show that the refined embedding not
only improves recommendation performance, achieving approximately $10\%$ to
$50\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized
Discounted Cumulative Gain (NDCG), but also enhances interpretability, as
evidenced by case studies.

</details>

### [142] [Generative Recommendation with Continuous-Token Diffusion](https://arxiv.org/abs/2504.12007)
*Haohao Qu,Wenqi Fan,Shanru Lin*

Main category: cs.IR

TLDR: 这篇论文提出DeftRec框架，使用去噪扩散模型提升LLM-based推荐系统的连续数据处理能力，解决离散方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-based推荐系统依赖离散空间，存在信息压缩和词汇限制问题，连续数据方法有潜力但开发不足。

Method: 提出DeftRec框架，包括带掩码和K-way架构的标记器、条件于LLM推理的去噪扩散模型、包含负交互的目标重构，以及基于分数的推荐生成。

Result: 实验显示DeftRec优于传统和新兴LLM-based推荐系统基准。

Conclusion: DeftRec框架有效改善推荐系统的性能，提供更准确的推荐。

Abstract: In recent years, there has been a significant trend toward using large
language model (LLM)-based recommender systems (RecSys). Current research
primarily focuses on representing complex user-item interactions within a
discrete space to align with the inherent discrete nature of language models.
However, this approach faces limitations due to its discrete nature: (i)
information is often compressed during discretization; (ii) the tokenization
and generation for the vast number of users and items in real-world scenarios
are constrained by a limited vocabulary. Embracing continuous data presents a
promising alternative to enhance expressive capabilities, though this approach
is still in its early stages. To address this gap, we propose a novel
framework, DeftRec, which incorporates \textbf{de}noising di\textbf{f}fusion
models to enable LLM-based RecSys to seamlessly support continuous
\textbf{t}oken as input and target. First, we introduce a robust tokenizer with
a masking operation and an additive K-way architecture to index users and
items, capturing their complex collaborative relationships into continuous
tokens. Crucially, we develop a denoising diffusion model to process user
preferences within continuous domains by conditioning on reasoning content from
pre-trained large language model. During the denoising process, we reformulate
the objective to include negative interactions, building a comprehensive
understanding of user preferences for effective and accurate recommendation
generation. Finally, given a continuous token as output, recommendations can be
easily generated through score-based retrieval. Extensive experiments
demonstrate the effectiveness of the proposed methods, showing that DeftRec
surpasses competitive benchmarks, including both traditional and emerging
LLM-based RecSys.

</details>

### [143] [Optimizing Compound Retrieval Systems](https://arxiv.org/abs/2504.12063)
*Harrie Oosterhuis,Rolf Jagerman,Zhen Qin,Xuanhui Wang*

Main category: cs.IR

TLDR: 这篇论文引入复合检索系统概念，通过结合多个模型如BM25和LLM，优化检索效率和效果，比级联方法更好。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统使用级联方法，但作者提出需要更广泛的模型交互方式，特别是整合大语言模型，以优化性能。

Method: 提出复合检索系统框架，通过学习模型应用位置和预测聚合方式，结合BM25和LLM的相对相关性预测，优化排名指标和效率目标。

Result: 实验结果显示，复合检索系统在有效性和效率之间提供了更好的权衡，即使在自监督方式下也优于级联方法。

Conclusion: 作者希望通过这一概念，激励信息检索领域进行更多创新性的模型交互设计。

Abstract: Modern retrieval systems do not rely on a single ranking model to construct
their rankings. Instead, they generally take a cascading approach where a
sequence of ranking models are applied in multiple re-ranking stages. Thereby,
they balance the quality of the top-K ranking with computational costs by
limiting the number of documents each model re-ranks. However, the cascading
approach is not the only way models can interact to form a retrieval system.
  We propose the concept of compound retrieval systems as a broader class of
retrieval systems that apply multiple prediction models. This encapsulates
cascading models but also allows other types of interactions than top-K
re-ranking. In particular, we enable interactions with large language models
(LLMs) which can provide relative relevance comparisons. We focus on the
optimization of compound retrieval system design which uniquely involves
learning where to apply the component models and how to aggregate their
predictions into a final ranking. This work shows how our compound approach can
combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM
relevance predictions, while optimizing a given ranking metric and efficiency
target. Our experimental results show optimized compound retrieval systems
provide better trade-offs between effectiveness and efficiency than cascading
approaches, even when applied in a self-supervised manner.
  With the introduction of compound retrieval systems, we hope to inspire the
information retrieval field to more out-of-the-box thinking on how prediction
models can interact to form rankings.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [144] [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
*Tobias Hallmen,Robin-Nico Kampa,Fabian Deuser,Norbert Oswald,Elisabeth André*

Main category: cs.CV

TLDR: 本研究使用音频、文本和视觉多模态方法提升BAH和EMI挑战的表现，取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 基于先前工作，整合文本和视觉模态以更好地理解情感分析中的语境。

Method: 利用Wav2Vec 2.0提取音频特征，结合VAD模块、BERT-like编码器、ViT和LSTM进行时序建模，并融合多种模态。

Result: 与基线方法相比，取得了显著的性能提升。

Conclusion: 模态结合提高了行为矛盾识别和情感模仿强度估计的准确性。

Abstract: In this study, we present our methodology for two tasks: the Behavioural
Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry
Intensity (EMI) Estimation Challenge, both conducted as part of the 8th
Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building
on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast
dataset to extract various audio features, capturing both linguistic and
paralinguistic information. Our approach incorporates a
valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like
encoder, and a vision transformer (ViT) with predictions subsequently processed
through a long short-term memory (LSTM) architecture for temporal modeling. In
this iteration, we integrate the textual and visual modality into our analysis,
recognizing that semantic content provides valuable contextual cues and
underscoring that the meaning of speech often conveys more critical insights
than its acoustic counterpart alone. Fusing in the vision modality helps in
some cases to interpret the textual modality more precisely. This combined
approach yields significant performance improvements over baseline methods.

</details>

### [145] [SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection](https://arxiv.org/abs/2504.11470)
*Huaxiang Zhang,Hao Zhang,Aoran Mei,Zhongxue Gan,Guo-Niu Zhu*

Main category: cs.CV

TLDR: 本文提出SO-DETR模型，针对小物体检测的挑战，通过双域混合编码器、增强查询选择机制和知识蒸馏策略，提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有检测Transformer方法在小物体检测中存在低级特征融合效率低和查询选择策略不适配的问题。

Method: 提出SO-DETR模型，包括双域混合编码器融合多尺度特征、增强查询选择机制使用扩展IoU动态选择锚框，以及知识蒸馏策略。

Result: 在VisDrone-2019-DET和UAVVaste数据集上，SO-DETR在类似计算需求下优于现有方法。

Conclusion: SO-DETR是一种高效的小物体检测方法，证明了其在性能和计算效率方面的优势。

Abstract: Detection Transformer-based methods have achieved significant advancements in
general object detection. However, challenges remain in effectively detecting
small objects. One key difficulty is that existing encoders struggle to
efficiently fuse low-level features. Additionally, the query selection
strategies are not effectively tailored for small objects. To address these
challenges, this paper proposes an efficient model, Small Object Detection
Transformer (SO-DETR). The model comprises three key components: a dual-domain
hybrid encoder, an enhanced query selection mechanism, and a knowledge
distillation strategy. The dual-domain hybrid encoder integrates spatial and
frequency domains to fuse multi-scale features effectively. This approach
enhances the representation of high-resolution features while maintaining
relatively low computational overhead. The enhanced query selection mechanism
optimizes query initialization by dynamically selecting high-scoring anchor
boxes using expanded IoU, thereby improving the allocation of query resources.
Furthermore, by incorporating a lightweight backbone network and implementing a
knowledge distillation strategy, we develop an efficient detector for small
objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets
demonstrate that SO-DETR outperforms existing methods with similar
computational demands. The project page is available at
https://github.com/ValiantDiligent/SO_DETR.

</details>

### [146] [Visual moral inference and communication](https://arxiv.org/abs/2504.11473)
*Warren Zhu,Aida Ramezani,Yang Xu*

Main category: cs.CV

TLDR: 本文提出一个计算框架，支持从图像中进行道德推理，应用于推断人类对视觉图像的道德判断和分析新闻图像中的道德模式。发现文本模型不足，语言-视觉融合模型更精确，并揭示新闻中的隐含偏差。


<details>
  <summary>Details</summary>
Motivation: 人类可从多种输入来源进行道德推理，而AI通常依赖文本输入，但道德可通过语言以外的模态传达，因此需要扩展到视觉输入。

Method: 提出计算框架，使用语言-视觉融合模型，应用于两个任务：推断人类对图像的道德判断和分析公共新闻图像中的道德内容。

Result: 文本模型无法捕捉视觉刺激的细粒度道德判断，融合模型提供更好精度；应用于新闻数据发现新闻类别和地缘政治讨论中的隐含偏差。

Conclusion: 这项工作为自动化视觉道德推理和发现公共媒体中视觉道德沟通模式开辟新途径。

Abstract: Humans can make moral inferences from multiple sources of input. In contrast,
automated moral inference in artificial intelligence typically relies on
language models with textual input. However, morality is conveyed through
modalities beyond language. We present a computational framework that supports
moral inference from natural images, demonstrated in two related tasks: 1)
inferring human moral judgment toward visual images and 2) analyzing patterns
in moral content communicated via images from public news. We find that models
based on text alone cannot capture the fine-grained human moral judgment toward
visual stimuli, but language-vision fusion models offer better precision in
visual moral inference. Furthermore, applications of our framework to news data
reveal implicit biases in news categories and geopolitical discussions. Our
work creates avenues for automating visual moral inference and discovering
patterns of visual moral communication in public media.

</details>

### [147] [SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification](https://arxiv.org/abs/2504.11477)
*Yunkai Zhang,Shiyin Wei,Yong Huang,Yawu Su,Shanshan Lu,Hui Li*

Main category: cs.CV

TLDR: 这篇论文提出SDIGLM模型，使用大型多模态模型改进结构损伤识别，解决现有CV模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有CV模型在损伤类型识别和语言描述方面存在限制，无法适应复杂工程环境，因此需要LMMs来统一处理文本和视觉数据。

Method: 基于VisualGLM-6B开发SDIGLM，整合U-Net语义分割生成视觉CoT，并构建多轮对话数据集及提示工程形成语言CoT。

Result: SDIGLM在各种基础设施上达到95.24%的准确率，并能描述损伤特征如孔洞大小、裂缝方向和腐蚀严重程度。

Conclusion: SDIGLM超越通用LMMs，在结构损伤识别中表现出色，具有实际工程应用潜力。

Abstract: Existing computer vision(CV)-based structural damage identification models
demonstrate notable accuracy in categorizing and localizing damage. However,
these models present several critical limitations that hinder their practical
application in civil engineering(CE). Primarily, their ability to recognize
damage types remains constrained, preventing comprehensive analysis of the
highly varied and complex conditions encountered in real-world CE structures.
Second, these models lack linguistic capabilities, rendering them unable to
articulate structural damage characteristics through natural language
descriptions. With the continuous advancement of artificial intelligence(AI),
large multi-modal models(LMMs) have emerged as a transformative solution,
enabling the unified encoding and alignment of textual and visual data. These
models can autonomously generate detailed descriptive narratives of structural
damage while demonstrating robust generalization across diverse scenarios and
tasks. This study introduces SDIGLM, an innovative LMM for structural damage
identification, developed based on the open-source VisualGLM-6B architecture.
To address the challenge of adapting LMMs to the intricate and varied operating
conditions in CE, this work integrates a U-Net-based semantic segmentation
module to generate defect segmentation maps as visual Chain of Thought(CoT).
Additionally, a multi-round dialogue fine-tuning dataset is constructed to
enhance logical reasoning, complemented by a language CoT formed through prompt
engineering. By leveraging this multi-modal CoT, SDIGLM surpasses
general-purpose LMMs in structural damage identification, achieving an accuracy
of 95.24% across various infrastructure types. Moreover, the model effectively
describes damage characteristics such as hole size, crack direction, and
corrosion severity.

</details>

### [148] [Flux Already Knows - Activating Subject-Driven Image Generation without Training](https://arxiv.org/abs/2504.11478)
*Hao Kang,Stathi Fotiadis,Liming Jiang,Qing Yan,Yumin Jia,Zichuan Liu,Min Jin Chong,Xin Lu*

Main category: cs.CV

TLDR: 提出了一种简单有效的零样本主体驱动图像生成框架，使用 vanilla Flux 模型，通过网格-based 图像完成和马赛克布局复制主体图像，实现高保真生成。


<details>
  <summary>Details</summary>
Motivation: 动机是提供一种无需额外数据、训练或微调的资源高效方法，开启轻量级定制的可能性。

Method: 方法包括将任务框架为基于网格的图像完成，简单复制主体图像在马赛克布局中，结合新型级联注意力设计和元提示技术。

Result: 结果显示在多个关键指标和人类偏好研究中优于基线，支持多样编辑如徽标插入、虚拟试穿等，尽管有某些方面的权衡。

Conclusion: 结论是预训练的文本到图像模型可以实现高质量、资源高效的主体驱动生成，为下游应用提供新可能。

Abstract: We propose a simple yet effective zero-shot framework for subject-driven
image generation using a vanilla Flux model. By framing the task as grid-based
image completion and simply replicating the subject image(s) in a mosaic
layout, we activate strong identity-preserving capabilities without any
additional data, training, or inference-time fine-tuning. This "free lunch"
approach is further strengthened by a novel cascade attention design and meta
prompting technique, boosting fidelity and versatility. Experimental results
show that our method outperforms baselines across multiple key metrics in
benchmarks and human preference studies, with trade-offs in certain aspects.
Additionally, it supports diverse edits, including logo insertion, virtual
try-on, and subject replacement or insertion. These results demonstrate that a
pre-trained foundational text-to-image model can enable high-quality,
resource-efficient subject-driven generation, opening new possibilities for
lightweight customization in downstream applications.

</details>

### [149] [snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing](https://arxiv.org/abs/2504.11482)
*Vidya Sudevan,Fakhreddine Zayer,Rizwana Kausar,Sajid Javed,Hamad Karki,Giulia De Masi,Jorge Dias*

Main category: cs.CV

TLDR: 这篇论文提出了一种轻量级脉冲神经网络（SNN）方法snnTrans-DHZ，用于水下图像去雾，实现了高效能耗性能。


<details>
  <summary>Details</summary>
Motivation: 水下图像去雾对基于视觉的海洋操作至关重要，因为光散射和吸收会降低能见度，需要低功耗高效方法。

Method: 使用SNN处理时间依赖图像序列，将静态图像转换为序列，转换到LAB颜色空间，并通过K估计器、背景光估计器和软重建模块训练。

Result: 在UIEB数据集上PSNR 21.68 dB、SSIM 0.8795；在EUVP上PSNR 23.46 dB、SSIM 0.8439；参数少、计算和能耗低。

Conclusion: snnTrans-DHZ高效性使其适合水下机器人、海洋勘探和环境监测应用。

Abstract: Underwater image dehazing is critical for vision-based marine operations
because light scattering and absorption can severely reduce visibility. This
paper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)
specifically designed for underwater dehazing. By leveraging the temporal
dynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image
sequences while maintaining low power consumption. Static underwater images are
first converted into time-dependent sequences by repeatedly inputting the same
image over user-defined timesteps. These RGB sequences are then transformed
into LAB color space representations and processed concurrently. The
architecture features three key modules: (i) a K estimator that extracts
features from multiple color space representations; (ii) a Background Light
Estimator that jointly infers the background light component from the RGB-LAB
images; and (iii) a soft image reconstruction module that produces haze-free,
visibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a
surrogate gradient-based backpropagation through time (BPTT) strategy alongside
a novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ
achieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it
yields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million
network parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the
algorithm significantly outperforms existing state-of-the-art methods in terms
of efficiency. These features make snnTrans-DHZ highly suitable for deployment
in underwater robotics, marine exploration, and environmental monitoring.

</details>

### [150] [TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification](https://arxiv.org/abs/2504.11500)
*Kaicong Huang,Talha Azfar,Jack Reilly,Ruimin Ke*

Main category: cs.CV

TLDR: 提出TransitReID框架，使用视觉重识别从公交车摄像头收集OD数据，实现了约90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统交通OD数据收集方法成本高、效率低的问题，并克服车载摄像头重识别中的遮挡和视角变化挑战。

Method: 包括遮挡鲁棒的重识别算法（使用变分自编码器引导的区域注意力机制）和分层存储动态匹配机制，以及多线程设计和专属数据集。

Result: 在公交路线模拟中达到约90%的准确率，处于最先进水平。

Conclusion: TransitReID框架有效地提高了交通OD数据收集的准确性、效率和隐私保护。

Abstract: Transit Origin-Destination (OD) data are essential for transit planning,
particularly in route optimization and demand-responsive paratransit systems.
Traditional methods, such as manual surveys, are costly and inefficient, while
Bluetooth and WiFi-based approaches require passengers to carry specific
devices, limiting data coverage. On the other hand, most transit vehicles are
equipped with onboard cameras for surveillance, offering an opportunity to
repurpose them for edge-based OD data collection through visual person
re-identification (ReID). However, such approaches face significant challenges,
including severe occlusion and viewpoint variations in transit environments,
which greatly reduce matching accuracy and hinder their adoption. Moreover,
designing effective algorithms that can operate efficiently on edge devices
remains an open challenge. To address these challenges, we propose TransitReID,
a novel framework for individual-level transit OD data collection. TransitReID
consists of two key components: (1) An occlusion-robust ReID algorithm
featuring a variational autoencoder guided region-attention mechanism that
adaptively focuses on visible body regions through reconstruction
loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic
Matching (HSDM) mechanism specifically designed for efficient and robust
transit OD matching which balances storage, speed, and accuracy. Additionally,
a multi-threaded design supports near real-time operation on edge devices,
which also ensuring privacy protection. We also introduce a ReID dataset
tailored for complex bus environments to address the lack of relevant training
data. Experimental results demonstrate that TransitReID achieves
state-of-the-art performance in ReID tasks, with an accuracy of approximately
90\% in bus route simulations.

</details>

### [151] [Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey](https://arxiv.org/abs/2504.11588)
*Siteng Ma,Honghui Du,Yu An,Jing Wang,Qinqin Wang,Haochang Wu,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TLDR: 这篇调查综述了医疗图像中不完全监督深度学习的研究，涵盖分类、分割和检测任务。


<details>
  <summary>Details</summary>
Motivation: 由于获取大型标注数据集的挑战，包括耗时和劳动密集型注释，促使对不完全、 inexact 和 absent 监督范式的兴趣。

Method: 通过分类和审阅约600篇自2018年以来的研究，涵盖脑、心脏和胸部成像等应用，提供正式定义和总结。

Result: 提供了各种学习机制的全面总结、研究关系分析，并识别了未来挑战。

Conclusion: 帮助读者理解当前研究景观，并讨论潜在的未来研究方向。

Abstract: Deep learning has achieved significant breakthroughs in medical imaging, but
these advancements are often dependent on large, well-annotated datasets.
However, obtaining such datasets poses a significant challenge, as it requires
time-consuming and labor-intensive annotations from medical experts.
Consequently, there is growing interest in learning paradigms such as
incomplete, inexact, and absent supervision, which are designed to operate
under limited, inexact, or missing labels. This survey categorizes and reviews
the evolving research in these areas, analyzing around 600 notable
contributions since 2018. It covers tasks such as image classification,
segmentation, and detection across various medical application areas, including
but not limited to brain, chest, and cardiac imaging. We attempt to establish
the relationships among existing research studies in related areas. We provide
formal definitions of different learning paradigms and offer a comprehensive
summary and interpretation of various learning mechanisms and strategies,
aiding readers in better understanding the current research landscape and
ideas. We also discuss potential future research challenges.

</details>

### [152] [Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics](https://arxiv.org/abs/2504.11686)
*Yiran He,Yun Cao,Bowen Yang,Zeyu Zhang*

Main category: cs.CV

TLDR: 本论文提出使用多模态LLM检测AI生成图像的伪造框架，通过提示工程和少样本学习实现高准确率。


<details>
  <summary>Details</summary>
Motivation: generative AI快速发展使图像操纵更易且更难检测，多模态LLM虽有丰富知识但不适合AIGC和局部伪造细节。

Method: 提出框架评估图像真实性、定位篡改区域、提供证据和追踪生成方法，使用语义线索、提示工程和少样本学习。

Result: 实验显示GPT4V在Autosplice上准确率92.1%，在LaMa上86.3%，与最先进方法竞争。

Conclusion: 讨论多模态LLM的局限性并提出潜在改进方向。

Abstract: The rapid development of generative AI facilitates content creation and makes
image manipulation easier and more difficult to detect. While multimodal Large
Language Models (LLMs) have encoded rich world knowledge, they are not
inherently tailored for combating AI-generated Content (AIGC) and struggle to
comprehend local forgery details. In this work, we investigate the application
of multimodal LLMs in forgery detection. We propose a framework capable of
evaluating image authenticity, localizing tampered regions, providing evidence,
and tracing generation methods based on semantic tampering clues. Our method
demonstrates that the potential of LLMs in forgery analysis can be effectively
unlocked through meticulous prompt engineering and the application of few-shot
learning techniques. We conduct qualitative and quantitative experiments and
show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in
LaMa, which is competitive with state-of-the-art AIGC detection methods. We
further discuss the limitations of multimodal LLMs in such tasks and propose
potential improvements.

</details>

### [153] [Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset](https://arxiv.org/abs/2504.11707)
*Muhammad Shahid Muneer,Simon S. Woo*

Main category: cs.CV

TLDR: 这篇论文提出一个百万规模的提示和图像数据集，以及一个针对多模态对抗攻击的鲁棒NSFW防御方法，以防止文本到图像模型的滥用。


<details>
  <summary>Details</summary>
Motivation: 动机是解决文本到图像模型生成NSFW内容的问题、现有防御方法的失效以及缺乏鲁棒的多模态NSFW数据集。

Method: 方法包括使用开源扩散模型生成数据集，并开发一个多模态防御模型来区分安全和NSFW内容，并抵抗对抗攻击。

Result: 结果显示，该模型在准确率、召回率上优于现有最先进方法，并显著降低了多模态对抗攻击的攻击成功率。

Conclusion: 结论是该方法有效缓解了当前挑战，并提供了开源代码实现。

Abstract: In the past years, we have witnessed the remarkable success of Text-to-Image
(T2I) models and their widespread use on the web. Extensive research in making
T2I models produce hyper-realistic images has led to new concerns, such as
generating Not-Safe-For-Work (NSFW) web content and polluting the web society.
To help prevent misuse of T2I models and create a safer web environment for
users features like NSFW filters and post-hoc security checks are used in these
models. However, recent work unveiled how these methods can easily fail to
prevent misuse. In particular, adversarial attacks on text and image modalities
can easily outplay defensive measures. %Exploiting such leads to the growing
concern of preventing adversarial attacks on text and image modalities.
Moreover, there is currently no robust multimodal NSFW dataset that includes
both prompt and image pairs and adversarial examples. This work proposes a
million-scale prompt and image dataset generated using open-source diffusion
models. Second, we develop a multimodal defense to distinguish safe and NSFW
text and images, which is robust against adversarial attacks and directly
alleviates current challenges. Our extensive experiments show that our model
performs well against existing SOTA NSFW detection methods in terms of accuracy
and recall, drastically reducing the Attack Success Rate (ASR) in multimodal
adversarial attack scenarios. Code:
https://github.com/shahidmuneer/multimodal-nsfw-defense.

</details>

### [154] [GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision](https://arxiv.org/abs/2504.11754)
*Zihui Zhang,Yafei Yang,Hongtao Wen,Bo Yang*

Main category: cs.CV

TLDR: This paper introduces GrabS, a new unsupervised method for 3D object segmentation in complex point clouds, using a two-stage approach with generative priors and an embodied agent, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised methods are limited to simple objects and have inferior performance due to the lack of objectness in pretrained 2D features or external signals.

Method: A two-stage pipeline: first, learn generative and discriminative object-centric priors from object datasets; second, use an embodied agent to discover objects by querying against these priors.

Result: The method achieves remarkable segmentation performance on two real-world datasets and a new synthetic dataset, surpassing all existing unsupervised methods.

Conclusion: GrabS demonstrates significant advancements in unsupervised 3D object segmentation by effectively utilizing pretrained priors and embodied agents.

Abstract: We study the hard problem of 3D object segmentation in complex point clouds
without requiring human labels of 3D scenes for supervision. By relying on the
similarity of pretrained 2D features or external signals such as motion to
group 3D points as objects, existing unsupervised methods are usually limited
to identifying simple objects like cars or their segmented objects are often
inferior due to the lack of objectness in pretrained features. In this paper,
we propose a new two-stage pipeline called GrabS. The core concept of our
method is to learn generative and discriminative object-centric priors as a
foundation from object datasets in the first stage, and then design an embodied
agent to learn to discover multiple objects by querying against the pretrained
generative priors in the second stage. We extensively evaluate our method on
two real-world datasets and a newly created synthetic dataset, demonstrating
remarkable segmentation performance, clearly surpassing all existing
unsupervised methods.

</details>

### [155] [ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model](https://arxiv.org/abs/2504.11781)
*Guanchun Wang,Xiangrong Zhang,Yifei Zhang,Zelin Peng,Tianyang Zhang,Xu Tang,Licheng Jiao*

Main category: cs.CV

TLDR: 本文提出ACMamba模型，减少高光谱图像异常检测的计算成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 当前高光谱图像异常检测因计算成本高而部署缓慢，本文通过巧妙采样策略来降低成本。

Method: 提出不对称共识状态空间模型（ACMamba），使用区域级实例、Mamba-based模块和共识学习策略进行背景重建和异常压缩。

Result: 通过理论分析和八个基准实验验证，ACMamba在速度和性能上优于现有最先进方法。

Conclusion: ACMamba显著提高了高光谱图像异常检测的效率和效果。

Abstract: Unsupervised anomaly detection in hyperspectral images (HSI), aiming to
detect unknown targets from backgrounds, is challenging for earth surface
monitoring. However, current studies are hindered by steep computational costs
due to the high-dimensional property of HSI and dense sampling-based training
paradigm, constraining their rapid deployment. Our key observation is that,
during training, not all samples within the same homogeneous area are
indispensable, whereas ingenious sampling can provide a powerful substitute for
reducing costs. Motivated by this, we propose an Asymmetrical Consensus State
Space Model (ACMamba) to significantly reduce computational costs without
compromising accuracy. Specifically, we design an asymmetrical anomaly
detection paradigm that utilizes region-level instances as an efficient
alternative to dense pixel-level samples. In this paradigm, a low-cost
Mamba-based module is introduced to discover global contextual attributes of
regions that are essential for HSI reconstruction. Additionally, we develop a
consensus learning strategy from the optimization perspective to simultaneously
facilitate background reconstruction and anomaly compression, further
alleviating the negative impact of anomaly reconstruction. Theoretical analysis
and extensive experiments across eight benchmarks verify the superiority of
ACMamba, demonstrating a faster speed and stronger performance over the
state-of-the-art.

</details>

### [156] [Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting](https://arxiv.org/abs/2504.11820)
*Delong Suzhang,Meng Yang*

Main category: cs.CV

TLDR: 本文提出了一种改进深度恢复泛化性的方法，通过处理原始深度图的结构失调，使用新的生成管道、不确定性模块和特征对齐模块。


<details>
  <summary>Details</summary>
Motivation: 原始深度图中低质量结构普遍存在，缺乏配对的原始-真实数据，现有方法无法充分考虑结构失调的多样性，导致泛化能力差。

Method: 设计了一个新的原始深度生成管道来丰富结构失调的多样性；开发了一个结构不确定性模块，使用深度基础模型（DFM）来更好地估计不确定性；设计了一个鲁棒的特征对齐模块，与RGB图像精确对齐。

Result: 在多个数据集上进行了广泛实验，证明了所提出方法的竞争性准确性和泛化能力。

Conclusion: 该方法在各种挑战性的原始深度图中实现了更好的准确性和泛化能力。

Abstract: The low-quality structure in raw depth maps is prevalent in real-world RGB-D
datasets, which makes real-world depth recovery a critical task in recent
years. However, the lack of paired raw-ground truth (raw-GT) data in the real
world poses challenges for generalized depth recovery. Existing methods
insufficiently consider the diversity of structure misalignment in raw depth
maps, which leads to poor generalization in real-world depth recovery. Notably,
random structure misalignments are not limited to raw depth data but also
affect GT depth in real-world datasets. In the proposed method, we tackle the
generalization problem from both input and output perspectives. For input, we
enrich the diversity of structure misalignment in raw depth maps by designing a
new raw depth generation pipeline, which helps the network avoid overfitting to
a specific condition. Furthermore, a structure uncertainty module is designed
to explicitly identify the misaligned structure for input raw depth maps to
better generalize in unseen scenarios. Notably the well-trained depth
foundation model (DFM) can help the structure uncertainty module estimate the
structure uncertainty better. For output, a robust feature alignment module is
designed to precisely align with the accurate structure of RGB images avoiding
the interference of inaccurate GT depth. Extensive experiments on multiple
datasets demonstrate the proposed method achieves competitive accuracy and
generalization capabilities across various challenging raw depth maps.

</details>

### [157] [Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement](https://arxiv.org/abs/2504.11896)
*Xingxing Yang,Jie Chen,Zaifeng Yang*

Main category: cs.CV

TLDR: 这篇论文提出了一种基于分解的物理信息先验的低光照图像增强新方法，通过颜色感知变换和内容-噪声分解网络处理照明变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在sRGB颜色空间直接映射低光照图像时，存在颜色预测不一致和对光谱功率分布变化高度敏感的问题，导致性能不稳定。

Method: 引入PiCat框架，包括颜色感知变换（CAT）将图像转换为照明不变描述符，以及内容-噪声分解网络（CNDN）减少噪声，共同作为物理先验指导变换过程。

Result: 在五个基准数据集上，表现出优于现有最先进方法的性能。

Conclusion: 该框架有效解决了复杂照明和光谱变化的挑战，提高了低光照图像增强的鲁棒性和效果。

Abstract: Image decomposition offers deep insights into the imaging factors of visual
data and significantly enhances various advanced computer vision tasks. In this
work, we introduce a novel approach to low-light image enhancement based on
decomposed physics-informed priors. Existing methods that directly map
low-light to normal-light images in the sRGB color space suffer from
inconsistent color predictions and high sensitivity to spectral power
distribution (SPD) variations, resulting in unstable performance under diverse
lighting conditions. To address these challenges, we introduce a
Physics-informed Color-aware Transform (PiCat), a learning-based framework that
converts low-light images from the sRGB color space into deep
illumination-invariant descriptors via our proposed Color-aware Transform
(CAT). This transformation enables robust handling of complex lighting and SPD
variations. Complementing this, we propose the Content-Noise Decomposition
Network (CNDN), which refines the descriptor distributions to better align with
well-lit conditions by mitigating noise and other distortions, thereby
effectively restoring content representations to low-light images. The CAT and
the CNDN collectively act as a physical prior, guiding the transformation
process from low-light to normal-light domains. Our proposed PiCat framework
demonstrates superior performance compared to state-of-the-art methods across
five benchmark datasets.

</details>

### [158] [Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions](https://arxiv.org/abs/2504.11967)
*Yifei Dong,Fengyi Wu,Sanjian Zhang,Guangyu Chen,Yuzhi Hu,Masumi Yano,Jingdong Sun,Siyu Huang,Feng Liu,Qi Dai,Zhi-Qi Cheng*

Main category: cs.CV

TLDR: 本文调查反无人机技术，焦点在分类、检测和跟踪，评估新兴方法和基准，指出性能差距并建议未来方向。


<details>
  <summary>Details</summary>
Motivation: 无人机在基础设施检查和监视中的重要性及其安全挑战，需开发反无人机系统。

Method: 系统评估分类、检测和跟踪，使用扩散基数据合成、多模态融合等方法，跨RGB、红外、音频、雷达和RF传感器管道。

Result: 发现实时性能、隐蔽检测和群集场景中的持久差距。

Conclusion: 强调需要鲁棒的自适应系统，并指导创新防御策略的发展。

Abstract: Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure
inspection, surveillance, and related tasks, yet they also introduce critical
security challenges. This survey provides a wide-ranging examination of the
anti-UAV domain, centering on three core objectives-classification, detection,
and tracking-while detailing emerging methodologies such as diffusion-based
data synthesis, multi-modal fusion, vision-language modeling, self-supervised
learning, and reinforcement learning. We systematically evaluate
state-of-the-art solutions across both single-modality and multi-sensor
pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss
large-scale as well as adversarially oriented benchmarks. Our analysis reveals
persistent gaps in real-time performance, stealth detection, and swarm-based
scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.
By highlighting open research directions, we aim to foster innovation and guide
the development of next-generation defense strategies in an era marked by the
extensive use of UAVs.

</details>

### [159] [Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets](https://arxiv.org/abs/2504.11777)
*Yongpei Ma,Pengyu Wang,Adam Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TLDR: 这篇论文提出SEQA框架，使用大语言模型增强医疗视觉问答系统的语义等价问题，并引入新指标评估一致性。


<details>
  <summary>Details</summary>
Motivation: 解决MVQA系统因问题表述语言变异性导致的一致性问题。

Method: 提出SEQA框架利用LLMs生成语义等价问题重述，引入TAR-SC等指标，并增强SLAKE、VQA-RAD和PathVQA数据集。

Result: 数据集多样性指标提升：ANQI平均增加86.1，ANQA增加85.1，ANQS增加46；模型准确率平均提高19.35%，TAR-SC提升11.61%。

Conclusion: 框架显著改善MVQA模型的一致性和性能。

Abstract: Medical Visual Question Answering (MVQA) systems can interpret medical images
in response to natural language queries. However, linguistic variability in
question phrasing often undermines the consistency of these systems. To address
this challenge, we propose a Semantically Equivalent Question Augmentation
(SEQA) framework, which leverages large language models (LLMs) to generate
diverse yet semantically equivalent rephrasings of questions. Specifically,
this approach enriches linguistic diversity while preserving semantic meaning.
We further introduce an evaluation metric, Total Agreement Rate with
Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a
model's capability to generate consistent and correct responses to semantically
equivalent linguistic variations. In addition, we also propose three other
diversity metrics - average number of QA items per image (ANQI), average number
of questions per image with the same answer (ANQA), and average number of
open-ended questions per image with the same semantics (ANQS). Using the SEQA
framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,
and PathVQA. As a result, all three datasets achieved significant improvements
by incorporating more semantically equivalent questions: ANQI increased by an
average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate
three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and
fine-tuning settings on the enhanced datasets. Experimental results in MVQA
datasets show that fine-tuned models achieve an average accuracy improvement of
19.35%, while our proposed TAR-SC metric shows an average improvement of 11.
61%, indicating a substantial enhancement in model consistency.

</details>

### [160] [RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model](https://arxiv.org/abs/2504.12039)
*Yizhuo Wu,Francesco Fioranelli,Chang Gao*

Main category: cs.CV

TLDR: 本篇论文引入RadMamba，一种轻量级Mamba SSM，用于基于雷达的人类活动识别（HAR），以少量参数实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有雷达-based HAR模型计算需求高、资源受限问题，并利用transformer优势提升效率。

Method: 开发RadMamba，一种参数高效的Mamba SSM，针对雷达微多普勒数据优化设计。

Result: 在DIAT数据集上达到99.8%准确率，仅用前模型1/400参数；在CI4R数据集上达到92.0%准确率，仅用其他模型1/10参数；在UoG2020数据集上以6.7k参数超越其他模型至少3%。

Conclusion: RadMamba提供高效的雷达-based HAR解决方案，适合资源受限场景。

Abstract: Radar-based HAR has emerged as a promising alternative to conventional
monitoring approaches, such as wearable devices and camera-based systems, due
to its unique privacy preservation and robustness advantages. However, existing
solutions based on convolutional and recurrent neural networks, although
effective, are computationally demanding during deployment. This limits their
applicability in scenarios with constrained resources or those requiring
multiple sensors. Advanced architectures, such as ViT and SSM architectures,
offer improved modeling capabilities and have made efforts toward lightweight
designs. However, their computational complexity remains relatively high. To
leverage the strengths of transformer architectures while simultaneously
enhancing accuracy and reducing computational complexity, this paper introduces
RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM
specifically tailored for radar-based HAR. Across three diverse datasets,
RadMamba matches the top-performing previous model's 99.8% classification
accuracy on Dataset DIAT with only 1/400 of its parameters and equals the
leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their
parameters. In scenarios with continuous sequences of actions evaluated on
Dataset UoG2020, RadMamba surpasses other models with significantly higher
parameter counts by at least 3%, achieving this with only 6.7k parameters. Our
code is available at: https://github.com/lab-emi/AIRHAR.

</details>

### [161] [AttentionDrop: A Novel Regularization Method for Transformer Models](https://arxiv.org/abs/2504.12088)
*Mirza Samad Ahmed Baig,Syeda Anshrah Gillani,Abdul Akbar Khan,Shahid Munir Shah*

Main category: cs.CV

TLDR: 提出AttentionDrop，一种针对Transformer模型的自注意力正则化技术，以减少过拟合。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在多种任务中表现出色，但其巨大容量易导致过拟合，尤其在训练数据有限或噪声大的情况下。

Method: 引入AttentionDrop家族的三种变体：Hard Attention Masking（随机屏蔽top-k注意力logits）、Blurred Attention Smoothing（动态高斯卷积平滑注意力logits）和Consistency-Regularized AttentionDrop（通过KL一致性损失强制输出稳定）。

Result: 方法旨在通过操作自注意力分布提高模型泛化能力，潜在改善性能（摘要未详述具体结果）。

Conclusion: AttentionDrop技术可增强Transformer模型的鲁棒性和泛化效果。

Abstract: Transformer-based architectures achieve state-of-the-art performance across a
wide range of tasks in natural language processing, computer vision, and
speech. However, their immense capacity often leads to overfitting, especially
when training data is limited or noisy. We propose AttentionDrop, a unified
family of stochastic regularization techniques that operate directly on the
self-attention distributions. We introduces three variants: 1. Hard Attention
Masking: randomly zeroes out top-k attention logits per query to encourage
diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic
Gaussian convolution over attention logits to diffuse overly peaked
distributions. 3. Consistency-Regularized AttentionDrop: enforces output
stability under multiple independent AttentionDrop perturbations via a KL-based
consistency loss.

</details>

### [162] [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
*Laura Fieback,Nishilkumar Balar,Jakob Spiegelberg,Hanno Gottschalk*

Main category: cs.CV

TLDR: 提出ECD方法缓解LVLM的幻觉生成问题。


<details>
  <summary>Details</summary>
Motivation: LVLM在生成响应时易出现与视觉输入不符的幻觉，需要有效缓解。

Method: 引入Efficient Contrastive Decoding (ECD)，通过对比token概率和幻觉分数抑制幻觉，不需额外训练。

Result: 实验显示ECD有效减少幻觉，在性能和计算时间上优于现有方法。

Conclusion: ECD是一个简单、有效的幻觉缓解方法，可应用于各种开源LVLM。

Abstract: Despite recent advances in Large Vision Language Models (LVLMs), these models
still suffer from generating hallucinatory responses that do not align with the
visual input provided. To mitigate such hallucinations, we introduce Efficient
Contrastive Decoding (ECD), a simple method that leverages probabilistic
hallucination detection to shift the output distribution towards contextually
accurate answers at inference time. By contrasting token probabilities and
hallucination scores, ECD subtracts hallucinated concepts from the original
distribution, effectively suppressing hallucinations. Notably, our proposed
method can be applied to any open-source LVLM and does not require additional
LVLM training. We evaluate our method on several benchmark datasets and across
different LVLMs. Our experiments show that ECD effectively mitigates
hallucinations, outperforming state-of-the-art methods with respect to
performance on LVLM benchmarks and computation time.

</details>

### [163] [Exploring Video-Based Driver Activity Recognition under Noisy Labels](https://arxiv.org/abs/2504.11966)
*Linjuan Fan,Di Wen,Kunyu Peng,Kailun Yang,Jiaming Zhang,Ruiping Liu,Yufan Chen,Junwei Zheng,Jiamin Wu,Xudong Han,Rainer Stiefelhagen*

Main category: cs.CV

TLDR: 本文提出了一种针对驾驶员活动识别的标签噪声学习方法，基于聚类假设优化模型性能，并在公共数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 真实世界视频数据常有错误标签，影响模型可靠性和性能，而驾驶员活动识别领域对此鲜有探索。

Method: 基于聚类假设，学习低维表示并进行集群内共精炼，提出灵活样本选择策略并加入自适应参数。

Result: 在Drive&Act数据集实验中，方法性能优于其他图像分类去噪方法。

Conclusion: 该方法有效提升了驾驶员活动识别中标签噪声问题的处理，代码已开源。

Abstract: As an open research topic in the field of deep learning, learning with noisy
labels has attracted much attention and grown rapidly over the past ten years.
Learning with label noise is crucial for driver distraction behavior
recognition, as real-world video data often contains mislabeled samples,
impacting model reliability and performance. However, label noise learning is
barely explored in the driver activity recognition field. In this paper, we
propose the first label noise learning approach for the driver activity
recognition task. Based on the cluster assumption, we initially enable the
model to learn clustering-friendly low-dimensional representations from given
videos and assign the resultant embeddings into clusters. We subsequently
perform co-refinement within each cluster to smooth the classifier outputs.
Furthermore, we propose a flexible sample selection strategy that combines two
selection criteria without relying on any hyperparameters to filter clean
samples from the training dataset. We also incorporate a self-adaptive
parameter into the sample selection process to enforce balancing across
classes. A comprehensive variety of experiments on the public Drive&Act dataset
for all granularity levels demonstrates the superior performance of our method
in comparison with other label-denoising methods derived from the image
classification field. The source code is available at
https://github.com/ilonafan/DAR-noisy-labels.

</details>

### [164] [pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild](https://arxiv.org/abs/2504.12045)
*Jonas Myhre Schiøtt,Viktor Sebastian Petersen,Dimitrios P. Papadopoulos*

Main category: cs.CV

TLDR: 这篇论文开发了pix2pockets系统，使用计算机视觉和强化学习检测台球桌和球，并建议最佳击球。


<details>
  <summary>Details</summary>
Motivation: 论文旨在利用计算机视觉和强化学习在体育领域的进展，构建RL辅助的8球台球教练。

Method: 方法包括构建195张图像的数据集、手动标注对象、开发对象检测模型和标准化RL环境，并比较RL算法和简单基线。

Result: 对象检测AP50为91.2，球位置误差0.4 cm；RL算法无法无犯规清台；基线每击成功率94.7%，30%概率单回合清台。

Conclusion: 论文设定了基线，并显示RL在击球建议任务上仍有改进空间。

Abstract: Computer vision models have seen increased usage in sports, and reinforcement
learning (RL) is famous for beating humans in strategic games such as Chess and
Go. In this paper, we are interested in building upon these advances and
examining the game of classic 8-ball pool. We introduce pix2pockets, a
foundation for an RL-assisted pool coach. Given a single image of a pool table,
we first aim to detect the table and the balls and then propose the optimal
shot suggestion. For the first task, we build a dataset with 195 diverse images
where we manually annotate all balls and table dots, leading to 5748 object
segmentation masks. For the second task, we build a standardized RL environment
that allows easy development and benchmarking of any RL algorithm. Our object
detection model yields an AP50 of 91.2 while our ball location pipeline obtains
an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set
a baseline for the shot suggestion task and we show that all of them fail to
pocket all balls without making a foul move. We also present a simple baseline
that achieves a per-shot success rate of 94.7% and clears a full game in a
single turn 30% of the time.

</details>

### [165] [Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing](https://arxiv.org/abs/2504.12215)
*Ilkin Sevgi Isler,David Mohaisen,Curtis Lisle,Damla Turgut,Ulas Bagci*

Main category: cs.CV

TLDR: 这篇论文提出了一种不确定性引导的粗到细分割框架，用于CT扫描中的肿瘤分割，通过整合解剖学先验和不确定性建模提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决肿瘤分割中的边界模糊、类别不平衡和解剖变异性等挑战。

Method: 采用两阶段框架：第一阶段生成粗略预测并进行基于肺重叠、接近肺表面和组件大小的解剖学过滤；第二阶段使用不确定性感知损失函数训练模型以改进模糊区域的准确性和边界校准。

Result: 在Orlando数据集上，将Swin UNETR的Dice分数从0.4690提高到0.6447，改善Hausdorff分数，减少假阳性，并增强空间可解释性。

Conclusion: 结合不确定性建模和解剖学先验可提升级联分割管道的鲁棒性和临床意义，用于肿瘤描绘。

Abstract: Reliable tumor segmentation in thoracic computed tomography (CT) remains
challenging due to boundary ambiguity, class imbalance, and anatomical
variability. We propose an uncertainty-guided, coarse-to-fine segmentation
framework that combines full-volume tumor localization with refined
region-of-interest (ROI) segmentation, enhanced by anatomically aware
post-processing. The first-stage model generates a coarse prediction, followed
by anatomically informed filtering based on lung overlap, proximity to lung
surfaces, and component size. The resulting ROIs are segmented by a
second-stage model trained with uncertainty-aware loss functions to improve
accuracy and boundary calibration in ambiguous regions. Experiments on private
and public datasets demonstrate improvements in Dice and Hausdorff scores, with
fewer false positives and enhanced spatial interpretability. These results
highlight the value of combining uncertainty modeling and anatomical priors in
cascaded segmentation pipelines for robust and clinically meaningful tumor
delineation. On the Orlando dataset, our framework improved Swin UNETR Dice
from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated
with segmentation gains, underscoring the value of anatomically informed
post-processing.

</details>

### [166] [RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning](https://arxiv.org/abs/2504.12167)
*Yuan Luo,Rudolf Hoffmann,Yan Xia,Olaf Wysocki,Benedikt Schwab,Thomas H. Kolbe,Daniel Cremers*

Main category: cs.CV

TLDR: 这篇论文引入RadarCity数据集和RADLER神经网络，使用对比自监督学习和语义3D城市模型提升雷达物体检测。


<details>
  <summary>Details</summary>
Motivation: 语义3D城市模型易获取且信息丰富，但尚未充分用于减少雷达检测中的噪声影响。

Method: 提出RADLER网络，通过对比自监督学习获取雷达特征，并融合语义3D城市模型的语义深度特征。

Result: 在RadarCity数据集上，mAP平均提升5.46%，mAR平均提升3.51%。

Conclusion: 这项工作将推动语义引导和地图支持的雷达物体检测研究。

Abstract: Semantic 3D city models are worldwide easy-accessible, providing accurate,
object-oriented, and semantic-rich 3D priors. To date, their potential to
mitigate the noise impact on radar object detection remains under-explored. In
this paper, we first introduce a unique dataset, RadarCity, comprising 54K
synchronized radar-image pairs and semantic 3D city models. Moreover, we
propose a novel neural network, RADLER, leveraging the effectiveness of
contrastive self-supervised learning (SSL) and semantic 3D city models to
enhance radar object detection of pedestrians, cyclists, and cars.
Specifically, we first obtain the robust radar features via a SSL network in
the radar-image pretext task. We then use a simple yet effective feature fusion
strategy to incorporate semantic-depth features from semantic 3D city models.
Having prior 3D information as guidance, RADLER obtains more fine-grained
details to enhance radar object detection. We extensively evaluate RADLER on
the collected RadarCity dataset and demonstrate average improvements of 5.46%
in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over
previous radar object detection methods. We believe this work will foster
further research on semantic-guided and map-supported radar object detection.
Our project page is publicly available
athttps://gpp-communication.github.io/RADLER .

</details>

### [167] [FLIP Reasoning Challenge](https://arxiv.org/abs/2504.12256)
*Andreas Plesner,Turlan Kuzhagaliyev,Roger Wattenhofer*

Main category: cs.CV

TLDR: 本文引入FLIP数据集，用于评估AI在图像排序任务中的推理能力，发现现有模型准确率远低于人类，并通过字幕和集成方法提升性能。


<details>
  <summary>Details</summary>
Motivation: AI在感知和生成任务上进步显著，但推理能力仍薄弱，因此需要基于人类验证任务的基准如FLIP来评估。

Method: 使用FLIP数据集，基于Idena区块链的任务，评估VLMs和LLMs在识别图像逻辑顺序方面的性能，并测试字幕模型和模型集成方法。

Result: 最先进模型零样本准确率最高77.9%，人类为95.3%；字幕模型提升准确率至75.2%，集成15个模型达85.2%。

Conclusion: 揭示现有模型局限性，强调需要像FLIP这样的多模态基准来推动AI推理发展。

Abstract: Over the past years, advances in artificial intelligence (AI) have
demonstrated how AI can solve many perception and generation tasks, such as
image classification and text writing, yet reasoning remains a challenge. This
paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning
capabilities based on human verification tasks on the Idena blockchain. FLIP
challenges present users with two orderings of 4 images, requiring them to
identify the logically coherent one. By emphasizing sequential reasoning,
visual storytelling, and common sense, FLIP provides a unique testbed for
multimodal AI systems. Our experiments evaluate state-of-the-art models,
leveraging both vision-language models (VLMs) and large language models (LLMs).
Results reveal that even the best open-sourced and closed-sourced models
achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot
settings, compared to human performance of 95.3%. Captioning models aid
reasoning models by providing text descriptions of images, yielding better
results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5
Pro. Combining the predictions from 15 models in an ensemble increases the
accuracy to 85.2%. These findings highlight the limitations of existing
reasoning models and the need for robust multimodal benchmarks like FLIP. The
full codebase and dataset will be available at
https://github.com/aplesner/FLIP-Reasoning-Challenge.

</details>

### [168] [CoMotion: Concurrent Multi-person 3D Motion](https://arxiv.org/abs/2504.12186)
*Alejandro Newell,Peiyun Hu,Lahav Lipson,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TLDR: 本论文提出一种从单目摄像头流中检测和跟踪多个人的详细3D姿势方法，能够在拥挤场景中处理困难姿势和遮挡。


<details>
  <summary>Details</summary>
Motivation: 为了在拥挤且有遮挡的场景中维持时间连贯的多人3D姿势预测，提高跟踪鲁棒性。

Method: 模型结合强有力的每帧检测和学习姿势更新，直接从新输入图像更新姿势，而非跨时间匹配检测。

Result: 模型在3D姿势估计准确性上匹配最先进系统，同时在多人跟踪中更快更准确，并提供了代码和权重。

Conclusion: 该方法改进了多人3D姿势跟踪性能，特别是在在线处理遮挡时的有效性。

Abstract: We introduce an approach for detecting and tracking detailed 3D poses of
multiple people from a single monocular camera stream. Our system maintains
temporally coherent predictions in crowded scenes filled with difficult poses
and occlusions. Our model performs both strong per-frame detection and a
learned pose update to track people from frame to frame. Rather than match
detections across time, poses are updated directly from a new input image,
which enables online tracking through occlusion. We train on numerous image and
video datasets leveraging pseudo-labeled annotations to produce a model that
matches state-of-the-art systems in 3D pose estimation accuracy while being
faster and more accurate in tracking multiple people through time. Code and
weights are provided at https://github.com/apple/ml-comotion

</details>

### [169] [How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions](https://arxiv.org/abs/2504.12284)
*Aditya Prakash,Benjamin Lundell,Dmitry Andreychuk,David Forsyth,Saurabh Gupta,Harpreet Sawhney*

Main category: cs.CV

TLDR: 本论文解决从单RGB视图、动作文本和3D接触点预测3D手部运动和接触图的新问题，使用VQVAE和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 动机是处理一个新颖的预测手部交互轨迹问题，以提升人机交互或虚拟现实中的应用。

Method: 方法包括使用VQVAE学习交互代码本和Transformer解码器预测交互轨迹。

Result: 实验结果显示该方法优于基线模型，并具有良好的泛化能力。

Conclusion: 结论是该方法有效解决了手部交互预测问题，并为相关研究提供了基准。

Abstract: We tackle the novel problem of predicting 3D hand motion and contact maps (or
Interaction Trajectories) given a single RGB view, action text, and a 3D
contact point on the object as input. Our approach consists of (1) Interaction
Codebook: a VQVAE model to learn a latent codebook of hand poses and contact
points, effectively tokenizing interaction trajectories, (2) Interaction
Predictor: a transformer-decoder module to predict the interaction trajectory
from test time inputs by using an indexer module to retrieve a latent
affordance from the learned codebook. To train our model, we develop a data
engine that extracts 3D hand poses and contact trajectories from the diverse
HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger
than existing works, in terms of diversity of objects and interactions
observed, and test for generalization of the model across object categories,
action categories, tasks, and scenes. Experimental results show the
effectiveness of our approach over transformer & diffusion baselines across all
settings.

</details>

### [170] [SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians](https://arxiv.org/abs/2504.12292)
*Liam Schoneveld,Zhe Chen,Davide Davoli,Jiapeng Tang,Saimon Terazawa,Ko Nishino,Matthias Nießner*

Main category: cs.CV

TLDR: 本文提出SHeaP方法，使用2D高斯函数的自监督学习来提升从单目图像中重建3D人头模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于3D真实数据稀缺且现有可微渲染方法存在局限性，需要改进自监督学习从2D视频中学习的方法。

Method: 预测3DMM网格和绑定到网格的2D高斯函数，然后通过再现动画匹配目标帧并反向传播光度损失来优化。

Result: 在NoW基准和新的非中性表情基准上超越现有自监督方法，在几何评估和情感分类中表现出色。

Conclusion: 使用高斯函数渲染显著提高了自监督3D人头重建的有效性，实现了更高的准确性和表现力。

Abstract: Accurate, real-time 3D reconstruction of human heads from monocular images
and videos underlies numerous visual applications. As 3D ground truth data is
hard to come by at scale, previous methods have sought to learn from abundant
2D videos in a self-supervised manner. Typically, this involves the use of
differentiable mesh rendering, which is effective but faces limitations. To
improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor
Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a
set of Gaussians that are rigged to this mesh. We then reanimate this rigged
head avatar to match a target frame, and backpropagate photometric losses to
both the 3DMM and Gaussian prediction networks. We find that using Gaussians
for rendering substantially improves the effectiveness of this self-supervised
approach. Training solely on 2D data, our method surpasses existing
self-supervised approaches in geometric evaluations on the NoW benchmark for
neutral faces and a new benchmark for non-neutral expressions. Our method also
produces highly expressive meshes, outperforming state-of-the-art in emotion
classification.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [171] [Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework](https://arxiv.org/abs/2504.11469)
*Guillaume Garret,Antoine Vacavant,Carole Frindel*

Main category: eess.IV

TLDR: 本文提出一种新的3D血管分割可解释性管道，分析深度学习模型依赖局部线索而非全局解剖结构。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分割中表现优异，但黑箱性质限制临床应用，且对全局解剖结构的利用不明。

Method: 结合梯度归因、图指导点选择和基于斑点的显著性图分析，使用ground truth提取血管图定义兴趣点，并评估输入体素贡献。

Result: 在IRCAD和Bullitt数据集上，模型决策主要由局部归因斑点主导，与血管属性相关性低，表明全局解剖推理利用有限。

Conclusion: 强调结构化可解释性工具的重要性，并指出分割模型在捕获全局血管上下文方面的局限性。

Abstract: Deep learning models have achieved impressive performance in medical image
segmentation, yet their black-box nature limits clinical adoption. In vascular
applications, trustworthy segmentation should rely on both local image cues and
global anatomical structures, such as vessel connectivity or branching.
However, the extent to which models leverage such global context remains
unclear. We present a novel explainability pipeline for 3D vessel segmentation,
combining gradient-based attribution with graph-guided point selection and a
blob-based analysis of Saliency maps. Using vascular graphs extracted from
ground truth, we define anatomically meaningful points of interest (POIs) and
assess the contribution of input voxels via Saliency maps. These are analyzed
at both global and local scales using a custom blob detector. Applied to IRCAD
and Bullitt datasets, our analysis shows that model decisions are dominated by
highly localized attribution blobs centered near POIs. Attribution features
show little correlation with vessel-level properties such as thickness,
tubularity, or connectivity -- suggesting limited use of global anatomical
reasoning. Our results underline the importance of structured explainability
tools and highlight the current limitations of segmentation models in capturing
global vascular context.

</details>

### [172] [Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD](https://arxiv.org/abs/2504.11474)
*Byunggun Kim,Younghun Kwon*

Main category: eess.IV

TLDR: 提出了一种基于Transformer的ADHD诊断模型，通过学习脑部时空生物标志物，提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见精神疾病，影响儿童和成人，需要更有效的诊断方法。

Method: 设计了CNN-based embedding block、local temporal attention和ROI-rank based masking，用于学习脑部局部BOLD信号和重要ROI特征。

Result: 在ADHD-200数据集上，模型性能为ACC 77.78%、SPE 76.60%、SEN 79.22%、AUC 79.30%，优于其他Transformer变体。

Conclusion: 时空增强Transformer在ADHD诊断中表现出色，提供更精确的生物标志物。

Abstract: In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of
the common mental diseases discovered not only in children but also in adults.
In this context, we propose a ADHD diagnosis transformer model that can
effectively simultaneously find important brain spatiotemporal biomarkers from
resting-state functional magnetic resonance (rs-fMRI). This model not only
learns spatiotemporal individual features but also learns the correlation with
full attention structures specialized in ADHD diagnosis. In particular, it
focuses on learning local blood oxygenation level dependent (BOLD) signals and
distinguishing important regions of interest (ROI) in the brain. Specifically,
the three proposed methods for ADHD diagnosis transformer are as follows.
First, we design a CNN-based embedding block to obtain more expressive
embedding features in brain region attention. It is reconstructed based on the
previously CNN-based ADHD diagnosis models for the transformer. Next, for
individual spatiotemporal feature attention, we change the attention method to
local temporal attention and ROI-rank based masking. For the temporal features
of fMRI, the local temporal attention enables to learn local BOLD signal
features with only simple window masking. For the spatial feature of fMRI,
ROI-rank based masking can distinguish ROIs with high correlation in ROI
relationships based on attention scores, thereby providing a more specific
biomarker for ADHD diagnosis. The experiment was conducted with various types
of transformer models. To evaluate these models, we collected the data from 939
individuals from all sites provided by the ADHD-200 competition. Through this,
the spatiotemporal enhanced transformer for ADHD diagnosis outperforms the
performance of other different types of transformer variants. (77.78ACC
76.60SPE 79.22SEN 79.30AUC)

</details>

### [173] [Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images](https://arxiv.org/abs/2504.11491)
*Mansoor Hayat,Supavadee Aramvith,Subrata Bhattacharjee,Nouman Ahmad*

Main category: eess.IV

TLDR: 本研究提出Attention GhostUNet++模型，用于腹部脂肪和肝脏精确分割，取得了高Dice系数。


<details>
  <summary>Details</summary>
Motivation: 准确分割腹部脂肪和肝脏有助于理解身体组成及健康风险，如糖尿病和心血管疾病。

Method: 提出Attention GhostUNet++，通过整合通道、空间和深度注意力机制到Ghost UNet++瓶颈中实现自动分割。

Result: 在AATTCT-IDS和LiTS数据集上，Dice系数分别为VAT 0.9430、SAT 0.9639和肝脏0.9652，优于基线模型，并提升了特征细化和效率。

Conclusion: 尽管边界细节有轻微限制，但模型提供稳健的身体组成分析解决方案，代码开源。

Abstract: Accurate segmentation of abdominal adipose tissue, including subcutaneous
(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is
essential for understanding body composition and associated health risks such
as type 2 diabetes and cardiovascular disease. This study proposes Attention
GhostUNet++, a novel deep learning model incorporating Channel, Spatial, and
Depth Attention mechanisms into the Ghost UNet++ bottleneck for automated,
precise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model
achieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for
liver segmentation, surpassing baseline models. Despite minor limitations in
boundary detail segmentation, the proposed model significantly enhances feature
refinement, contextual understanding, and computational efficiency, offering a
robust solution for body composition analysis. The implementation of the
proposed Attention GhostUNet++ model is available
at:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus.

</details>

### [174] [Learned enclosure method for experimental EIT data](https://arxiv.org/abs/2504.11512)
*Sara Sippola,Siiri Rautio,Andreas Hauptmann,Takanori Ide,Samuli Siltanen*

Main category: eess.IV

TLDR: 本论文结合Ikehata的包围方法与神经网络，改进电阻抗层析成像的逆问题求解。


<details>
  <summary>Details</summary>
Motivation: 电阻抗层析成像的逆问题是高度病态和非线性的，难于准确求解，且对结合分析方法与机器学习的兴趣日益增加。

Method: 提出一种方法，将Ikehata的包围方法与神经网络结合，从边界测量估计包凸壳。

Result: 在模拟和实验数据上，表现优于经典包围方法结合最小二乘拟合。

Conclusion: 学到的包凸壳方法在性能上 superior，提升了逆问题求解的准确性。

Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method with
diverse applications, including medical imaging and non-destructive testing.
The inverse problem of reconstructing internal electrical conductivity from
boundary measurements is nonlinear and highly ill-posed, making it difficult to
solve accurately. In recent years, there has been growing interest in combining
analytical methods with machine learning to solve inverse problems. In this
paper, we propose a method for estimating the convex hull of inclusions from
boundary measurements by combining the enclosure method proposed by Ikehata
with neural networks. We demonstrate its performance using experimental data.
Compared to the classical enclosure method with least squares fitting, the
learned convex hull achieves superior performance on both simulated and
experimental data.

</details>

### [175] [Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography](https://arxiv.org/abs/2504.12249)
*Zhijin He,Alan B. McMillan*

Main category: eess.IV

TLDR: 本研究比较了放射组学和深度学习方法在胸部X光检测COVID-19、肺不透明和病毒性肺炎中的诊断性能。


<details>
  <summary>Details</summary>
Motivation: AI在医疗成像中的应用革命性，需要比较不同方法以在数据有限场景下提升诊断准确性，并指导临床实践。

Method: 系统比较放射组学模型（如决策树、梯度提升、随机森林、SVM、MLP）和深度学习模型（如CNN、ViT）的诊断准确性和鲁棒性，在不同样本大小下评估性能。

Result: 结果揭示了各模型的效能和适用情境，突出了特定AI方法在某些临床环境中的优势。

Conclusion: 研究填补了AI模型选择指导的空白，促进AI在自动化、高通量临床诊断中的应用。

Abstract: The application of artificial intelligence (AI) in medical imaging has
revolutionized diagnostic practices, enabling advanced analysis and
interpretation of radiological data. This study presents a comprehensive
evaluation of radiomics-based and deep learning-based approaches for disease
detection in chest radiography, focusing on COVID-19, lung opacity, and viral
pneumonia. While deep learning models, particularly convolutional neural
networks (CNNs) and vision transformers (ViTs), learn directly from image data,
radiomics-based models extract and analyze quantitative features, potentially
providing advantages in data-limited scenarios. This study systematically
compares the diagnostic accuracy and robustness of various AI models, including
Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines
(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against
state-of-the-art computer vision deep learning architectures. Performance
metrics across varying sample sizes reveal insights into each model's efficacy,
highlighting the contexts in which specific AI approaches may offer enhanced
diagnostic capabilities. The results aim to inform the integration of AI-driven
diagnostic tools in clinical practice, particularly in automated and
high-throughput environments where timely, reliable diagnosis is critical. This
comparative study addresses an essential gap, establishing guidance for the
selection of AI models based on clinical and operational needs.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [176] [Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder](https://arxiv.org/abs/2504.12005)
*Soobin Suh,Dabi Ahn,Heewoong Park,Jonghun Park*

Main category: cs.SD

TLDR: 这篇论文提出了一种使用条件变分自编码器（CVAE）和逆自回归流（IAF）来实现语音转换中多样化语调的方法。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换模型仅能为每个源输入产生一个结果，而说话者可从单一脚本产生不同语调的 utterances，因此需要克服这一限制。

Method: 提出使用CVAE将说话者风格特征映射到高斯分布的潜在空间，并通过IAF使潜在空间后验更复杂，以实现多样化语调的语音转换。

Result: 实验显示，转换语音不仅具有多样化语调，还比无CVAE模型有更好的音质。

Conclusion: 该方法成功提升了语音转换的语调多样性和音质。

Abstract: Voice conversion is a task of synthesizing an utterance with target speaker's
voice while maintaining linguistic information of the source utterance. While a
speaker can produce varying utterances from a single script with different
intonations, conventional voice conversion models were limited to producing
only one result per source input. To overcome this limitation, we propose a
novel approach for voice conversion with diverse intonations using conditional
variational autoencoder (CVAE). Experiments have shown that the speaker's style
feature can be mapped into a latent space with Gaussian distribution. We have
also been able to convert voices with more diverse intonation by making the
posterior of the latent space more complex with inverse autoregressive flow
(IAF). As a result, the converted voice not only has a diversity of
intonations, but also has better sound quality than the model without CVAE.

</details>

### [177] [Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML](https://arxiv.org/abs/2504.12272)
*Kong Ka Hing,Mehran Behjati*

Main category: cs.SD

TLDR: 本研究使用TinyML在边缘设备上分类马来西亚犀鸟叫声，实现高效实时野生动物监测。


<details>
  <summary>Details</summary>
Motivation: 犀鸟面临栖息地丧失、偷猎和环境变化威胁，传统监测资源密集，TinyML提供高效实时分析解决方案。

Method: 音频数据预处理、提取Mel-Frequency Energy特征、在Arduino Nano 33 BLE上部署模型，使用Edge Impulse训练和验证。

Result: 模型在真实世界测试中实现高精度犀鸟物种识别。

Conclusion: 强调TinyML在环境监测和生态保护中的潜力，为TinyML和野生动物保护领域贡献。

Abstract: Hornbills, an iconic species of Malaysia's biodiversity, face threats from
habi-tat loss, poaching, and environmental changes, necessitating accurate and
real-time population monitoring that is traditionally challenging and re-source
intensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to
transform wildlife monitoring by enabling efficient, real-time da-ta analysis
directly on edge devices. Addressing the challenge of wildlife conservation,
this research paper explores the pivotal role of machine learn-ing,
specifically TinyML, in the classification and monitoring of hornbill calls in
Malaysia. Leveraging audio data from the Xeno-canto database, the study aims to
develop a speech recognition system capable of identifying and classifying
hornbill vocalizations. The proposed methodology involves pre-processing the
audio data, extracting features using Mel-Frequency Energy (MFE), and deploying
the model on an Arduino Nano 33 BLE, which is adept at edge computing. The
research encompasses foundational work, in-cluding a comprehensive
introduction, literature review, and methodology. The model is trained using
Edge Impulse and validated through real-world tests, achieving high accuracy in
hornbill species identification. The project underscores the potential of
TinyML for environmental monitoring and its broader application in ecological
conservation efforts, contributing to both the field of TinyML and wildlife
conservation.

</details>

### [178] [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
*Mikhail Osipov*

Main category: cs.SD

TLDR: 本篇论文提出了一种基于几何的谱图变换方法，用于规范化构音障碍语音，实现零样本泛化并显著改善自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 为了处理构音障碍导致的语音畸变，提高自动语音识别在运动性言语障碍下的鲁棒性。

Method: 使用局部李群变换建模谱图的时间、频率和幅度畸变，通过神经网络从合成畸变数据中推断参数，并应用逆变换规范化真实输入。

Result: 在TORGO数据集上实现了高达16个百分点的词错误率减少，同时在干净语音上无性能下降，展示了零样本泛化的有效性。

Conclusion: 引入了一种原则性强、可解释的途径，提升了在运动性言语障碍下的语音识别鲁棒性。

Abstract: We present a geometry-driven method for normalizing dysarthric speech using
local Lie group transformations of spectrograms. Time, frequency, and amplitude
distortions are modeled as smooth, invertible deformations, parameterized by
scalar fields and applied via exponential maps. A neural network is trained to
infer these fields from synthetic distortions of typical speech-without using
any pathological data. At test time, the model applies an approximate inverse
to real dysarthric inputs. Despite zero-shot generalization, we observe
substantial ASR gains, including up to 16 percentage points WER reduction on
challenging TORGO samples, with no degradation on clean speech. This work
introduces a principled, interpretable approach for robust speech recognition
under motor speech disorders

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [179] [The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs](https://arxiv.org/abs/2504.11711)
*Haonan Li,Hang Zhang,Kexin Pei,Zhiyun Qian*

Main category: cs.SE

TLDR: BugLens 通过 LLM 提升静态分析精度，从 0.10/0.50 提高到 0.72。


<details>
  <summary>Details</summary>
Motivation: 静态分析在精度和可扩展性间权衡，常有高假阳性率；LLM 在代码理解上有潜力但直接应用不可靠。

Method: 引入 BugLens 框架，指导 LLM 遵循传统分析步骤，评估错误代码模式的安全影响并验证约束。

Result: 在 Linux 内核漏洞上，精度从 0.10 和 0.50 提高到 0.72，减少假阳性，发现四个新漏洞。

Conclusion: 结构化的 LLM 工作流可显著提升静态分析工具的有效性。

Abstract: Static analysis is a cornerstone for software vulnerability detection, yet it
often struggles with the classic precision-scalability trade-off. In practice,
such tools often produce high false positive rates, particularly in large
codebases like the Linux kernel. This imprecision can arise from simplified
vulnerability modeling and over-approximation of path and data constraints.
While large language models (LLMs) show promise in code understanding, their
naive application to program analysis yields unreliable results due to inherent
reasoning limitations. We introduce BugLens, a post-refinement framework that
significantly improves static analysis precision. BugLens guides an LLM to
follow traditional analysis steps by assessing buggy code patterns for security
impact and validating the constraints associated with static warnings.
Evaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10
(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing
false positives and revealing four previously unreported vulnerabilities. Our
results suggest that a structured LLM-based workflow can meaningfully enhance
the effectiveness of static analysis tools.

</details>

### [180] [Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph](https://arxiv.org/abs/2504.11502)
*Jatin Nainani,Chia-Tung Ho,Anirudh Dhurka,Haoxing Ren*

Main category: cs.SE

TLDR: 本文提出了一种基于多大型语言模型的时序分析代理，使用分层规划和Agentic RAG方法，实现了高通过率的VLSI电路时序报告调试。


<details>
  <summary>Details</summary>
Motivation: 技术进步导致VLSI电路时序调试挑战增大，需要高效智能方法；LLM在语言理解和决策中的潜力。

Method: 提出时序分析代理，结合多LLM分层规划、Timing Debug Relation Graph (TDRG)和Agentic RAG方法。

Result: 在工业设计基准上，单报告平均98%通过率，多报告90%通过率。

Conclusion: 证明了方法的有效性和适应性。

Abstract: Timing analysis is an essential and demanding verification method for Very
Large Scale Integrated (VLSI) circuit design and optimization. In addition, it
also serves as the cornerstone of the final sign-off, determining whether the
chip is ready to be sent to the semiconductor foundry for fabrication.
Recently, as the technology advance relentlessly, smaller metal pitches and the
increasing number of devices have led to greater challenges and longer
turn-around-time for experienced human designers to debug timing issues from
the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient
and intelligent methodology is highly necessary and essential for debugging
timing issues and reduce the turnaround times. Recently, Large Language Models
(LLMs) have shown great promise across various tasks in language understanding
and interactive decision-making, incorporating reasoning and actions. In this
work, we propose a timing analysis agent, that is empowered by multi-LLMs task
solving, and incorporates a novel hierarchical planning and solving flow to
automate the analysis of timing reports from commercial tool. In addition, we
build a Timing Debug Relation Graph (TDRG) that connects the reports with the
relationships of debug traces from experienced timing engineers. The timing
analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG)
approach, that includes agent and coding to retrieve data accurately, on the
developed TDRG. In our studies, the proposed timing analysis agent achieves an
average 98% pass-rate on a single-report benchmark and a 90% pass-rate for
multi-report benchmark from industrial designs, demonstrating its effectiveness
and adaptability.

</details>

### [181] [Agile Retrospectives: What went well? What didn't go well? What should we do?](https://arxiv.org/abs/2504.11780)
*Maria Spichkova,Hina Lee,Kevin Iwan,Madeleine Zwart,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TLDR: 这篇论文探讨了在敏捷/Scrum回顾会议中使用生成式AI进行信息交互和可视化，并介绍了原型工具RetroAI++。


<details>
  <summary>Details</summary>
Motivation: 动机是分析生成式AI在回顾会议中的潜在应用，以提升软件开发团队的信息交互和可视化效果。

Method: 方法包括对AI使用进行分析，并开发原型工具RetroAI++，专注于回顾会议的相关功能。

Result: 结果是呈现了原型工具RetroAI++，展示了工作的进展，但未提供具体成果。

Conclusion: 结论是这项工作仍在进行中，旨在展示AI在回顾会议中的潜力。

Abstract: In Agile/Scrum software development, the idea of retrospective meetings
(retros) is one of the core elements of the project process. In this paper, we
present our work in progress focusing on two aspects: analysis of potential
usage of generative AI for information interaction within retrospective
meetings, and visualisation of retros' information to software development
teams. We also present our prototype tool RetroAI++, focusing on retros-related
functionalities.

</details>

### [182] [Unravelling Technical debt topics through Time, Programming Languages and Repository](https://arxiv.org/abs/2504.11714)
*Karthik Shivashankar,Antonio Martini*

Main category: cs.SE

TLDR: 简而言之，本研究通过分析2015年至2023年GitHub问题，使用BERTopic主题建模和情感分析，探索软件工程中技术债务主题的演变、多样性和趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究广泛涉及技术债务的识别和量化，但对技术债务主题多样性和时间发展的理解仍存在显著空白。

Method: 采用BERTopic进行主题建模，并对2015年9月至2023年GitHub问题中提取的技术债务数据进行探索性分析，同时融入情感分析。

Result: 研究分类了技术债务主题，跟踪其时间演变，并通过情感分析揭示了这些主题在时间、编程语言和仓库中的感知趋势和变化。

Conclusion: 本研究深化了对技术债务动态景观的理解，有助于改进软件工程中的技术债务管理。

Abstract: This study explores the dynamic landscape of Technical Debt (TD) topics in
software engineering by examining its evolution across time, programming
languages, and repositories. Despite the extensive research on identifying and
quantifying TD, there remains a significant gap in understanding the diversity
of TD topics and their temporal development. To address this, we have conducted
an explorative analysis of TD data extracted from GitHub issues spanning from
2015 to September 2023. We employed BERTopic for sophisticated topic modelling.
This study categorises the TD topics and tracks their progression over time.
Furthermore, we have incorporated sentiment analysis for each identified topic,
providing a deeper insight into the perceptions and attitudes associated with
these topics. This offers a more nuanced understanding of the trends and shifts
in TD topics through time, programming language, and repository.

</details>

### [183] [On the calibration of Just-in-time Defect Prediction](https://arxiv.org/abs/2504.12051)
*Xhulja Shahini,Jone Bartel,Klaus Pohl*

Main category: cs.SE

TLDR: 本研究评估了Just in Time缺陷预测（JIT DP）模型的校准问题，发现存在失准现象，且后校准方法未能一致改善。


<details>
  <summary>Details</summary>
Motivation: 动机是减轻JIT DP中错误分类错误，提高预测可靠性和资源分配效率。

Method: 方法包括评估三个JIT DP技术的校准水平，并测试后校准方法的有效性。

Result: 结果显示模型的ECE在2-35%范围内有失准，后校准方法不一致地改善校准。

Conclusion: 结论是JIT DP模型存在校准问题，需要进一步优化以提升预测可靠性。

Abstract: Just in time defect prediction (JIT DP) leverages ML to identify defect-prone
code commits, enabling quality assurance (QA) teams to allocate resources more
efficiently by focusing on commits that are most likely to contain defects.
Although JIT DP techniques have introduced improvements in terms of predictive
accuracy, they are still susceptible to misclassification errors such as false
positives and negatives. This can lead to wasted resources or undetected
defects, a particularly critical concern when QA resources are limited. To
mitigate these challenges and preserve the practical utility of JIT DP tools,
it becomes essential to estimate the reliability of the predictions, i.e.,
computing confidence scores. Such scores can help practitioners determine the
trustworthiness of predictions and thus prioritize them efficiently. A simple
approach to computing confidence scores is to extract, alongside each
prediction, the corresponding prediction probabilities and use them as
indicators of confidence. However, for these probabilities to reliably serve as
confidence scores, the predictive model must be well-calibrated. This means
that the prediction probabilities must accurately represent the true likelihood
of each prediction being correct. Miscalibration, common in modern ML models,
distorts probability scores such that they do not align with the actual
correctness probability. In this study, we evaluate the calibration of three
JIT DP techniques to determine whether and to what extent they exhibit poor
calibration. Furthermore, we assess whether post-calibration methods can
improve the calibration of existing JIT defect prediction models. Our results
reveal that all evaluated JIT DP models exhibit some level of miscalibration,
with ECE ranging from 2-35%. Furthermore, post-calibration methods do not
consistently improve the calibration.

</details>

### [184] [From Requirements to Architecture: Semi-Automatically Generating Software Architectures](https://arxiv.org/abs/2504.12192)
*Tobias Eisenreich*

Main category: cs.SE

TLDR: 本论文提出利用大语言模型（LLMs）支持建筑师创建架构的新方法，通过工具协作实现时间节省。


<details>
  <summary>Details</summary>
Motivation: 为了支持初级和高级建筑师，充分利用LLMs的演进能力来提升架构设计过程。

Method: 方法涉及建筑师与LLM驱动工具的紧密合作，涵盖领域模型创建、用例规范、架构决策和架构评估，建筑师可完全控制或遵循过程以获得支持。

Result: 初步结果显示该方法可行，并能为建筑师节省大量时间。

Conclusion: 该方法具有可行性，并展示了在架构设计中应用LLMs的潜力。

Abstract: To support junior and senior architects, I propose developing a new
architecture creation method that leverages LLMs' evolving capabilities to
support the architect. This method involves the architect's close collaboration
with LLM-fueled tooling over the whole process. The architect is guided through
Domain Model creation, Use Case specification, architectural decisions, and
architecture evaluation. While the architect can take complete control of the
process and the results, and use the tooling as a building set, they can follow
the intended process for maximum tooling support. The preliminary results
suggest the feasibility of this process and indicate major time savings for the
architect.

</details>

<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [185] [Proof-Carrying Neuro-Symbolic Code](https://arxiv.org/abs/2504.12031)
*Ekaterina Komendantskaya*

Main category: cs.PL

TLDR: 本文介绍'证明携带神经符号代码'的概念，从神经和符号角度解释其意义和价值，并概述这一新研究领域的初步成功和挑战。


<details>
  <summary>Details</summary>
Motivation: 桥接神经和符号AI之间的差距，通过引入确保可证明正确性的概念，以满足可靠AI系统需求。

Method: 通过概念介绍、从不同视角的解释以及成功和挑战的概述，可能涉及理论讨论或案例分析。

Result: 概述了概念应用的初步成功，并指出了面临的关键挑战。

Conclusion: 这一新研究领域具有潜力，但需要克服挑战以实现进一步发展。

Abstract: This invited paper introduces the concept of "proof-carrying neuro-symbolic
code" and explains its meaning and value, from both the "neural" and the
"symbolic" perspectives. The talk outlines the first successes and challenges
that this new area of research faces.

</details>