<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 30]
- [cs.LG](#cs.LG) [Total: 96]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [math.ST](#math.ST) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [math.CA](#math.CA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 32]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 6]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 30]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [econ.GN](#econ.GN) [Total: 3]
- [cs.CL](#cs.CL) [Total: 22]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Toward Super Agent System with Hybrid AI Routers](https://arxiv.org/abs/2504.10519)
*Yuhang Yao,Haixin Wang,Yibo Chen,Jiawen Wang,Min Chang Jordan Ren,Bosheng Ding,Salman Avestimehr,Chaoyang He*

Main category: cs.AI

TLDR: 这篇论文设计了一个超级代理系统，使用混合本地和云端模型来高效处理用户任务。


<details>
  <summary>Details</summary>
Motivation: 为了使基于大语言模型的AI超级代理在现实世界中可部署，需要优化效率、降低成本并解决隐私问题。

Method: 系统通过检测用户意图、路由到专用代理或生成工作流，并动态选择本地或云端模型。

Result: 提出了一个增强型本地设备的超级代理蓝图，减少了对云端的依赖。

Conclusion: 展望未来，超级代理将通过本地计算和云端协作无缝融入日常生活。

Abstract: AI Agents powered by Large Language Models are transforming the world through
enormous applications. A super agent has the potential to fulfill diverse user
needs, such as summarization, coding, and research, by accurately understanding
user intent and leveraging the appropriate tools to solve tasks. However, to
make such an agent viable for real-world deployment and accessible at scale,
significant optimizations are required to ensure high efficiency and low cost.
This paper presents a design of the Super Agent System. Upon receiving a user
prompt, the system first detects the intent of the user, then routes the
request to specialized task agents with the necessary tools or automatically
generates agentic workflows. In practice, most applications directly serve as
AI assistants on edge devices such as phones and robots. As different language
models vary in capability and cloud-based models often entail high
computational costs, latency, and privacy concerns, we then explore the hybrid
mode where the router dynamically selects between local and cloud models based
on task complexity. Finally, we introduce the blueprint of an on-device super
agent enhanced with cloud. With advances in multi-modality models and edge
hardware, we envision that most computations can be handled locally, with cloud
collaboration only as needed. Such architecture paves the way for super agents
to be seamlessly integrated into everyday life in the near future.

</details>

### [2] [Explainable Artificial Intelligence techniques for interpretation of food datasets: a review](https://arxiv.org/abs/2504.10527)
*Leonardo Arrighi,Ingrid Alves de Moraes,Marco Zullich,Michele Simonato,Douglas Fernandes Barbin,Sylvio Barbon Junior*

Main category: cs.AI

TLDR: 这篇调查介绍了在食品工程中使用可解释AI（XAI）技术的分类法，以提高AI模型的可靠性和透明度，并指导研究人员选择合适的方法。


<details>
  <summary>Details</summary>
Motivation: AI在食品工程中的应用日益复杂，导致可靠性问题，而XAI尚未被充分利用，需要提升AI决策的透明度以满足食品质量标准。

Method: 呈现一个基于数据类型和解释方法的分类法，用于分类使用XAI技术的食品质量研究。

Result: 突出了XAI在食品工程中的趋势、挑战和机会，以鼓励其采用。

Conclusion: 通过提供分类法和见解，指导研究人员并推动XAI在食品工程中的应用。

Abstract: Artificial Intelligence (AI) has become essential for analyzing complex data
and solving highly-challenging tasks. It is being applied across numerous
disciplines beyond computer science, including Food Engineering, where there is
a growing demand for accurate and trustworthy predictions to meet stringent
food quality standards. However, this requires increasingly complex AI models,
raising reliability concerns. In response, eXplainable AI (XAI) has emerged to
provide insights into AI decision-making, aiding model interpretation by
developers and users. Nevertheless, XAI remains underutilized in Food
Engineering, limiting model reliability. For instance, in food quality control,
AI models using spectral imaging can detect contaminants or assess freshness
levels, but their opaque decision-making process hinders adoption. XAI
techniques such as SHAP (Shapley Additive Explanations) and Grad-CAM
(Gradient-weighted Class Activation Mapping) can pinpoint which spectral
wavelengths or image regions contribute most to a prediction, enhancing
transparency and aiding quality control inspectors in verifying AI-generated
assessments. This survey presents a taxonomy for classifying food quality
research using XAI techniques, organized by data types and explanation methods,
to guide researchers in choosing suitable approaches. We also highlight trends,
challenges, and opportunities to encourage the adoption of XAI in Food
Engineering.

</details>

### [3] [Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics](https://arxiv.org/abs/2504.10649)
*Matthew Zalesak,Hins Hu,Samitha Samaranayake*

Main category: cs.AI

TLDR: 这篇论文开源了乘车池模拟器，引入新算法，提升了效率，并在真实数据上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 缺乏开源实现导致基准测试困难，因此开发了开源工具以便算法比较。

Method: 实现了乘车池模拟器，包括车辆路由和再平衡组件，引入交换-based局部搜索启发式，并提出LA-MR-CE算法。

Result: 实验显示LA-MR-CE算法在服务率和计算时间上优越，所有短视算法受容量瓶颈限制。

Conclusion: 建议整合未来信息以克服性能瓶颈，提升算法效果。

Abstract: On-demand ride-pooling has emerged as a popular urban transportation
solution, addressing the efficiency limitations of traditional ride-hailing
services by grouping multiple riding requests with spatiotemporal proximity
into a single vehicle. Although numerous algorithms have been developed for the
Ride-pool Assignment Problem (RAP) -- a core component of ride-pooling systems,
there is a lack of open-source implementations, making it difficult to
benchmark these algorithms on a common dataset and objective. In this paper, we
present the implementation details of a ride-pool simulator that encompasses
several key ride-pool assignment algorithms, along with associated components
such as vehicle routing and rebalancing. We also open-source a highly optimized
and modular C++ codebase, designed to facilitate the extension of new
algorithms and features. Additionally, we introduce a family of swapping-based
local-search heuristics to enhance existing ride-pool assignment algorithms,
achieving a better balance between performance and computational efficiency.
Extensive experiments on a large-scale, real-world dataset from Manhattan, NYC
reveal that while all selected algorithms perform comparably, the newly
proposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE)
algorithm achieves a state-of-the-art service rate with significantly reduced
computational time. Furthermore, an in-depth analysis suggests that a
performance barrier exists for all myopic ride-pool assignment algorithms due
to the system's capacity bottleneck, and incorporating future information could
be key to overcoming this limitation.

</details>

### [4] [Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control](https://arxiv.org/abs/2504.10831)
*Hyojun Ahn,Seungcheol Oh,Gyu Seon Kim,Soyi Jung,Soohyun Park,Joongheon Kim*

Main category: cs.AI

TLDR: 本论文提出SafeGPT框架，结合GPT和RL提升UAV最后一段交付的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效可靠的无人机(UAV)最后一段交付，解决GPT可能产生的幻觉问题。

Method: SafeGPT采用两层结构：Global GPT分配高水平任务，On-Device GPT处理实时路线规划；RL安全过滤器监控并覆盖不安全决策；双重回放缓冲区优化策略。

Result: 模拟结果显示，SafeGPT比GPT-only基准有更高交付成功率，并显著减少电池消耗和旅行距离。

Conclusion: 验证了结合GPT语义推理与正式安全保证的有效性，为鲁棒节能的UAV物流提供解决方案。

Abstract: This paper proposes SafeGPT, a two-tiered framework that integrates
generative pretrained transformers (GPTs) with reinforcement learning (RL) for
efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In
the proposed design, a Global GPT module assigns high-level tasks such as
sector allocation, while an On-Device GPT manages real-time local route
planning. An RL-based safety filter monitors each GPT decision and overrides
unsafe actions that could lead to battery depletion or duplicate visits,
effectively mitigating hallucinations. Furthermore, a dual replay buffer
mechanism helps both the GPT modules and the RL agent refine their strategies
over time. Simulation results demonstrate that SafeGPT achieves higher delivery
success rates compared to a GPT-only baseline, while substantially reducing
battery consumption and travel distance. These findings validate the efficacy
of combining GPT-based semantic reasoning with formal safety guarantees,
contributing a viable solution for robust and energy-efficient UAV logistics.

</details>

### [5] [Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration](https://arxiv.org/abs/2504.10865)
*Han-Dong Lim,Donghwan Lee*

Main category: cs.AI

TLDR: 本文研究投影Bellman方程的理论属性、线性Q学习和近似值迭代算法，探讨解的存在性和收敛条件。


<details>
  <summary>Details</summary>
Motivation: 动机是分析投影Bellman方程解的存在以及算法收敛的充分条件。

Method: 方法包括使用严格负行主导对角假设和AVI收敛条件，检查线性Q学习的收敛，并观察ε-贪婪策略下的解。

Result: 结果显示SNRDD假设确保线性Q学习的收敛，并探讨其与AVI收敛的关系，提供相关观察。

Conclusion: 结论是对投影Bellman方程理论性质的深入理解和算法改进的启示。

Abstract: In this paper, we study the theoretical properties of the projected Bellman
equation (PBE) and two algorithms to solve this equation: linear Q-learning and
approximate value iteration (AVI). We consider two sufficient conditions for
the existence of a solution to PBE : strictly negatively row dominating
diagonal (SNRDD) assumption and a condition motivated by the convergence of
AVI. The SNRDD assumption also ensures the convergence of linear Q-learning,
and its relationship with the convergence of AVI is examined. Lastly, several
interesting observations on the solution of PBE are provided when using
$\epsilon$-greedy policy.

</details>

### [6] [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
*Yize Zhang,Tianshu Wang,Sirui Chen,Kun Wang,Xingyu Zeng,Hongyu Lin,Xianpei Han,Le Sun,Chaochao Lu*

Main category: cs.AI

TLDR: 本文提出ARise框架，通过风险评估、动态RAG和Monte Carlo树搜索优化LLMs推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在开放式知识密集推理中受限，现有方法存在泛化问题、错误传播和验证瓶颈。

Method: ARise框架整合中间状态风险评估、动态检索增强生成和Monte Carlo树搜索范式。

Result: 实验结果显示ARise比最先进KAR方法提升23.10%，比RAG增强模型提升25.37%。

Conclusion: ARise框架有效解决推理局限性，优化多分支假设推理计划。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities and
are receiving increasing attention to enhance their reasoning through scaling
test--time compute. However, their application in open--ended,
knowledge--intensive, complex reasoning scenarios is still limited.
Reasoning--oriented methods struggle to generalize to open--ended scenarios due
to implicit assumptions of complete world knowledge. Meanwhile,
knowledge--augmented reasoning (KAR) methods fail to address two core
challenges: 1) error propagation, where errors in early steps cascade through
the chain, and 2) verification bottleneck, where the explore--exploit tradeoff
arises in multi--branch decision processes. To overcome these limitations, we
introduce ARise, a novel framework that integrates risk assessment of
intermediate reasoning states with dynamic retrieval--augmented generation
(RAG) within a Monte Carlo tree search paradigm. This approach enables
effective construction and optimization of reasoning plans across multiple
maintained hypothesis branches. Experimental results show that ARise
significantly outperforms the state--of--the--art KAR methods by up to 23.10%,
and the latest RAG-equipped large reasoning models by up to 25.37%.

</details>

### [7] [Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior](https://arxiv.org/abs/2504.11075)
*Dongmin Kim,Hoshinori Kanazawa,Naoto Yoshida,Yasuo Kuniyoshi*

Main category: cs.AI

TLDR: 本论文提出'self-prior'密度模型，通过最小化过去和当前感官体验差异，诱导代理内在目标导向行为；在模拟环境中，代理自发伸手触碰触觉刺激，展示了内在动机行为的涌现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注探索如何获得外部奖励，本论文探讨纯内在过程如何驱动无外部奖励的 spontaneous 探索和学习。

Method: 提出'self-prior'多模态感官体验密度模型，并整合到基于自由能量原理的主动推理框架中；在模拟环境中测试代理行为。

Result: 代理自发地伸手触碰触觉刺激，证实了内在动机机制的有效性。

Conclusion: 展示了代理通过自身感官体验塑造目标导向行为，类似于婴儿早期发展的 intentional 行为涌现。

Abstract: Infants often exhibit goal-directed behaviors, such as reaching for a sensory
stimulus, even when no external reward criterion is provided. These
intrinsically motivated behaviors facilitate spontaneous exploration and
learning of the body and environment during early developmental stages.
Although computational modeling can offer insight into the mechanisms
underlying such behaviors, many existing studies on intrinsic motivation focus
primarily on how exploration contributes to acquiring external rewards. In this
paper, we propose a novel density model for an agent's own multimodal sensory
experiences, called the "self-prior," and investigate whether it can
autonomously induce goal-directed behavior. Integrated within an active
inference framework based on the free energy principle, the self-prior
generates behavioral references purely from an intrinsic process that minimizes
mismatches between average past sensory experiences and current observations.
This mechanism is also analogous to the acquisition and utilization of a body
schema through continuous interaction with the environment. We examine this
approach in a simulated environment and confirm that the agent spontaneously
reaches toward a tactile stimulus. Our study implements intrinsically motivated
behavior shaped by the agent's own sensory experiences, demonstrating the
spontaneous emergence of intentional behavior during early development.

</details>

### [8] [C-SHAP for time series: An approach to high-level temporal explanations](https://arxiv.org/abs/2504.11159)
*Annemarie Jutte,Faizan Ahmed,Jeroen Linssen,Maurice van Keulen*

Main category: cs.AI

TLDR: 本文提出C-SHAP方法，用于基于概念解释时间序列AI模型，提供高级模式解释。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法仅解释低级模式，忽略高水平模式的影响，因此需要概念-based方法提升AI可靠性。

Method: 提出C-SHAP方法，并使用时间序列分解作为示例实现。

Result: 通过能源领域用例证明方法有效性。

Conclusion: C-SHAP提升了时间序列AI模型的可解释性，捕捉高水平模式。

Abstract: Time series are ubiquitous in domains such as energy forecasting, healthcare,
and industry. Using AI systems, some tasks within these domains can be
efficiently handled. Explainable AI (XAI) aims to increase the reliability of
AI solutions by explaining model reasoning. For time series, many XAI methods
provide point- or sequence-based attribution maps. These methods explain model
reasoning in terms of low-level patterns. However, they do not capture
high-level patterns that may also influence model reasoning. We propose a
concept-based method to provide explanations in terms of these high-level
patterns. In this paper, we present C-SHAP for time series, an approach which
determines the contribution of concepts to a model outcome. We provide a
general definition of C-SHAP and present an example implementation using time
series decomposition. Additionally, we demonstrate the effectiveness of the
methodology through a use case from the energy domain.

</details>

### [9] [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.AI

TLDR: 这篇论文提出了一种逻辑增强生成框架，使用语义知识图来提升大型语言模型在隐喻检测和理解中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型难以从自然语言中提取隐含知识，因此使用语义知识图作为概念空间来提高推理效率和可解释性。

Method: 应用逻辑增强生成框架，结合语义知识图和提示启发式，生成隐含的类比连接，并用于隐喻检测任务。

Result: 方法超越基线，在视觉隐喻理解上优于人类，但对领域特定隐喻有局限，并进行了错误分析。

Conclusion: 该方法提高了推理的可解释性，但仍存在挑战，建议改进隐喻注释和评估方法。

Abstract: Recent advances in Large Language Models have demonstrated their capabilities
across a variety of tasks. However, automatically extracting implicit knowledge
from natural language remains a significant challenge, as machines lack active
experience with the physical world. Given this scenario, semantic knowledge
graphs can serve as conceptual spaces that guide the automated text generation
reasoning process to achieve more efficient and explainable results. In this
paper, we apply a logic-augmented generation (LAG) framework that leverages the
explicit representation of a text through a semantic knowledge graph and
applies it in combination with prompt heuristics to elicit implicit analogical
connections. This method generates extended knowledge graph triples
representing implicit meaning, enabling systems to reason on unlabeled
multimodal data regardless of the domain. We validate our work through three
metaphor detection and understanding tasks across four datasets, as they
require deep analogical reasoning capabilities. The results show that this
integrated approach surpasses current baselines, performs better than humans in
understanding visual metaphors, and enables more explainable reasoning
processes, though still has inherent limitations in metaphor understanding,
especially for domain-specific metaphors. Furthermore, we propose a thorough
error analysis, discussing issues with metaphorical annotations and current
evaluation methods.

</details>

### [10] [Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs](https://arxiv.org/abs/2504.11200)
*Irene Celino,Mario Scrocca,Agnese Chiatti*

Main category: cs.AI

TLDR: 这篇论文探讨了神经符号人工智能（NeSy AI）如何通过结合符号知识和数据驱动学习来提升人类与系统之间的相互理解，涵盖知识分享、交换和治理三个维度，并通过案例分析识别挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是强调NeSy AI在促进人类与系统相互理解中的重要性，通过定义三个关键维度来表征和改进这一过程。

Method: 方法包括引入知识分享、交换和治理三个维度，呈现NeSy AI和知识图谱的应用案例，并分析这些方法在不同维度的覆盖情况。

Result: 结果突出了NeSy AI的潜力与挑战，并识别了未来研究中相互理解的空白和不足。

Conclusion: 结论是，这种分析有助于指导未来研究，解决当前方法在相互理解方面的局限性。

Abstract: This chapter investigates the concept of mutual understanding between humans
and systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)
methods can significantly enhance this mutual understanding by leveraging
explicit symbolic knowledge representations with data-driven learning models.
We start by introducing three critical dimensions to characterize mutual
understanding: sharing knowledge, exchanging knowledge, and governing
knowledge. Sharing knowledge involves aligning the conceptual models of
different agents to enable a shared understanding of the domain of interest.
Exchanging knowledge relates to ensuring the effective and accurate
communication between agents. Governing knowledge concerns establishing rules
and processes to regulate the interaction between agents. Then, we present
several different use case scenarios that demonstrate the application of NeSy
AI and Knowledge Graphs to aid meaningful exchanges between human, artificial,
and robotic agents. These scenarios highlight both the potential and the
challenges of combining top-down symbolic reasoning with bottom-up neural
learning, guiding the discussion of the coverage provided by current solutions
along the dimensions of sharing, exchanging, and governing knowledge.
Concurrently, this analysis facilitates the identification of gaps and less
developed aspects in mutual understanding to address in future research.

</details>

### [11] [Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs](https://arxiv.org/abs/2504.11239)
*Chang Yang,Ruiyu Wang,Junzhe Jiang,Qi Jiang,Qinggang Zhang,Yanchen Deng,Shuxin Li,Shuyue Hu,Bo Li,Florian T. Pokorny,Xiao Huang,Xinrun Wang*

Main category: cs.AI

TLDR: 本文提出NPPC，一个针对大语言模型的永续扩展推理基准测试，专注于NP完全问题，旨在不可击溃、不可黑客攻击、自动可验证和通用。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试容易被快速击溃和黑客攻击，因此需要构建一个永续扩展、不可击溃的基准测试。

Method: 提出NPPC框架，包括npgym（生成NP完全问题实例）、npsolver（评估LLMs性能）和npeval（分析性能指标）三个模块。

Result: 实验显示NPPC将高级LLMs性能降至10%以下，DeepSeek-R1表现最佳，并观察到令牌数和顿悟时刻随难度变化的模式。

Conclusion: NPPC是首个永续扩展推理基准测试，可作为AGI发展的不可击溃测试平台。

Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due
to the rapid progress of LLMs, there are two main issues of current benchmarks:
i) these benchmarks can be crushed in a short time (less than 1 year), and ii)
these benchmarks may be easily hacked. To handle these issues, we propose the
ever-scalingness for building the benchmarks which are uncrushable, unhackable,
auto-verifiable and general. This paper presents Nondeterministic
Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark
for LLMs. Specifically, the NPPC has three main modules: i) npgym, which
provides a unified interface of 25 well-known NP-complete problems and can
generate any number of instances with any levels of complexities, ii) npsolver:
which provides a unified interface to evaluate the problem instances with both
online and offline models via APIs and local deployments, respectively, and
iii) npeval: which provides the comprehensive and ready-to-use tools to analyze
the performances of LLMs over different problems, the number of tokens, the aha
moments, the reasoning errors and the solution errors. Extensive experiments
over widely-used LLMs demonstrate: i) NPPC can successfully decrease the
performances of advanced LLMs' performances to below 10%, demonstrating that
NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the
most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and
o1/o3-mini in most NP-complete problems considered, and iii) the numbers of
tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and
DeepSeek-R1, are observed first to increase and then decrease when the problem
instances become more and more difficult. We believe that NPPC is the first
ever-scaling reasoning benchmark, serving as the uncrushable and unhackable
testbed for LLMs toward artificial general intelligence (AGI).

</details>

### [12] [Towards Automated Safety Requirements Derivation Using Agent-based RAG](https://arxiv.org/abs/2504.11243)
*Balahari Vignesh Balu,Florian Geissler,Francesco Carella,Joao-Vitor Zacchi,Josef Jiru,Nuria Mata,Reinhard Stolle*

Main category: cs.AI

TLDR: 本文提出使用代理-based RAG结合LLM自动推导自驾车安全要求，解决了传统方法在复杂查询中的相关性问题，并在Apollo案例中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统LLM缺乏领域知识，现有RAG在处理安全相关复杂查询时性能下降，本文旨在通过代理-based RAG提升信息检索相关性。

Method: 提出代理-based RAG方法，应用于汽车标准和Apollo案例，使用从Apollo数据提取的安全要求问题集进行实现。

Result: 通过RAG指标评估，代理-based 方法比默认RAG检索信息更相关，并讨论了其优势。

Conclusion: 代理-based RAG在自驾车安全要求推导中更有效，提供更好的性能和相关性。

Abstract: We study the automated derivation of safety requirements in a self-driving
vehicle use case, leveraging LLMs in combination with agent-based
retrieval-augmented generation. Conventional approaches that utilise
pre-trained LLMs to assist in safety analyses typically lack domain-specific
knowledge. Existing RAG approaches address this issue, yet their performance
deteriorates when handling complex queries and it becomes increasingly harder
to retrieve the most relevant information. This is particularly relevant for
safety-relevant applications. In this paper, we propose the use of agent-based
RAG to derive safety requirements and show that the retrieved information is
more relevant to the queries. We implement an agent-based approach on a
document pool of automotive standards and the Apollo case study, as a
representative example of an automated driving perception system. Our solution
is tested on a data set of safety requirement questions and answers, extracted
from the Apollo data. Evaluating a set of selected RAG metrics, we present and
discuss advantages of a agent-based approach compared to default RAG methods.

</details>

### [13] [Learning to Be A Doctor: Searching for Effective Medical Agent Architectures](https://arxiv.org/abs/2504.11301)
*Yangyang Zhuang,Wenjia Jiang,Jiayu Zhang,Ze Yang,Joey Tianyi Zhou,Chi Zhang*

Main category: cs.AI

TLDR: 这篇论文提出了一种自动设计医疗代理架构的框架，使用大型语言模型（LLM），通过分层搜索空间实现动态适应和诊断准确性提升。


<details>
  <summary>Details</summary>
Motivation: 现有医疗代理系统依赖静态工作流，缺乏灵活性；受AutoML成功启发，需要自动设计以适应多样化诊断需求。

Method: 定义分层代理搜索空间，将医疗代理视为基于图的架构，支持节点、结构和框架级修改，以及诊断反馈引导的迭代自改进。

Result: 在皮肤病诊断任务的实验中，方法有效演化工作流结构，并显著提高诊断准确性。

Conclusion: 这是第一个完全自动化的医疗代理架构设计框架，为在真实临床环境中部署智能代理提供可扩展的基础。

Abstract: Large Language Model (LLM)-based agents have demonstrated strong capabilities
across a wide range of tasks, and their application in the medical domain holds
particular promise due to the demand for high generalizability and reliance on
interdisciplinary knowledge. However, existing medical agent systems often rely
on static, manually crafted workflows that lack the flexibility to accommodate
diverse diagnostic requirements and adapt to emerging clinical scenarios.
Motivated by the success of automated machine learning (AutoML), this paper
introduces a novel framework for the automated design of medical agent
architectures. Specifically, we define a hierarchical and expressive agent
search space that enables dynamic workflow adaptation through structured
modifications at the node, structural, and framework levels. Our framework
conceptualizes medical agents as graph-based architectures composed of diverse,
functional node types and supports iterative self-improvement guided by
diagnostic feedback. Experimental results on skin disease diagnosis tasks
demonstrate that the proposed method effectively evolves workflow structures
and significantly enhances diagnostic accuracy over time. This work represents
the first fully automated framework for medical agent architecture design and
offers a scalable, adaptable foundation for deploying intelligent agents in
real-world clinical environments.

</details>

### [14] [Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.11354)
*Haiming Wang,Mert Unsal,Xiaohan Lin,Mantas Baksys,Junqi Liu,Marco Dos Santos,Flood Sung,Marina Vinyes,Zhenzhe Ying,Zekai Zhu,Jianqiao Lu,Hugues de Saxcé,Bolton Bailey,Chendong Song,Chenjun Xiao,Dehao Zhang,Ebony Zhang,Frederick Pu,Han Zhu,Jiawei Liu,Jonas Bayer,Julien Michel,Longhui Yu,Léo Dreyfus-Schmidt,Lewis Tunstall,Luigi Pagani,Moreira Machado,Pauline Bourigault,Ran Wang,Stanislas Polu,Thibaut Barroyer,Wen-Ding Li,Yazhe Niu,Yann Fleureau,Yangyang Hu,Zhouliang Yu,Zihan Wang,Zhilin Yang,Zhengying Liu,Jia Li*

Main category: cs.AI

TLDR: Kimina-Prover 是一个新型推理驱动的正式定理证明模型，使用强化学习训练，在 Lean 4 证明生成中达到 miniF2F 基准 80.7% pass@8192 的新纪录，并开源精简版本。


<details>
  <summary>Details</summary>
Motivation: 创新推理范式，模拟人类问题解决策略，桥接正式验证与非正式数学直觉的差距。

Method: 采用大规模强化学习管道从 Qwen2.5-72B 训练，使用 '正式推理模式' 迭代生成和完善证明步骤。

Result: 在 miniF2F 基准上达到 80.7% pass@8192 的新状态-of-the-art，高样本效率、模型大小性能 scaling，以及独特推理风格。

Conclusion: 展示了样本效率、性能 scaling 的优势，并有潜力弥合正式验证与数学直觉的差距；开源 1.5B 和 7B 参数版本。

Abstract: We introduce Kimina-Prover Preview, a large language model that pioneers a
novel reasoning-driven exploration paradigm for formal theorem proving, as
showcased in this preview release. Trained with a large-scale reinforcement
learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong
performance in Lean 4 proof generation by employing a structured reasoning
pattern we term \textit{formal reasoning pattern}. This approach allows the
model to emulate human problem-solving strategies in Lean, iteratively
generating and refining proof steps. Kimina-Prover sets a new state-of-the-art
on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved
benchmark performance, our work yields several key insights: (1) Kimina-Prover
exhibits high sample efficiency, delivering strong results even with minimal
sampling (pass@1) and scaling effectively with computational budget, stemming
from its unique reasoning pattern and RL training; (2) we demonstrate clear
performance scaling with model size, a trend previously unobserved for neural
theorem provers in formal mathematics; (3) the learned reasoning style,
distinct from traditional search algorithms, shows potential to bridge the gap
between formal verification and informal mathematical intuition. We open source
distilled versions with 1.5B and 7B parameters of Kimina-Prover

</details>

### [15] [Embodied World Models Emerge from Navigational Task in Open-Ended Environments](https://arxiv.org/abs/2504.11419)
*Li Jin,Liu Jia*

Main category: cs.AI

TLDR: 本研究通过元强化学习和神经网络，展示了AI代理如何通过主动互动学习空间意识，并通过因果验证确认神经表示的有效性。


<details>
  <summary>Details</summary>
Motivation: AI研究中，空间意识和推理发展长期面临挑战，传统模型依赖被动观察，而具身认知理论强调主动互动的重要性。

Method: 使用门控循环单元(GRU)结合元强化学习(Meta-RL)，引入混合动力系统(HDS)建模交互，脊表示映射路径，规范相关分析(CCA)确认对齐，并进行干预实验。

Result: 代理学会编码方向、距离和障碍避免，显示稳定极限环，CCA确认神经状态与行为空间强对齐，干预实验证实特定神经维度与导航性能的因果联系。

Conclusion: 桥接行动与感知的鸿沟，提供构建适应性、可解释AI模型的洞见，并为理解和控制AI内部机制开辟新途径。

Abstract: Understanding how artificial systems can develop spatial awareness and
reasoning has long been a challenge in AI research. Traditional models often
rely on passive observation, but embodied cognition theory suggests that deeper
understanding emerges from active interaction with the environment. This study
investigates whether neural networks can autonomously internalize spatial
concepts through interaction, focusing on planar navigation tasks. Using Gated
Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we
show that agents can learn to encode spatial properties like direction,
distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)
to model the agent-environment interaction as a closed dynamical system,
revealing stable limit cycles that correspond to optimal navigation strategies.
Ridge Representation allows us to map navigation paths into a fixed-dimensional
behavioral space, enabling comparison with neural states. Canonical Correlation
Analysis (CCA) confirms strong alignment between these representations,
suggesting that the agent's neural states actively encode spatial knowledge.
Intervention experiments further show that specific neural dimensions are
causally linked to navigation performance. This work provides an approach to
bridging the gap between action and perception in AI, offering new insights
into building adaptive, interpretable models that can generalize across complex
environments. The causal validation of neural representations also opens new
avenues for understanding and controlling the internal mechanisms of AI
systems, pushing the boundaries of how machines learn and reason in dynamic,
real-world scenarios.

</details>

### [16] [Toward Super Agent System with Hybrid AI Routers](https://arxiv.org/abs/2504.10519)
*Yuhang Yao,Haixin Wang,Yibo Chen,Jiawen Wang,Min Chang Jordan Ren,Bosheng Ding,Salman Avestimehr,Chaoyang He*

Main category: cs.AI

TLDR: 这篇论文介绍了超级代理系统的设计，旨在通过意图检测、任务路由和混合计算模式实现高效、低成本的AI应用。


<details>
  <summary>Details</summary>
Motivation: AI代理需要优化以实现大规模部署，解决效率、成本和隐私问题。

Method: 系统检测用户意图，路由请求到专用代理或生成工作流，并动态选择本地或云端模型。

Result: 提出了一种增强云端的本地超级代理蓝图。

Conclusion: 展望超级代理通过技术进步无缝融入日常生活。

Abstract: AI Agents powered by Large Language Models are transforming the world through
enormous applications. A super agent has the potential to fulfill diverse user
needs, such as summarization, coding, and research, by accurately understanding
user intent and leveraging the appropriate tools to solve tasks. However, to
make such an agent viable for real-world deployment and accessible at scale,
significant optimizations are required to ensure high efficiency and low cost.
This paper presents a design of the Super Agent System. Upon receiving a user
prompt, the system first detects the intent of the user, then routes the
request to specialized task agents with the necessary tools or automatically
generates agentic workflows. In practice, most applications directly serve as
AI assistants on edge devices such as phones and robots. As different language
models vary in capability and cloud-based models often entail high
computational costs, latency, and privacy concerns, we then explore the hybrid
mode where the router dynamically selects between local and cloud models based
on task complexity. Finally, we introduce the blueprint of an on-device super
agent enhanced with cloud. With advances in multi-modality models and edge
hardware, we envision that most computations can be handled locally, with cloud
collaboration only as needed. Such architecture paves the way for super agents
to be seamlessly integrated into everyday life in the near future.

</details>

### [17] [Explainable Artificial Intelligence techniques for interpretation of food datasets: a review](https://arxiv.org/abs/2504.10527)
*Leonardo Arrighi,Ingrid Alves de Moraes,Marco Zullich,Michele Simonato,Douglas Fernandes Barbin,Sylvio Barbon Junior*

Main category: cs.AI

TLDR: 这篇调查通过分类法组织XAI在食品质量研究中的应用，并讨论趋势、挑战和机会。


<details>
  <summary>Details</summary>
Motivation: AI模型在食品工程中复杂性增加，可靠性担忧突出，XAI使用不足，需要提升透明度。

Method: 提出基于数据类型和解释方法的分类法，用于指导研究人员选择XAI方法。

Result: 突出了XAI在食品工程中的趋势、挑战和机会，以促进其采用。

Conclusion: 鼓励在食品工程中采用XAI，通过分类法和讨论提供指导。

Abstract: Artificial Intelligence (AI) has become essential for analyzing complex data
and solving highly-challenging tasks. It is being applied across numerous
disciplines beyond computer science, including Food Engineering, where there is
a growing demand for accurate and trustworthy predictions to meet stringent
food quality standards. However, this requires increasingly complex AI models,
raising reliability concerns. In response, eXplainable AI (XAI) has emerged to
provide insights into AI decision-making, aiding model interpretation by
developers and users. Nevertheless, XAI remains underutilized in Food
Engineering, limiting model reliability. For instance, in food quality control,
AI models using spectral imaging can detect contaminants or assess freshness
levels, but their opaque decision-making process hinders adoption. XAI
techniques such as SHAP (Shapley Additive Explanations) and Grad-CAM
(Gradient-weighted Class Activation Mapping) can pinpoint which spectral
wavelengths or image regions contribute most to a prediction, enhancing
transparency and aiding quality control inspectors in verifying AI-generated
assessments. This survey presents a taxonomy for classifying food quality
research using XAI techniques, organized by data types and explanation methods,
to guide researchers in choosing suitable approaches. We also highlight trends,
challenges, and opportunities to encourage the adoption of XAI in Food
Engineering.

</details>

### [18] [Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics](https://arxiv.org/abs/2504.10649)
*Matthew Zalesak,Hins Hu,Samitha Samaranayake*

Main category: cs.AI

TLDR: 本文提供开源乘车池模拟器，实现了关键算法，并提出高效新算法LA-MR-CE。


<details>
  <summary>Details</summary>
Motivation: 缺乏乘车池分配算法开源实现，难以进行基准测试。

Method: 实现C++基于模拟器，包括车辆路由、再平衡和交换式局部搜索启发式方法，以及LA-MR-CE算法。

Result: 曼哈顿数据集实验显示LA-MR-CE算法服务率高、计算时间短；所有短视算法存在容量瓶颈。

Conclusion: 克服性能限制需纳入未来信息。

Abstract: On-demand ride-pooling has emerged as a popular urban transportation
solution, addressing the efficiency limitations of traditional ride-hailing
services by grouping multiple riding requests with spatiotemporal proximity
into a single vehicle. Although numerous algorithms have been developed for the
Ride-pool Assignment Problem (RAP) -- a core component of ride-pooling systems,
there is a lack of open-source implementations, making it difficult to
benchmark these algorithms on a common dataset and objective. In this paper, we
present the implementation details of a ride-pool simulator that encompasses
several key ride-pool assignment algorithms, along with associated components
such as vehicle routing and rebalancing. We also open-source a highly optimized
and modular C++ codebase, designed to facilitate the extension of new
algorithms and features. Additionally, we introduce a family of swapping-based
local-search heuristics to enhance existing ride-pool assignment algorithms,
achieving a better balance between performance and computational efficiency.
Extensive experiments on a large-scale, real-world dataset from Manhattan, NYC
reveal that while all selected algorithms perform comparably, the newly
proposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE)
algorithm achieves a state-of-the-art service rate with significantly reduced
computational time. Furthermore, an in-depth analysis suggests that a
performance barrier exists for all myopic ride-pool assignment algorithms due
to the system's capacity bottleneck, and incorporating future information could
be key to overcoming this limitation.

</details>

### [19] [Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control](https://arxiv.org/abs/2504.10831)
*Hyojun Ahn,Seungcheol Oh,Gyu Seon Kim,Soyi Jung,Soohyun Park,Joongheon Kim*

Main category: cs.AI

TLDR: 本文提出SafeGPT框架，将GPT与RL结合，用于高效可靠的UAV末端配送，提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 动机是解决UAV配送中的效率和安全问题，特别是GPT模型的幻觉问题。

Method: 方法包括两层GPT框架（全球GPT分配任务、设备端GPT实时规划）、RL-based安全过滤器和双重重放缓冲机制。

Result: 结果显示SafeGPT比GPT-only基线有更高配送成功率，并减少电池消耗和旅行距离。

Conclusion: 结论验证了GPT语义推理与安全保证结合的有效性，为稳健节能的UAV物流提供方案。

Abstract: This paper proposes SafeGPT, a two-tiered framework that integrates
generative pretrained transformers (GPTs) with reinforcement learning (RL) for
efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In
the proposed design, a Global GPT module assigns high-level tasks such as
sector allocation, while an On-Device GPT manages real-time local route
planning. An RL-based safety filter monitors each GPT decision and overrides
unsafe actions that could lead to battery depletion or duplicate visits,
effectively mitigating hallucinations. Furthermore, a dual replay buffer
mechanism helps both the GPT modules and the RL agent refine their strategies
over time. Simulation results demonstrate that SafeGPT achieves higher delivery
success rates compared to a GPT-only baseline, while substantially reducing
battery consumption and travel distance. These findings validate the efficacy
of combining GPT-based semantic reasoning with formal safety guarantees,
contributing a viable solution for robust and energy-efficient UAV logistics.

</details>

### [20] [Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration](https://arxiv.org/abs/2504.10865)
*Han-Dong Lim,Donghwan Lee*

Main category: cs.AI

TLDR: 本论文研究投影Bellman方程（PBE）的理论性质、解的存在条件，以及线性Q学习和近似值迭代（AVI）的收敛。


<details>
  <summary>Details</summary>
Motivation: 为了探索PBE解的存在和算法收敛的理论基础，以提升强化学习算法的可靠性和性能。

Method: 通过分析严格负行主导对角（SNRDD）假设和AVI收敛条件，考察线性Q学习和AVI算法。

Result: SNRDD假设确保线性Q学习的收敛，并探讨了其与AVI收敛的关系；提供了ε-贪婪策略下PBE解的观察。

Conclusion: 论文为PBE和相关算法提供了理论洞见，有助于强化学习领域的算法设计和应用。

Abstract: In this paper, we study the theoretical properties of the projected Bellman
equation (PBE) and two algorithms to solve this equation: linear Q-learning and
approximate value iteration (AVI). We consider two sufficient conditions for
the existence of a solution to PBE : strictly negatively row dominating
diagonal (SNRDD) assumption and a condition motivated by the convergence of
AVI. The SNRDD assumption also ensures the convergence of linear Q-learning,
and its relationship with the convergence of AVI is examined. Lastly, several
interesting observations on the solution of PBE are provided when using
$\epsilon$-greedy policy.

</details>

### [21] [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
*Yize Zhang,Tianshu Wang,Sirui Chen,Kun Wang,Xingyu Zeng,Hongyu Lin,Xianpei Han,Le Sun,Chaochao Lu*

Main category: cs.AI

TLDR: 这篇论文引入ARise框架，通过风险评估和动态RAG整合Monte Carlo树搜索，提升LLM在开放式推理中的性能，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在开放式、知识密集型推理中受限，现有方法存在知识不完整、错误传播和验证瓶颈问题。

Method: 提出ARise框架，结合中间状态风险评估、动态RAG和Monte Carlo树搜索，优化多分支推理计划。

Result: 实验显示，ARise比最先进知识增强方法提高多达23.10%，比RAG增强模型提高多达25.37%。

Conclusion: ARise有效解决推理挑战，显著提升性能，具有推广潜力。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities and
are receiving increasing attention to enhance their reasoning through scaling
test--time compute. However, their application in open--ended,
knowledge--intensive, complex reasoning scenarios is still limited.
Reasoning--oriented methods struggle to generalize to open--ended scenarios due
to implicit assumptions of complete world knowledge. Meanwhile,
knowledge--augmented reasoning (KAR) methods fail to address two core
challenges: 1) error propagation, where errors in early steps cascade through
the chain, and 2) verification bottleneck, where the explore--exploit tradeoff
arises in multi--branch decision processes. To overcome these limitations, we
introduce ARise, a novel framework that integrates risk assessment of
intermediate reasoning states with dynamic retrieval--augmented generation
(RAG) within a Monte Carlo tree search paradigm. This approach enables
effective construction and optimization of reasoning plans across multiple
maintained hypothesis branches. Experimental results show that ARise
significantly outperforms the state--of--the--art KAR methods by up to 23.10%,
and the latest RAG-equipped large reasoning models by up to 25.37%.

</details>

### [22] [Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior](https://arxiv.org/abs/2504.11075)
*Dongmin Kim,Hoshinori Kanazawa,Naoto Yoshida,Yasuo Kuniyoshi*

Main category: cs.AI

TLDR: 本研究提出一种基于自身感官经验的密度模型，诱导自主目标导向行为，模拟婴儿早期发展阶段的自发探索。


<details>
  <summary>Details</summary>
Motivation: 现有模型多关注外部奖励，本文旨在通过纯内在动机机制理解婴儿自发探索和学习行为。

Method: 提出'自先验'密度模型，整合到基于自由能量原理的主动推理框架中，通过最小化过去与当前感官经验的不匹配生成行为参考，并在模拟环境中验证。

Result: 代理自发地伸手触碰触觉刺激，展示了基于自身感官的内在动机行为。

Conclusion: 研究证实了通过自身感官经验可以自主诱导目标导向行为，模拟了婴儿早期发展的意图行为 emerg。

Abstract: Infants often exhibit goal-directed behaviors, such as reaching for a sensory
stimulus, even when no external reward criterion is provided. These
intrinsically motivated behaviors facilitate spontaneous exploration and
learning of the body and environment during early developmental stages.
Although computational modeling can offer insight into the mechanisms
underlying such behaviors, many existing studies on intrinsic motivation focus
primarily on how exploration contributes to acquiring external rewards. In this
paper, we propose a novel density model for an agent's own multimodal sensory
experiences, called the "self-prior," and investigate whether it can
autonomously induce goal-directed behavior. Integrated within an active
inference framework based on the free energy principle, the self-prior
generates behavioral references purely from an intrinsic process that minimizes
mismatches between average past sensory experiences and current observations.
This mechanism is also analogous to the acquisition and utilization of a body
schema through continuous interaction with the environment. We examine this
approach in a simulated environment and confirm that the agent spontaneously
reaches toward a tactile stimulus. Our study implements intrinsically motivated
behavior shaped by the agent's own sensory experiences, demonstrating the
spontaneous emergence of intentional behavior during early development.

</details>

### [23] [C-SHAP for time series: An approach to high-level temporal explanations](https://arxiv.org/abs/2504.11159)
*Annemarie Jutte,Faizan Ahmed,Jeroen Linssen,Maurice van Keulen*

Main category: cs.AI

TLDR: 本篇论文提出C-SHAP方法，用于时间序列数据的概念-based XAI，解释高层次模式，并在能源领域验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法仅关注低层次模式，忽略高层次模式对模型推理的影响，因此需要基于概念的解释方法。

Method: 提出C-SHAP方法，包括一般定义和使用时间序列分解的实现示例。

Result: 通过能源领域的用例证明了方法的有效性。

Conclusion: C-SHAP提升了时间序列AI的可解释性，促进了AI在实际应用中的可靠性。

Abstract: Time series are ubiquitous in domains such as energy forecasting, healthcare,
and industry. Using AI systems, some tasks within these domains can be
efficiently handled. Explainable AI (XAI) aims to increase the reliability of
AI solutions by explaining model reasoning. For time series, many XAI methods
provide point- or sequence-based attribution maps. These methods explain model
reasoning in terms of low-level patterns. However, they do not capture
high-level patterns that may also influence model reasoning. We propose a
concept-based method to provide explanations in terms of these high-level
patterns. In this paper, we present C-SHAP for time series, an approach which
determines the contribution of concepts to a model outcome. We provide a
general definition of C-SHAP and present an example implementation using time
series decomposition. Additionally, we demonstrate the effectiveness of the
methodology through a use case from the energy domain.

</details>

### [24] [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.AI

TLDR: 本文提出LAG框架，使用语义知识图提升语言模型在隐含知识提取和比喻理解方面的性能。


<details>
  <summary>Details</summary>
Motivation: 机器难以从自然语言中提取隐含知识，缺乏物理世界经验，因此需要语义知识图作为概念空间指导推理。

Method: 应用逻辑增强生成（LAG）框架，结合语义知识图和提示启发式，生成扩展的知识图三元组以提取隐含类比连接。

Result: 在三个比喻检测任务上优于基线和人类，尤其在视觉比喻理解中更可解释，但存在领域特定比喻的局限性。

Conclusion: 通过错误分析讨论了比喻标注和评估方法的不足，并指出改进方向。

Abstract: Recent advances in Large Language Models have demonstrated their capabilities
across a variety of tasks. However, automatically extracting implicit knowledge
from natural language remains a significant challenge, as machines lack active
experience with the physical world. Given this scenario, semantic knowledge
graphs can serve as conceptual spaces that guide the automated text generation
reasoning process to achieve more efficient and explainable results. In this
paper, we apply a logic-augmented generation (LAG) framework that leverages the
explicit representation of a text through a semantic knowledge graph and
applies it in combination with prompt heuristics to elicit implicit analogical
connections. This method generates extended knowledge graph triples
representing implicit meaning, enabling systems to reason on unlabeled
multimodal data regardless of the domain. We validate our work through three
metaphor detection and understanding tasks across four datasets, as they
require deep analogical reasoning capabilities. The results show that this
integrated approach surpasses current baselines, performs better than humans in
understanding visual metaphors, and enables more explainable reasoning
processes, though still has inherent limitations in metaphor understanding,
especially for domain-specific metaphors. Furthermore, we propose a thorough
error analysis, discussing issues with metaphorical annotations and current
evaluation methods.

</details>

### [25] [Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs](https://arxiv.org/abs/2504.11200)
*Irene Celino,Mario Scrocca,Agnese Chiatti*

Main category: cs.AI

TLDR: 本章探讨神经符号AI（NeSy AI）如何通过结合符号知识和数据驱动模型提升人类与系统相互理解，引入共享、交换和治理知识三个维度，并通过用例场景展示应用潜力、挑战和研究空白。


<details>
  <summary>Details</summary>
Motivation: 动机是解决人类与AI系统相互理解的不足，利用NeSy AI增强知识共享、交换和治理。

Method: 方法包括定义三个关键维度，并使用NeSy AI和知识图谱在用例场景中演示符号推理与神经学习的结合。

Result: 结果展示了NeSy AI的潜力与挑战，识别了在三个维度上的覆盖不足和未来研究方向。

Conclusion: 结论强调NeSy AI可改善相互理解，但需进一步研究以填补空白。

Abstract: This chapter investigates the concept of mutual understanding between humans
and systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)
methods can significantly enhance this mutual understanding by leveraging
explicit symbolic knowledge representations with data-driven learning models.
We start by introducing three critical dimensions to characterize mutual
understanding: sharing knowledge, exchanging knowledge, and governing
knowledge. Sharing knowledge involves aligning the conceptual models of
different agents to enable a shared understanding of the domain of interest.
Exchanging knowledge relates to ensuring the effective and accurate
communication between agents. Governing knowledge concerns establishing rules
and processes to regulate the interaction between agents. Then, we present
several different use case scenarios that demonstrate the application of NeSy
AI and Knowledge Graphs to aid meaningful exchanges between human, artificial,
and robotic agents. These scenarios highlight both the potential and the
challenges of combining top-down symbolic reasoning with bottom-up neural
learning, guiding the discussion of the coverage provided by current solutions
along the dimensions of sharing, exchanging, and governing knowledge.
Concurrently, this analysis facilitates the identification of gaps and less
developed aspects in mutual understanding to address in future research.

</details>

### [26] [Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs](https://arxiv.org/abs/2504.11239)
*Chang Yang,Ruiyu Wang,Junzhe Jiang,Qi Jiang,Qinggang Zhang,Yanchen Deng,Shuxin Li,Shuyue Hu,Bo Li,Florian T. Pokorny,Xiao Huang,Xinrun Wang*

Main category: cs.AI

TLDR: 这篇论文提出NPPC，一个可扩展的推理基准，用于测试大型语言模型（LLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准容易被快速超越和被hack，因此需要构建一个ever-scaling、不可被超越和不可被hack的基准。

Method: NPPC包括三个模块：npgym提供NP-complete问题的接口和实例生成、npsolver用于评估模型、npeval用于分析性能。

Result: 实验显示NPPC将先进LLM性能降至10%以下，DeepSeek-R1等模型表现出色，并观察到难度增加时token数和aha moments的变化。

Conclusion: NPPC是第一个ever-scaling推理基准，可作为LLM向AGI发展的测试平台。

Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due
to the rapid progress of LLMs, there are two main issues of current benchmarks:
i) these benchmarks can be crushed in a short time (less than 1 year), and ii)
these benchmarks may be easily hacked. To handle these issues, we propose the
ever-scalingness for building the benchmarks which are uncrushable, unhackable,
auto-verifiable and general. This paper presents Nondeterministic
Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark
for LLMs. Specifically, the NPPC has three main modules: i) npgym, which
provides a unified interface of 25 well-known NP-complete problems and can
generate any number of instances with any levels of complexities, ii) npsolver:
which provides a unified interface to evaluate the problem instances with both
online and offline models via APIs and local deployments, respectively, and
iii) npeval: which provides the comprehensive and ready-to-use tools to analyze
the performances of LLMs over different problems, the number of tokens, the aha
moments, the reasoning errors and the solution errors. Extensive experiments
over widely-used LLMs demonstrate: i) NPPC can successfully decrease the
performances of advanced LLMs' performances to below 10%, demonstrating that
NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the
most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and
o1/o3-mini in most NP-complete problems considered, and iii) the numbers of
tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and
DeepSeek-R1, are observed first to increase and then decrease when the problem
instances become more and more difficult. We believe that NPPC is the first
ever-scaling reasoning benchmark, serving as the uncrushable and unhackable
testbed for LLMs toward artificial general intelligence (AGI).

</details>

### [27] [Towards Automated Safety Requirements Derivation Using Agent-based RAG](https://arxiv.org/abs/2504.11243)
*Balahari Vignesh Balu,Florian Geissler,Francesco Carella,Joao-Vitor Zacchi,Josef Jiru,Nuria Mata,Reinhard Stolle*

Main category: cs.AI

TLDR: 本论文提出使用基于代理的RAG方法来自动推导自驾车的安全要求，提高信息检索的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM缺乏领域特定知识，现有RAG方法在处理复杂查询时性能下降，尤其在安全相关应用中。

Method: 使用基于代理的RAG方法，应用于汽车标准和Apollo案例研究的数据集，并通过选定的RAG指标进行评估。

Result: 代理-based RAG方法在检索相关信息方面表现出优势，比默认RAG方法更好。

Conclusion: 代理-based方法在安全要求推导中更有效，特别是在处理复杂查询时。

Abstract: We study the automated derivation of safety requirements in a self-driving
vehicle use case, leveraging LLMs in combination with agent-based
retrieval-augmented generation. Conventional approaches that utilise
pre-trained LLMs to assist in safety analyses typically lack domain-specific
knowledge. Existing RAG approaches address this issue, yet their performance
deteriorates when handling complex queries and it becomes increasingly harder
to retrieve the most relevant information. This is particularly relevant for
safety-relevant applications. In this paper, we propose the use of agent-based
RAG to derive safety requirements and show that the retrieved information is
more relevant to the queries. We implement an agent-based approach on a
document pool of automotive standards and the Apollo case study, as a
representative example of an automated driving perception system. Our solution
is tested on a data set of safety requirement questions and answers, extracted
from the Apollo data. Evaluating a set of selected RAG metrics, we present and
discuss advantages of a agent-based approach compared to default RAG methods.

</details>

### [28] [Learning to Be A Doctor: Searching for Effective Medical Agent Architectures](https://arxiv.org/abs/2504.11301)
*Yangyang Zhuang,Wenjia Jiang,Jiayu Zhang,Ze Yang,Joey Tianyi Zhou,Chi Zhang*

Main category: cs.AI

TLDR: 本论文引入了一个自动设计医疗代理架构的框架，使用大语言模型，实现了动态适应和诊断准确性的提升，在皮肤病诊断任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 受AutoML成功启发，针对现有医疗代理系统缺乏灵活性和适应性的问题，以及医疗领域对高泛化性和跨学科知识的需求。

Method: 定义了一个分层代理搜索空间，通过节点、结构和框架级别的修改实现动态工作流适应；将代理视为图-based架构，支持诊断反馈引导的迭代改进。

Result: 在皮肤病诊断任务的实验中，方法有效地演化工作流并显著提高诊断准确性。

Conclusion: 这是第一个完全自动化的医疗代理架构设计框架，为真实临床环境中部署智能代理提供了可扩展的适应性基础。

Abstract: Large Language Model (LLM)-based agents have demonstrated strong capabilities
across a wide range of tasks, and their application in the medical domain holds
particular promise due to the demand for high generalizability and reliance on
interdisciplinary knowledge. However, existing medical agent systems often rely
on static, manually crafted workflows that lack the flexibility to accommodate
diverse diagnostic requirements and adapt to emerging clinical scenarios.
Motivated by the success of automated machine learning (AutoML), this paper
introduces a novel framework for the automated design of medical agent
architectures. Specifically, we define a hierarchical and expressive agent
search space that enables dynamic workflow adaptation through structured
modifications at the node, structural, and framework levels. Our framework
conceptualizes medical agents as graph-based architectures composed of diverse,
functional node types and supports iterative self-improvement guided by
diagnostic feedback. Experimental results on skin disease diagnosis tasks
demonstrate that the proposed method effectively evolves workflow structures
and significantly enhances diagnostic accuracy over time. This work represents
the first fully automated framework for medical agent architecture design and
offers a scalable, adaptable foundation for deploying intelligent agents in
real-world clinical environments.

</details>

### [29] [Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.11354)
*Haiming Wang,Mert Unsal,Xiaohan Lin,Mantas Baksys,Junqi Liu,Marco Dos Santos,Flood Sung,Marina Vinyes,Zhenzhe Ying,Zekai Zhu,Jianqiao Lu,Hugues de Saxcé,Bolton Bailey,Chendong Song,Chenjun Xiao,Dehao Zhang,Ebony Zhang,Frederick Pu,Han Zhu,Jiawei Liu,Jonas Bayer,Julien Michel,Longhui Yu,Léo Dreyfus-Schmidt,Lewis Tunstall,Luigi Pagani,Moreira Machado,Pauline Bourigault,Ran Wang,Stanislas Polu,Thibaut Barroyer,Wen-Ding Li,Yazhe Niu,Yann Fleureau,Yangyang Hu,Zhouliang Yu,Zihan Wang,Zhilin Yang,Zhengying Liu,Jia Li*

Main category: cs.AI

TLDR: Kimina-Prover 是一种新型大型语言模型，通过创新的推理驱动探索范式，在形式定理证明中达到一流性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在改进形式定理证明的效率，模拟人类问题解决策略，并桥接形式验证与非正式数学直觉。

Method: 使用大规模强化学习管道从 Qwen2.5-72B 训练，采用 '形式推理模式' 在 Lean 4 中迭代生成和完善证明步骤。

Result: 在 miniF2F 基准测试中达到 80.7% 的 pass@8192 成绩，展示高样本效率、模型大小性能 scaling，并开源 1.5B 和 7B 参数的简化版本。

Conclusion: Kimina-Prover 的工作揭示了高效证明生成、性能 scaling 的潜力，并为形式与非形式数学整合提供新见解。

Abstract: We introduce Kimina-Prover Preview, a large language model that pioneers a
novel reasoning-driven exploration paradigm for formal theorem proving, as
showcased in this preview release. Trained with a large-scale reinforcement
learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong
performance in Lean 4 proof generation by employing a structured reasoning
pattern we term \textit{formal reasoning pattern}. This approach allows the
model to emulate human problem-solving strategies in Lean, iteratively
generating and refining proof steps. Kimina-Prover sets a new state-of-the-art
on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved
benchmark performance, our work yields several key insights: (1) Kimina-Prover
exhibits high sample efficiency, delivering strong results even with minimal
sampling (pass@1) and scaling effectively with computational budget, stemming
from its unique reasoning pattern and RL training; (2) we demonstrate clear
performance scaling with model size, a trend previously unobserved for neural
theorem provers in formal mathematics; (3) the learned reasoning style,
distinct from traditional search algorithms, shows potential to bridge the gap
between formal verification and informal mathematical intuition. We open source
distilled versions with 1.5B and 7B parameters of Kimina-Prover

</details>

### [30] [Embodied World Models Emerge from Navigational Task in Open-Ended Environments](https://arxiv.org/abs/2504.11419)
*Li Jin,Liu Jia*

Main category: cs.AI

TLDR: 神经网络通过主动互动学习空间意识，使用Meta-RL等方法，并验证神经表示的因果关系。


<details>
  <summary>Details</summary>
Motivation: AI研究中，理解人工系统如何发展空间意识和推理是一大挑战，传统模型依赖被动观察，而具身认知理论强调主动互动的重要性。

Method: 使用门控循环单元(GRU)结合元强化学习(Meta-RL)，引入混合动力系统(HDS)建模，采用脊表示、典型相关分析(CCA)和干预实验。

Result: 代理学习编码方向、距离和避障等空间属性；发现稳定极限环对应最优导航策略；CCA显示神经状态与行为表示高度一致；干预实验证实特定神经维度与导航性能有因果联系。

Conclusion: 本研究桥接行动与感知的差距，提供构建适应性、可解释AI模型的新途径，并为理解和控制AI内部机制开辟新方向。

Abstract: Understanding how artificial systems can develop spatial awareness and
reasoning has long been a challenge in AI research. Traditional models often
rely on passive observation, but embodied cognition theory suggests that deeper
understanding emerges from active interaction with the environment. This study
investigates whether neural networks can autonomously internalize spatial
concepts through interaction, focusing on planar navigation tasks. Using Gated
Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we
show that agents can learn to encode spatial properties like direction,
distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)
to model the agent-environment interaction as a closed dynamical system,
revealing stable limit cycles that correspond to optimal navigation strategies.
Ridge Representation allows us to map navigation paths into a fixed-dimensional
behavioral space, enabling comparison with neural states. Canonical Correlation
Analysis (CCA) confirms strong alignment between these representations,
suggesting that the agent's neural states actively encode spatial knowledge.
Intervention experiments further show that specific neural dimensions are
causally linked to navigation performance. This work provides an approach to
bridging the gap between action and perception in AI, offering new insights
into building adaptive, interpretable models that can generalize across complex
environments. The causal validation of neural representations also opens new
avenues for understanding and controlling the internal mechanisms of AI
systems, pushing the boundaries of how machines learn and reason in dynamic,
real-world scenarios.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA](https://arxiv.org/abs/2504.10490)
*Gabriel Bo,Marc Bernardino,Justin Gu*

Main category: cs.LG

TLDR: 本研究尝试将KAN和图表示整合到GPT-2中提升多任务学习准确性，但发现LoRA优化方法更有效。


<details>
  <summary>Details</summary>
Motivation: 受KAN和GAT在CoT模型应用增加以及与MLP比较的争议启发，旨在提高多任务学习准确性。

Method: 使用LoRA增强transformer，进行超参数微调和L2正则化；开发Graph LoRA和Hybrid-KAN LoRA变体，并进行系统评估。

Result: LoRA增强transformer在SST测试集准确率55.249%，CFIMDB dev集99.18%，释义检测89.9%，十四行诗生成CHRF分数42.097；变体未优于基准。

Conclusion: 高效参数适配通过LoRA是针对情感分析、释义检测和十四行诗生成的最有效策略。

Abstract: We explore the potential of integrating learnable and interpretable
modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based
representations--within a pre-trained GPT-2 model to enhance multi-task
learning accuracy. Motivated by the recent surge in using KAN and graph
attention (GAT) architectures in chain-of-thought (CoT) models and debates over
their benefits compared to simpler architectures like MLPs, we begin by
enhancing a standard self-attention transformer using Low-Rank Adaptation
(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This
approach yields significant improvements. To further boost interpretability and
richer representations, we develop two variants that attempt to improve the
standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,
systematic evaluations reveal that neither variant outperforms the optimized
LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,
99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On
sonnet generation, we get a CHRF score of 42.097. These findings highlight that
efficient parameter adaptation via LoRA remains the most effective strategy for
our tasks: sentiment analysis, paraphrase detection, and sonnet generation.

</details>

### [32] [Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP](https://arxiv.org/abs/2504.10536)
*Lihong Zhang,Yue Li*

Main category: cs.LG

TLDR: 本论文提出层跳跃联邦学习方法，应用于医疗NLP，减少通信开销70%，性能接近集中式训练。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在医疗NLP中的隐私问题，以及通信开销和数据异质性挑战。

Method: 提出层跳跃联邦学习，仅微调预训练LLM（如LLaMA 3.2-1B）的选定层，其他层冻结。

Result: 通信成本减少约70%，性能与集中式训练相差不到2%，优于基线，处理非IID数据并与差分隐私兼容。

Conclusion: 这种方法为医疗NLP隐私保护协作学习提供实用解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
organizations without sharing raw data, addressing crucial privacy concerns in
healthcare natural language processing (NLP). However, training large language
models (LLMs) in federated settings faces significant challenges, including
communication overhead and data heterogeneity. We propose Layer-Skipping
Federated Learning, where only selected layers of a pre-trained LLM are
fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,
our approach reduces communication costs by approximately 70% while maintaining
performance within 2% of centralized training. We evaluate our method on
clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our
experiments demonstrate that Layer-Skipping FL outperforms competitive
baselines, handles non-IID clinical data distributions effectively, and shows
robustness when combined with differential privacy. This approach represents a
practical solution for privacy-preserving collaborative learning in healthcare
NLP.

</details>

### [33] [MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers](https://arxiv.org/abs/2504.10551)
*Lili Zhao,Qi Liu,Wei Chen,Liyi Chen,Ruijun Sun,Min Hou,Yang Wang,Shijin Wang*

Main category: cs.LG

TLDR: 本论文提出MiMu方法，缓解Empirical Risk Minimization (ERM)模型的多重捷径学习问题，提高鲁棒性泛化性能。


<details>
  <summary>Details</summary>
Motivation: ERM模型依赖虚假相关性导致捷径学习，现有方法仅针对单一捷径，而现实中捷径多样且未知，模型对强捷径过度依赖影响泛化。

Method: 提出MiMu方法，包括自校准策略（防止依赖捷径）和自提升策略（随机掩码及自适应注意力对齐）。

Result: 实验在NLP和CV领域显示MiMu有效提升了模型的鲁棒性泛化能力。

Conclusion: MiMu通过减轻多重捷径学习行为，显著改善了模型的泛化性能。

Abstract: Empirical Risk Minimization (ERM) models often rely on spurious correlations
between features and labels during the learning process, leading to shortcut
learning behavior that undermines robustness generalization performance.
Current research mainly targets identifying or mitigating a single shortcut;
however, in real-world scenarios, cues within the data are diverse and unknown.
In empirical studies, we reveal that the models rely to varying extents on
different shortcuts. Compared to weak shortcuts, models depend more heavily on
strong shortcuts, resulting in their poor generalization ability. To address
these challenges, we propose MiMu, a novel method integrated with
Transformer-based ERMs designed to Mitigate Multiple shortcut learning
behavior, which incorporates self-calibration strategy and self-improvement
strategy. In the source model, we preliminarily propose the self-calibration
strategy to prevent the model from relying on shortcuts and make overconfident
predictions. Then, we further design self-improvement strategy in target model
to reduce the reliance on multiple shortcuts. The random mask strategy involves
randomly masking partial attention positions to diversify the focus of target
model other than concentrating on a fixed region. Meanwhile, the adaptive
attention alignment module facilitates the alignment of attention weights to
the calibrated source model, without the need for post-hoc attention maps or
supervision. Finally, extensive experiments conducted on Natural Language
Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu
in improving robustness generalization abilities.

</details>

### [34] [LEMUR Neural Network Dataset: Towards Seamless AutoML](https://arxiv.org/abs/2504.10552)
*Arash Torabi Goodarzi,Roman Kochnev,Waleed Khalid,Furui Qin,Tolgay Atinc Uzun,Yashkumar Sanjaybhai Dhameliya,Yash Kanubhai Kathiriya,Zofia Antonina Bentyn,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TLDR: LEMUR 是一个开源数据集，包含各种神经网络模型代码，用于 AutoML 和模型分析，提供结构化表示和性能数据。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集对神经网络发展至关重要，需支持基准测试、AutoML 和模型分析。

Method: 引入 LEMUR，使用 Python 和 PyTorch 构建，集成 Optuna 框架进行评估、优化和统计分析。

Result: 启用 LLM 微调、提供评估工具、API 和边设备部署，支持模型开发和性能统计。

Conclusion: LEMUR 将开源发布，帮助研究者和从业者高效开发、测试和分析神经网络模型。

Abstract: Neural networks are fundamental in artificial intelligence, driving progress
in computer vision and natural language processing. High-quality datasets are
crucial for their development, and there is growing interest in datasets
composed of neural networks themselves to support benchmarking, automated
machine learning (AutoML), and model analysis. We introduce LEMUR, an open
source dataset of neural network models with well-structured code for diverse
architectures across tasks such as object detection, image classification,
segmentation, and natural language processing. LEMUR is primarily designed to
enable fine-tuning of large language models (LLMs) for AutoML tasks, providing
a rich source of structured model representations and associated performance
data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new
datasets and models while maintaining consistency. It integrates an
Optuna-powered framework for evaluation, hyperparameter optimization,
statistical analysis, and graphical insights. LEMUR provides an extension that
enables models to run efficiently on edge devices, facilitating deployment in
resource-constrained environments. Providing tools for model evaluation,
preprocessing, and database management, LEMUR supports researchers and
practitioners in developing, testing, and analyzing neural networks.
Additionally, it offers an API that delivers comprehensive information about
neural network models and their complete performance statistics with a single
request, which can be used in experiments with code-generating large language
models. The LEMUR will be released as an open source project under the MIT
license upon acceptance of the paper.

</details>

### [35] [Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains](https://arxiv.org/abs/2504.10555)
*Marco Salmè,Lorenzo Tronchin,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.LG

TLDR: 本研究探讨深度生成模型在数据稀缺环境下的应用，扩展生成学习三难困境并评估VAE、GAN和DM的性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺阻碍了医学和精准农业等领域的技术进步，需要合成数据解决生成学习三难困境的限制。

Method: 评估VAE、GAN和DM在保真度、多样性和采样效率下的性能，并提出框架评估合成数据的实用性、鲁棒性和隐私性。

Result: 不同DGM模型显示出应用上下文相关的独特优势。

Conclusion: 扩展生成学习三难困境，提供选择DGM模型的指导，以适应真实世界需求。

Abstract: Data scarcity remains a critical bottleneck impeding technological
advancements across various domains, including but not limited to medicine and
precision agriculture. To address this challenge, we explore the potential of
Deep Generative Models (DGMs) in producing synthetic data that satisfies the
Generative Learning Trilemma: fidelity, diversity, and sampling efficiency.
However, recognizing that these criteria alone are insufficient for practical
applications, we extend the trilemma to include utility, robustness, and
privacy, factors crucial for ensuring the applicability of DGMs in real-world
scenarios. Evaluating these metrics becomes particularly challenging in
data-scarce environments, as DGMs traditionally rely on large datasets to
perform optimally. This limitation is especially pronounced in domains like
medicine and precision agriculture, where ensuring acceptable model performance
under data constraints is vital. To address these challenges, we assess the
Generative Learning Trilemma in data-scarcity settings using state-of-the-art
evaluation metrics, comparing three prominent DGMs: Variational Autoencoders
(VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs).
Furthermore, we propose a comprehensive framework to assess utility,
robustness, and privacy in synthetic data generated by DGMs. Our findings
demonstrate varying strengths among DGMs, with each model exhibiting unique
advantages based on the application context. This study broadens the scope of
the Generative Learning Trilemma, aligning it with real-world demands and
providing actionable guidance for selecting DGMs tailored to specific
applications.

</details>

### [36] [VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification](https://arxiv.org/abs/2504.10556)
*Lucas Heublein,Simon Kocher,Tobias Feigl,Alexander Rügamer,Christopher Mutschler,Felix Ott*

Main category: cs.LG

TLDR: 本论文使用变分自编码器（VAE）实现GNSS干扰分类的模型压缩和数据增强，达到高达99.92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 分布式学习和Edge AI需要高效数据处理、低延迟通信、去中心化训练和数据隐私；在GNSS应用中，需准确监控和分类干扰以提升 situational awareness，挑战是压缩ML模型同时保持高准确性。

Method: 提出VAE的disentanglement方法提取latent features，用于干扰分类；包括vanilla、factorized和conditional generative三种变体；用于数据压缩和增强，通过latent表示插值；评估四个数据集，并进行超参数优化。

Result: 数据压缩率从512到8192；准确率高达99.92%。

Conclusion: VAE方法有效实现了高压缩率和高准确率的干扰分类，证明了在分布式环境中的可行性。

Abstract: Distributed learning and Edge AI necessitate efficient data processing,
low-latency communication, decentralized model training, and stringent data
privacy to facilitate real-time intelligence on edge devices while reducing
dependency on centralized infrastructure and ensuring high model performance.
In the context of global navigation satellite system (GNSS) applications, the
primary objective is to accurately monitor and classify interferences that
degrade system performance in distributed environments, thereby enhancing
situational awareness. To achieve this, machine learning (ML) models can be
deployed on low-resource devices, ensuring minimal communication latency and
preserving data privacy. The key challenge is to compress ML models while
maintaining high classification accuracy. In this paper, we propose variational
autoencoders (VAEs) for disentanglement to extract essential latent features
that enable accurate classification of interferences. We demonstrate that the
disentanglement approach can be leveraged for both data compression and data
augmentation by interpolating the lower-dimensional latent representations of
signal power. To validate our approach, we evaluate three VAE variants -
vanilla, factorized, and conditional generative - on four distinct datasets,
including two collected in controlled indoor environments and two real-world
highway datasets. Additionally, we conduct extensive hyperparameter searches to
optimize performance. Our proposed VAE achieves a data compression rate ranging
from 512 to 8,192 and achieves an accuracy up to 99.92%.

</details>

### [37] [Efficient Process Reward Model Training via Active Learning](https://arxiv.org/abs/2504.10559)
*Keyu Duan,Zichen Liu,Xin Mao,Tianyu Pang,Changyu Chen,Qiguang Chen,Michael Qizhe Shieh,Longxu Dou*

Main category: cs.LG

TLDR: ActPRM 是一种主动学习方法，通过减少 50% 的标注成本，同时保持或提高性能，并在特定基准上达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 训练数据标注的规模化对人类和 LLM 都具有挑战性，因此提出 ActPRM 来解决这一限制。

Method: 提出主动学习方法 ActPRM，选择不确定性最高的样本，使用 PRM 估计不确定性，由高成本的推理模型标注数据，然后计算损失并更新 PRM 的权重。

Result: 减少 50% 的标注成本，同时达到相当或更好的性能；通过过滤超过 100 万的数学推理轨迹，保留 60% 的数据，在 ProcessBench (75.0%) 和 PRMBench (65.5%) 上达到新的 SOTA。

Conclusion: ActPRM 在减少标注成本和提高性能方面表现出色，并实现了新的状态-of-the-art 结果。

Abstract: Process Reward Models (PRMs) provide step-level supervision to large language
models (LLMs), but scaling up training data annotation remains challenging for
both humans and LLMs. To address this limitation, we propose an active learning
approach, ActPRM, which proactively selects the most uncertain samples for
training, substantially reducing labeling costs. During training, we use the
PRM to estimate uncertainty after the forward pass, retaining only highly
uncertain data. A capable yet costly reasoning model then labels this data.
Then we compute the loss with respect to the labels and update the PRM's
weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active
learning setting, demonstrating that ActPRM reduces 50% annotation, but
achieving the comparable or even better performance. Beyond annotation
efficiency, we further advance the actively trained PRM by filtering over 1M+
math reasoning trajectories with ActPRM, retaining 60% of the data. A
subsequent training on this selected dataset yields a new state-of-the-art
(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same
sized models.

</details>

### [38] [Self-Controlled Dynamic Expansion Model for Continual Learning](https://arxiv.org/abs/2504.10561)
*Runqing Wu,Fei Ye,Rongyao Hu,Guoxi Huang*

Main category: cs.LG

TLDR: 本文提出自控动态扩展模型（SCDEM）来提升持续学习性能，通过多骨干网动态适应新任务，并引入优化机制实现知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法使用静态Vision Transformer骨干网，无法有效适应新任务和数据域，导致参数利用不足。

Method: 引入SCDEM模型，使用多ViT骨干网动态生成新专家，结合协作优化机制（COM）、特征分布一致性（FDC）及动态层级特征注意力机制（DLWFAM）来优化学习过程。

Result: 实验结果显示，该方法在各种基准上达到了最先进性能。

Conclusion: 该方法有效地解决了持续学习中的适应性和知识保留问题，并通过实证验证了其优越性。

Abstract: Continual Learning (CL) epitomizes an advanced training paradigm wherein
prior data samples remain inaccessible during the acquisition of new tasks.
Numerous investigations have delved into leveraging a pre-trained Vision
Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless,
these approaches typically utilize a singular, static backbone, which
inadequately adapts to novel tasks, particularly when engaging with diverse
data domains, due to a substantial number of inactive parameters. This paper
addresses this limitation by introducing an innovative Self-Controlled Dynamic
Expansion Model (SCDEM), which orchestrates multiple distinct trainable
pre-trained ViT backbones to furnish diverse and semantically enriched
representations. Specifically, by employing the multi-backbone architecture as
a shared module, the proposed SCDEM dynamically generates a new expert with
minimal parameters to accommodate a new task. A novel Collaborative
Optimization Mechanism (COM) is introduced to synergistically optimize multiple
backbones by harnessing prediction signals from historical experts, thereby
facilitating new task learning without erasing previously acquired knowledge.
Additionally, a novel Feature Distribution Consistency (FDC) approach is
proposed to align semantic similarity between previously and currently learned
representations through an optimal transport distance-based mechanism,
effectively mitigating negative knowledge transfer effects. Furthermore, to
alleviate over-regularization challenges, this paper presents a novel Dynamic
Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the
penalization intensity on each trainable representation layer. An extensive
series of experiments have been conducted to evaluate the proposed
methodology's efficacy, with empirical results corroborating that the approach
attains state-of-the-art performance.

</details>

### [39] [Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling](https://arxiv.org/abs/2504.10612)
*Michal Balcerak,Tamaz Amiranashvili,Suprosanna Shit,Antonio Terpin,Sebastian Kaltenbach,Petros Koumoutsakos,Bjoern Menze*

Main category: cs.LG

TLDR: 本文提出Energy Matching框架，统一流方法和能量模型，提高生成性能，在CIFAR-10上FID达3.97。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理部分观察或额外先验时复杂，受到Wasserstein梯度流进展启发，旨在解决这些局限性。

Method: 提出Energy Matching框架，使用单一时间无关标量场，结合流方法和能量模型灵活性，无需时间条件或额外网络。

Result: 在CIFAR-10生成上显著优于现有EBM（FID 3.97 vs 8.61），保留无模拟训练，并引入交互能量支持多样模式探索。

Conclusion: 该方法标志着EBM的重大创新，简化框架，提升能力，促进在各种领域的广泛应用。

Abstract: Generative models often map noise to data by matching flows or scores, but
these approaches become cumbersome for incorporating partial observations or
additional priors. Inspired by recent advances in Wasserstein gradient flows,
we propose Energy Matching, a framework that unifies flow-based approaches with
the flexibility of energy-based models (EBMs). Far from the data manifold,
samples move along curl-free, optimal transport paths from noise to data. As
they approach the data manifold, an entropic energy term guides the system into
a Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97
compared to 8.61), while retaining the simulation-free training of
transport-based approaches away from the data manifold. Additionally, we
exploit the flexibility of our method and introduce an interaction energy for
diverse mode exploration. Our approach focuses on learning a static scalar
potential energy -- without time conditioning, auxiliary generators, or
additional networks -- marking a significant departure from recent EBM methods.
We believe this simplified framework significantly advances EBM capabilities
and paves the way for their broader adoption in generative modeling across
diverse domains.

</details>

### [40] [Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning](https://arxiv.org/abs/2504.10677)
*Muhammad Al-Zafar Khan,Jamal Al-Karaki*

Main category: cs.LG

TLDR: 这篇论文提出了一种多智能体强化学习框架，用于优化使用工程生物代理的组织修复过程。


<details>
  <summary>Details</summary>
Motivation: 为了优化组织修复过程，提高生物医学应用中的效率和效果。

Method: 整合了随机反应-扩散系统、神经样电化学通信（包括Hebbian可塑性）、生物学信息奖励函数（结合化学梯度跟踪、神经同步和鲁棒惩罚）、以及课程学习方案。

Result: 通过in silico实验，展示了紧急修复策略，包括动态分泌控制和空间协调。

Conclusion: 该框架证明了在组织修复中的有效性，突显了紧急行为和优化潜力。

Abstract: In this paper, we present a multi-agent reinforcement learning (MARL)
framework for optimizing tissue repair processes using engineered biological
agents. Our approach integrates: (1) stochastic reaction-diffusion systems
modeling molecular signaling, (2) neural-like electrochemical communication
with Hebbian plasticity, and (3) a biologically informed reward function
combining chemical gradient tracking, neural synchronization, and robust
penalties. A curriculum learning scheme guides the agent through progressively
complex repair scenarios. In silico experiments demonstrate emergent repair
strategies, including dynamic secretion control and spatial coordination.

</details>

### [41] [The Jailbreak Tax: How Useful are Your Jailbreak Outputs?](https://arxiv.org/abs/2504.10694)
*Kristina Nikolić,Luze Sun,Jie Zhang,Florian Tramèr*

Main category: cs.LG

TLDR: 这篇论文评估了越狱攻击对大语言模型的效用，引入了'越狱税'概念，强调攻击成功可能导致输出质量下降。


<details>
  <summary>Details</summary>
Motivation: 质疑现有越狱攻击的输出是否真正有用，因为它们可能产生有害但低效用的响应。

Method: 构建新评估集，通过让模型拒绝良性主题（如生物或数学）的问题，并评估八种代表性越狱攻击在五个效用基准上的表现。

Result: 发现越狱响应中模型效用一致下降，称为越狱税，例如数学准确率下降高达92%。

Conclusion: 提出越狱税作为AI安全新指标，并提供基准，发布在https://github.com/ethz-spylab/jailbreak-tax。

Abstract: Jailbreak attacks bypass the guardrails of large language models to produce
harmful outputs. In this paper, we ask whether the model outputs produced by
existing jailbreaks are actually useful. For example, when jailbreaking a model
to give instructions for building a bomb, does the jailbreak yield good
instructions? Since the utility of most unsafe answers (e.g., bomb
instructions) is hard to evaluate rigorously, we build new jailbreak evaluation
sets with known ground truth answers, by aligning models to refuse questions
related to benign and easy-to-evaluate topics (e.g., biology or math). Our
evaluation of eight representative jailbreaks across five utility benchmarks
reveals a consistent drop in model utility in jailbroken responses, which we
term the jailbreak tax. For example, while all jailbreaks we tested bypass
guardrails in models aligned to refuse to answer math, this comes at the
expense of a drop of up to 92% in accuracy. Overall, our work proposes the
jailbreak tax as a new important metric in AI safety, and introduces benchmarks
to evaluate existing and future jailbreaks. We make the benchmark available at
https://github.com/ethz-spylab/jailbreak-tax

</details>

### [42] [Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)](https://arxiv.org/abs/2504.10720)
*Kamaljyoti Nath,Khemraj Shukla,Victor C. Tsai,Umair bin Waheed,Christian Huber,Omer Alpak,Chuen-Song Chen,Ligang Lu,Amik St-Cyr*

Main category: cs.LG

TLDR: 本研究使用Deep Operator Networks (DeepONet)改进Full Waveform Inversion (FWI)，以提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统FWI计算需求高，且逆问题因数据有限可能非唯一，需要更高效方法。

Method: 提出DeepONet反演地震波形获取地下速度场，包括处理噪声数据、出分布预测，并作为传统FWI起始模型。

Result: DeepONet捕获关键特征，在噪声数据和某些速度模型中准确性优于其他机器学习方法，并加速FWI收敛。

Conclusion: 整合DeepONet可加速FWI过程，并提高其鲁棒性和可靠性。

Abstract: Full Waveform Inversion (FWI) is an important geophysical technique
considered in subsurface property prediction. It solves the inverse problem of
predicting high-resolution Earth interior models from seismic data. Traditional
FWI methods are computationally demanding. Inverse problems in geophysics often
face challenges of non-uniqueness due to limited data, as data are often
collected only on the surface. In this study, we introduce a novel methodology
that leverages Deep Operator Networks (DeepONet) to attempt to improve both the
efficiency and accuracy of FWI. The proposed DeepONet methodology inverts
seismic waveforms for the subsurface velocity field. This approach is able to
capture some key features of the subsurface velocity field. We have shown that
the architecture can be applied to noisy seismic data with an accuracy that is
better than some other machine learning methods. We also test our proposed
method with out-of-distribution prediction for different velocity models. The
proposed DeepONet shows comparable and better accuracy in some velocity models
than some other machine learning methods. To improve the FWI workflow, we
propose using the DeepONet output as a starting model for conventional FWI and
that it may improve FWI performance. While we have only shown that DeepONet
facilitates faster convergence than starting with a homogeneous velocity field,
it may have some benefits compared to other approaches to constructing starting
models. This integration of DeepONet into FWI may accelerate the inversion
process and may also enhance its robustness and reliability.

</details>

### [43] [Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization](https://arxiv.org/abs/2504.10735)
*Timur Carstensen,Neeratyoy Mallik,Frank Hutter,Martin Rapp*

Main category: cs.LG

TLDR: 本论文提出了一种新的多保真度超参数优化（MF-HPO）方法，使用训练或冻结层数作为保真度来源，以节省计算资源，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增长，高效的超参数优化方法变得至关重要，现有的保真度来源在低计算资源下表现不佳。

Method: 提出将训练或冻结层数作为保真度来源，通过冻结部分层减少计算和内存消耗，同时保持超参数的相关性。

Result: 在ResNets和Transformers上实证评估显示了良好相关性，并分析了与其他保真度来源结合的效用。

Conclusion: 这项工作为使用硬件资源作为保真度的MF-HPO打开新应用，并为改进联合保真度空间的算法提供了机会。

Abstract: As model sizes grow, finding efficient and cost-effective hyperparameter
optimization (HPO) methods becomes increasingly crucial for deep learning
pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources
required for DL training with lower fidelity estimations, existing fidelity
sources often fail under lower compute and memory constraints. We propose a
novel fidelity source: the number of layers that are trained or frozen during
training. For deep networks, this approach offers significant compute and
memory savings while preserving rank correlations between hyperparameters at
low fidelities compared to full model training. We demonstrate this in our
empirical evaluation across ResNets and Transformers and additionally analyze
the utility of frozen layers as a fidelity in using GPU resources as a fidelity
in HPO, and for a combined MF-HPO with other fidelity sources. This
contribution opens new applications for MF-HPO with hardware resources as a
fidelity and creates opportunities for improved algorithms navigating joint
fidelity spaces.

</details>

### [44] [Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity](https://arxiv.org/abs/2504.10752)
*Neil Mehta,Ines Goncalves,Alberto Montagna,Mathis Fleury,Gustavo Caetano,Ines Esteves,Athanasios Vourvopoulos,Pulkit Grover,Patricia Figueiredo*

Main category: cs.LG

TLDR: 这项研究证明EEG可以预测fMRI运动脑网络信号，在任务和自发条件下跨不同天有统计学意义，并有神经反馈应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨EEG和fMRI是否捕捉共享神经活动信息，特别是预测fMRI信号的程度。

Method: 使用Sparse Group Lasso正则化的可解释模型，针对个体受试者训练，并在不同天测试，与空模型和传统EEG传感器运动节律比较。

Result: 在大多数受试者中获得显著预测，任务条件下更频繁，解释了EEG通道、频率和血流动力学延迟。

Conclusion: 证明EEG可跨不同天预测fMRI活动，具有个体统计学意义，并为EEG神经反馈应用提供潜力。

Abstract: Simultaneous EEG-fMRI recordings are increasingly used to investigate brain
activity by leveraging the complementary high spatial and high temporal
resolution of fMRI and EEG signals respectively. It remains unclear, however,
to what degree these two imaging modalities capture shared information about
neural activity. Here, we investigate whether it is possible to predict both
task-evoked and spontaneous fMRI signals of motor brain networks from EEG
time-varying spectral power using interpretable models trained for individual
subjects with Sparse Group Lasso regularization. Critically, we test the
trained models on data acquired from each subject on a different day and obtain
statistical validation by comparison with appropriate null models as well as
the conventional EEG sensorimotor rhythm. We find significant prediction
results in most subjects, although less frequently for resting-state compared
to task-based conditions. Furthermore, we interpret the model learned
parameters to understand representations of EEG-fMRI coupling in terms of
predictive EEG channels, frequencies, and haemodynamic delays. In conclusion,
our work provides evidence of the ability to predict fMRI motor brain activity
from EEG recordings alone across different days, in both task-evoked and
spontaneous conditions, with statistical significance in individual subjects.
These results present great potential for translation to EEG neurofeedback
applications.

</details>

### [45] [auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory](https://arxiv.org/abs/2504.10754)
*Arjun Subramonian,Elvis Dohmatob*

Main category: cs.LG

TLDR: 本论文引入auto-fpt工具，使用自由概率理论自动计算高维随机矩阵期望迹，简化机器学习计算。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习理论中高维计算复杂，需要自动化工具简化符号计算。

Method: 开发基于Python和SymPy的auto-fpt工具，自动生成并求解固定点方程系统。

Result: 应用于神经网络高维误差分析，复现已知结果。

Conclusion: 希望auto-fpt简化高维分析，帮助机器学习社区复现和发现新现象。

Abstract: A large part of modern machine learning theory often involves computing the
high-dimensional expected trace of a rational expression of large rectangular
random matrices. To symbolically compute such quantities using free probability
theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that
can automatically produce a reduced system of fixed-point equations which can
be solved for the quantities of interest, and effectively constitutes a theory.
We overview the algorithmic ideas underlying auto-fpt and its applications to
various interesting problems, such as the high-dimensional error of linearized
feed-forward neural networks, recovering well-known results. We hope that
auto-fpt streamlines the majority of calculations involved in high-dimensional
analysis, while helping the machine learning community reproduce known and
uncover new phenomena.

</details>

### [46] [How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766)
*Ming Li,Yanhong Li,Ziyue Li,Tianyi Zhou*

Main category: cs.LG

TLDR: 本研究通过梯度谱分析统一数据质量指标，为LLM后训练提供新见解。


<details>
  <summary>Details</summary>
Motivation: 探索不同数据质量对LLM微调动态的影响，因为此领域研究不足。

Method: 使用层级梯度的奇异值分解(SVD)进行谱分析，评估指令和推理数据的质量。

Result: 高质量数据具有较低核范数和较高有效秩；有效秩更能捕捉微妙差异；同一模型家族梯度模式相似，不同家族差异显著。

Conclusion: 提供数据质量影响的统一视角，帮助开发更好的数据探索策略。

Abstract: As the post-training of large language models (LLMs) advances from
instruction-following to complex reasoning tasks, understanding how different
data affect finetuning dynamics remains largely unexplored. In this paper, we
present a spectral analysis of layer-wise gradients induced by low/high-quality
instruction and reasoning data for LLM post-training. Our analysis reveals that
widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and
Reward, can be explained and unified by spectral properties computed from
gradients' singular value decomposition (SVD). Specifically, higher-quality
data are usually associated with lower nuclear norms and higher effective
ranks. Notably, effective rank exhibits better robustness and resolution than
nuclear norm in capturing subtle quality differences. For example, reasoning
data achieves substantially higher effective ranks than instruction data,
implying richer gradient structures on more complex tasks. Our experiments also
highlight that models within the same family share similar gradient patterns
regardless of their sizes, whereas different model families diverge
significantly. Providing a unified view on the effects of data quality across
instruction and reasoning data, this work illuminates the interplay between
data quality and training stability, shedding novel insights into developing
better data exploration strategies for post-training.

</details>

### [47] [MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation](https://arxiv.org/abs/1908.08652)
*Abhay Kumar,Nishant Jain,Suraj Tripathi,Chirag Singh,Kamal Krishna*

Main category: cs.LG

TLDR: 提出了一种多任务学习网络MTCNet，用于人群密度和计数估计，通过辅助任务提升了主任务的性能。


<details>
  <summary>Details</summary>
Motivation: 人群计数估计面临非均匀尺度变化和任意视角的挑战，因此使用多任务学习来捕捉相关尺度信息。

Method: 模型包括主任务（使用VGG-16提取特征和扩张CNN生成密度图）和辅助任务（共享前端的CNN分类器）。

Result: 在ShanghaiTech数据集上，MAE比最先进方法低5.8%和14.9%（无数据增强），在UCF_CC_50数据集上低10.5%。

Conclusion: 证明了多任务学习方法在人群计数中的有效性，取得了更好的性能。

Abstract: We propose a Multi-Task Learning (MTL) paradigm based deep neural network
architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and
count estimation. Crowd count estimation is challenging due to the non-uniform
scale variations and the arbitrary perspective of an individual image. The
proposed model has two related tasks, with Crowd Density Estimation as the main
task and Crowd-Count Group Classification as the auxiliary task. The auxiliary
task helps in capturing the relevant scale-related information to improve the
performance of the main task. The main task model comprises two blocks: VGG-16
front-end for feature extraction and a dilated Convolutional Neural Network for
density map generation. The auxiliary task model shares the same front-end as
the main task, followed by a CNN classifier. Our proposed network achieves 5.8%
and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on
ShanghaiTech dataset without using any data augmentation. Our model also
outperforms with 10.5% lower MAE on UCF_CC_50 dataset.

</details>

### [48] [Collaborative Bayesian Optimization via Wasserstein Barycenters](https://arxiv.org/abs/2504.10770)
*Donglin Zhan,Haoting Zhang,Rhonda Righter,Zeyu Zheng,James Anderson*

Main category: cs.LG

TLDR: 本论文提出协作贝叶斯优化框架，解决黑箱优化和数据隐私问题，代理共享GP模型使用Wasserstein重心，证明有效性。


<details>
  <summary>Details</summary>
Motivation: 由于黑箱优化和数据隐私需求增长。

Method: 引入协作BO框架，代理共享GP模型不共享数据，构建Wasserstein重心中心模型和协作获取函数。

Result: 证明算法渐进一致和数值准确，实验中优于基线框架，与集中式方法竞争。

Conclusion: 该方法在隐私约束下实现高效协作优化。

Abstract: Motivated by the growing need for black-box optimization and data privacy, we
introduce a collaborative Bayesian optimization (BO) framework that addresses
both of these challenges. In this framework agents work collaboratively to
optimize a function they only have oracle access to. In order to mitigate
against communication and privacy constraints, agents are not allowed to share
their data but can share their Gaussian process (GP) surrogate models. To
enable collaboration under these constraints, we construct a central model to
approximate the objective function by leveraging the concept of Wasserstein
barycenters of GPs. This central model integrates the shared models without
accessing the underlying data. A key aspect of our approach is a collaborative
acquisition function that balances exploration and exploitation, allowing for
the optimization of decision variables collaboratively in each iteration. We
prove that our proposed algorithm is asymptotically consistent and that its
implementation via Monte Carlo methods is numerically accurate. Through
numerical experiments, we demonstrate that our approach outperforms other
baseline collaborative frameworks and is competitive with centralized
approaches that do not consider data privacy.

</details>

### [49] [AtlasD: Automatic Local Symmetry Discovery](https://arxiv.org/abs/2504.10777)
*Manu Bhat,Jonghyun Park,Jianke Yang,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TLDR: 本论文提出AtlasD方法，通过发现局部对称性来改进机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略局部邻域对称性，导致对称群 misrepresentation。

Method: 形式化局部对称性为图集等变性，训练局部预测网络并学习Lie群基。

Result: 在top-quark tagging和PDE实验中发现多连通分量局部对称群，提升气候分割和视觉任务性能。

Conclusion: 局部对称性作为归纳偏差，有助于改进下游任务效果。

Abstract: Existing symmetry discovery methods predominantly focus on global
transformations across the entire system or space, but they fail to consider
the symmetries in local neighborhoods. This may result in the reported symmetry
group being a misrepresentation of the true symmetry. In this paper, we
formalize the notion of local symmetry as atlas equivariance. Our proposed
pipeline, automatic local symmetry discovery (AtlasD), recovers the local
symmetries of a function by training local predictor networks and then learning
a Lie group basis to which the predictors are equivariant. We demonstrate
AtlasD is capable of discovering local symmetry groups with multiple connected
components in top-quark tagging and partial differential equation experiments.
The discovered local symmetry is shown to be a useful inductive bias that
improves the performance of downstream tasks in climate segmentation and vision
tasks.

</details>

### [50] [Power-scaled Bayesian Inference with Score-based Generative mModels](https://arxiv.org/abs/2504.10807)
*Huseyin Tuna Erdinc,Yunlin Zeng,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: cs.LG

TLDR: 本论文提出基于得分的生成算法，用于贝叶斯推理中幂缩放先验和似然的采样，应用于地震速度模型合成。


<details>
  <summary>Details</summary>
Motivation: 动机是实现对先验和似然影响的灵活控制，无需重新训练，并通过敏感性分析评估其对后验分布的影响。

Method: 方法使用基于得分的生成算法，从中间幂后验中采样，专注于条件地震图像的地震速度模型。

Result: 结果显示，增加似然幂提高样本数据保真度，减少先验幂增加结构多样性；适度似然缩放降低射击数据残差。

Conclusion: 结论是，该算法在后验细化和敏感性分析中具有实用价值。

Abstract: We propose a score-based generative algorithm for sampling from power-scaled
priors and likelihoods within the Bayesian inference framework. Our algorithm
enables flexible control over prior-likelihood influence without requiring
retraining for different power-scaling configurations. Specifically, we focus
on synthesizing seismic velocity models conditioned on imaged seismic. Our
method enables sensitivity analysis by sampling from intermediate power
posteriors, allowing us to assess the relative influence of the prior and
likelihood on samples of the posterior distribution. Through a comprehensive
set of experiments, we evaluate the effects of varying the power parameter in
different settings: applying it solely to the prior, to the likelihood of a
Bayesian formulation, and to both simultaneously. The results show that
increasing the power of the likelihood up to a certain threshold improves the
fidelity of posterior samples to the conditioning data (e.g., seismic images),
while decreasing the prior power promotes greater structural diversity among
samples. Moreover, we find that moderate scaling of the likelihood leads to a
reduced shot data residual, confirming its utility in posterior refinement.

</details>

### [51] [FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare](https://arxiv.org/abs/2504.10817)
*Penghao Wang,Qian Chen,Teng Zhang,Yingwei Zhang,Wang Lu,Yiqiang Chen*

Main category: cs.LG

TLDR: 本研究开发了Federated Healthcare Benchmark (FHBench)，一个针对真实医疗应用的基准测试，并引入了Efficient Personalized Federated Learning with Adaptive LoRA (EPFL)，以提升联邦学习在医疗领域的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据集多为多模态，计算资源有限，现有的联邦学习方法存在挑战，需要填补基准测试的空白。

Method: 开发了FHBench，涵盖神经、心血管、呼吸系统和一般病理领域的诊断任务，并引入EPFL框架，使用Adaptive LoRA实现高效的个性化联邦学习。

Result: 结果显示FHBench作为基准测试的稳健性，以及EPFL在各种医疗模式中的高效性和有效性。

Conclusion: EPFL框架解决了现有方法的局限性，推动了医疗领域联邦学习的创新发展。

Abstract: Federated Learning (FL) has emerged as an effective solution for
multi-institutional collaborations without sharing patient data, offering a
range of methods tailored for diverse applications. However, real-world medical
datasets are often multimodal, and computational resources are limited, posing
significant challenges for existing FL approaches. Recognizing these
limitations, we developed the Federated Healthcare Benchmark(FHBench), a
benchmark specifically designed from datasets derived from real-world
healthcare applications. FHBench encompasses critical diagnostic tasks across
domains such as the nervous, cardiovascular, and respiratory systems and
general pathology, providing comprehensive support for multimodal healthcare
evaluations and filling a significant gap in existing benchmarks. Building on
FHBench, we introduced Efficient Personalized Federated Learning with Adaptive
LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency
and effectiveness across various healthcare modalities. Our results highlight
the robustness of FHBench as a benchmarking tool and the potential of EPFL as
an innovative approach to advancing healthcare-focused FL, addressing key
limitations of existing methods.

</details>

### [52] [Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations](https://arxiv.org/abs/2504.10833)
*Shubham Kumar,Dwip Dalal,Narendra Ahuja*

Main category: cs.LG

TLDR: 这篇论文引入了Surrogate Faithfulness (SF)方法，以改进无监督概念解释方法的忠实度评估，并通过Optimally Faithful (OF)解释展示了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有忠实度指标忽略了概念的空间分布，导致解释不准确，因此需要更精确的评价方法。

Method: 提出SF方法，包括空间感知代理和两个新忠实度指标，并开发OF解释来最大化忠实度。

Result: 实验结果表明，添加空间感知提高了忠实度；OF解释的错误率降低了30%以上；OF概念在域外数据上泛化良好，并对对抗样本更鲁棒。

Conclusion: 新方法显著提升了解释的忠实度、泛化和鲁棒性。

Abstract: Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a
promising tool for generating semantic explanations of the decision-making
processes in deep neural networks, having applications in both model
improvement and understanding. It is vital that the explanation is accurate, or
faithful, to the model, yet we identify several limitations of prior
faithfulness metrics that inhibit an accurate evaluation; most notably, prior
metrics involve only the set of concepts present, ignoring how they may be
spatially distributed. We address these limitations with Surrogate Faithfulness
(SF), an evaluation method that introduces a spatially-aware surrogate and two
novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)
explanations, where concepts are found that maximize faithfulness. Our
experiments show that (1) adding spatial-awareness to prior U-CBEMs increases
faithfulness in all cases; (2) OF produces significantly more faithful
explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's
learned concepts generalize well to out-of-domain data and are more robust to
adversarial examples, where prior U-CBEMs struggle.

</details>

### [53] [How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?](https://arxiv.org/abs/2504.10850)
*Meiqi Liu,Zhuoqun Huang,Yue Xing*

Main category: cs.LG

TLDR: 本文提出一种无需访问基础模型权重的方法，通过训练鲁棒自编码器作为数据预处理，提高下游任务的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动机：对抗训练计算复杂度高，无法对基础模型进行微调，因此需要一种不更新模型权重的方法来提升下游任务的鲁棒性。

Method: 方法：基于理论分析，设计鲁棒自编码器作为数据预处理工具，训练时不访问基础模型。

Result: 结果：实验证明该方法有效提升下游任务鲁棒性，并验证特征鲁棒性与下游鲁棒性之间的联系。

Conclusion: 结论：该方法简单有效，证实了理论洞见。

Abstract: With the rise of powerful foundation models, a pre-training-fine-tuning
paradigm becomes increasingly popular these days: A foundation model is
pre-trained using a huge amount of data from various sources, and then the
downstream users only need to fine-tune and adapt it to specific downstream
tasks. However, due to the high computation complexity of adversarial training,
it is not feasible to fine-tune the foundation model to improve its robustness
on the downstream task. Observing the above challenge, we want to improve the
downstream robustness without updating/accessing the weights in the foundation
model. Inspired from existing literature in robustness inheritance (Kim et al.,
2020), through theoretical investigation, we identify a close relationship
between robust contrastive learning with the adversarial robustness of
supervised learning. To further validate and utilize this theoretical insight,
we design a simple-yet-effective robust auto-encoder as a data pre-processing
method before feeding the data into the foundation model. The proposed approach
has zero access to the foundation model when training the robust auto-encoder.
Extensive experiments demonstrate the effectiveness of the proposed method in
improving the robustness of downstream tasks, verifying the connection between
the feature robustness (implied by small adversarial contrastive loss) and the
robustness of the downstream task.

</details>

### [54] [ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning](https://arxiv.org/abs/2504.10851)
*Ruochen Jin,Boning Tong,Shu Yang,Bojian Hou,Li Shen*

Main category: cs.LG

TLDR: 本文引入ICAFS，一种新的垂直联邦学习特征选择方法，通过考虑客户端间交互，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法主要关注客户端内部，忽略了客户端间特征交互，导致模型性能不佳。

Method: 提出ICAFS多阶段集成方法，使用条件特征合成和可学习特征选择器，通过合成嵌入进行集成特征选择，绕过私有梯度共享限制。

Result: 在多个真实数据集实验中，ICAFS在预测准确性上超过了现有最先进方法。

Conclusion: ICAFS通过考虑客户端间交互，提供了一种有效的垂直联邦学习特征选择方案，提升了模型性能。

Abstract: Vertical federated learning (VFL) enables a paradigm for vertically
partitioned data across clients to collaboratively train machine learning
models. Feature selection (FS) plays a crucial role in Vertical Federated
Learning (VFL) due to the unique nature that data are distributed across
multiple clients. In VFL, different clients possess distinct subsets of
features for overlapping data samples, making the process of identifying and
selecting the most relevant features a complex yet essential task. Previous FS
efforts have primarily revolved around intra-client feature selection,
overlooking vital feature interaction across clients, leading to subpar model
outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for
effective FS in VFL by considering inter-client interactions. By employing
conditional feature synthesis alongside multiple learnable feature selectors,
ICAFS facilitates ensemble FS over these selectors using synthetic embeddings.
This method bypasses the limitations of private gradient sharing and allows for
model training using real data with refined embeddings. Experiments on multiple
real-world datasets demonstrate that ICAFS surpasses current state-of-the-art
methods in prediction accuracy.

</details>

### [55] [Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization](https://arxiv.org/abs/2504.10900)
*Peiliang Gong,Emadeldeen Eldele,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TLDR: 本论文提出ProtoNorm机制，改进Transformer在时间序列预训练中的规范化策略，解决数据分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 预训练大型数据集时，数据分布不匹配问题在时间序列数据中尤为严重，需要适应性强的规范化方法。

Method: 在Transformer中用原型引导的动态规范化(ProtoNorm)替换LayerNorm，通过样本与原型的亲和度选择规范化层。

Result: 实验显示，该方法在分类和预测任务上显著优于传统技术，并缓解分布偏移，在真实基准上证明鲁棒性和泛化性。

Conclusion: 该方法促进了更通用的时间序列基础模型发展。

Abstract: Foundation models have achieved remarkable success across diverse
machine-learning domains through large-scale pretraining on large, diverse
datasets. However, pretraining on such datasets introduces significant
challenges due to substantial mismatches in data distributions, a problem
particularly pronounced with time series data. In this paper, we tackle this
issue by proposing a domain-aware adaptive normalization strategy within the
Transformer architecture. Specifically, we replace the traditional LayerNorm
with a prototype-guided dynamic normalization mechanism (ProtoNorm), where
learned prototypes encapsulate distinct data distributions, and
sample-to-prototype affinity determines the appropriate normalization layer.
This mechanism effectively captures the heterogeneity of time series
characteristics, aligning pretrained representations with downstream tasks.
Through comprehensive empirical evaluation, we demonstrate that our method
significantly outperforms conventional pretraining techniques across both
classification and forecasting tasks, while effectively mitigating the adverse
effects of distribution shifts during pretraining. Incorporating ProtoNorm is
as simple as replacing a single line of code. Extensive experiments on diverse
real-world time series benchmarks validate the robustness and generalizability
of our approach, advancing the development of more versatile time series
foundation models.

</details>

### [56] [Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs](https://arxiv.org/abs/2504.10902)
*Rui Dai,Sile Hu,Xu Shen,Yonggang Zhang,Xinmei Tian,Jieping Ye*

Main category: cs.LG

TLDR: 本论文提出基于子模块线性性的模型合并策略，提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 作者认为子模块线性性高于整体模型，因此开发独立合并方法。

Method: 独立合并子模块（如层、self-attention和MLP），并推导最优合并权重的闭式解。

Result: 实验显示该方法优于标准任务算术和其他基准，在不同规模和任务中表现突出。

Conclusion: 强调利用子模块线性性的优势，为多任务模型合并提供新视角。

Abstract: Task arithmetic is a straightforward yet highly effective strategy for model
merging, enabling the resultant model to exhibit multi-task capabilities.
Recent research indicates that models demonstrating linearity enhance the
performance of task arithmetic. In contrast to existing methods that rely on
the global linearization of the model, we argue that this linearity already
exists within the model's submodules. In particular, we present a statistical
analysis and show that submodules (e.g., layers, self-attentions, and MLPs)
exhibit significantly higher linearity than the overall model. Based on these
findings, we propose an innovative model merging strategy that independently
merges these submodules. Especially, we derive a closed-form solution for
optimal merging weights grounded in the linear properties of these submodules.
Experimental results demonstrate that our method consistently outperforms the
standard task arithmetic approach and other established baselines across
different model scales and various tasks. This result highlights the benefits
of leveraging the linearity of submodules and provides a new perspective for
exploring solutions for effective and practical multi-task model merging.

</details>

### [57] [Towards A Universal Graph Structural Encoder](https://arxiv.org/abs/2504.10917)
*Jialin Chen,Haolan Zuo,Haoyu Peter Wang,Siqi Miao,Pan Li,Rex Ying*

Main category: cs.LG

TLDR: GFSE 是一种通用图结构编码器，通过自监督预训练提升跨域图表示学习性能，并在多数情况下达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕获和转移不同图域的结构信息，且无法有效处理复杂拓扑。

Method: 提出 GFSE，使用基于图 Transformer 的注意力机制和多自监督学习目标进行预训练，可与下游模型无缝整合。

Result: 实验显示 GFSE 显著提升性能，在 81.6% 的评估案例中达到最先进水平，并减少微调需求。

Conclusion: GFSE 作为强大通用的图结构编码器，具有广泛应用于图结构数据的潜力。

Abstract: Recent advancements in large-scale pre-training have shown the potential to
learn generalizable representations for downstream tasks. In the graph domain,
however, capturing and transferring structural information across different
graph domains remains challenging, primarily due to the inherent differences in
topological patterns across various contexts. Additionally, most existing
models struggle to capture the complexity of rich graph structures, leading to
inadequate exploration of the embedding space. To address these challenges, we
propose GFSE, a universal graph structural encoder designed to capture
transferable structural patterns across diverse domains such as molecular
graphs, social networks, and citation networks. GFSE is the first cross-domain
graph structural encoder pre-trained with multiple self-supervised learning
objectives. Built on a Graph Transformer, GFSE incorporates attention
mechanisms informed by graph inductive bias, enabling it to encode intricate
multi-level and fine-grained topological features. The pre-trained GFSE
produces generic and theoretically expressive positional and structural
encoding for graphs, which can be seamlessly integrated with various downstream
graph feature encoders, including graph neural networks for vectorized features
and Large Language Models for text-attributed graphs. Comprehensive experiments
on synthetic and real-world datasets demonstrate GFSE's capability to
significantly enhance the model's performance while requiring substantially
less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art
performance in 81.6% evaluated cases, spanning diverse graph models and
datasets, highlighting its potential as a powerful and versatile encoder for
graph-structured data.

</details>

### [58] [Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting](https://arxiv.org/abs/2504.10923)
*Mingyi Zhu,Zhaoxin Li,Qiao Lin,Li Ding*

Main category: cs.LG

TLDR: 本文提出Fast-Powerformer模型，用于中期风力发电预测，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 风力发电预测面临气象随机性和输出波动带来的准确性和计算效率挑战。

Method: 基于Reformer架构，添加轻量级LSTM嵌入、输入转置和FECAM机制，以增强特征提取和减少复杂度。

Result: 实验显示模型在真实数据集上精度和效率优于主流方法，并具有快速推理和低内存消耗。

Conclusion: 模型在实际部署中具有高实用价值。

Abstract: Wind power forecasting (WPF), as a significant research topic within
renewable energy, plays a crucial role in enhancing the security, stability,
and economic operation of power grids. However, due to the high stochasticity
of meteorological factors (e.g., wind speed) and significant fluctuations in
wind power output, mid-term wind power forecasting faces a dual challenge of
maintaining high accuracy and computational efficiency. To address these
issues, this paper proposes an efficient and lightweight mid-term wind power
forecasting model, termed Fast-Powerformer. The proposed model is built upon
the Reformer architecture, incorporating structural enhancements such as a
lightweight Long Short-Term Memory (LSTM) embedding module, an input
transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism
(FECAM). These improvements enable the model to strengthen temporal feature
extraction, optimize dependency modeling across variables, significantly reduce
computational complexity, and enhance sensitivity to periodic patterns and
dominant frequency components. Experimental results conducted on multiple
real-world wind farm datasets demonstrate that the proposed Fast-Powerformer
achieves superior prediction accuracy and operational efficiency compared to
mainstream forecasting approaches. Furthermore, the model exhibits fast
inference speed and low memory consumption, highlighting its considerable
practical value for real-world deployment scenarios.

</details>

### [59] [Transfer Learning for Temporal Link Prediction](https://arxiv.org/abs/2504.10925)
*Ayan Chatterjee,Barbara Ikica,Babak Ravandi,John Palowitch*

Main category: cs.LG

TLDR: 本论文研究了时间链接预测的转移学习任务，通过添加结构映射模块，使模型能够转移到新图上，并为无记忆基础模型铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有TLP模型无法直接应用于新图，因为记忆模块只存储训练时的节点信息，且结构信号对任务具有重要性。

Method: 在现有TLP模型架构中添加结构映射模块，学习从图结构特征到记忆嵌入的映射。

Result: 开发了转移有效的记忆负载模型，为时间链接预测的无记忆基础模型提供了可能性。

Conclusion: 这项工作推动了TLP任务中模型的泛化能力和基础模型的发展。

Abstract: Link prediction on graphs has applications spanning from recommender systems
to drug discovery. Temporal link prediction (TLP) refers to predicting future
links in a temporally evolving graph and adds additional complexity related to
the dynamic nature of graphs. State-of-the-art TLP models incorporate memory
modules alongside graph neural networks to learn both the temporal mechanisms
of incoming nodes and the evolving graph topology. However, memory modules only
store information about nodes seen at train time, and hence such models cannot
be directly transferred to entirely new graphs at test time and deployment. In
this work, we study a new transfer learning task for temporal link prediction,
and develop transfer-effective methods for memory-laden models. Specifically,
motivated by work showing the informativeness of structural signals for the TLP
task, we augment a structural mapping module to the existing TLP model
architectures, which learns a mapping from graph structural (topological)
features to memory embeddings. Our work paves the way for a memory-free
foundation model for TLP.

</details>

### [60] [GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA](https://arxiv.org/abs/2504.10490)
*Gabriel Bo,Marc Bernardino,Justin Gu*

Main category: cs.LG

TLDR: 本研究探索将KAN和图表示整合到GPT-2中以提升多任务学习准确性，但发现优化LoRA方法更有效。


<details>
  <summary>Details</summary>
Motivation: 受KAN和GAT在CoT模型中应用的 surge 以及与MLP比较的辩论驱动，旨在提升多任务学习准确性。

Method: 使用LoRA增强自注意力transformer，进行超参数微调和L2正则化；开发Graph LoRA和Hybrid-KAN LoRA变体。

Result: 优化LoRA transformer在SST上55.249%准确率，CFIMDB上99.18%，释义检测上89.9%，十四行诗生成CHRF得分为42.097；变体未优于基准。

Conclusion: LoRA的efficient参数适配是针对情感分析、释义检测和十四行诗生成的最有效策略。

Abstract: We explore the potential of integrating learnable and interpretable
modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based
representations--within a pre-trained GPT-2 model to enhance multi-task
learning accuracy. Motivated by the recent surge in using KAN and graph
attention (GAT) architectures in chain-of-thought (CoT) models and debates over
their benefits compared to simpler architectures like MLPs, we begin by
enhancing a standard self-attention transformer using Low-Rank Adaptation
(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This
approach yields significant improvements. To further boost interpretability and
richer representations, we develop two variants that attempt to improve the
standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,
systematic evaluations reveal that neither variant outperforms the optimized
LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,
99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On
sonnet generation, we get a CHRF score of 42.097. These findings highlight that
efficient parameter adaptation via LoRA remains the most effective strategy for
our tasks: sentiment analysis, paraphrase detection, and sonnet generation.

</details>

### [61] [Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP](https://arxiv.org/abs/2504.10536)
*Lihong Zhang,Yue Li*

Main category: cs.LG

TLDR: 提出Layer-Skipping Federated Learning方法，减少联邦学习中大语言模型的通信开销，同时在医疗NLP任务中保持高性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可解决医疗NLP中的隐私问题，但面临通信开销和数据异质性挑战。

Method: 提出仅微调预训练LLM选定层的Layer-Skipping FL方法，使用LLaMA 3.2-1B模型，在i2b2和MIMIC-III数据集上评估临床命名实体识别和分类任务。

Result: 减少约70%通信成本，性能仅比集中式训练低2%，优于基线，处理非IID数据并与差分隐私兼容。

Conclusion: 这种方法是医疗NLP中隐私保护协作学习的实用解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
organizations without sharing raw data, addressing crucial privacy concerns in
healthcare natural language processing (NLP). However, training large language
models (LLMs) in federated settings faces significant challenges, including
communication overhead and data heterogeneity. We propose Layer-Skipping
Federated Learning, where only selected layers of a pre-trained LLM are
fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,
our approach reduces communication costs by approximately 70% while maintaining
performance within 2% of centralized training. We evaluate our method on
clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our
experiments demonstrate that Layer-Skipping FL outperforms competitive
baselines, handles non-IID clinical data distributions effectively, and shows
robustness when combined with differential privacy. This approach represents a
practical solution for privacy-preserving collaborative learning in healthcare
NLP.

</details>

### [62] [MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers](https://arxiv.org/abs/2504.10551)
*Lili Zhao,Qi Liu,Wei Chen,Liyi Chen,Ruijun Sun,Min Hou,Yang Wang,Shijin Wang*

Main category: cs.LG

TLDR: 本论文提出MiMu方法，用于缓解经验风险最小化（ERM）模型中的多种捷径学习问题，提高鲁棒性泛化性能。


<details>
  <summary>Details</summary>
Motivation: ERM模型常依赖虚假相关性和捷径学习，导致泛化能力差；真实数据中存在多种未知捷径，模型更依赖强捷径。

Method: 提出MiMu方法，整合Transformer-based ERM，包括源模型的自校准策略和目标模型的自提升策略（如随机掩码和自适应注意力对齐模块）。

Result: 在NLP和CV领域的广泛实验中，MiMu有效提升了模型的鲁棒性泛化能力。

Conclusion: MiMu成功缓解了多个捷径学习问题，提高了模型的整体性能。

Abstract: Empirical Risk Minimization (ERM) models often rely on spurious correlations
between features and labels during the learning process, leading to shortcut
learning behavior that undermines robustness generalization performance.
Current research mainly targets identifying or mitigating a single shortcut;
however, in real-world scenarios, cues within the data are diverse and unknown.
In empirical studies, we reveal that the models rely to varying extents on
different shortcuts. Compared to weak shortcuts, models depend more heavily on
strong shortcuts, resulting in their poor generalization ability. To address
these challenges, we propose MiMu, a novel method integrated with
Transformer-based ERMs designed to Mitigate Multiple shortcut learning
behavior, which incorporates self-calibration strategy and self-improvement
strategy. In the source model, we preliminarily propose the self-calibration
strategy to prevent the model from relying on shortcuts and make overconfident
predictions. Then, we further design self-improvement strategy in target model
to reduce the reliance on multiple shortcuts. The random mask strategy involves
randomly masking partial attention positions to diversify the focus of target
model other than concentrating on a fixed region. Meanwhile, the adaptive
attention alignment module facilitates the alignment of attention weights to
the calibrated source model, without the need for post-hoc attention maps or
supervision. Finally, extensive experiments conducted on Natural Language
Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu
in improving robustness generalization abilities.

</details>

### [63] [LEMUR Neural Network Dataset: Towards Seamless AutoML](https://arxiv.org/abs/2504.10552)
*Arash Torabi Goodarzi,Roman Kochnev,Waleed Khalid,Furui Qin,Tolgay Atinc Uzun,Yashkumar Sanjaybhai Dhameliya,Yash Kanubhai Kathiriya,Zofia Antonina Bentyn,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TLDR: 本文介绍了LEMUR，这是一个开源神经网络模型数据集，用于支持AutoML、基准测试和模型分析。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集对神经网络发展至关重要，本文旨在提供一个由神经网络模型组成的结构化数据集，以推动基准测试、AutoML和模型分析。

Method: 使用Python和PyTorch构建LEMUR数据集，提供结构化代码和性能数据，集成Optuna框架进行评估、超参数优化和分析，并支持边缘设备部署。

Result: LEMUR启用了LLM的微调、提供了API访问模型信息和性能统计，支持研究人员在开发、测试和分析神经网络时使用。

Conclusion: LEMUR将作为MIT许可开源项目发布，帮助研究者和从业者在资源受限环境中高效部署和分析神经网络模型。

Abstract: Neural networks are fundamental in artificial intelligence, driving progress
in computer vision and natural language processing. High-quality datasets are
crucial for their development, and there is growing interest in datasets
composed of neural networks themselves to support benchmarking, automated
machine learning (AutoML), and model analysis. We introduce LEMUR, an open
source dataset of neural network models with well-structured code for diverse
architectures across tasks such as object detection, image classification,
segmentation, and natural language processing. LEMUR is primarily designed to
enable fine-tuning of large language models (LLMs) for AutoML tasks, providing
a rich source of structured model representations and associated performance
data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new
datasets and models while maintaining consistency. It integrates an
Optuna-powered framework for evaluation, hyperparameter optimization,
statistical analysis, and graphical insights. LEMUR provides an extension that
enables models to run efficiently on edge devices, facilitating deployment in
resource-constrained environments. Providing tools for model evaluation,
preprocessing, and database management, LEMUR supports researchers and
practitioners in developing, testing, and analyzing neural networks.
Additionally, it offers an API that delivers comprehensive information about
neural network models and their complete performance statistics with a single
request, which can be used in experiments with code-generating large language
models. The LEMUR will be released as an open source project under the MIT
license upon acceptance of the paper.

</details>

### [64] [Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains](https://arxiv.org/abs/2504.10555)
*Marco Salmè,Lorenzo Tronchin,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.LG

TLDR: 本文探讨了在数据稀缺场景下使用深度生成模型生成合成数据，扩展评估标准并比较VAE、GAN和DM。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺阻碍了医学和精准农业等领域的技术进步，需要合成数据满足保真度、多样性、采样效率、实用性、鲁棒性和隐私性。

Method: 使用最先进指标评估生成学习三难困境，比较VAE、GAN和DM，并提出框架评估合成数据的实用性、鲁棒性和隐私性。

Result: 发现不同DGM在应用中具有独特优势。

Conclusion: 扩展三难困境以适应现实需求，并为选择合适的DGM提供指导。

Abstract: Data scarcity remains a critical bottleneck impeding technological
advancements across various domains, including but not limited to medicine and
precision agriculture. To address this challenge, we explore the potential of
Deep Generative Models (DGMs) in producing synthetic data that satisfies the
Generative Learning Trilemma: fidelity, diversity, and sampling efficiency.
However, recognizing that these criteria alone are insufficient for practical
applications, we extend the trilemma to include utility, robustness, and
privacy, factors crucial for ensuring the applicability of DGMs in real-world
scenarios. Evaluating these metrics becomes particularly challenging in
data-scarce environments, as DGMs traditionally rely on large datasets to
perform optimally. This limitation is especially pronounced in domains like
medicine and precision agriculture, where ensuring acceptable model performance
under data constraints is vital. To address these challenges, we assess the
Generative Learning Trilemma in data-scarcity settings using state-of-the-art
evaluation metrics, comparing three prominent DGMs: Variational Autoencoders
(VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs).
Furthermore, we propose a comprehensive framework to assess utility,
robustness, and privacy in synthetic data generated by DGMs. Our findings
demonstrate varying strengths among DGMs, with each model exhibiting unique
advantages based on the application context. This study broadens the scope of
the Generative Learning Trilemma, aligning it with real-world demands and
providing actionable guidance for selecting DGMs tailored to specific
applications.

</details>

### [65] [VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification](https://arxiv.org/abs/2504.10556)
*Lucas Heublein,Simon Kocher,Tobias Feigl,Alexander Rügamer,Christopher Mutschler,Felix Ott*

Main category: cs.LG

TLDR: 本篇论文提出使用变分自编码器（VAEs）进行特征解缠结，以压缩和增强数据，用于高精度分类GNSS干扰。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习和Edge AI在GNSS应用中的挑战，包括高效数据处理、低延迟、数据隐私和模型压缩，同时保持高性能。

Method: 提出VAEs解缠结方法，包括基础、因子化和条件生成变体，在四个数据集上评估，并进行超参数优化。

Result: 实现数据压缩率512至8192，分类准确率高达99.92%。

Conclusion: VAE方法成功实现了高精度干扰分类和显著数据压缩，提升了分布式环境下的性能。

Abstract: Distributed learning and Edge AI necessitate efficient data processing,
low-latency communication, decentralized model training, and stringent data
privacy to facilitate real-time intelligence on edge devices while reducing
dependency on centralized infrastructure and ensuring high model performance.
In the context of global navigation satellite system (GNSS) applications, the
primary objective is to accurately monitor and classify interferences that
degrade system performance in distributed environments, thereby enhancing
situational awareness. To achieve this, machine learning (ML) models can be
deployed on low-resource devices, ensuring minimal communication latency and
preserving data privacy. The key challenge is to compress ML models while
maintaining high classification accuracy. In this paper, we propose variational
autoencoders (VAEs) for disentanglement to extract essential latent features
that enable accurate classification of interferences. We demonstrate that the
disentanglement approach can be leveraged for both data compression and data
augmentation by interpolating the lower-dimensional latent representations of
signal power. To validate our approach, we evaluate three VAE variants -
vanilla, factorized, and conditional generative - on four distinct datasets,
including two collected in controlled indoor environments and two real-world
highway datasets. Additionally, we conduct extensive hyperparameter searches to
optimize performance. Our proposed VAE achieves a data compression rate ranging
from 512 to 8,192 and achieves an accuracy up to 99.92%.

</details>

### [66] [Efficient Process Reward Model Training via Active Learning](https://arxiv.org/abs/2504.10559)
*Keyu Duan,Zichen Liu,Xin Mao,Tianyu Pang,Changyu Chen,Qiguang Chen,Michael Qizhe Shieh,Longxu Dou*

Main category: cs.LG

TLDR: 本文提出ActPRM主动学习方法，减少PRMs训练数据标注成本50%，并通过数据过滤提升性能至SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 解决PRMs步级监督训练数据标注规模化困难的问题，因为人类和LLM标注成本高且挑战性大。

Method: 使用ActPRM估计样本不确定性，选择高不确定数据，由高效推理模型标注，然后计算损失更新PRM权重。

Result: ActPRM减少50%标注量，性能相当或更好；过滤100+万数学轨迹，保留60%数据，PRM在ProcessBench达75.0%、PRMBench达65.5%的新SOTA。

Conclusion: ActPRM提高了标注效率，并通过主动学习提升模型性能，实现了高效的PRMs训练。

Abstract: Process Reward Models (PRMs) provide step-level supervision to large language
models (LLMs), but scaling up training data annotation remains challenging for
both humans and LLMs. To address this limitation, we propose an active learning
approach, ActPRM, which proactively selects the most uncertain samples for
training, substantially reducing labeling costs. During training, we use the
PRM to estimate uncertainty after the forward pass, retaining only highly
uncertain data. A capable yet costly reasoning model then labels this data.
Then we compute the loss with respect to the labels and update the PRM's
weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active
learning setting, demonstrating that ActPRM reduces 50% annotation, but
achieving the comparable or even better performance. Beyond annotation
efficiency, we further advance the actively trained PRM by filtering over 1M+
math reasoning trajectories with ActPRM, retaining 60% of the data. A
subsequent training on this selected dataset yields a new state-of-the-art
(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same
sized models.

</details>

### [67] [Self-Controlled Dynamic Expansion Model for Continual Learning](https://arxiv.org/abs/2504.10561)
*Runqing Wu,Fei Ye,Rongyao Hu,Guoxi Huang*

Main category: cs.LG

TLDR: 这篇论文提出了一种自控动态扩展模型（SCDEM），用于视觉Transformer的持续学习，通过多个骨干网络和优化机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决现有持续学习方法中，使用单一静态预训练视觉Transformer骨干网络无法良好适应新任务的问题，尤其是在数据域多样时，存在许多不活跃参数和负知识转移。

Method: 方法包括引入SCDEM，使用多个可训练的ViT骨干网络；协作优化机制（COM）利用历史专家预测信号优化学习；特征分布一致性（FDC）通过最优传输距离对齐语义相似性；动态层级特征注意力机制（DLWFAM）自动调整惩罚强度。

Result: 实验结果显示，该方法在持续学习任务中达到了最先进性能。

Conclusion: 该方法有效地缓解了过度正则化和负知识转移问题，证明了动态多骨干网络在持续学习中的优越性。

Abstract: Continual Learning (CL) epitomizes an advanced training paradigm wherein
prior data samples remain inaccessible during the acquisition of new tasks.
Numerous investigations have delved into leveraging a pre-trained Vision
Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless,
these approaches typically utilize a singular, static backbone, which
inadequately adapts to novel tasks, particularly when engaging with diverse
data domains, due to a substantial number of inactive parameters. This paper
addresses this limitation by introducing an innovative Self-Controlled Dynamic
Expansion Model (SCDEM), which orchestrates multiple distinct trainable
pre-trained ViT backbones to furnish diverse and semantically enriched
representations. Specifically, by employing the multi-backbone architecture as
a shared module, the proposed SCDEM dynamically generates a new expert with
minimal parameters to accommodate a new task. A novel Collaborative
Optimization Mechanism (COM) is introduced to synergistically optimize multiple
backbones by harnessing prediction signals from historical experts, thereby
facilitating new task learning without erasing previously acquired knowledge.
Additionally, a novel Feature Distribution Consistency (FDC) approach is
proposed to align semantic similarity between previously and currently learned
representations through an optimal transport distance-based mechanism,
effectively mitigating negative knowledge transfer effects. Furthermore, to
alleviate over-regularization challenges, this paper presents a novel Dynamic
Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the
penalization intensity on each trainable representation layer. An extensive
series of experiments have been conducted to evaluate the proposed
methodology's efficacy, with empirical results corroborating that the approach
attains state-of-the-art performance.

</details>

### [68] [Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling](https://arxiv.org/abs/2504.10612)
*Michal Balcerak,Tamaz Amiranashvili,Suprosanna Shit,Antonio Terpin,Sebastian Kaltenbach,Petros Koumoutsakos,Bjoern Menze*

Main category: cs.LG

TLDR: 本文提出Energy Matching框架，将生成模型的流方法与能量模型结合，提高生成性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理部分观察和额外先验时效率低下，作者受Wasserstein梯度流启发，开发新框架。

Method: 使用单一时间无关的标量场，引导样本从噪声通过最优传输路径到数据，并由熵能引导到Boltzmann平衡。

Result: 在CIFAR-10上FID分数为3.97，优于现有方法8.61；保留无模拟训练，并引入交互能量探索模式。

Conclusion: 简化EBM框架，提升其能力和应用范围，促进生成建模的广泛采用。

Abstract: Generative models often map noise to data by matching flows or scores, but
these approaches become cumbersome for incorporating partial observations or
additional priors. Inspired by recent advances in Wasserstein gradient flows,
we propose Energy Matching, a framework that unifies flow-based approaches with
the flexibility of energy-based models (EBMs). Far from the data manifold,
samples move along curl-free, optimal transport paths from noise to data. As
they approach the data manifold, an entropic energy term guides the system into
a Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97
compared to 8.61), while retaining the simulation-free training of
transport-based approaches away from the data manifold. Additionally, we
exploit the flexibility of our method and introduce an interaction energy for
diverse mode exploration. Our approach focuses on learning a static scalar
potential energy -- without time conditioning, auxiliary generators, or
additional networks -- marking a significant departure from recent EBM methods.
We believe this simplified framework significantly advances EBM capabilities
and paves the way for their broader adoption in generative modeling across
diverse domains.

</details>

### [69] [Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning](https://arxiv.org/abs/2504.10677)
*Muhammad Al-Zafar Khan,Jamal Al-Karaki*

Main category: cs.LG

TLDR: 本文提出多智能体强化学习框架优化组织修复，整合生物模型和学习方法，实验显示有效策略。


<details>
  <summary>Details</summary>
Motivation: 优化使用工程化生物代理的组织修复过程。

Method: 整合随机反应-扩散系统、Hebbian可塑性通信、生物奖励函数和课程学习方案。

Result: 模拟实验展示动态分泌控制和空间协调等紧急修复策略。

Conclusion: 框架在虚拟实验中有效，可能应用于实际组织修复优化。

Abstract: In this paper, we present a multi-agent reinforcement learning (MARL)
framework for optimizing tissue repair processes using engineered biological
agents. Our approach integrates: (1) stochastic reaction-diffusion systems
modeling molecular signaling, (2) neural-like electrochemical communication
with Hebbian plasticity, and (3) a biologically informed reward function
combining chemical gradient tracking, neural synchronization, and robust
penalties. A curriculum learning scheme guides the agent through progressively
complex repair scenarios. In silico experiments demonstrate emergent repair
strategies, including dynamic secretion control and spatial coordination.

</details>

### [70] [The Jailbreak Tax: How Useful are Your Jailbreak Outputs?](https://arxiv.org/abs/2504.10694)
*Kristina Nikolić,Luze Sun,Jie Zhang,Florian Tramèr*

Main category: cs.LG

TLDR: 这篇论文评估了越狱攻击对大语言模型的效用，发现越狱会导致输出质量下降，称为“越狱税”，并提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 作者质疑现有越狱攻击的输出是否真正有用，尤其是在难以评估的不安全主题上，因此构建了基于良性主题的评估集。

Method: 通过让模型拒绝回答生物学或数学等主题的问题，创建新评估集，并测试八种代表性越狱攻击在五个效用基准上的表现。

Result: 发现越狱攻击导致模型效用显著下降，例如数学准确率下降高达92%，证实了“越狱税”的存在。

Conclusion: 提出“越狱税”作为AI安全新指标，并发布开源基准以评估未来越狱攻击。

Abstract: Jailbreak attacks bypass the guardrails of large language models to produce
harmful outputs. In this paper, we ask whether the model outputs produced by
existing jailbreaks are actually useful. For example, when jailbreaking a model
to give instructions for building a bomb, does the jailbreak yield good
instructions? Since the utility of most unsafe answers (e.g., bomb
instructions) is hard to evaluate rigorously, we build new jailbreak evaluation
sets with known ground truth answers, by aligning models to refuse questions
related to benign and easy-to-evaluate topics (e.g., biology or math). Our
evaluation of eight representative jailbreaks across five utility benchmarks
reveals a consistent drop in model utility in jailbroken responses, which we
term the jailbreak tax. For example, while all jailbreaks we tested bypass
guardrails in models aligned to refuse to answer math, this comes at the
expense of a drop of up to 92% in accuracy. Overall, our work proposes the
jailbreak tax as a new important metric in AI safety, and introduces benchmarks
to evaluate existing and future jailbreaks. We make the benchmark available at
https://github.com/ethz-spylab/jailbreak-tax

</details>

### [71] [Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)](https://arxiv.org/abs/2504.10720)
*Kamaljyoti Nath,Khemraj Shukla,Victor C. Tsai,Umair bin Waheed,Christian Huber,Omer Alpak,Chuen-Song Chen,Ligang Lu,Amik St-Cyr*

Main category: cs.LG

TLDR: 本研究使用Deep Operator Networks (DeepONet) 改进Full Waveform Inversion (FWI) 的效率和准确性，处理地震数据逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统FWI计算需求高，且逆问题因数据有限存在非唯一性挑战。

Method: 引入DeepONet方法逆转地震波形以获取地下速度场，应用于噪声数据和分布外预测，并建议用DeepONet输出作为传统FWI的起始模型。

Result: DeepONet在噪声数据和某些速度模型中比其他机器学习方法更准确，并加速FWI收敛。

Conclusion: 将DeepONet整合到FWI中可能加速逆过程，提高鲁棒性和可靠性。

Abstract: Full Waveform Inversion (FWI) is an important geophysical technique
considered in subsurface property prediction. It solves the inverse problem of
predicting high-resolution Earth interior models from seismic data. Traditional
FWI methods are computationally demanding. Inverse problems in geophysics often
face challenges of non-uniqueness due to limited data, as data are often
collected only on the surface. In this study, we introduce a novel methodology
that leverages Deep Operator Networks (DeepONet) to attempt to improve both the
efficiency and accuracy of FWI. The proposed DeepONet methodology inverts
seismic waveforms for the subsurface velocity field. This approach is able to
capture some key features of the subsurface velocity field. We have shown that
the architecture can be applied to noisy seismic data with an accuracy that is
better than some other machine learning methods. We also test our proposed
method with out-of-distribution prediction for different velocity models. The
proposed DeepONet shows comparable and better accuracy in some velocity models
than some other machine learning methods. To improve the FWI workflow, we
propose using the DeepONet output as a starting model for conventional FWI and
that it may improve FWI performance. While we have only shown that DeepONet
facilitates faster convergence than starting with a homogeneous velocity field,
it may have some benefits compared to other approaches to constructing starting
models. This integration of DeepONet into FWI may accelerate the inversion
process and may also enhance its robustness and reliability.

</details>

### [72] [Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization](https://arxiv.org/abs/2504.10735)
*Timur Carstensen,Neeratyoy Mallik,Frank Hutter,Martin Rapp*

Main category: cs.LG

TLDR: 这篇论文提出了一种新的保真度来源，用于多保真度超参数优化（MF-HPO），通过训练或冻结层数来节省计算和内存，同时保持超参数相关性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增长，高效的超参数优化方法至关重要，而现有保真度来源在低计算和内存约束下往往失效。

Method: 提出使用训练或冻结层数作为MF-HPO中的保真度来源。

Result: 实证评估在ResNets和Transformers上显示了计算和内存节省，并保持了超参数秩相关性；还分析了与GPU资源结合的效用和与其他保真度来源的联合使用。

Conclusion: 为使用硬件资源作为保真度的MF-HPO开辟新应用，并为改进算法提供了机会。

Abstract: As model sizes grow, finding efficient and cost-effective hyperparameter
optimization (HPO) methods becomes increasingly crucial for deep learning
pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources
required for DL training with lower fidelity estimations, existing fidelity
sources often fail under lower compute and memory constraints. We propose a
novel fidelity source: the number of layers that are trained or frozen during
training. For deep networks, this approach offers significant compute and
memory savings while preserving rank correlations between hyperparameters at
low fidelities compared to full model training. We demonstrate this in our
empirical evaluation across ResNets and Transformers and additionally analyze
the utility of frozen layers as a fidelity in using GPU resources as a fidelity
in HPO, and for a combined MF-HPO with other fidelity sources. This
contribution opens new applications for MF-HPO with hardware resources as a
fidelity and creates opportunities for improved algorithms navigating joint
fidelity spaces.

</details>

### [73] [Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity](https://arxiv.org/abs/2504.10752)
*Neil Mehta,Ines Goncalves,Alberto Montagna,Mathis Fleury,Gustavo Caetano,Ines Esteves,Athanasios Vourvopoulos,Pulkit Grover,Patricia Figueiredo*

Main category: cs.LG

TLDR: 本文使用可解释模型从EEG预测fMRI信号，跨天验证，在个体水平上显示显著预测能力。


<details>
  <summary>Details</summary>
Motivation: 探讨EEG和fMRI在神经活动信息共享程度的不确定性，并为EEG神经反馈应用提供潜力。

Method: 采用Sparse Group Lasso正则化训练可解释模型，针对个体受试者，并在不同天的数据上测试，与null模型和传统EEG比较。

Result: 在大多数受试者中预测显著，任务条件多于静息状态，并解释EEG通道、频率和血流动力学延迟。

Conclusion: 证明了从EEG单独预测fMRI运动脑活动的能力，跨任务和自发条件，统计显著，具有神经反馈应用的潜力。

Abstract: Simultaneous EEG-fMRI recordings are increasingly used to investigate brain
activity by leveraging the complementary high spatial and high temporal
resolution of fMRI and EEG signals respectively. It remains unclear, however,
to what degree these two imaging modalities capture shared information about
neural activity. Here, we investigate whether it is possible to predict both
task-evoked and spontaneous fMRI signals of motor brain networks from EEG
time-varying spectral power using interpretable models trained for individual
subjects with Sparse Group Lasso regularization. Critically, we test the
trained models on data acquired from each subject on a different day and obtain
statistical validation by comparison with appropriate null models as well as
the conventional EEG sensorimotor rhythm. We find significant prediction
results in most subjects, although less frequently for resting-state compared
to task-based conditions. Furthermore, we interpret the model learned
parameters to understand representations of EEG-fMRI coupling in terms of
predictive EEG channels, frequencies, and haemodynamic delays. In conclusion,
our work provides evidence of the ability to predict fMRI motor brain activity
from EEG recordings alone across different days, in both task-evoked and
spontaneous conditions, with statistical significance in individual subjects.
These results present great potential for translation to EEG neurofeedback
applications.

</details>

### [74] [auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory](https://arxiv.org/abs/2504.10754)
*Arjun Subramonian,Elvis Dohmatob*

Main category: cs.LG

TLDR: auto-fpt 是一个基于自由概率理论的工具，用于自动计算高维随机矩阵期望迹，并应用于机器学习问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习理论常需计算高维随机矩阵期望迹，auto-fpt 旨在简化这些计算，帮助社区重现和发现新现象。

Method: 使用 Python 和 SymPy 开发，自动生成基于自由概率理论的固定点方程系统。

Result: 应用于神经网络高维误差等问题，成功恢复已知结果。

Conclusion: 希望简化高维分析计算，促进机器学习社区发展。

Abstract: A large part of modern machine learning theory often involves computing the
high-dimensional expected trace of a rational expression of large rectangular
random matrices. To symbolically compute such quantities using free probability
theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that
can automatically produce a reduced system of fixed-point equations which can
be solved for the quantities of interest, and effectively constitutes a theory.
We overview the algorithmic ideas underlying auto-fpt and its applications to
various interesting problems, such as the high-dimensional error of linearized
feed-forward neural networks, recovering well-known results. We hope that
auto-fpt streamlines the majority of calculations involved in high-dimensional
analysis, while helping the machine learning community reproduce known and
uncover new phenomena.

</details>

### [75] [How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766)
*Ming Li,Yanhong Li,Ziyue Li,Tianyi Zhou*

Main category: cs.LG

TLDR: 本文通过梯度谱分析统一数据质量指标，并揭示LLM后训练中数据质量与训练稳定性的关系。


<details>
  <summary>Details</summary>
Motivation: 探索不同数据质量如何影响LLM从指令跟随到复杂推理任务的微调动态，这方面研究尚不充分。

Method: 使用层级梯度奇异值分解(SVD)进行谱分析，解释指标如核范数和有效秩。

Result: 高质量数据具有较低核范数和较高有效秩；推理数据有效秩更高；同一模型家族梯度模式相似。

Conclusion: 提供数据质量统一视角，阐明其与训练稳定性的互动，并为优化数据探索策略提供新见解。

Abstract: As the post-training of large language models (LLMs) advances from
instruction-following to complex reasoning tasks, understanding how different
data affect finetuning dynamics remains largely unexplored. In this paper, we
present a spectral analysis of layer-wise gradients induced by low/high-quality
instruction and reasoning data for LLM post-training. Our analysis reveals that
widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and
Reward, can be explained and unified by spectral properties computed from
gradients' singular value decomposition (SVD). Specifically, higher-quality
data are usually associated with lower nuclear norms and higher effective
ranks. Notably, effective rank exhibits better robustness and resolution than
nuclear norm in capturing subtle quality differences. For example, reasoning
data achieves substantially higher effective ranks than instruction data,
implying richer gradient structures on more complex tasks. Our experiments also
highlight that models within the same family share similar gradient patterns
regardless of their sizes, whereas different model families diverge
significantly. Providing a unified view on the effects of data quality across
instruction and reasoning data, this work illuminates the interplay between
data quality and training stability, shedding novel insights into developing
better data exploration strategies for post-training.

</details>

### [76] [MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation](https://arxiv.org/abs/1908.08652)
*Abhay Kumar,Nishant Jain,Suraj Tripathi,Chirag Singh,Kamal Krishna*

Main category: cs.LG

TLDR: 简而言之，我们提出了一种基于多任务学习的深度神经网络MTCNet，用于人群密度和计数估计。


<details>
  <summary>Details</summary>
Motivation: 人群计数估计因非均匀尺度变化和任意视角而具有挑战性。

Method: 提出MTCNet，包括主要任务（密度估计，使用VGG-16和扩张CNN生成密度图）和辅助任务（计数组分类，共享前端并用CNN分类器）。

Result: 在ShanghaiTech数据集上MAE降低5.8%和14.9%，在UCF_CC_50数据集上降低10.5%，无需数据增强。

Conclusion: 模型在人群计数任务中表现出色，优于现有方法。

Abstract: We propose a Multi-Task Learning (MTL) paradigm based deep neural network
architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and
count estimation. Crowd count estimation is challenging due to the non-uniform
scale variations and the arbitrary perspective of an individual image. The
proposed model has two related tasks, with Crowd Density Estimation as the main
task and Crowd-Count Group Classification as the auxiliary task. The auxiliary
task helps in capturing the relevant scale-related information to improve the
performance of the main task. The main task model comprises two blocks: VGG-16
front-end for feature extraction and a dilated Convolutional Neural Network for
density map generation. The auxiliary task model shares the same front-end as
the main task, followed by a CNN classifier. Our proposed network achieves 5.8%
and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on
ShanghaiTech dataset without using any data augmentation. Our model also
outperforms with 10.5% lower MAE on UCF_CC_50 dataset.

</details>

### [77] [Collaborative Bayesian Optimization via Wasserstein Barycenters](https://arxiv.org/abs/2504.10770)
*Donglin Zhan,Haoting Zhang,Rhonda Righter,Zeyu Zheng,James Anderson*

Main category: cs.LG

TLDR: 本论文提出了一种协作贝叶斯优化框架，解决黑箱优化和数据隐私问题，通过共享高斯过程模型而非数据，使用Wasserstein重心进行协作，实验证明性能优异。


<details>
  <summary>Details</summary>
Motivation: 受黑箱优化和数据隐私需求增长的驱动，论文引入代理协作优化框架。

Method: 构建中心模型使用Wasserstein重心整合共享高斯过程模型，设计协作采集函数，并通过蒙特卡罗方法实现。

Result: 证明算法渐近一致性和数值准确性，实验显示优于其他协作框架，并与集中式方法竞争。

Conclusion: 该方法在数据隐私约束下实现了有效的协作优化。

Abstract: Motivated by the growing need for black-box optimization and data privacy, we
introduce a collaborative Bayesian optimization (BO) framework that addresses
both of these challenges. In this framework agents work collaboratively to
optimize a function they only have oracle access to. In order to mitigate
against communication and privacy constraints, agents are not allowed to share
their data but can share their Gaussian process (GP) surrogate models. To
enable collaboration under these constraints, we construct a central model to
approximate the objective function by leveraging the concept of Wasserstein
barycenters of GPs. This central model integrates the shared models without
accessing the underlying data. A key aspect of our approach is a collaborative
acquisition function that balances exploration and exploitation, allowing for
the optimization of decision variables collaboratively in each iteration. We
prove that our proposed algorithm is asymptotically consistent and that its
implementation via Monte Carlo methods is numerically accurate. Through
numerical experiments, we demonstrate that our approach outperforms other
baseline collaborative frameworks and is competitive with centralized
approaches that do not consider data privacy.

</details>

### [78] [AtlasD: Automatic Local Symmetry Discovery](https://arxiv.org/abs/2504.10777)
*Manu Bhat,Jonghyun Park,Jianke Yang,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TLDR: 本论文提出AtlasD方法，通过形式化局部对称性，发现并应用它来提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略局部邻域对称性，导致对称群误报，需要开发局部对称性发现机制。

Method: 训练局部预测器网络并学习李群基，实现函数的局部对称性（图集等变性）。

Result: 在顶夸克标记和偏微分方程实验中发现多连通分量局部对称群，并改善气候分割和视觉任务性能。

Conclusion: 局部对称性作为归纳偏差，能显著提升下游任务的性能。

Abstract: Existing symmetry discovery methods predominantly focus on global
transformations across the entire system or space, but they fail to consider
the symmetries in local neighborhoods. This may result in the reported symmetry
group being a misrepresentation of the true symmetry. In this paper, we
formalize the notion of local symmetry as atlas equivariance. Our proposed
pipeline, automatic local symmetry discovery (AtlasD), recovers the local
symmetries of a function by training local predictor networks and then learning
a Lie group basis to which the predictors are equivariant. We demonstrate
AtlasD is capable of discovering local symmetry groups with multiple connected
components in top-quark tagging and partial differential equation experiments.
The discovered local symmetry is shown to be a useful inductive bias that
improves the performance of downstream tasks in climate segmentation and vision
tasks.

</details>

### [79] [Power-scaled Bayesian Inference with Score-based Generative mModels](https://arxiv.org/abs/2504.10807)
*Huseyin Tuna Erdinc,Yunlin Zeng,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: cs.LG

TLDR: 本文提出一种基于得分的生成算法，用于从幂缩放的先验和似然中采样，实现贝叶斯推理中对后验分布的灵活控制，特别是针对地震速度模型。


<details>
  <summary>Details</summary>
Motivation: 为了在贝叶斯推理中灵活控制先验和似然的相对影响，而无需重新训练，并通过敏感性分析评估其对后验分布的影响。

Method: 使用基于得分的生成算法，从幂缩放的先验和似然中采样，专注于条件地震图像的地震速度模型合成。

Result: 实验显示，提高似然幂可提升后验样本对数据的保真度，降低先验幂可增加样本结构多样性，中等似然缩放可减少射线数据残差。

Conclusion: 该方法有助于后验分布的细化和敏感性分析，提高了贝叶斯推理的实用性。

Abstract: We propose a score-based generative algorithm for sampling from power-scaled
priors and likelihoods within the Bayesian inference framework. Our algorithm
enables flexible control over prior-likelihood influence without requiring
retraining for different power-scaling configurations. Specifically, we focus
on synthesizing seismic velocity models conditioned on imaged seismic. Our
method enables sensitivity analysis by sampling from intermediate power
posteriors, allowing us to assess the relative influence of the prior and
likelihood on samples of the posterior distribution. Through a comprehensive
set of experiments, we evaluate the effects of varying the power parameter in
different settings: applying it solely to the prior, to the likelihood of a
Bayesian formulation, and to both simultaneously. The results show that
increasing the power of the likelihood up to a certain threshold improves the
fidelity of posterior samples to the conditioning data (e.g., seismic images),
while decreasing the prior power promotes greater structural diversity among
samples. Moreover, we find that moderate scaling of the likelihood leads to a
reduced shot data residual, confirming its utility in posterior refinement.

</details>

### [80] [FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare](https://arxiv.org/abs/2504.10817)
*Penghao Wang,Qian Chen,Teng Zhang,Yingwei Zhang,Wang Lu,Yiqiang Chen*

Main category: cs.LG

TLDR: 本论文开发了医疗联邦学习基准FHBench和EPFL框架，以处理多模态医疗数据并提高联邦学习效率。


<details>
  <summary>Details</summary>
Motivation: 针对真实医疗数据集的多模态性和计算资源限制，现有的联邦学习方法面临挑战，需要新的基准和方法。

Method: 开发了FHBench基准，涵盖神经、心血管、呼吸系统和一般病理领域，并引入EPFL框架，使用Adaptive LoRA进行个性化联邦学习。

Result: EPFL在各种医疗模式中显示出优越的效率和效果，FHBench作为基准工具证明了其稳健性。

Conclusion: 这项工作解决了现有联邦学习方法的局限性，推进了医疗领域的联邦学习应用。

Abstract: Federated Learning (FL) has emerged as an effective solution for
multi-institutional collaborations without sharing patient data, offering a
range of methods tailored for diverse applications. However, real-world medical
datasets are often multimodal, and computational resources are limited, posing
significant challenges for existing FL approaches. Recognizing these
limitations, we developed the Federated Healthcare Benchmark(FHBench), a
benchmark specifically designed from datasets derived from real-world
healthcare applications. FHBench encompasses critical diagnostic tasks across
domains such as the nervous, cardiovascular, and respiratory systems and
general pathology, providing comprehensive support for multimodal healthcare
evaluations and filling a significant gap in existing benchmarks. Building on
FHBench, we introduced Efficient Personalized Federated Learning with Adaptive
LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency
and effectiveness across various healthcare modalities. Our results highlight
the robustness of FHBench as a benchmarking tool and the potential of EPFL as
an innovative approach to advancing healthcare-focused FL, addressing key
limitations of existing methods.

</details>

### [81] [Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations](https://arxiv.org/abs/2504.10833)
*Shubham Kumar,Dwip Dalal,Narendra Ahuja*

Main category: cs.LG

TLDR: 本论文提出Surrogate Faithfulness (SF)方法来评估和优化概念-based解释的忠实度，实验显示显著改善。


<details>
  <summary>Details</summary>
Motivation: 现有忠实度指标忽略概念空间分布，导致评估不准确，需要更全面的方法。

Method: 引入SF框架和Optimally Faithful (OF)解释，通过空间感知代理和新指标最大化忠实度。

Result: 实验结果显示，添加空间感知提高忠实度；OF解释错误率降低30%以上；OF在泛化和对抗样本上表现更佳。

Conclusion: 新方法有效提升了解释的忠实度、泛化和鲁棒性。

Abstract: Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a
promising tool for generating semantic explanations of the decision-making
processes in deep neural networks, having applications in both model
improvement and understanding. It is vital that the explanation is accurate, or
faithful, to the model, yet we identify several limitations of prior
faithfulness metrics that inhibit an accurate evaluation; most notably, prior
metrics involve only the set of concepts present, ignoring how they may be
spatially distributed. We address these limitations with Surrogate Faithfulness
(SF), an evaluation method that introduces a spatially-aware surrogate and two
novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)
explanations, where concepts are found that maximize faithfulness. Our
experiments show that (1) adding spatial-awareness to prior U-CBEMs increases
faithfulness in all cases; (2) OF produces significantly more faithful
explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's
learned concepts generalize well to out-of-domain data and are more robust to
adversarial examples, where prior U-CBEMs struggle.

</details>

### [82] [How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?](https://arxiv.org/abs/2504.10850)
*Meiqi Liu,Zhuoqun Huang,Yue Xing*

Main category: cs.LG

TLDR: 这篇论文提出了一种不需微调基础模型就提高下游任务鲁棒性的方法，使用鲁棒自编码器作为数据预处理。


<details>
  <summary>Details</summary>
Motivation: 对抗训练计算复杂度高，无法微调基础模型提升鲁棒性，需要无权访问模型权重的方法。

Method: 设计鲁棒自编码器作为数据预处理，基于鲁棒对比学习与对抗鲁棒性的理论联系。

Result: 实验显示方法有效，提高下游任务鲁棒性，并验证特征鲁棒性与下游鲁棒性的关联。

Conclusion: 方法简单有效，证实理论见解。

Abstract: With the rise of powerful foundation models, a pre-training-fine-tuning
paradigm becomes increasingly popular these days: A foundation model is
pre-trained using a huge amount of data from various sources, and then the
downstream users only need to fine-tune and adapt it to specific downstream
tasks. However, due to the high computation complexity of adversarial training,
it is not feasible to fine-tune the foundation model to improve its robustness
on the downstream task. Observing the above challenge, we want to improve the
downstream robustness without updating/accessing the weights in the foundation
model. Inspired from existing literature in robustness inheritance (Kim et al.,
2020), through theoretical investigation, we identify a close relationship
between robust contrastive learning with the adversarial robustness of
supervised learning. To further validate and utilize this theoretical insight,
we design a simple-yet-effective robust auto-encoder as a data pre-processing
method before feeding the data into the foundation model. The proposed approach
has zero access to the foundation model when training the robust auto-encoder.
Extensive experiments demonstrate the effectiveness of the proposed method in
improving the robustness of downstream tasks, verifying the connection between
the feature robustness (implied by small adversarial contrastive loss) and the
robustness of the downstream task.

</details>

### [83] [ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning](https://arxiv.org/abs/2504.10851)
*Ruochen Jin,Boning Tong,Shu Yang,Bojian Hou,Li Shen*

Main category: cs.LG

TLDR: 本论文提出ICAFS，一种考虑客户端间交互的垂直联邦学习特征选择方法，提高了模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法忽略客户端间特征交互，导致模型性能不佳。

Method: ICAFS使用条件特征合成和多个可学习选择器的集成方法，通过合成嵌入进行特征选择，避免私有梯度共享。

Result: 实验在真实数据集上显示，ICAFS在预测准确性上优于现有最先进方法。

Conclusion: ICAFS改进了垂直联邦学习中的特征选择，提升了模型性能并保护了数据隐私。

Abstract: Vertical federated learning (VFL) enables a paradigm for vertically
partitioned data across clients to collaboratively train machine learning
models. Feature selection (FS) plays a crucial role in Vertical Federated
Learning (VFL) due to the unique nature that data are distributed across
multiple clients. In VFL, different clients possess distinct subsets of
features for overlapping data samples, making the process of identifying and
selecting the most relevant features a complex yet essential task. Previous FS
efforts have primarily revolved around intra-client feature selection,
overlooking vital feature interaction across clients, leading to subpar model
outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for
effective FS in VFL by considering inter-client interactions. By employing
conditional feature synthesis alongside multiple learnable feature selectors,
ICAFS facilitates ensemble FS over these selectors using synthetic embeddings.
This method bypasses the limitations of private gradient sharing and allows for
model training using real data with refined embeddings. Experiments on multiple
real-world datasets demonstrate that ICAFS surpasses current state-of-the-art
methods in prediction accuracy.

</details>

### [84] [Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization](https://arxiv.org/abs/2504.10900)
*Peiliang Gong,Emadeldeen Eldele,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TLDR: 提出ProtoNorm方法，通过自适应归一化提升时间序列基础模型性能，解决预训练分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练中数据分布不匹配的挑战，尤其在时间序列数据中更显著。

Method: 在Transformer架构中，替换LayerNorm为原型引导动态归一化（ProtoNorm），使用学习原型和样本亲和度进行归一化。

Result: 在分类和预测任务中显著优于传统方法，缓解分布偏移影响。

Conclusion: 推进更通用、鲁棒的时间序列基础模型发展，易于实现。

Abstract: Foundation models have achieved remarkable success across diverse
machine-learning domains through large-scale pretraining on large, diverse
datasets. However, pretraining on such datasets introduces significant
challenges due to substantial mismatches in data distributions, a problem
particularly pronounced with time series data. In this paper, we tackle this
issue by proposing a domain-aware adaptive normalization strategy within the
Transformer architecture. Specifically, we replace the traditional LayerNorm
with a prototype-guided dynamic normalization mechanism (ProtoNorm), where
learned prototypes encapsulate distinct data distributions, and
sample-to-prototype affinity determines the appropriate normalization layer.
This mechanism effectively captures the heterogeneity of time series
characteristics, aligning pretrained representations with downstream tasks.
Through comprehensive empirical evaluation, we demonstrate that our method
significantly outperforms conventional pretraining techniques across both
classification and forecasting tasks, while effectively mitigating the adverse
effects of distribution shifts during pretraining. Incorporating ProtoNorm is
as simple as replacing a single line of code. Extensive experiments on diverse
real-world time series benchmarks validate the robustness and generalizability
of our approach, advancing the development of more versatile time series
foundation models.

</details>

### [85] [Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs](https://arxiv.org/abs/2504.10902)
*Rui Dai,Sile Hu,Xu Shen,Yonggang Zhang,Xinmei Tian,Jieping Ye*

Main category: cs.LG

TLDR: 本文提出基于子模块线性性的模型合并策略，提升任务算术的多任务能力，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局线性化，但子模块已具有更高线性性，因此需开发新策略。

Method: 提出独立合并子模块的方法，并基于其线性特性推导最优合并权重的闭式解。

Result: 实验结果显示，该方法在不同模型规模和任务上 consistently 优于标准任务算术和其他基线。

Conclusion: 强调利用子模块线性性的优势，为多任务模型合并提供新视角。

Abstract: Task arithmetic is a straightforward yet highly effective strategy for model
merging, enabling the resultant model to exhibit multi-task capabilities.
Recent research indicates that models demonstrating linearity enhance the
performance of task arithmetic. In contrast to existing methods that rely on
the global linearization of the model, we argue that this linearity already
exists within the model's submodules. In particular, we present a statistical
analysis and show that submodules (e.g., layers, self-attentions, and MLPs)
exhibit significantly higher linearity than the overall model. Based on these
findings, we propose an innovative model merging strategy that independently
merges these submodules. Especially, we derive a closed-form solution for
optimal merging weights grounded in the linear properties of these submodules.
Experimental results demonstrate that our method consistently outperforms the
standard task arithmetic approach and other established baselines across
different model scales and various tasks. This result highlights the benefits
of leveraging the linearity of submodules and provides a new perspective for
exploring solutions for effective and practical multi-task model merging.

</details>

### [86] [Towards A Universal Graph Structural Encoder](https://arxiv.org/abs/2504.10917)
*Jialin Chen,Haolan Zuo,Haoyu Peter Wang,Siqi Miao,Pan Li,Rex Ying*

Main category: cs.LG

TLDR: GFSE 是一个通用的图结构编码器，通过自监督学习捕捉跨域结构模式，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉和转移图结构信息，特别是拓扑模式差异和嵌入空间探索不足。

Method: 提出 GFSE，使用基于 Graph Transformer 的注意力机制，多目标自监督预训练，编码多层次拓扑特征。

Result: 实验显示 GFSE 显著提升性能，在 81.6% 的情况下达到最先进水平，减少细调需求。

Conclusion: GFSE 作为强大通用的图结构编码器，有潜力应用于各种图数据。

Abstract: Recent advancements in large-scale pre-training have shown the potential to
learn generalizable representations for downstream tasks. In the graph domain,
however, capturing and transferring structural information across different
graph domains remains challenging, primarily due to the inherent differences in
topological patterns across various contexts. Additionally, most existing
models struggle to capture the complexity of rich graph structures, leading to
inadequate exploration of the embedding space. To address these challenges, we
propose GFSE, a universal graph structural encoder designed to capture
transferable structural patterns across diverse domains such as molecular
graphs, social networks, and citation networks. GFSE is the first cross-domain
graph structural encoder pre-trained with multiple self-supervised learning
objectives. Built on a Graph Transformer, GFSE incorporates attention
mechanisms informed by graph inductive bias, enabling it to encode intricate
multi-level and fine-grained topological features. The pre-trained GFSE
produces generic and theoretically expressive positional and structural
encoding for graphs, which can be seamlessly integrated with various downstream
graph feature encoders, including graph neural networks for vectorized features
and Large Language Models for text-attributed graphs. Comprehensive experiments
on synthetic and real-world datasets demonstrate GFSE's capability to
significantly enhance the model's performance while requiring substantially
less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art
performance in 81.6% evaluated cases, spanning diverse graph models and
datasets, highlighting its potential as a powerful and versatile encoder for
graph-structured data.

</details>

### [87] [Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting](https://arxiv.org/abs/2504.10923)
*Mingyi Zhu,Zhaoxin Li,Qiao Lin,Li Ding*

Main category: cs.LG

TLDR: 本论文提出了一种高效轻量级的中期风力发电预测模型Fast-Powerformer，以提高预测准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决风力发电预测中气象因素随机性和输出波动带来的准确性和效率双重挑战。

Method: 基于Reformer架构，加入轻量LSTM嵌入模块、输入转置机制和频率增强通道注意力机制(FECAM)。

Result: 实验结果显示，模型在真实数据集上实现更高预测准确性、操作效率、快速推理速度和低内存消耗。

Conclusion: 模型具有显著的实际部署价值。

Abstract: Wind power forecasting (WPF), as a significant research topic within
renewable energy, plays a crucial role in enhancing the security, stability,
and economic operation of power grids. However, due to the high stochasticity
of meteorological factors (e.g., wind speed) and significant fluctuations in
wind power output, mid-term wind power forecasting faces a dual challenge of
maintaining high accuracy and computational efficiency. To address these
issues, this paper proposes an efficient and lightweight mid-term wind power
forecasting model, termed Fast-Powerformer. The proposed model is built upon
the Reformer architecture, incorporating structural enhancements such as a
lightweight Long Short-Term Memory (LSTM) embedding module, an input
transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism
(FECAM). These improvements enable the model to strengthen temporal feature
extraction, optimize dependency modeling across variables, significantly reduce
computational complexity, and enhance sensitivity to periodic patterns and
dominant frequency components. Experimental results conducted on multiple
real-world wind farm datasets demonstrate that the proposed Fast-Powerformer
achieves superior prediction accuracy and operational efficiency compared to
mainstream forecasting approaches. Furthermore, the model exhibits fast
inference speed and low memory consumption, highlighting its considerable
practical value for real-world deployment scenarios.

</details>

### [88] [Transfer Learning for Temporal Link Prediction](https://arxiv.org/abs/2504.10925)
*Ayan Chatterjee,Barbara Ikica,Babak Ravandi,John Palowitch*

Main category: cs.LG

TLDR: 本论文研究时序链接预测的转移学习任务，通过添加结构映射模块使模型可转移。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法应用于新图，因为记忆模块只存储训练时的节点信息。

Method: 在时序链接预测模型中添加结构映射模块，学习从图拓扑特征到记忆嵌入的映射。

Result: 开发了可转移的模型，并为无记忆基础模型奠定基础。

Conclusion: 这项工作推动了时序链接预测向记忆无关的通用模型发展。

Abstract: Link prediction on graphs has applications spanning from recommender systems
to drug discovery. Temporal link prediction (TLP) refers to predicting future
links in a temporally evolving graph and adds additional complexity related to
the dynamic nature of graphs. State-of-the-art TLP models incorporate memory
modules alongside graph neural networks to learn both the temporal mechanisms
of incoming nodes and the evolving graph topology. However, memory modules only
store information about nodes seen at train time, and hence such models cannot
be directly transferred to entirely new graphs at test time and deployment. In
this work, we study a new transfer learning task for temporal link prediction,
and develop transfer-effective methods for memory-laden models. Specifically,
motivated by work showing the informativeness of structural signals for the TLP
task, we augment a structural mapping module to the existing TLP model
architectures, which learns a mapping from graph structural (topological)
features to memory embeddings. Our work paves the way for a memory-free
foundation model for TLP.

</details>

### [89] [Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions](https://arxiv.org/abs/2504.10932)
*Bo Wang,Lizuo Liu,Wei Cai*

Main category: cs.LG

TLDR: 本论文提出多尺度DeepOnet（Mscale-DeepOnet），以减少DeepOnet在高频映射学习中的谱偏差，并应用于Helmholtz方程的系数与解映射。数值结果显示其在高频波散射问题上的显著改善。


<details>
  <summary>Details</summary>
Motivation: 减少DeepOnet在学习高频振荡函数映射时的谱偏差，以更好地处理高频成分。

Method: 在DeepOnet的branch和trunk网络中引入多尺度神经网络。

Result: Mscale-DeepOnet在高频波散射问题上比普通DeepOnet有显著改善，参数数量相似。

Conclusion: Mscale-DeepOnet能捕获高频成分，提高高频映射的学习能力。

Abstract: In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce
the spectral bias of the DeepOnet in learning high-frequency mapping between
highly oscillatory functions, with an application to the nonlinear mapping
between the coefficient of the Helmholtz equation and its solution. The
Mscale-DeepOnet introduces the multiscale neural network in the branch and
trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown
to be able to capture various high-frequency components of the mapping itself
and its image. Numerical results demonstrate the substantial improvement of the
Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime
over the normal DeepOnet with a similar number of network parameters.

</details>

### [90] [Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs](https://arxiv.org/abs/2504.10936)
*Yuni Susanti,Michael Färber*

Main category: cs.LG

TLDR: 这篇论文探讨了使用大型语言模型（LLM）结合观测数据进行因果发现，通过实验证明了其有效性，F1分数显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现依赖统计方法和大样本数据，而LLM提供了领域专家知识，但其处理观测数据的能力尚不明确。

Method: 本研究使用配对提示和BFS-based提示策略，将观测数据直接融入LLM提示中，以推断因果关系。

Result: 实验结果显示，融入观测数据后，F1分数最高提升0.11，并比传统统计方法高出最高0.52。

Conclusion: 研究突出了LLM在因果发现中的潜力和局限性，为未来LLM驱动的因果发现奠定了基础。

Abstract: Causal discovery traditionally relies on statistical methods applied to
observational data, often requiring large datasets and assumptions about
underlying causal structures. Recent advancements in Large Language Models
(LLMs) have introduced new possibilities for causal discovery by providing
domain expert knowledge. However, it remains unclear whether LLMs can
effectively process observational data for causal discovery. In this work, we
explore the potential of LLMs for data-driven causal discovery by integrating
observational data for LLM-based reasoning. Specifically, we examine whether
LLMs can effectively utilize observational data through two prompting
strategies: pairwise prompting and breadth first search (BFS)-based prompting.
In both approaches, we incorporate the observational data directly into the
prompt to assess LLMs' ability to infer causal relationships from such data.
Experiments on benchmark datasets show that incorporating observational data
enhances causal discovery, boosting F1 scores by up to 0.11 point using both
pairwise and BFS LLM-based prompting, while outperforming traditional
statistical causal discovery baseline by up to 0.52 points. Our findings
highlight the potential and limitations of LLMs for data-driven causal
discovery, demonstrating their ability to move beyond textual metadata and
effectively interpret and utilize observational data for more informed causal
reasoning. Our studies lays the groundwork for future advancements toward fully
LLM-driven causal discovery.

</details>

### [91] [When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers](https://arxiv.org/abs/2504.10957)
*Hongkang Li,Yihua Zhang,Shuai Zhang,Meng Wang,Sijia Liu,Pin-Yu Chen*

Main category: cs.LG

TLDR: 任务算术通过添加任务向量加权和编辑预训练模型，本文首次为非线性Transformer提供泛化理论保证，并验证实际应用。


<details>
  <summary>Details</summary>
Motivation: 任务算术在模型编辑中高效，但理论理解有限，本文旨在解决非凸优化问题，提供泛化保证的理论表征。

Method: 采用概念学习设置，二元分类任务，证明任务添加、否定和系数选择的有效性，支持稠密和低秩参数近似。

Result: 证明任务算术在多任务学习、遗忘和泛化方面的成功，并通过Phi-1.5模型验证机器遗忘任务。

Conclusion: 本文建立了任务算术的理论基础，并通过实验确认其实际有效性，推进了模型编辑领域的理解。

Abstract: Task arithmetic refers to editing the pre-trained model by adding a weighted
sum of task vectors, each of which is the weight update from the pre-trained
model to fine-tuned models for certain tasks. This approach recently gained
attention as a computationally efficient inference method for model editing,
e.g., multi-task learning, forgetting, and out-of-domain generalization
capabilities. However, the theoretical understanding of why task vectors can
execute various conceptual operations remains limited, due to the highly
non-convexity of training Transformer-based models. To the best of our
knowledge, this paper provides the first theoretical characterization of the
generalization guarantees of task vector methods on nonlinear Transformers. We
consider a conceptual learning setting, where each task is a binary
classification problem based on a discriminative pattern. We theoretically
prove the effectiveness of task addition in simultaneously learning a set of
irrelevant or aligned tasks, as well as the success of task negation in
unlearning one task from irrelevant or contradictory tasks. Moreover, we prove
the proper selection of linear coefficients for task arithmetic to achieve
guaranteed generalization to out-of-domain tasks. All of our theoretical
results hold for both dense-weight parameters and their low-rank
approximations. Although established in a conceptual setting, our theoretical
findings were validated on a practical machine unlearning task using the large
language model Phi-1.5 (1.3B).

</details>

### [92] [Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits](https://arxiv.org/abs/2504.10959)
*Xiaoyang He,Xiaoxia Huang*

Main category: cs.LG

TLDR: 本论文提出DK-UCB算法，使用过去上下文估计mmWave信道瞬时传输率，减少频繁信道估计成本。


<details>
  <summary>Details</summary>
Motivation: 车辆需要及时信道条件选择基站通信，但频繁估计快速衰落mmWave信道成本高。

Method: 提出DK-UCB算法，利用过去上下文和传输率映射到RKHS，使用新型核函数改进准确性，并鼓励信息共享加速学习。

Result: 提高了估计准确性，加速学习过程，同时保持可负担通信成本。

Conclusion: DK-UCB算法有效减少信道估计成本并提升性能。

Abstract: Vehicles require timely channel conditions to determine the base station (BS)
to communicate with, but it is costly to estimate the fast-fading mmWave
channels frequently. Without additional channel estimations, the proposed
Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the
current instantaneous transmission rates utilizing past contexts, such as the
vehicle's location and velocity, along with past instantaneous transmission
rates. To capture the nonlinear mapping from a context to the instantaneous
transmission rate, DK-UCB maps a context into the reproducing kernel Hilbert
space (RKHS) where a linear mapping becomes observable. To improve estimation
accuracy, we propose a novel kernel function in RKHS which incorporates the
propagation characteristics of the mmWave signals. Moreover, DK-UCB encourages
a vehicle to share necessary information when it has conducted significant
explorations, which speeds up the learning process while maintaining affordable
communication costs.

</details>

### [93] [ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings](https://arxiv.org/abs/2504.10983)
*Zitai Kong,Yiheng Zhu,Yinlong Xu,Hanjing Zhou,Mingzhe Yin,Jialu Wu,Hongxia Xu,Chang-Yu Hsieh,Tingjun Hou,Jian Wu*

Main category: cs.LG

TLDR: ProtFlow 是一种基于流匹配的蛋白质序列设计框架，提高了生成效率和性能，在多种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成方法存在低效率、大模型空间和高训练成本等问题，需要一种更高效的框架来加速蛋白质序列设计。

Method: 引入 ProtFlow，利用蛋白质语言模型的潜在空间进行流匹配，压缩平滑空间，并通过 reflow 技术实现单步生成和多链蛋白联合设计。

Result: ProtFlow 在一般肽、多链蛋白、抗菌肽和抗体等任务中优于特定方法，展示了更好的性能。

Conclusion: ProtFlow 具有广阔的应用潜力，可提升计算蛋白质序列设计和分析的效率和适用性。

Abstract: The design of protein sequences with desired functionalities is a fundamental
task in protein engineering. Deep generative methods, such as autoregressive
models and diffusion models, have greatly accelerated the discovery of novel
protein sequences. However, these methods mainly focus on local or shallow
residual semantics and suffer from low inference efficiency, large modeling
space and high training cost. To address these challenges, we introduce
ProtFlow, a fast flow matching-based protein sequence design framework that
operates on embeddings derived from semantically meaningful latent space of
protein language models. By compressing and smoothing the latent space,
ProtFlow enhances performance while training on limited computational
resources. Leveraging reflow techniques, ProtFlow enables high-quality
single-step sequence generation. Additionally, we develop a joint design
pipeline for the design scene of multichain proteins. We evaluate ProtFlow
across diverse protein design tasks, including general peptides and long-chain
proteins, antimicrobial peptides, and antibodies. Experimental results
demonstrate that ProtFlow outperforms task-specific methods in these
applications, underscoring its potential and broad applicability in
computational protein sequence design and analysis.

</details>

### [94] [Leveraging Vertical Public-Private Split for Improved Synthetic Data Generation](https://arxiv.org/abs/2504.10987)
*Samuel Maddock,Shripad Gade,Graham Cormode,Will Bullock*

Main category: cs.LG

TLDR: 本论文提出一种框架，将水平公共辅助方法适应到垂直公共-私有分区，以提升差分隐私合成数据生成的质量，并与条件生成方法比较，突出了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅处理水平分区（使用少量公共行），但现实数据集常有垂直分区（公共和私有属性），需要更实用方法来提高合成数据质量。

Method: 提出框架适应垂直设置，并使用条件生成作为备选方法进行比较。

Result: 发现公共数据辅助方法的初始限制，并建议未来研究方向以解决这些挑战。

Conclusion: 强调需要进一步研究来改进方法，解决垂直分区中的问题，提高合成数据质量。

Abstract: Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of
private and secure tabular-data sharing, producing artificial data that carries
through the underlying statistical properties of the input data. This typically
involves adding carefully calibrated statistical noise to guarantee individual
privacy, at the cost of synthetic data quality. Recent literature has explored
scenarios where a small amount of public data is used to help enhance the
quality of synthetic data. These methods study a horizontal public-private
partitioning which assumes access to a small number of public rows that can be
used for model initialization, providing a small utility gain. However,
realistic datasets often naturally consist of public and private attributes,
making a vertical public-private partitioning relevant for practical synthetic
data deployments. We propose a novel framework that adapts horizontal
public-assisted methods into the vertical setting. We compare this framework
against our alternative approach that uses conditional generation, highlighting
initial limitations of public-data assisted methods and proposing future
research directions to address these challenges.

</details>

### [95] [Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset](https://arxiv.org/abs/2504.11022)
*Joana Reuss,Jan Macdonald,Simon Becker,Konrad Schultka,Lorenz Richter,Marco Körner*

Main category: cs.LG

TLDR: 这项研究对转移学习和元学习算法在真实世界作物类型分类中的性能进行了基准测试，发现元学习算法准确性略高但计算成本增加，并突出了跨区域知识转移的挑战。


<details>
  <summary>Details</summary>
Motivation: 空间数据不平衡导致远程 sensing 分类困难，需要评估转移和元学习算法在真实场景中的性能。

Method: 在EuroCropsML数据集上基准测试转移学习和元学习算法（如FO-MAML、ANIL、TIML），使用爱沙尼亚、拉脱维亚和葡萄牙的Sentinel-2卫星数据。

Result: MAML-based算法在爱沙尼亚任务上预训练后准确性略高，但计算需求增加；爱沙尼亚和葡萄牙间知识转移对所有算法都具挑战。

Conclusion: 强调准确性和资源权衡、区域知识转移难度，并提供基准促进未来研究，代码公开在GitHub。

Abstract: Spatial imbalances in crop type data pose significant challenges for accurate
classification in remote sensing applications. Algorithms aiming at
transferring knowledge from data-rich to data-scarce tasks have thus surged in
popularity. However, despite their effectiveness in previous evaluations, their
performance in challenging real-world applications is unclear and needs to be
evaluated. This study benchmarks transfer learning and several meta-learning
algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),
Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the
real-world EuroCropsML time series dataset, which combines farmer-reported crop
data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.
Our findings indicate that MAML-based meta-learning algorithms achieve slightly
higher accuracy compared to simpler transfer learning methods when applied to
crop type classification tasks in Estonia after pre-training on data from
Latvia. However, this improvement comes at the cost of increased computational
demands and training time. Moreover, we find that the transfer of knowledge
between geographically disparate regions, such as Estonia and Portugal, poses
significant challenges to all investigated algorithms. These insights
underscore the trade-offs between accuracy and computational resource
requirements in selecting machine learning methods for real-world crop type
classification tasks and highlight the difficulties of transferring knowledge
between different regions of the Earth. To facilitate future research in this
domain, we present the first comprehensive benchmark for evaluating transfer
and meta-learning methods for crop type classification under real-world
conditions. The corresponding code is publicly available at
https://github.com/dida-do/eurocrops-meta-learning.

</details>

### [96] [A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications](https://arxiv.org/abs/2504.11026)
*Alexandru Vasilache,Jona Scholz,Vincent Schilling,Sven Nitzsche,Florian Kaelber,Johannes Korsch,Juergen Becker*

Main category: cs.LG

TLDR: 这篇论文介绍了一个开源的PyTorch兼容框架，用于SNN脉冲编码，并评估多种方法，发现SF效率最高。


<details>
  <summary>Details</summary>
Motivation: SNN处理稀疏脉冲时能量效率高，但与传统数据集不兼容，需要高效编码方法。

Method: 开发支持LIF、SF、PWM、BSA等算法的框架，并用C/C++在嵌入式硬件上评估性能。

Result: SF重建错误最低，能量效率和速度最高，脉冲稀疏性第二好；其他方法在特定场景有优势。

Conclusion: 框架和分析为选择最佳编码策略以实现SNN能量高效应用提供资源。

Abstract: Spiking Neural Networks (SNNs) offer promising energy efficiency advantages,
particularly when processing sparse spike trains. However, their
incompatibility with traditional datasets, which consist of batches of input
vectors rather than spike trains, necessitates the development of efficient
encoding methods. This paper introduces a novel, open-source PyTorch-compatible
Python framework for spike encoding, designed for neuromorphic applications in
machine learning and reinforcement learning. The framework supports a range of
encoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward
(SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well
as specialized encoding strategies covering population coding and reinforcement
learning scenarios. Furthermore, we investigate the performance trade-offs of
each method on embedded hardware using C/C++ implementations, considering
energy consumption, computation time, spike sparsity, and reconstruction
accuracy. Our findings indicate that SF typically achieves the lowest
reconstruction error and offers the highest energy efficiency and fastest
encoding speed, achieving the second-best spike sparsity. At the same time,
other methods demonstrate particular strengths depending on the signal
characteristics. This framework and the accompanying empirical analysis provide
valuable resources for selecting optimal encoding strategies for
energy-efficient SNN applications.

</details>

### [97] [Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models](https://arxiv.org/abs/2504.11054)
*Andrea Tirinzoni,Ahmed Touati,Jesse Farebrother,Mateusz Guzek,Anssi Kanervisto,Yingchen Xu,Alessandro Lazaric,Matteo Pirotta*

Main category: cs.LG

TLDR: 本论文提出一种新颖的无监督强化学习方法，通过模仿无标签行为数据集来预训练代理，实现下游任务的零样本泛化，并在人形机器人控制中验证。


<details>
  <summary>Details</summary>
Motivation: 现有无监督RL方法存在局限性，如需针对下游任务运行RL、依赖特定数据集或预训练策略与任务相关性差，因此需开发更有效的预训练方法。

Method: 引入Forward-Backward Representations with Conditional-Policy Regularization算法，通过训练前向-后向表示嵌入轨迹，并使用条件鉴别器鼓励策略覆盖无标签数据集中的状态。

Result: 训练出Meta Motivo模型，能处理人形机器人任务如运动跟踪、目标到达和奖励优化，表现出类人行为，并优于现有无监督RL和模型基线。

Conclusion: 该方法提升了策略与行为的对齐性，同时保留泛化能力，在复杂环境中实现高效性能。

Abstract: Unsupervised reinforcement learning (RL) aims at pre-training agents that can
solve a wide range of downstream tasks in complex environments. Despite recent
advancements, existing approaches suffer from several limitations: they may
require running an RL process on each downstream task to achieve a satisfactory
performance, they may need access to datasets with good coverage or
well-curated task-specific samples, or they may pre-train policies with
unsupervised losses that are poorly correlated with the downstream tasks of
interest. In this paper, we introduce a novel algorithm regularizing
unsupervised RL towards imitating trajectories from unlabeled behavior
datasets. The key technical novelty of our method, called Forward-Backward
Representations with Conditional-Policy Regularization, is to train
forward-backward representations to embed the unlabeled trajectories to the
same latent space used to represent states, rewards, and policies, and use a
latent-conditional discriminator to encourage policies to ``cover'' the states
in the unlabeled behavior dataset. As a result, we can learn policies that are
well aligned with the behaviors in the dataset, while retaining zero-shot
generalization capabilities for reward-based and imitation tasks. We
demonstrate the effectiveness of this new approach in a challenging humanoid
control problem: leveraging observation-only motion capture datasets, we train
Meta Motivo, the first humanoid behavioral foundation model that can be
prompted to solve a variety of whole-body tasks, including motion tracking,
goal reaching, and reward optimization. The resulting model is capable of
expressing human-like behaviors and it achieves competitive performance with
task-specific methods while outperforming state-of-the-art unsupervised RL and
model-based baselines.

</details>

### [98] [Dynamical errors in machine learning forecasts](https://arxiv.org/abs/2504.11074)
*Zhou Fang,Gianmarco Mengaldo*

Main category: cs.LG

TLDR: 本论文探讨机器学习预测中标准错误指标（如MAE和MSE）的局限性，提出基于动态指标（如瞬时维度d和逆持久性θ）的度量方法，以评估预测的动态一致性，并通过实验揭示ML预测的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 动机是解决机器学习预测是否保留底层系统的动态行为问题，这在科学和工程应用中至关重要，以评估模型的保真度和识别失败模式。

Method: 方法包括使用动态指标d和θ，提出基于这些指标的错误度量，并分析直接和递归预测策略在Lorenz、Kuramoto-Sivashinsky方程、Kolmogorov流动和天气预测数据集上的表现。

Result: 结果显示，较大的预测错误与更高的动态复杂性和更低的持久性相关，ML预测在长预测时段内存在显著的动态属性扭曲。

Conclusion: 结论是，这些动态指标-based度量提供了互补信息，可用于改进ML模型的预测保真度。

Abstract: In machine learning forecasting, standard error metrics such as mean absolute
error (MAE) and mean squared error (MSE) quantify discrepancies between
predictions and target values. However, these metrics do not directly evaluate
the physical and/or dynamical consistency of forecasts, an increasingly
critical concern in scientific and engineering applications.
  Indeed, a fundamental yet often overlooked question is whether machine
learning forecasts preserve the dynamical behavior of the underlying system.
Addressing this issue is essential for assessing the fidelity of machine
learning models and identifying potential failure modes, particularly in
applications where maintaining correct dynamical behavior is crucial.
  In this work, we investigate the relationship between standard forecasting
error metrics, such as MAE and MSE, and the dynamical properties of the
underlying system. To achieve this goal, we use two recently developed
dynamical indices: the instantaneous dimension ($d$), and the inverse
persistence ($\theta$). Our results indicate that larger forecast errors --
e.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity)
and higher $\theta$ (lower persistence). To further assess dynamical
consistency, we propose error metrics based on the dynamical indices that
measure the discrepancy of the forecasted $d$ and $\theta$ versus their correct
values. Leveraging these dynamical indices-based metrics, we analyze direct and
recursive forecasting strategies for three canonical datasets -- Lorenz,
Kuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world
weather forecasting task. Our findings reveal substantial distortions in
dynamical properties in ML forecasts, especially for long forecast lead times
or long recursive simulations, providing complementary information on ML
forecast fidelity that can be used to improve ML models.

</details>

### [99] [InfoClus: Informative Clustering of High-dimensional Data Embeddings](https://arxiv.org/abs/2504.11089)
*Fuyin Lai,Edith Heiter,Guillaume Bied,Jefrey Lijffijt*

Main category: cs.LG

TLDR: 本篇论文引入InfoClus方法，用于对高维数据的低维嵌入进行分区和解释，以提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维数据的低维嵌入往往难以解释，因此需要一种分区和解释的方法。

Method: 提出基于信息论的目标函数，通过InfoClus在层次聚类上使用贪婪搜索联合优化分区和解释。

Result: 在三个数据集上的定性和定量分析显示，InfoClus优于RVX和VERA方法，并能提供良好的分析起点。

Conclusion: InfoClus能有效促进嵌入的可探索性和可解释性，并具有显著优势。

Abstract: Developing an understanding of high-dimensional data can be facilitated by
visualizing that data using dimensionality reduction. However, the
low-dimensional embeddings are often difficult to interpret. To facilitate the
exploration and interpretation of low-dimensional embeddings, we introduce a
new concept named partitioning with explanations. The idea is to partition the
data shown through the embedding into groups, each of which is given a sparse
explanation using the original high-dimensional attributes. We introduce an
objective function that quantifies how much we can learn through observing the
explanations of the data partitioning, using information theory, and also how
complex the explanations are. Through parameterization of the complexity, we
can tune the solutions towards the desired granularity. We propose InfoClus,
which optimizes the partitioning and explanations jointly, through greedy
search constrained over a hierarchical clustering. We conduct a qualitative and
quantitative analysis of InfoClus on three data sets. We contrast the results
on the Cytometry data with published manual analysis results, and compare with
two other recent methods for explaining embeddings (RVX and VERA). These
comparisons highlight that InfoClus has distinct advantages over existing
procedures and methods. We find that InfoClus can automatically create good
starting points for the analysis of dimensionality-reduction-based scatter
plots.

</details>

### [100] [Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay](https://arxiv.org/abs/2504.11118)
*Henrik Krauss,Takehisa Yairi*

Main category: cs.LG

TLDR: 这项研究提出一种新方法，仅用游戏数据通过强化学习技术揭示人类隐蔽注意力模式。


<details>
  <summary>Details</summary>
Motivation: 为了仅用游戏数据理解人类注意力模式，与代理注意力差异，并开发增强型强化学习代理。

Method: 提出CTR注意力网络，从Atari游戏数据生成注意力地图，并与基于眼动追踪的TIOA模型比较。

Result: 人类CTR地图关注附近对象，与TIOA更接近；代理地图则广泛而一致，定量分析证实差异。

Conclusion: CTR网络有效揭示人类隐蔽注意力，无需额外数据，有助于理解人类-代理注意力差异和RL代理开发。

Abstract: This study introduces a novel method for revealing human covert attention
patterns using gameplay data alone, utilizing offline attention techniques from
reinforcement learning (RL). We propose the contextualized, task-relevant (CTR)
attention network, which generates attention maps from both human and RL agent
gameplay in Atari environments. These maps are sparse yet retain the necessary
information for the current player's decision making. We compare the
CTR-derived attention maps with a temporally integrated overt attention (TIOA)
model based on eye-tracking data, serving as a point of comparison and
discussion. Visual inspection reveals distinct attention patterns: human CTR
maps focus on the player and rather nearby opponents, occasionally shifting
between stronger focus and broader views - sometimes even attending to empty
space ahead. In contrast, agent maps maintain a consistent broad focus on most
objects, including distant ones and the player. Quantitative analysis further
demonstrates that human CTR maps align more closely with TIOA than agent maps
do. Our findings indicate that the CTR attention network can effectively reveal
human covert attention patterns from gameplay alone, without the need for
additional data like brain activity recordings. This work contributes to
understanding human-agent attention differences and enables the development of
RL agents augmented with human covert attention.

</details>

### [101] [Divergence of Empirical Neural Tangent Kernel in Classification Problems](https://arxiv.org/abs/2504.11130)
*Zixiong Yu,Songtao Tian,Guhan Chen*

Main category: cs.LG

TLDR: 本论文证明，FCN和ResNet在分类问题中不能被基于NTK的核逻辑回归近似，因为经验NTK在过度训练时发散。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络在分类问题中的行为，强调与回归问题的差异，揭示NTK理论的局限性。

Method: 首先证明NTK的严格正定性，然后证明参数发散，并通过反证法和实验验证经验NTK不收敛。

Result: 经验NTK在训练时发散，不均匀收敛于NTK；通过合成数据和MNIST实验得到验证。

Conclusion: NTK理论不适用于分类问题，对神经网络理论有重要启示。

Abstract: This paper demonstrates that in classification problems, fully connected
neural networks (FCNs) and residual neural networks (ResNets) cannot be
approximated by kernel logistic regression based on the Neural Tangent Kernel
(NTK) under overtraining (i.e., when training time approaches infinity).
Specifically, when using the cross-entropy loss, regardless of how large the
network width is (as long as it is finite), the empirical NTK diverges from the
NTK on the training samples as training time increases. To establish this
result, we first demonstrate the strictly positive definiteness of the NTKs for
multi-layer FCNs and ResNets. Then, we prove that during training, % with the
cross-entropy loss, the neural network parameters diverge if the smallest
eigenvalue of the empirical NTK matrix (Gram matrix) with respect to training
samples is bounded below by a positive constant. This behavior contrasts
sharply with the lazy training regime commonly observed in regression problems.
Consequently, using a proof by contradiction, we show that the empirical NTK
does not uniformly converge to the NTK across all times on the training samples
as the network width increases. We validate our theoretical results through
experiments on both synthetic data and the MNIST classification task. This
finding implies that NTK theory is not applicable in this context, with
significant theoretical implications for understanding neural networks in
classification problems.

</details>

### [102] [R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](https://arxiv.org/abs/2504.11195)
*Lijun Sheng,Jian Liang,Zilei Wang,Ran He*

Main category: cs.LG

TLDR: 本文提出R-TPT方法，在推理阶段防御视觉语言模型对抗性攻击，无需标记数据，提供灵活性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型易受对抗性攻击，且现有防御方法依赖标记数据和缺乏灵活性。

Method: 重新制定边际熵目标，只保留点熵最小化，并引入基于可靠性的加权集成策略。

Result: 在各种基准和攻击上的实验证明R-TPT的有效性。

Conclusion: R-TPT增强防御能力，无需标记数据，并提供高灵活性。

Abstract: Vision-language models (VLMs), such as CLIP, have gained significant
popularity as foundation models, with numerous fine-tuning methods developed to
enhance performance on downstream tasks. However, due to their inherent
vulnerability and the common practice of selecting from a limited set of
open-source models, VLMs suffer from a higher risk of adversarial attacks than
traditional vision models. Existing defense techniques typically rely on
adversarial fine-tuning during training, which requires labeled data and lacks
of flexibility for downstream tasks. To address these limitations, we propose
robust test-time prompt tuning (R-TPT), which mitigates the impact of
adversarial attacks during the inference stage. We first reformulate the
classic marginal entropy objective by eliminating the term that introduces
conflicts under adversarial conditions, retaining only the pointwise entropy
minimization. Furthermore, we introduce a plug-and-play reliability-based
weighted ensembling strategy, which aggregates useful information from reliable
augmented views to strengthen the defense. R-TPT enhances defense against
adversarial attacks without requiring labeled training data while offering high
flexibility for inference tasks. Extensive experiments on widely used
benchmarks with various attacks demonstrate the effectiveness of R-TPT. The
code is available in https://github.com/TomSheng21/R-TPT.

</details>

### [103] [Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance](https://arxiv.org/abs/2504.11197)
*Shangyu Liu,Zhenzhe Zheng,Xiaoyao Huang,Fan Wu,Jie Wu*

Main category: cs.LG

TLDR: DRAGON 是一个分布式 RAG 框架，用于在不泄露隐私的情况下提升边缘设备上小型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在资源受限设备上部署高效，但性能有限；现有 RAG 实现集中式，无法处理云端和设备端数据分离的问题。

Method: 提出 DRAGON 框架，将 RAG 分解为并行 token 生成，使用双侧推测聚合算法避免同步，并引入调度算法基于网络条件优化。

Result: 实验显示性能提升高达 1.9 倍，降低每个 token 延迟，TTFT 开销可忽略。

Conclusion: DRAGON 框架有效提升 SLM 性能，同时确保隐私和效率。

Abstract: Small language models (SLMs) support efficient deployments on
resource-constrained edge devices, but their limited capacity compromises
inference performance. Retrieval-augmented generation (RAG) is a promising
solution to enhance model performance by integrating external databases,
without requiring intensive on-device model retraining. However, large-scale
public databases and user-specific private contextual documents are typically
located on the cloud and the device separately, while existing RAG
implementations are primarily centralized. To bridge this gap, we propose
DRAGON, a distributed RAG framework to enhance on-device SLMs through both
general and personal knowledge without the risk of leaking document privacy.
Specifically, DRAGON decomposes multi-document RAG into multiple parallel token
generation processes performed independently and locally on the cloud and the
device, and employs a newly designed Speculative Aggregation, a dual-side
speculative algorithm to avoid frequent output synchronization between the
cloud and device. A new scheduling algorithm is further introduced to identify
the optimal aggregation side based on real-time network conditions. Evaluations
on real-world hardware testbed demonstrate a significant performance
improvement of DRAGON-up to 1.9x greater gains over standalone SLM compared to
the centralized RAG, substantial reduction in per-token latency, and negligible
Time to First Token (TTFT) overhead.

</details>

### [104] [Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models](https://arxiv.org/abs/2504.11216)
*Gergely D. Németh,Eros Fanì,Yeat Jeng Ng,Barbara Caputo,Miguel Ángel Lozano,Nuria Oliver,Novi Quadrianto*

Main category: cs.LG

TLDR: 本论文提出FedDiverse算法处理联邦学习中数据异质性问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据非独立同分布和不平衡，导致模型泛化差、收敛慢和性能降低。

Method: 提出6个数据异质性度量指标、创建7个计算机视觉数据集、并开发FedDiverse客户端选择算法。

Result: 实验显示FedDiverse提升联邦学习方法的性能和鲁棒性，同时通信和计算开销低。

Conclusion: FedDiverse是管理数据异质性的有效策略。

Abstract: Federated Learning (FL) enables decentralized training of machine learning
models on distributed data while preserving privacy. However, in real-world FL
settings, client data is often non-identically distributed and imbalanced,
resulting in statistical data heterogeneity which impacts the generalization
capabilities of the server's model across clients, slows convergence and
reduces performance. In this paper, we address this challenge by first
proposing a characterization of statistical data heterogeneity by means of 6
metrics of global and client attribute imbalance, class imbalance, and spurious
correlations. Next, we create and share 7 computer vision datasets for binary
and multiclass image classification tasks in Federated Learning that cover a
broad range of statistical data heterogeneity and hence simulate real-world
situations. Finally, we propose FedDiverse, a novel client selection algorithm
in FL which is designed to manage and leverage data heterogeneity across
clients by promoting collaboration between clients with complementary data
distributions. Experiments on the seven proposed FL datasets demonstrate
FedDiverse's effectiveness in enhancing the performance and robustness of a
variety of FL methods while having low communication and computational
overhead.

</details>

### [105] [The Forward-Forward Algorithm: Characterizing Training Behavior](https://arxiv.org/abs/2504.11229)
*Reece Adamson*

Main category: cs.LG

TLDR: 这篇论文研究了Forward-Forward网络的训练动态，发现浅层准确率提升更快，并与整体模型准确率高度相关。


<details>
  <summary>Details</summary>
Motivation: 为了机械性地理解Forward-Forward网络的内部行为。

Method: 通过应用各种系统特征处理，调查训练过程中层和整体模型准确率的变化，以及准确率与层深度的关系。

Result: 经验结果表明，网络中更深的层在准确率提升上存在延迟，浅层准确率与整体模型准确率高度相关。

Conclusion: 浅层准确率更早改善并强烈影响整体性能，深层有延迟。

Abstract: The Forward-Forward algorithm is an alternative learning method which
consists of two forward passes rather than a forward and backward pass employed
by backpropagation. Forward-Forward networks employ layer local loss functions
which are optimized based on the layer activation for each forward pass rather
than a single global objective function. This work explores the dynamics of
model and layer accuracy changes in Forward-Forward networks as training
progresses in pursuit of a mechanistic understanding of their internal
behavior. Treatments to various system characteristics are applied to
investigate changes in layer and overall model accuracy as training progresses,
how accuracy is impacted by layer depth, and how strongly individual layer
accuracy is correlated with overall model accuracy. The empirical results
presented suggest that layers deeper within Forward-Forward networks experience
a delay in accuracy improvement relative to shallower layers and that shallower
layer accuracy is strongly correlated with overall model accuracy.

</details>

### [106] [A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes](https://arxiv.org/abs/2504.11250)
*Jeroen Middelhuis,Zaharah Bukhsh,Ivo Adan,Remco Dijkman*

Main category: cs.LG

TLDR: 这篇论文提出了一种基于rollout的深度强化学习算法和奖励函数，直接优化业务流程资源分配以最小化平均周期时间，并展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习算法不适合动态业务环境，且奖励函数可能导致次优策略，因此需要改进。

Method: 提出rollout-based DRL算法和直接分解目标函数的奖励函数，通过迭代评估执行轨迹来改进策略。

Result: 在所有六个评估的业务流程中学习到最优策略，优于最先进算法，后者仅在两个流程中成功。

Conclusion: 该方法保证目标函数最小化，无需复杂的奖励工程。

Abstract: Resource allocation plays a critical role in minimizing cycle time and
improving the efficiency of business processes. Recently, Deep Reinforcement
Learning (DRL) has emerged as a powerful tool to optimize resource allocation
policies in business processes. In the DRL framework, an agent learns a policy
through interaction with the environment, guided solely by reward signals that
indicate the quality of its decisions. However, existing algorithms are not
suitable for dynamic environments such as business processes. Furthermore,
existing DRL-based methods rely on engineered reward functions that approximate
the desired objective, but a misalignment between reward and objective can lead
to undesired decisions or suboptimal policies. To address these issues, we
propose a rollout-based DRL algorithm and a reward function to optimize the
objective directly. Our algorithm iteratively improves the policy by evaluating
execution trajectories following different actions. Our reward function
directly decomposes the objective function of minimizing the mean cycle time.
Maximizing our reward function guarantees that the objective function is
minimized without requiring extensive reward engineering. The results show that
our method consistently learns the optimal policy in all six evaluated business
processes, outperforming the state-of-the-art algorithm that can only learn the
optimal policy in two of the evaluated processes.

</details>

### [107] [Reconstructing Fine-Grained Network Data using Autoencoder Architectures with Domain Knowledge Penalties](https://arxiv.org/abs/2504.11255)
*Mark Cheung,Sridhar Venkatesan*

Main category: cs.LG

TLDR: 本论文提出使用机器学习和形式方法从粗粒度特征向量重建细粒度网络会话数据，以提升网络安全模型，同时解决数据存储挑战。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大规模收集和存储原始网络流量带来的困难，特别是捕获稀有网络攻击样本的挑战，这阻碍了全面数据集用于模型训练和威胁检测。

Method: 方法是采用自编码器模型结合领域知识的惩罚项，通过形式方法指导，从结构化特征表示中推断PCAP会话头。

Result: 结果显示，通过约束-based损失项融入领域知识显著提高了重建准确性，尤其在类别特征的会话级编码上。

Conclusion: 结论是这种方法实现了高效重建，促进数据高效训练，并保持隐私和存储效率。

Abstract: The ability to reconstruct fine-grained network session data, including
individual packets, from coarse-grained feature vectors is crucial for
improving network security models. However, the large-scale collection and
storage of raw network traffic pose significant challenges, particularly for
capturing rare cyberattack samples. These challenges hinder the ability to
retain comprehensive datasets for model training and future threat detection.
To address this, we propose a machine learning approach guided by formal
methods to encode and reconstruct network data. Our method employs autoencoder
models with domain-informed penalties to impute PCAP session headers from
structured feature representations. Experimental results demonstrate that
incorporating domain knowledge through constraint-based loss terms
significantly improves reconstruction accuracy, particularly for categorical
features with session-level encodings. By enabling efficient reconstruction of
detailed network sessions, our approach facilitates data-efficient model
training while preserving privacy and storage efficiency.

</details>

### [108] [DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction](https://arxiv.org/abs/2504.11264)
*Ruochi Zhang,Qian Yang,Xiaoyang Wang,Haoran Wu,Qiong Zhou,Yu Wang,Kewei Li,Yueying Wang,Yusi Fan,Jiale Zhang,Lan Huang,Chang Liu,Fengfeng Zhou*

Main category: cs.LG

TLDR: 本论文提出DeepSelective框架，使用EHR数据预测患者预后，提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习缺乏鲁棒表示学习、依赖专家特征，以及深度学习可解释性不足的问题。

Method: 提出DeepSelective框架，结合数据压缩技术和创新特征选择方法，集成自定义模块。

Result: 实验显示提高了预测准确性和可解释性。

Conclusion: 这是一种有价值的临床决策工具，并提供了开源代码。

Abstract: The rapid accumulation of Electronic Health Records (EHRs) has transformed
healthcare by providing valuable data that enhance clinical predictions and
diagnoses. While conventional machine learning models have proven effective,
they often lack robust representation learning and depend heavily on
expert-crafted features. Although deep learning offers powerful solutions, it
is often criticized for its lack of interpretability. To address these
challenges, we propose DeepSelective, a novel end to end deep learning
framework for predicting patient prognosis using EHR data, with a strong
emphasis on enhancing model interpretability. DeepSelective combines data
compression techniques with an innovative feature selection approach,
integrating custom-designed modules that work together to improve both accuracy
and interpretability. Our experiments demonstrate that DeepSelective not only
enhances predictive accuracy but also significantly improves interpretability,
making it a valuable tool for clinical decision-making. The source code is
freely available at http://www.healthinformaticslab.org/supp/resources.php .

</details>

### [109] [Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation](https://arxiv.org/abs/2504.11284)
*Michal Lukasik,Lin Chen,Harikrishna Narasimhan,Aditya Krishna Menon,Wittawat Jitkrittum,Felix X. Yu,Sashank J. Reddi,Gang Fu,Mohammadhossein Bateni,Sanjiv Kumar*

Main category: cs.LG

TLDR: 这篇论文分析了在二部排名问题中，面对多个二元目标标签时，损失聚合和标签聚合的比较，结果显示标签聚合更可取，以避免标签独裁。


<details>
  <summary>Details</summary>
Motivation: 动机是解决二部排名中多个二元标签（如不同标注者）的合成问题，以获得最大AUC的排名。

Method: 方法是通过表征Bayes最优解来正式分析损失聚合和标签聚合两种方法。

Result: 结果表明两种方法均可产生Pareto最优解，但损失聚合可能导致标签独裁，经验验证支持标签聚合更优。

Conclusion: 结论是标签聚合比损失聚合更可取，以防止无意中偏好某个标签。

Abstract: Bipartite ranking is a fundamental supervised learning problem, with the goal
of learning a ranking over instances with maximal area under the ROC curve
(AUC) against a single binary target label. However, one may often observe
multiple binary target labels, e.g., from distinct human annotators. How can
one synthesize such labels into a single coherent ranking? In this work, we
formally analyze two approaches to this problem -- loss aggregation and label
aggregation -- by characterizing their Bayes-optimal solutions. Based on this,
we show that while both methods can yield Pareto-optimal solutions, loss
aggregation can exhibit label dictatorship: one can inadvertently (and
undesirably) favor one label over others. This suggests that label aggregation
can be preferable to loss aggregation, which we empirically verify.

</details>

### [110] [Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints](https://arxiv.org/abs/2504.11320)
*Ruicheng Ao,Gan Luo,David Simchi-Levi,Xinshang Wang*

Main category: cs.LG

TLDR: 本文提出WAIT和Nested WAIT算法优化LLM推理调度，提高内存约束下的效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理因KV缓存增长而资源密集，传统调度无效，需要新方法。

Method: 形式化为多阶段在线调度问题，使用流体动力学近似基准，开发WAIT和Nested WAIT算法。

Result: 算法近似最优，实验显示Llama-7B模型吞吐量和延迟改善。

Conclusion: 桥接运筹学与机器学习，提供高效LLM部署框架。

Abstract: Large Language Models (LLMs) are indispensable in today's applications, but
their inference procedure -- generating responses by processing text in
segments and using a memory-heavy Key-Value (KV) cache -- demands significant
computational resources, particularly under memory constraints. This paper
formulates LLM inference optimization as a multi-stage online scheduling
problem where sequential prompt arrivals and KV cache growth render
conventional scheduling ineffective. We develop a fluid dynamics approximation
to provide a tractable benchmark that guides algorithm design. Building on
this, we propose the Waiting for Accumulated Inference Threshold (WAIT)
algorithm, which uses multiple thresholds to schedule incoming prompts
optimally when output lengths are known, and extend it to Nested WAIT for cases
with unknown output lengths. Theoretical analysis shows that both algorithms
achieve near-optimal performance against the fluid benchmark in heavy traffic
conditions, balancing throughput, latency, and Time to First Token (TTFT).
Experiments with the Llama-7B model on an A100 GPU using both synthetic and
real-world datasets demonstrate improved throughput and reduced latency
relative to established baselines like vLLM and Sarathi. This work bridges
operations research and machine learning, offering a rigorous framework for the
efficient deployment of LLMs under memory constraints.

</details>

### [111] [Subset-Contrastive Multi-Omics Network Embedding](https://arxiv.org/abs/2504.11321)
*Pedro Henrique da Costa Avelar,Min Wu,Sophia Tsoka*

Main category: cs.LG

TLDR: SCONE 是一种使用对比学习的可扩展子图对比方法，用于omics数据网络嵌入，在单细胞和批量多组学整合中表现出色。


<details>
  <summary>Details</summary>
Motivation: 网络-based omics数据分析在单细胞场景下内存密集，且基于相似性的网络缺乏离散拓扑，降低了方法的有效性。

Method: 提出SCONE方法，通过可扩展的子图对比学习技术，利用网络相似性，实现大规模omics数据的可扩展分析。

Result: 在单细胞数据中展示协同omics整合用于细胞类型聚类，在批量多组学整合中性能与最先进方法相当，尽管使用有限的数据视图。

Conclusion: 预期这将激发对子集对比方法在omics数据中进一步的研究。

Abstract: Motivation: Network-based analyses of omics data are widely used, and while
many of these methods have been adapted to single-cell scenarios, they often
remain memory- and space-intensive. As a result, they are better suited to
batch data or smaller datasets. Furthermore, the application of network-based
methods in multi-omics often relies on similarity-based networks, which lack
structurally-discrete topologies. This limitation may reduce the effectiveness
of graph-based methods that were initially designed for topologies with better
defined structures. Results: We propose Subset-Contrastive multi-Omics Network
Embedding (SCONE), a method that employs contrastive learning techniques on
large datasets through a scalable subgraph contrastive approach. By exploiting
the pairwise similarity basis of many network-based omics methods, we
transformed this characteristic into a strength, developing an approach that
aims to achieve scalable and effective analysis. Our method demonstrates
synergistic omics integration for cell type clustering in single-cell data.
Additionally, we evaluate its performance in a bulk multi-omics integration
scenario, where SCONE performs comparable to the state-of-the-art despite
utilising limited views of the original data. We anticipate that our findings
will motivate further research into the use of subset contrastive methods for
omics data.

</details>

### [112] [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
*Abitha Thankaraj,Yiding Jiang,J. Zico Kolter,Yonatan Bisk*

Main category: cs.LG

TLDR: 本文提出Trelawney技术，通过重排训练数据序列来改善因果语言模型训练，与人类写作过程更匹配，提升规划、算法推理和故事生成等任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决因果语言模型训练假设与人类写作过程（目标先于内容）的失配问题，避免通过架构修改来处理。

Method: Trelawney技术，重排和处理训练数据序列，并开发相应的推理算法。

Result: 在规划、算法推理和故事生成基准上提升性能，并免费启用长期目标生成，进一步改善规划和推理能力。

Conclusion: Trelawney可能开启超越当前语言建模范式的全新能力。

Abstract: The structure of causal language model training assumes that each token can
be accurately predicted from the previous context. This contrasts with humans'
natural writing and reasoning process, where goals are typically known before
the exact argument or phrasings. While this mismatch has been well studied in
the literature, the working assumption has been that architectural changes are
needed to address this mismatch. We argue that rearranging and processing the
training data sequences can allow models to more accurately imitate the true
data-generating process, and does not require any other changes to the
architecture or training infrastructure. We demonstrate that this technique,
Trelawney, and the inference algorithms derived from it allow us to improve
performance on several key benchmarks that span planning, algorithmic
reasoning, and story generation tasks. Finally, our method naturally enables
the generation of long-term goals at no additional cost. We investigate how
using the model's goal-generation capability can further improve planning and
reasoning. Additionally, we believe Trelawney could potentially open doors to
new capabilities beyond the current language modeling paradigm.

</details>

### [113] [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/abs/2504.11343)
*Wei Xiong,Jiarui Yao,Yuhui Xu,Bo Pang,Lei Wang,Doyen Sahoo,Junnan Li,Nan Jiang,Tong Zhang,Caiming Xiong,Hanze Dong*

Main category: cs.LG

TLDR: 论文分析GRPO的有效性，发现RAFT基线性能竞争性强，提出Reinforce-Rej方法，并建议未来关注负样本处理。


<details>
  <summary>Details</summary>
Motivation: GRPO在强化学习中表现出色，但其有效性来源尚未被充分理解。

Method: 从强化学习角度重新审视GRPO，比较RAFT和PPO，进行消融实验，并提出Reinforce-Rej方法。

Result: RAFT基线与GRPO和PPO性能相当，GRPO优势主要来自丢弃完全错误响应，Reinforce-Rej提高了KL效率和稳定性。

Conclusion: 推荐RAFT作为稳健基线，建议未来工作关注更原则性的负样本整合设计。

Abstract: Reinforcement learning (RL) has become a prevailing approach for fine-tuning
large language models (LLMs) on complex reasoning tasks. Among recent methods,
GRPO stands out for its empirical success in training models such as
DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In
this work, we revisit GRPO from a reinforce-like algorithm perspective and
analyze its core components. Surprisingly, we find that a simple rejection
sampling baseline, RAFT, which trains only on positively rewarded samples,
yields competitive performance than GRPO and PPO. Our ablation studies reveal
that GRPO's main advantage arises from discarding prompts with entirely
incorrect responses, rather than from its reward normalization. Motivated by
this insight, we propose Reinforce-Rej, a minimal extension of policy gradient
that filters both entirely incorrect and entirely correct samples.
Reinforce-Rej improves KL efficiency and stability, serving as a lightweight
yet effective alternative to more complex RL algorithms. We advocate RAFT as a
robust and interpretable baseline, and suggest that future advances should
focus on more principled designs for incorporating negative samples, rather
than relying on them indiscriminately. Our findings provide guidance for future
work in reward-based LLM post-training.

</details>

### [114] [Interpretable Hybrid-Rule Temporal Point Processes](https://arxiv.org/abs/2504.11344)
*Yunyang Cao,Juekai Lin,Hongye Wang,Wenhao Li,Bo Jin*

Main category: cs.LG

TLDR: 本文提出HRTPP框架，结合时间逻辑规则和数值特征，提高Temporal Point Processes在医疗事件建模中的可解释性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: Temporal Point Processes虽能捕获时间动态，但缺乏可解释性；现有可解释方法无法整合数值特征，限制了预测精度。

Method: 提出HRTPP框架，包括基本强度、基于规则的强度和数值特征强度；采用两阶段规则挖掘策略和贝叶斯优化。

Result: 在真实医疗数据集上，HRTPP在预测性能和临床可解释性上优于现有方法；案例研究中，提取规则解释了疾病进展。

Conclusion: HRTPP为医疗诊断提供宝贵贡献，提升了模型的可解释性和准确性。

Abstract: Temporal Point Processes (TPPs) are widely used for modeling event sequences
in various medical domains, such as disease onset prediction, progression
analysis, and clinical decision support. Although TPPs effectively capture
temporal dynamics, their lack of interpretability remains a critical challenge.
Recent advancements have introduced interpretable TPPs. However, these methods
fail to incorporate numerical features, thereby limiting their ability to
generate precise predictions. To address this issue, we propose Hybrid-Rule
Temporal Point Processes (HRTPP), a novel framework that integrates temporal
logic rules with numerical features, improving both interpretability and
predictive accuracy in event modeling. HRTPP comprises three key components:
basic intensity for intrinsic event likelihood, rule-based intensity for
structured temporal dependencies, and numerical feature intensity for dynamic
probability modulation. To effectively discover valid rules, we introduce a
two-phase rule mining strategy with Bayesian optimization. To evaluate our
method, we establish a multi-criteria assessment framework, incorporating rule
validity, model fitting, and temporal predictive accuracy. Experimental results
on real-world medical datasets demonstrate that HRTPP outperforms
state-of-the-art interpretable TPPs in terms of predictive performance and
clinical interpretability. In case studies, the rules extracted by HRTPP
explain the disease progression, offering valuable contributions to medical
diagnosis.

</details>

### [115] [Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation function](https://arxiv.org/abs/2504.11345)
*Luis Miguel Pardo,Daniel Sebastián*

Main category: cs.LG

TLDR: 简而言之，本文扩展Erzeugungsgrad概念，连接代数封闭域的仿射交集理论与计算学习理论的VC理论，证明VC维数和Krull维数线性相关，并应用于理性激活函数的神经网络。


<details>
  <summary>Details</summary>
Motivation: 动机是桥接交集理论和VC理论，分析参数化构造集族的复杂性，以提升机器学习中分类器的理论基础。

Method: 方法包括扩展Heintz (1983)的概念，使用Pardo-Sebastián (2022)的度量，证明维数关系，并应用于逃避品种和神经网络。

Result: 结果显示VC维数和Krull维数线性相关（考虑对数因子），并研究了正确测试序列的密度。

Conclusion: 结论是通过理论联系，改进了对参数化神经网络的分析和应用。

Abstract: The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound
the number of non-empty cells occurring after a process of quantifier
elimination. We extend this notion and the combinatorial bounds of Theorem 2 in
Heintz (1983) using the degree for constructible sets defined in
Pardo-Sebasti\'an (2022). We show that the Erzeugungsgrad is the key ingredient
to connect affine Intersection Theory over algebraically closed fields and the
VC-Theory of Computational Learning Theory for families of classifiers given by
parameterized families of constructible sets. In particular, we prove that the
VC-dimension and the Krull dimension are linearly related up to logarithmic
factors based on Intersection Theory. Using this relation, we study the density
of correct test sequences in evasive varieties. We apply these ideas to analyze
parameterized families of neural networks with rational activation function.

</details>

### [116] [An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2504.11353)
*Jundi Huang,Dawei Zhan*

Main category: cs.LG

TLDR: 本文提出AdaDropout算法，通过自适应丢弃获取函数变量来改善高维Bayesian优化问题。


<details>
  <summary>Details</summary>
Motivation: Bayesian优化在高维问题上性能下降，由于获取函数的高维性。

Method: 自适应地沿迭代丢弃获取函数变量，逐步减少维度。

Result: 数值实验显示AdaDropout有效处理高维挑战，提高解决方案质量，并优于标准和最先进方法。

Conclusion: 这项工作为高维昂贵优化问题提供了一个简单而高效的解决方案。

Abstract: Bayesian optimization (BO) is a widely used algorithm for solving expensive
black-box optimization problems. However, its performance decreases
significantly on high-dimensional problems due to the inherent
high-dimensionality of the acquisition function. In the proposed algorithm, we
adaptively dropout the variables of the acquisition function along the
iterations. By gradually reducing the dimension of the acquisition function,
the proposed approach has less and less difficulty to optimize the acquisition
function. Numerical experiments demonstrate that AdaDropout effectively tackle
high-dimensional challenges and improve solution quality where standard
Bayesian optimization methods often struggle. Moreover, it achieves superior
results when compared with state-of-the-art high-dimensional Bayesian
optimization approaches. This work provides a simple yet efficient solution for
high-dimensional expensive optimization.

</details>

### [117] [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
*Tianwei Ni,Allen Nie,Sapana Chaudhary,Yao Liu,Huzefa Rangwala,Rasool Fakoor*

Main category: cs.LG

TLDR: 本文提出了一种微调大型语言模型的方法，通过使用成功和失败的推理路径整合搜索能力，大大减少了推理时间，同时改进了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少大型语言模型在解决复杂推理问题时，推理时搜索带来的高计算成本和推理时间。

Method: 使用从各种搜索方法中获得的成功和失败推理路径数据对模型进行微调，并采用较小的学习率来防止搜索能力的退化。

Result: 在Game-of-24和Countdown基准测试中，优于标准微调和推理时搜索基线，并实现了高达180倍的推理时间减少。

Conclusion: 该方法有效地将搜索能力整合到模型中，增强了推理任务的效率和性能。

Abstract: Leveraging inference-time search in large language models has proven
effective in further enhancing a trained model's capability to solve complex
mathematical and reasoning problems. However, this approach significantly
increases computational costs and inference time, as the model must generate
and evaluate multiple candidate solutions to identify a viable reasoning path.
To address this, we propose an effective approach that integrates search
capabilities directly into the model by fine-tuning it using both successful
(learning) and failed reasoning paths (forgetting) derived from diverse search
methods. While fine-tuning the model with these data might seem
straightforward, we identify a critical issue: the model's search capability
tends to degrade rapidly if fine-tuning is performed naively. We show that this
degradation can be substantially mitigated by employing a smaller learning
rate. Extensive experiments on the challenging Game-of-24 and Countdown
mathematical reasoning benchmarks show that our approach not only outperforms
both standard fine-tuning and inference-time search baselines but also
significantly reduces inference time by 180$\times$.

</details>

### [118] [Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition](https://arxiv.org/abs/2504.11383)
*Wei Wanga,Maryam Hakimzadeh,Haihui Ruan,Somdatta Goswami*

Main category: cs.LG

TLDR: 本工作提出了一种新型混合框架，将DeepONet与有限元方法相结合，用于高效求解偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程数值求解器在计算成本与准确性间难以平衡，尤其在多尺度动态系统中；神经算子虽可加速模拟，但存在错误积累和泛化性差等问题。

Method: 引入物理信息DeepONet与有限元方法混合框架，通过域分解和Schwarz交替方法自适应耦合，将Newmark时间步进方案集成到DeepONet，并实现自适应子域演化。

Result: 在固体力学问题中验证，收敛率提高20%、错误小于1%，保持解连续性、减少计算成本、缓解错误积累，并自动适应演化现象。

Conclusion: 桥接数值方法与AI驱动代理，提供可扩展的高保真模拟路径，用于工程和科学应用。

Abstract: Numerical solvers for partial differential equations (PDEs) face challenges
balancing computational cost and accuracy, especially in multiscale and dynamic
systems. Neural operators can significantly speed up simulations; however, they
often face challenges such as error accumulation and limited generalization in
multiphysics problems. This work introduces a novel hybrid framework that
integrates physics-informed DeepONet with FEM through domain decomposition. The
core innovation lies in adaptively coupling FEM and DeepONet subdomains via a
Schwarz alternating method. This methodology strategically allocates
computationally demanding regions to a pre-trained Deep Operator Network, while
the remaining computational domain is solved through FEM. To address dynamic
systems, we integrate the Newmark time-stepping scheme directly into the
DeepONet, significantly mitigating error accumulation in long-term simulations.
Furthermore, an adaptive subdomain evolution enables the ML-resolved region to
expand dynamically, capturing emerging fine-scale features without remeshing.
The framework's efficacy has been validated across a range of solid mechanics
problems, including static, quasi-static, and dynamic regimes, demonstrating
accelerated convergence rates (up to 20% improvement compared to FE-FE
approaches), while preserving solution fidelity with error < 1%. Our case
studies show that our proposed hybrid solver: (1) maintains solution continuity
across subdomain interfaces, (2) reduces computational costs by eliminating
fine mesh requirements, (3) mitigates error accumulation in time-dependent
simulations, and (4) enables automatic adaptation to evolving physical
phenomena. This work bridges the gap between numerical methods and AI-driven
surrogates, offering a scalable pathway for high-fidelity simulations in
engineering and scientific applications.

</details>

### [119] [Trajectory Encoding Temporal Graph Networks](https://arxiv.org/abs/2504.11386)
*Jiafeng Xiong,Rizos Sakellariou*

Main category: cs.LG

TLDR: 本研究提出Trajectory Encoding TGN (TETGN)，通过引入可扩展节点标识符和消息传递机制，平衡动态图任务中的传递式和归纳式性能。


<details>
  <summary>Details</summary>
Motivation: 现有Temporal Graph Networks (TGNs) 在传递式和归纳式场景下存在困境：匿名模型归纳能力强但无法区分已知节点，非匿名模型在传递式任务中出色但无法适应新节点。

Method: 提出TETGN，使用自动可扩展的节点标识符作为可学习的temporal positional features，并通过这些标识符进行消息传递，与标准TGN结合多头注意力机制。

Result: 在三个真实数据集上的实验显示，TETGN 在链接预测和节点分类任务中显著优于强基线模型。

Conclusion: TETGN 成功统一匿名和非匿名模型的优势，提升了动态图学习的整体性能。

Abstract: Temporal Graph Networks (TGNs) have demonstrated significant success in
dynamic graph tasks such as link prediction and node classification. Both tasks
comprise transductive settings, where the model predicts links among known
nodes, and in inductive settings, where it generalises learned patterns to
previously unseen nodes. Existing TGN designs face a dilemma under these dual
scenarios. Anonymous TGNs, which rely solely on temporal and structural
information, offer strong inductive generalisation but struggle to distinguish
known nodes. In contrast, non-anonymous TGNs leverage node features to excel in
transductive tasks yet fail to adapt to new nodes. To address this challenge,
we propose Trajectory Encoding TGN (TETGN). Our approach introduces
automatically expandable node identifiers (IDs) as learnable temporal
positional features and performs message passing over these IDs to capture each
node's historical context. By integrating this trajectory-aware module with a
standard TGN using multi-head attention, TETGN effectively balances
transductive accuracy with inductive generalisation. Experimental results on
three real-world datasets show that TETGN significantly outperforms strong
baselines on both link prediction and node classification tasks, demonstrating
its ability to unify the advantages of anonymous and non-anonymous models for
dynamic graph learning.

</details>

### [120] [DataDecide: How to Predict Best Pretraining Data with Small Experiments](https://arxiv.org/abs/2504.11393)
*Ian Magnusson,Nguyen Tai,Ben Bogin,David Heineman,Jena D. Hwang,Luca Soldaini,Akshita Bhagia,Jiacheng Liu,Dirk Groeneveld,Oyvind Tafjord,Noah A. Smith,Pang Wei Koh,Jesse Dodge*

Main category: cs.LG

TLDR: 这篇论文介绍了DataDecide套件，通过小规模实验预测大规模语言模型性能，发现单规模预测已很准确，且使用连续似然指标能高效预测基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练成本高，因此使用小规模实验选择数据以降低成本至关重要。研究如何用小规模性能准确预测最佳数据集。

Method: 发布DataDecide开放套件，包括模型、数据和评估。进行控制预训练实验，涉及25个语料库、不同去重过滤、规模至100B tokens、模型大小至1B参数、3个随机种子，并比较8种基准方法。

Result: 发现单小规模模型排名能预测目标大规模模型性能（约80%正确）。无缩放定律方法优于单规模预测。使用连续似然指标作为代理，能以0.01%计算预测MMLU等基准>80%准确。

Conclusion: DataDecide促进未来缩放定律改进，证实小规模实验能有效预测大规模性能，特别是使用特定指标。

Abstract: Because large language models are expensive to pretrain on different
datasets, using smaller-scale experiments to decide on data is crucial for
reducing costs. Which benchmarks and methods of making decisions from observed
performance at small scale most accurately predict the datasets that yield the
best large models? To empower open exploration of this question, we release
models, data, and evaluations in DataDecide -- the most extensive open suite of
models over differences in data and scale. We conduct controlled pretraining
experiments across 25 corpora with differing sources, deduplication, and
filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random
seeds. We find that the ranking of models at a single, small size (e.g., 150M
parameters) is a strong baseline for predicting best models at our larger
target scale (1B) (~80% of com parisons correct). No scaling law methods among
8 baselines exceed the compute-decision frontier of single-scale predictions,
but DataDecide can measure improvement in future scaling laws. We also identify
that using continuous likelihood metrics as proxies in small experiments makes
benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable
at the target 1B scale with just 0.01% of the compute.

</details>

### [121] [MLPs and KANs for data-driven learning in physical problems: A performance comparison](https://arxiv.org/abs/2504.11397)
*Raghav Pant,Sikan Li,Xingjian Li,Hassan Iqbal,Krishna Kumar*

Main category: cs.LG

TLDR: 本文比较了Kolmogorov-Arnold Networks (KANs) 和多层感知机 (MLPs) 在求解偏微分方程 (PDEs) 方面的性能，发现KANs在浅层网络中表现出色。


<details>
  <summary>Details</summary>
Motivation: 动机是探索KANs是否能在物理系统中超越MLPs的性能，响应机器学习求解PDEs的日益兴趣和关键问题。

Method: 方法包括在深度操作符网络 (DeepONet) 和图网络模拟器 (GNS) 中比较KANs和MLPs的表现，并测试不同规模和复杂度的物理问题，考察浅层和深层网络。

Result: 结果显示KANs在深层网络中不一致地优于MLPs，但在浅层网络中准确性更高。

Conclusion: 结论是KANs在物理系统应用中具有前景，提供效率和准确性的平衡。

Abstract: There is increasing interest in solving partial differential equations (PDEs)
by casting them as machine learning problems. Recently, there has been a spike
in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional
neural networks represented by Multi-Layer Perceptrons (MLPs). While showing
promise, their performance advantages in physics-based problems remain largely
unexplored. Several critical questions persist: Can KANs capture complex
physical dynamics and under what conditions might they outperform traditional
architectures? In this work, we present a comparative study of KANs and MLPs
for learning physical systems governed by PDEs. We assess their performance
when applied in deep operator networks (DeepONet) and graph network-based
simulators (GNS), and test them on physical problems that vary significantly in
scale and complexity. Drawing inspiration from the Kolmogorov Representation
Theorem, we examine the behavior of KANs and MLPs across shallow and deep
network architectures. Our results reveal that although KANs do not
consistently outperform MLPs when configured as deep neural networks, they
demonstrate superior expressiveness in shallow network settings, significantly
outpacing MLPs in accuracy over our test cases. This suggests that KANs are a
promising choice, offering a balance of efficiency and accuracy in applications
involving physical systems.

</details>

### [122] [Measures of Variability for Risk-averse Policy Gradient](https://arxiv.org/abs/2504.11412)
*Yudong Luo,Yangchen Pan,Jiaqi Tan,Pascal Poupart*

Main category: cs.LG

TLDR: 本论文全面研究了九种变异性度量在风险厌恶强化学习中的应用，导出了新梯度公式，并通过经验研究比较了性能。


<details>
  <summary>Details</summary>
Motivation: 现有风险厌恶强化学习主要关注风险度量如CVaR，而变异性度量未被充分探索。

Method: 导出了四种未研究度量的策略梯度公式，改进了Gini Deviation梯度估计，分析了梯度属性，并与REINFORCE和PPO框架结合以惩罚回报离散。

Result: 经验研究显示，方差-based度量不稳定，而CVaR Deviation和Gini Deviation表现一致，提供高回报并有效学习风险厌恶策略。

Conclusion: 本工作提供了变异性度量在风险厌恶强化学习中的全面概述，为风险感知决策提供洞见，并指导未来研究。

Abstract: Risk-averse reinforcement learning (RARL) is critical for decision-making
under uncertainty, which is especially valuable in high-stake applications.
However, most existing works focus on risk measures, e.g., conditional
value-at-risk (CVaR), while measures of variability remain underexplored. In
this paper, we comprehensively study nine common measures of variability,
namely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation,
Standard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and
Semi_Standard Deviation. Among them, four metrics have not been previously
studied in RARL. We derive policy gradient formulas for these unstudied
metrics, improve gradient estimation for Gini Deviation, analyze their gradient
properties, and incorporate them with the REINFORCE and PPO frameworks to
penalize the dispersion of returns.
  Our empirical study reveals that variance-based metrics lead to unstable
policy updates. In contrast, CVaR Deviation and Gini Deviation show consistent
performance across different randomness and evaluation domains, achieving high
returns while effectively learning risk-averse policies. Mean Deviation and
Semi_Standard Deviation are also competitive across different scenarios. This
work provides a comprehensive overview of variability measures in RARL,
offering practical insights for risk-aware decision-making and guiding future
research on risk metrics and RARL algorithms.

</details>

### [123] [Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition](https://arxiv.org/abs/2504.11433)
*Indu Kant Deo,Rajeev K. Jaiman*

Main category: cs.LG

TLDR: 本文提出MI2A框架，通过结合自编码器和注意力机制的深度学习方法，提高流体波传播预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决自回归预测中相位和幅度错误积累的问题，并从经典多步方法中汲取灵感以提升长期预测性能。

Method: 使用去噪卷积自编码器进行潜在表示，注意力-based RNN with LSTM 进行时间演化，并引入损失函数分解策略分离相位和幅度。

Result: 在线性对流、Burgers方程和浅水系统等基准问题上，MI2A显著改善了预测准确性、稳定性和泛化能力。

Conclusion: MI2A是实时波建模的理想工具，优于标准LSTM和注意力模型。

Abstract: In this paper, we present a physics-based deep learning framework for
data-driven prediction of wave propagation in fluid media. The proposed
approach, termed Multistep Integration-Inspired Attention (MI2A), combines a
denoising-based convolutional autoencoder for reduced latent representation
with an attention-based recurrent neural network with long-short-term memory
cells for time evolution of reduced coordinates. This proposed architecture
draws inspiration from classical linear multistep methods to enhance stability
and long-horizon accuracy in latent-time integration. Despite the efficiency of
hybrid neural architectures in modeling wave dynamics, autoregressive
predictions are often prone to accumulating phase and amplitude errors over
time. To mitigate this issue within the MI2A framework, we introduce a novel
loss decomposition strategy that explicitly separates the training loss
function into distinct phase and amplitude components. We assess the
performance of MI2A against two baseline reduced-order models trained with
standard mean-squared error loss: a sequence-to-sequence recurrent neural
network and a variant using Luong-style attention. To demonstrate the
effectiveness of the MI2A model, we consider three benchmark wave propagation
problems of increasing complexity, namely one-dimensional linear convection,
the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant
shallow water system. Our results demonstrate that the MI2A framework
significantly improves the accuracy and stability of long-term predictions,
accurately preserving wave amplitude and phase characteristics. Compared to the
standard long-short term memory and attention-based models, MI2A-based deep
learning exhibits superior generalization and temporal accuracy, making it a
promising tool for real-time wave modeling.

</details>

### [124] [Mamba-Based Ensemble learning for White Blood Cell Classification](https://arxiv.org/abs/2504.11438)
*Lewis Clifton,Xin Tian,Duangdao Palasuwan,Phandee Watanaboonyongcharoen,Ponlapat Rojnuckarin,Nantheera Anantrasirichai*

Main category: cs.LG

TLDR: 这篇论文引入了一个使用Mamba模型和集成学习的新框架来改善白细胞分类，并提供了一个新数据集Chula-WBC-8，以提高效率而不降低准确性。


<details>
  <summary>Details</summary>
Motivation: 手动白细胞分类劳动密集且易出错，深度学习面临数据不平衡和计算需求挑战，尤其是Transformer模型的扩展性问题。

Method: 引入Mamba模型（线性复杂度）与集成学习相结合的框架，并创建新数据集Chula-WBC-8用于基准测试。

Result: 验证了Mamba模型的有效性，提高了分类效率而不降低准确性。

Conclusion: 展示了Mamba模型在资源受限环境中的潜力，并提供了源代码链接。

Abstract: White blood cell (WBC) classification assists in assessing immune health and
diagnosing various diseases, yet manual classification is labor-intensive and
prone to inconsistencies. Recent advancements in deep learning have shown
promise over traditional methods; however, challenges such as data imbalance
and the computational demands of modern technologies, such as Transformer-based
models which do not scale well with input size, limit their practical
application. This paper introduces a novel framework that leverages Mamba
models integrated with ensemble learning to improve WBC classification. Mamba
models, known for their linear complexity, provide a scalable alternative to
Transformer-based approaches, making them suitable for deployment in
resource-constrained environments. Additionally, we introduce a new WBC
dataset, Chula-WBC-8, for benchmarking. Our approach not only validates the
effectiveness of Mamba models in this domain but also demonstrates their
potential to significantly enhance classification efficiency without
compromising accuracy. The source code can be found at
https://github.com/LewisClifton/Mamba-WBC-Classification.

</details>

### [125] [A Clean Slate for Offline Reinforcement Learning](https://arxiv.org/abs/2504.11453)
*Matthew Thomas Jackson,Uljad Berdica,Jarek Liesen,Shimon Whiteson,Jakob Nicolaus Foerster*

Main category: cs.LG

TLDR: 本文解决离线强化学习中的问题，提出统一框架Unifloral和新算法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的进展受模糊问题定义和算法设计纠缠影响，导致实现不一致、评估不公和依赖在线调优。

Method: 引入严格分类法、透明评估协议，提供简洁实现，提出Unifloral算法统一多种方法，并开发TD3-AWR和MoBRAC算法。

Result: 新算法TD3-AWR和MoBRAC显著优于现有基线。

Conclusion: 通过统一框架和严格评估，提高离线强化学习的研究效率和公平性，实现已公开。

Abstract: Progress in offline reinforcement learning (RL) has been impeded by ambiguous
problem definitions and entangled algorithmic designs, resulting in
inconsistent implementations, insufficient ablations, and unfair evaluations.
Although offline RL explicitly avoids environment interaction, prior methods
frequently employ extensive, undocumented online evaluation for hyperparameter
tuning, complicating method comparisons. Moreover, existing reference
implementations differ significantly in boilerplate code, obscuring their core
algorithmic contributions. We address these challenges by first introducing a
rigorous taxonomy and a transparent evaluation protocol that explicitly
quantifies online tuning budgets. To resolve opaque algorithmic design, we
provide clean, minimalistic, single-file implementations of various model-free
and model-based offline RL methods, significantly enhancing clarity and
achieving substantial speed-ups. Leveraging these streamlined implementations,
we propose Unifloral, a unified algorithm that encapsulates diverse prior
approaches within a single, comprehensive hyperparameter space, enabling
algorithm development in a shared hyperparameter space. Using Unifloral with
our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR
(model-free) and MoBRAC (model-based) - which substantially outperform
established baselines. Our implementation is publicly available at
https://github.com/EmptyJackson/unifloral.

</details>

### [126] [Elucidating the Design Space of Multimodal Protein Language Models](https://arxiv.org/abs/2504.11454)
*Cheng-Yen,Hsieh,Xinyou Wang,Daiheng Zhang,Dongyu Xue,Fei Ye,Shujian Huang,Zaixiang Zheng,Quanquan Gu*

Main category: cs.LG

TLDR: 这篇论文通过改进多模态蛋白质语言模型，解决标记化损失问题，提高结构建模和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在将3D结构离散化为标记时丢失细粒度细节和相关性，需要克服标记化损失和结构预测不准的瓶颈。

Method: 阐述设计空间，包括改进生成模型、结构感知架构、表示学习、数据探索和更细粒度的监督。

Result: 将RMSD从5.52降低到2.36，提高结构生成多样性，优于3B基线模型，并与专业折叠模型相当。

Conclusion: 基于标记的多模态蛋白质语言模型通过这些改进可实现稳健的结构建模。

Abstract: Multimodal protein language models (PLMs) integrate sequence and token-based
structural information, serving as a powerful foundation for protein modeling,
generation, and design. However, the reliance on tokenizing 3D structures into
discrete tokens causes substantial loss of fidelity about fine-grained
structural details and correlations. In this paper, we systematically elucidate
the design space of multimodal PLMs to overcome their limitations. We identify
tokenization loss and inaccurate structure token predictions by the PLMs as
major bottlenecks. To address these, our proposed design space covers improved
generative modeling, structure-aware architectures and representation learning,
and data exploration. Our advancements approach finer-grained supervision,
demonstrating that token-based multimodal PLMs can achieve robust structural
modeling. The effective design methods dramatically improve the structure
generation diversity, and notably, folding abilities of our 650M model by
reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B
baselines and on par with the specialized folding models.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [127] [A Review of Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving](https://arxiv.org/abs/2504.11372)
*Zhengbing He,Jorge Laval,Yu Han,Ryosuke Nishi,Cathy Wu*

Main category: physics.soc-ph

TLDR: 这篇论文回顾了可变限速（VSL）和拥堵吸收驾驶（JAD）在抑制高速公路停车-启动波方面的进展，桥接了这两个领域，并指出了研究机会。


<details>
  <summary>Details</summary>
Motivation: 停车-启动波导致交通效率降低、驾驶风险增加和车辆排放升高，因此需要人工干预。论文旨在综合VSL和JAD的碎片化进展。

Method: 论文首先全面回顾VSL和JAD的成就，然后从基础图、交通动态建模、交通状态估计和预测、随机性、策略验证场景以及现场测试和实际部署等角度桥接两个领域并识别研究机会。

Result: 通过回顾和桥接，论文促进了VSL和JAD之间的互补，推进了高速公路停车-启动波抑制的整体研究目标。

Conclusion: 期望通过这种桥接，一个领域可以利用另一个领域的优势来克服其局限性，从而推动停车-启动波抑制的进步。

Abstract: The main form of freeway traffic congestion is the familiar stop-and-go wave,
characterized by wide moving jams that propagate indefinitely upstream provided
enough traffic demand. They cause severe, long-lasting adverse effects, such as
reduced traffic efficiency, increased driving risks, and higher vehicle
emissions. This underscores the crucial importance of artificial intervention
in the propagation of stop-and-go waves. Over the past two decades, two
prominent strategies for stop-and-go wave suppression have emerged: variable
speed limit (VSL) and jam-absorption driving (JAD). Although they share similar
research motivations, objectives, and theoretical foundations, the development
of these strategies has remained relatively disconnected. To synthesize
fragmented advances and drive the field forward, this paper first provides a
comprehensive review of the achievements in the stop-and-go wave
suppression-oriented VSL and JAD, respectively. It then focuses on bridging the
two areas and identifying research opportunities from the following
perspectives: fundamental diagrams, traffic dynamics modeling, traffic state
estimation and prediction, stochasticity, scenarios for strategy validation,
and field tests and practical deployment. We expect that through this review,
one area can effectively address its limitations by identifying and leveraging
the strengths of the other, thus promoting the overall research goal of freeway
stop-and-go wave suppression.

</details>

### [128] [A Review of Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving](https://arxiv.org/abs/2504.11372)
*Zhengbing He,Jorge Laval,Yu Han,Ryosuke Nishi,Cathy Wu*

Main category: physics.soc-ph

TLDR: 这篇论文回顾了可变速度限制（VSL）和拥堵吸收驾驶（JAD）在抑制高速公路停车-起步波方面的进展，并提出桥接两个领域以推动研究。


<details>
  <summary>Details</summary>
Motivation: 抑制停车-起步波以提高交通效率、减少驾驶风险和降低车辆排放。

Method: 全面回顾VSL和JAD的成就，并从基本图、交通动态建模、交通状态估计和预测、随机性、策略验证场景以及现场测试和实际部署角度桥接两个领域。

Result: 期望通过桥接VSL和JAD，解决各自局限性，促进停车-起步波抑制研究。

Conclusion: 通过这个回顾，一个领域可利用另一个优势，推动整体研究目标实现。

Abstract: The main form of freeway traffic congestion is the familiar stop-and-go wave,
characterized by wide moving jams that propagate indefinitely upstream provided
enough traffic demand. They cause severe, long-lasting adverse effects, such as
reduced traffic efficiency, increased driving risks, and higher vehicle
emissions. This underscores the crucial importance of artificial intervention
in the propagation of stop-and-go waves. Over the past two decades, two
prominent strategies for stop-and-go wave suppression have emerged: variable
speed limit (VSL) and jam-absorption driving (JAD). Although they share similar
research motivations, objectives, and theoretical foundations, the development
of these strategies has remained relatively disconnected. To synthesize
fragmented advances and drive the field forward, this paper first provides a
comprehensive review of the achievements in the stop-and-go wave
suppression-oriented VSL and JAD, respectively. It then focuses on bridging the
two areas and identifying research opportunities from the following
perspectives: fundamental diagrams, traffic dynamics modeling, traffic state
estimation and prediction, stochasticity, scenarios for strategy validation,
and field tests and practical deployment. We expect that through this review,
one area can effectively address its limitations by identifying and leveraging
the strengths of the other, thus promoting the overall research goal of freeway
stop-and-go wave suppression.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [129] [Xpose: Bi-directional Engineering for Hidden Query Extraction](https://arxiv.org/abs/2504.10898)
*Ahana Pradhan,Jayant Haritsa*

Main category: cs.DB

TLDR: 本论文介绍了Xpose系统，它结合逆向和正向工程从不透明可执行文件中提取复杂SQL查询，提高了隐藏查询提取的覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的隐藏查询提取工具只能处理简单查询，限制了查询结构和操作符；本文针对工业应用如查询恢复、数据库安全和供应商迁移的需要，提出扩展提取范围。

Method: 提出Xpose系统，包括扩展逆向工程以支持联合、代数过滤和析取，以及利用大语言模型进行正向工程从业务描述获取指导。

Result: 在E-TPCH和STACK基准测试中，Xpose准确提取了复杂查询，展示了其有效性。

Conclusion: Xpose在隐藏查询提取方面取得了重大进展，扩展了查询覆盖范围。

Abstract: Query reverse engineering (QRE) aims to synthesize a SQL query to connect a
given database and result instance. A recent variation of QRE is where an
additional input, an opaque executable containing a ground-truth query, is
provided, and the goal is to non-invasively extract this specific query through
only input-output examples. This variant, called Hidden Query Extraction (HQE),
has a spectrum of industrial use-cases including query recovery, database
security, and vendor migration. The reverse engineering (RE) tools developed
for HQE, which are based on database mutation and generation techniques, can
only extract flat queries with key-based equi joins and conjunctive arithmetic
filter predicates, making them limited wrt both query structure and query
operators. In this paper, we present Xpose, a HQE solution that elevates the
extraction scope to realistic complex queries, such as those found in the TPCH
benchmark. A two-pronged approach is taken: (1) The existing RE scope is
substantially extended to incorporate union connectors, algebraic filter
predicates, and disjunctions for both values and predicates. (2) The predictive
power of LLMs is leveraged to convert business descriptions of the opaque
application into extraction guidance, representing ``forward engineering" (FE).
The FE module recognizes common constructs, such as nesting of sub-queries,
outer joins, and scalar functions. In essence, FE establishes the broad query
contours, while RE fleshes out the fine-grained details. We have evaluated
Xpose on (a) E-TPCH, a query suite comprising the complete TPCH benchmark
extended with queries featuring unions, diverse join types, and sub-queries;
and (b) the real-world STACK benchmark. The experimental results demonstrate
that its bi-directional engineering approach accurately extracts these complex
queries, representing a significant step forward with regard to HQE coverage.

</details>

### [130] [Auto-Test: Learning Semantic-Domain Constraints for Unsupervised Error Detection in Tables](https://arxiv.org/abs/2504.10762)
*Qixu Chen,Yeye He,Raymond Chi-Wing Wong,Weiwei Cui,Song Ge,Haidong Zhang,Dongmei Zhang,Surajit Chaudhuri*

Main category: cs.DB

TLDR: 这篇论文提出了一种新的数据质量约束——语义域约束，能够自动推断和应用，而不需领域专家手动指定。


<details>
  <summary>Details</summary>
Motivation: 数据清洗是长期挑战，现有的算法依赖专家手动定义约束，因此需要自动化的方法来提高效率。

Method: 开发了一个框架，使用大规模统计测试从表格语料库学习约束，并通过优化框架提炼核心约束，具有质量保证。

Result: 评估显示，这种约束可检测真实表格错误，并增强现有数据清洗技术；还提供了基准数据集和代码。

Conclusion: 这项工作证明了自动约束的有效性，并为未来研究提供了资源。

Abstract: Data cleaning is a long-standing challenge in data management. While powerful
logic and statistical algorithms have been developed to detect and repair data
errors in tables, existing algorithms predominantly rely on domain-experts to
first manually specify data-quality constraints specific to a given table,
before data cleaning algorithms can be applied.
  In this work, we propose a new class of data-quality constraints that we call
Semantic-Domain Constraints, which can be reliably inferred and automatically
applied to any tables, without requiring domain-experts to manually specify on
a per-table basis. We develop a principled framework to systematically learn
such constraints from table corpora using large-scale statistical tests, which
can further be distilled into a core set of constraints using our optimization
framework, with provable quality guarantees. Extensive evaluations show that
this new class of constraints can be used to both (1) directly detect errors on
real tables in the wild, and (2) augment existing expert-driven data-cleaning
techniques as a new class of complementary constraints.
  Our extensively labeled benchmark dataset with 2400 real data columns, as
well as our code are available at https://github.com/qixuchen/AutoTest to
facilitate future research.

</details>

### [131] [Morphing-based Compression for Data-centric ML Pipelines](https://arxiv.org/abs/2504.11067)
*Sebastian Baunsgaard,Matthias Boehm*

Main category: cs.DB

TLDR: BWARE 扩展 AWARE 的工作负载感知无损矩阵压缩，用于数据中心 ML 管道，通过特征转换和压缩形态转换显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法难以有效重新发现结构数据冗余，而数据清洗、增强和特征工程提供了数据特性的信息，可用于优化压缩。

Method: 引入 BWARE，将压缩推送到特征转换和工程中，并开发一种新型技术，在不解压的情况下将压缩表示形态化为工作负载优化的表示。

Result: BWARE 显著改善端到端运行时性能，将数据中心 ML 管道的训练时间从天减少到小时。

Conclusion: BWARE 提升了数据中心 ML 管道的效率，展示了通过利用结构信息优化压缩的潜力。

Abstract: Data-centric ML pipelines extend traditional machine learning (ML) pipelines
-- of feature transformations and ML model training -- by outer loops for data
cleaning, augmentation, and feature engineering to create high-quality input
data. Existing lossless matrix compression applies lightweight compression
schemes to numeric matrices and performs linear algebra operations such as
matrix-vector multiplications directly on the compressed representation but
struggles to efficiently rediscover structural data redundancy. Compressed
operations are effective at fitting data in available memory, reducing I/O
across the storage-memory-cache hierarchy, and improving instruction
parallelism. The applied data cleaning, augmentation, and feature
transformations provide a rich source of information about data characteristics
such as distinct items, column sparsity, and column correlations. In this
paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless
matrix compression -- that pushes compression through feature transformations
and engineering to leverage information about structural transformations.
Besides compressed feature transformations, we introduce a novel technique for
lightweight morphing of a compressed representation into workload-optimized
compressed representations without decompression. BWARE shows substantial
end-to-end runtime improvements, reducing the execution time for training
data-centric ML pipelines from days to hours.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [132] [Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification](https://arxiv.org/abs/2504.10521)
*Pardis Moradbeiki,Mohammad Ali Zare Chahooki*

Main category: cs.SI

TLDR: 本研究通过整合文本分析、用户互动和个人资料，提高社交媒体情感分析准确率，尤其针对重大事件。


<details>
  <summary>Details</summary>
Motivation: 社交网络用户众多，传统方法仅依赖文本内容，无法有效捕捉复杂互动和事件相关情感。

Method: 采用混合方法，包括文本分析、通信树模型映射互动、用户个人资料和关注者分析。

Result: 与传统方法比较，情感分布模式提高准确率12%，用户资料提高15%。

Conclusion: 证明整合多源数据能更好地捕捉细微情感动态，传统方法不足以处理重大事件分析。

Abstract: As the popularity and reach of social networks continue to surge, a vast
reservoir of opinions and sentiments across various subjects inundates these
platforms. Among these, X social network (formerly Twitter) stands as a
juggernaut, boasting approximately 420 million active users. Extracting users'
emotional and mental states from their expressed opinions on social media has
become a common pursuit. While past methodologies predominantly focused on the
textual content of messages to analyze user sentiment, the interactive nature
of these platforms suggests a deeper complexity. This study employs hybrid
methodologies, integrating textual analysis, profile examination, follower
analysis, and emotion dissemination patterns. Initially, user interactions are
leveraged to refine emotion classification within messages, encompassing
exchanges where users respond to each other. Introducing the concept of a
communication tree, a model is extracted to map these interactions.
Subsequently, users' bios and interests from this tree are juxtaposed with
message text to enrich analysis. Finally, influential figures are identified
among users' followers in the communication tree, categorized into different
topics to gauge interests. The study highlights that traditional sentiment
analysis methodologies, focusing solely on textual content, are inadequate in
discerning sentiment towards significant events, notably the presidential
election. Comparative analysis with conventional methods reveals a substantial
improvement in accuracy with the incorporation of emotion distribution patterns
and user profiles. The proposed approach yields a 12% increase in accuracy with
emotion distribution patterns and a 15% increase when considering user
profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.

</details>

### [133] [Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification](https://arxiv.org/abs/2504.10521)
*Pardis Moradbeiki,Mohammad Ali Zare Chahooki*

Main category: cs.SI

TLDR: 这项研究通过整合用户互动、个人资料和情感传播模式改进了Twitter情感分析，准确率提升12%至15%。


<details>
  <summary>Details</summary>
Motivation: 社交网络用户激增，传统文本分析方法不足以捕捉重大事件如总统选举的情感动态。

Method: 采用混合方法，包括文本分析、个人资料检查、关注者分析和情感传播模式；引入通信树模型映射用户互动。

Result: 与传统方法相比，准确率提高12%（使用情感分布模式）和15%（考虑用户个人资料）。

Conclusion: 提出方法更有效地捕捉细微情感，强调传统方法的局限性。

Abstract: As the popularity and reach of social networks continue to surge, a vast
reservoir of opinions and sentiments across various subjects inundates these
platforms. Among these, X social network (formerly Twitter) stands as a
juggernaut, boasting approximately 420 million active users. Extracting users'
emotional and mental states from their expressed opinions on social media has
become a common pursuit. While past methodologies predominantly focused on the
textual content of messages to analyze user sentiment, the interactive nature
of these platforms suggests a deeper complexity. This study employs hybrid
methodologies, integrating textual analysis, profile examination, follower
analysis, and emotion dissemination patterns. Initially, user interactions are
leveraged to refine emotion classification within messages, encompassing
exchanges where users respond to each other. Introducing the concept of a
communication tree, a model is extracted to map these interactions.
Subsequently, users' bios and interests from this tree are juxtaposed with
message text to enrich analysis. Finally, influential figures are identified
among users' followers in the communication tree, categorized into different
topics to gauge interests. The study highlights that traditional sentiment
analysis methodologies, focusing solely on textual content, are inadequate in
discerning sentiment towards significant events, notably the presidential
election. Comparative analysis with conventional methods reveals a substantial
improvement in accuracy with the incorporation of emotion distribution patterns
and user profiles. The proposed approach yields a 12% increase in accuracy with
emotion distribution patterns and a 15% increase when considering user
profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.

</details>

### [134] [Influence Maximization in Temporal Social Networks with a Cold-Start Problem: A Supervised Approach](https://arxiv.org/abs/2504.11245)
*Laixin Xie,Ying Zhang,Xiyuan Wang,Shiyi Liu,Shenghan Gao,Xingxing Xing,Wei Wan,Haipeng Zhang,Quan Li*

Main category: cs.SI

TLDR: 本论文针对时间图中的影响最大化问题，提出基于影响传播路径的种子选择方法，解决了冷启动问题，并通过实验验证了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 通过影响传播路径定义种子以扩大网络规模，并解决时间网络中常见的冷启动问题。

Method: 引入基于主题的标记方法和张量化时间图网络（TGN），并使用历史数据为冷启动节点添加新邻居。

Result: 离线实验显示了预测准确性和训练效率的提升，在线A/B测试验证了实际网络增长和冷启动问题的有效解决。

Conclusion: 该方法提高了影响最大化的性能，并有效处理了冷启动问题。

Abstract: Influence Maximization (IM) in temporal graphs focuses on identifying
influential "seeds" that are pivotal for maximizing network expansion. We
advocate defining these seeds through Influence Propagation Paths (IPPs), which
is essential for scaling up the network. Our focus lies in efficiently labeling
IPPs and accurately predicting these seeds, while addressing the
often-overlooked cold-start issue prevalent in temporal networks. Our strategy
introduces a motif-based labeling method and a tensorized Temporal Graph
Network (TGN) tailored for multi-relational temporal graphs, bolstering
prediction accuracy and computational efficiency. Moreover, we augment
cold-start nodes with new neighbors from historical data sharing similar IPPs.
The recommendation system within an online team-based gaming environment
presents subtle impact on the social network, forming multi-relational (i.e.,
weak and strong) temporal graphs for our empirical IM study. We conduct offline
experiments to assess prediction accuracy and model training efficiency,
complemented by online A/B testing to validate practical network growth and the
effectiveness in addressing the cold-start issue.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [135] [Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning](https://arxiv.org/abs/2504.10489)
*Vikranth Udandarao,Noel Abraham Tiju,Muthuraj Vairamuthu,Harsh Mistry,Dhruv Kumar*

Main category: cs.HC

TLDR: 本文介绍了Roamify，一个AI驱动的旅行助手，使用大型语言模型生成个性化行程，用户调查显示AI方法更受欢迎，并强调设计考虑。


<details>
  <summary>Details</summary>
Motivation: 简化旅行规划过程，用户调查显示所有年龄组更偏好AI辅助，验证了此类助手的必要性。

Method: 使用Llama和T5等大型语言模型生成行程，结合网络爬虫获取实时目的地信息，并基于用户偏好调整推荐系统。

Result: 用户调查结果显示AI方法更受欢迎，验证了旅行助手的潜在需求。

Conclusion: Roamify可改善并简化不同年龄组的旅行规划体验。

Abstract: In this paper, we present Roamify, an Artificial Intelligence powered travel
assistant that aims to ease the process of travel planning. We have tested and
used multiple Large Language Models like Llama and T5 to generate personalised
itineraries per user preferences. Results from user surveys highlight the
preference for AI powered mediums over existing methods to help in travel
planning across all user age groups. These results firmly validate the
potential need of such a travel assistant. We highlight the two primary design
considerations for travel assistance: D1) incorporating a web-scraping method
to gather up-to-date news articles about destinations from various blog
sources, which significantly improves our itinerary suggestions, and D2)
utilising user preferences to create customised travel experiences along with a
recommendation system which changes the itinerary according to the user needs.
Our findings suggest that Roamify has the potential to improve and simplify how
users across multiple age groups plan their travel experiences.

</details>

### [136] [Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning](https://arxiv.org/abs/2504.10489)
*Vikranth Udandarao,Noel Abraham Tiju,Muthuraj Vairamuthu,Harsh Mistry,Dhruv Kumar*

Main category: cs.HC

TLDR: 本文介绍了Roamify，一款AI驱动的旅行助手，使用LLM生成个性化行程，用户调查证实其需求和有效性。


<details>
  <summary>Details</summary>
Motivation: 简化旅行规划过程，响应用户对AI辅助的偏好，满足各年龄段需求。

Method: 使用Llama和T5等LLM生成行程，结合网络爬虫获取实时目的地信息，并基于用户偏好和推荐系统定制体验。

Result: 用户调查显示AI辅助优于传统方法，验证旅行助手的必要性，并证明其改善行程建议的效果。

Conclusion: Roamify有潜力简化多年龄段用户的旅行规划，提供更个性化的体验。

Abstract: In this paper, we present Roamify, an Artificial Intelligence powered travel
assistant that aims to ease the process of travel planning. We have tested and
used multiple Large Language Models like Llama and T5 to generate personalised
itineraries per user preferences. Results from user surveys highlight the
preference for AI powered mediums over existing methods to help in travel
planning across all user age groups. These results firmly validate the
potential need of such a travel assistant. We highlight the two primary design
considerations for travel assistance: D1) incorporating a web-scraping method
to gather up-to-date news articles about destinations from various blog
sources, which significantly improves our itinerary suggestions, and D2)
utilising user preferences to create customised travel experiences along with a
recommendation system which changes the itinerary according to the user needs.
Our findings suggest that Roamify has the potential to improve and simplify how
users across multiple age groups plan their travel experiences.

</details>

### [137] [Rethinking Theory of Mind Benchmarks for LLMs: Towards A User-Centered Perspective](https://arxiv.org/abs/2504.10839)
*Qiaosi Wang,Xuhui Zhou,Maarten Sap,Jodi Forlizzi,Hong Shen*

Main category: cs.HC

TLDR: 本文批评了使用人类ToM任务评估LLM社会智能的局限性，并从HCI视角提出更动态的交互评估方法。


<details>
  <summary>Details</summary>
Motivation: 动机是针对当前ToM基准评估LLM的理论、方法论和评估问题，通过心理学和AI文献进行总结，以改进评估方法。

Method: 方法是通过文献综述分析ToM任务的局限性，并采用HCI视角重新定义ToM基准。

Result: 结果是识别了这些局限性，并建议在评估中考虑用户偏好、需求和体验。

Conclusion: 结论是概述了朝动态交互方向的潜在机会和挑战。

Abstract: The last couple of years have witnessed emerging research that appropriates
Theory-of-Mind (ToM) tasks designed for humans to benchmark LLM's ToM
capabilities as an indication of LLM's social intelligence. However, this
approach has a number of limitations. Drawing on existing psychology and AI
literature, we summarize the theoretical, methodological, and evaluation
limitations by pointing out that certain issues are inherently present in the
original ToM tasks used to evaluate human's ToM, which continues to persist and
exacerbated when appropriated to benchmark LLM's ToM. Taking a human-computer
interaction (HCI) perspective, these limitations prompt us to rethink the
definition and criteria of ToM in ToM benchmarks in a more dynamic,
interactional approach that accounts for user preferences, needs, and
experiences with LLMs in such evaluations. We conclude by outlining potential
opportunities and challenges towards this direction.

</details>

### [138] [Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images](https://arxiv.org/abs/2504.10662)
*Sina Elahimanesh,Mohammadali Mohammadkhani,Shohreh Kasaei*

Main category: cs.HC

TLDR: 本研究分析了波斯社区在X平台上情感表达与现实世界的差异，发现推文相似度高于图像。


<details>
  <summary>Details</summary>
Motivation: 探讨现实世界和在线平台之间情感表达的差异。

Method: 设计管道使用Transformers-based模型分析推文和图像情感，结合朋友反馈及距离标准比较。

Result: 图像与现实情感相似度28.67%，推文相似度75.88%，差异具有统计学意义。

Conclusion: 证实在线情感表达与现实世界存在差异，推文更接近现实情感。

Abstract: In contemporary society, widespread social media usage is evident in people's
daily lives. Nevertheless, disparities in emotional expressions between the
real world and online platforms can manifest. We comprehensively analyzed
Persian community on X to explore this phenomenon. An innovative pipeline was
designed to measure the similarity between emotions in the real world compared
to social media. Accordingly, recent tweets and images of participants were
gathered and analyzed using Transformers-based text and image sentiment
analysis modules. Each participant's friends also provided insights into the
their real-world emotions. A distance criterion was used to compare real-world
feelings with virtual experiences. Our study encompassed N=105 participants,
393 friends who contributed their perspectives, over 8,300 collected tweets,
and 2,000 media images. Results indicated a 28.67% similarity between images
and real-world emotions, while tweets exhibited a 75.88% alignment with
real-world feelings. Additionally, the statistical significance confirmed that
the observed disparities in sentiment proportions.

</details>

### [139] [Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students](https://arxiv.org/abs/2504.10961)
*Audrey Zhang,Yifei Gao,Wannapon Suraworachet,Tanya Nazaretsky,Mutlu Cukurova*

Main category: cs.HC

TLDR: 研究显示学生更偏好AI和AI-人类合作反馈，基于经验和性别有差异，为AI在教育中的整合提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI改变教育反馈实践，理解学生对不同反馈提供者的看法至关重要，本研究填补了这一空白。

Method: 采用91名参与者的内部受试者实验，调查学生区分反馈类型的能力、反馈质量感知和对AI参与的偏见。

Result: 学生在有用性和客观性上更偏好AI和合作反馈；揭示来源时AI反馈的真实性下降；教育AI经验提升识别和信任；一般AI经验降低有用性和可信度；男性学生对所有反馈的评价较低。

Conclusion: 这些洞见为将AI整合到高等教育反馈系统中提供循证指南，同时解决信任问题并培养学生的AI素养。

Abstract: As generative AI transforms educational feedback practices, understanding
students' perceptions of different feedback providers becomes crucial for
effective implementation. This study addresses a critical gap by comparing
undergraduate students' trust in AI-generated, human-created, and human-AI
co-produced feedback, informing how institutions can adapt feedback practices
in this new era. Through a within-subject experiment with 91 participants, we
investigated factors predicting students' ability to distinguish between
feedback types, perception of feedback quality, and potential biases to AI
involvement. Findings revealed that students generally preferred AI and
co-produced feedback over human feedback in terms of perceived usefulness and
objectivity. Only AI feedback suffered a decline in perceived genuineness when
feedback sources were revealed, while co-produced feedback maintained its
positive perception. Educational AI experience improved students' ability to
identify AI feedback and increased their trust in all feedback types, while
general AI experience decreased perceived usefulness and credibility. Male
students consistently rated all feedback types as less valuable than their
female and non-binary counterparts. These insights inform evidence-based
guidelines for integrating AI into higher education feedback systems while
addressing trust concerns and fostering AI literacy among students.

</details>

### [140] ["Even explanations will not help in trusting [this] fundamentally biased system": A Predictive Policing Case-Study](https://arxiv.org/abs/2504.11020)
*Siddharth Mehrotra,Ujwal Gadiraju,Eva Bittner,Folkert van Delden,Catholijn M. Jonker,Myrthe L. Tielman*

Main category: cs.HC

TLDR: 本研究探讨不同解释形式（文本、可视、混合）和用户专业性对AI预测性警务信任的影响，发现混合解释提升专家主观信任但未改善决策，无解释形式帮助建立适当信任，呼吁重新评估解释在AI信任中的作用。


<details>
  <summary>Details</summary>
Motivation: AI在高风险领域信任问题突出，用户可能过度或不足信任，过去研究显示解释有助于理解信任时机，但不同解释形式的影响尤其在高风险领域仍需探索。

Method: 通过实验比较文本、可视和混合解释形式，以及专家（退休警察）和非专家用户，在AI预测性警务中对信任的影响。

Result: 混合解释提高了专家用户的主观信任，但未改善决策；任何解释形式均未帮助建立适当信任。

Conclusion: 强调需要重新评估解释在构建AI适当信任中的作用，尤其在有疑问的系统中；基于结果提出挑战和政策推荐，以设计高风险AI系统的适当信任。

Abstract: In today's society, where Artificial Intelligence (AI) has gained a vital
role, concerns regarding user's trust have garnered significant attention. The
use of AI systems in high-risk domains have often led users to either
under-trust it, potentially causing inadequate reliance or over-trust it,
resulting in over-compliance. Therefore, users must maintain an appropriate
level of trust. Past research has indicated that explanations provided by AI
systems can enhance user understanding of when to trust or not trust the
system. However, the utility of presentation of different explanations forms
still remains to be explored especially in high-risk domains. Therefore, this
study explores the impact of different explanation types (text, visual, and
hybrid) and user expertise (retired police officers and lay users) on
establishing appropriate trust in AI-based predictive policing. While we
observed that the hybrid form of explanations increased the subjective trust in
AI for expert users, it did not led to better decision-making. Furthermore, no
form of explanations helped build appropriate trust. The findings of our study
emphasize the importance of re-evaluating the use of explanations to build
[appropriate] trust in AI based systems especially when the system's use is
questionable. Finally, we synthesize potential challenges and policy
recommendations based on our results to design for appropriate trust in
high-risk based AI-based systems.

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [141] [On the Contractivity of Stochastic Interpolation Flow](https://arxiv.org/abs/2504.10653)
*Max Daniels*

Main category: math.ST

TLDR: 本文研究随机插值框架用于高维采样，证明其Lipschitz性质并讨论实际应用。


<details>
  <summary>Details</summary>
Motivation: 探索一种类似于扩散模型的采样方法，并建立功能不等式以解决高维采样问题。

Method: 证明高斯基分布和强对数凹目标分布下的随机插值流映射Lipschitz性质，并构建非高斯分布间的Lipschitz传输映射。

Result: 随机插值流映射的Lipschitz常数与Caffarelli定理匹配，并推广到非高斯分布。

Conclusion: 为随机插值的采样和估计问题提供了实际含义和潜在应用。

Abstract: We investigate stochastic interpolation, a recently introduced framework for
high dimensional sampling which bears many similarities to diffusion modeling.
Stochastic interpolation generates a data sample by first randomly initializing
a particle drawn from a simple base distribution, then simulating deterministic
or stochastic dynamics such that in finite time the particle's distribution
converges to the target. We show that for a Gaussian base distribution and a
strongly log-concave target distribution, the stochastic interpolation flow map
is Lipschitz with a sharp constant which matches that of Caffarelli's theorem
for optimal transport maps. We are further able to construct Lipschitz
transport maps between non-Gaussian distributions, generalizing some recent
constructions in the literature on transport methods for establishing
functional inequalities. We discuss the practical implications of our theorem
for the sampling and estimation problems required by stochastic interpolation.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [142] [HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems](https://arxiv.org/abs/2504.10699)
*Nan Wang,Ricardo G. Sanfelice*

Main category: cs.RO

TLDR: 本文提出HyRRT-Connect算法，用于混合系统的运动规划，通过双向传播提高效率，并通过示例验证。


<details>
  <summary>Details</summary>
Motivation: 解决混合系统运动规划问题，特别是处理混合动力学中的不连续性和提高计算效率。

Method: 提出HyRRT-Connect算法，在混合时间内双向传播检测重叠，构建运动规划，并通过前向模拟重建后向部分消除不连续性。

Result: 应用于带驱动的弹跳球系统和步行机器人示例，展示了计算效率的显著提升。

Conclusion: HyRRT-Connect算法有效地解决了混合系统运动规划中的挑战，提高了整体性能。

Abstract: This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring that the motion plan satisfies the given hybrid dynamics. To
address the potential discontinuity along the flow caused by tolerating some
distance between the forward and backward partial motion plans, we reconstruct
the backward partial motion plan by a forward-in-hybrid-time simulation from
the final state of the forward partial motion plan. effectively eliminating the
discontinuity. The proposed algorithm is applied to an actuated bouncing ball
system and a walking robot example to highlight its computational improvement.

</details>

### [143] [$π$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles](https://arxiv.org/abs/2504.10962)
*Edvin Martin Andrejev,Amith Manoharan,Karl-Eerik Unt,Arun Kumar Singh*

Main category: cs.RO

TLDR: 本论文提出π-MPPI方法，通过添加投影过滤器改善MPPI算法的控制平滑性，适用于固定翼飞行器。


<details>
  <summary>Details</summary>
Motivation: 解决MPPI算法中控制序列不平滑导致系统振荡的问题，现有后处理方法无法有效约束控制导数。

Method: 引入投影过滤器π对控制样本进行最小修正，确保控制幅度和导数边界，并使用神经加速优化器减少计算开销。

Result: π-MPPI在固定翼飞行器上调优更容易，获得更平滑和鲁棒的性能。

Conclusion: π-MPPI提供简单实现任意平滑度控制序列的方法，可整合到任何MPPI管道中。

Abstract: Model Predictive Path Integral (MPPI) is a popular sampling-based Model
Predictive Control (MPC) algorithm for nonlinear systems. It optimizes
trajectories by sampling control sequences and averaging them. However, a key
issue with MPPI is the non-smoothness of the optimal control sequence, leading
to oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing
solutions use post-hoc smoothing, which fails to bound control derivatives.
This paper introduces a new approach: we add a projection filter $\pi$ to
minimally correct control samples, ensuring bounds on control magnitude and
higher-order derivatives. The filtered samples are then averaged using MPPI,
leading to our $\pi$-MPPI approach. We minimize computational overhead by using
a neural accelerated custom optimizer for the projection filter. $\pi$-MPPI
offers a simple way to achieve arbitrary smoothness in control sequences. While
we focus on FWVs, this projection filter can be integrated into any MPPI
pipeline. Applied to FWVs, $\pi$-MPPI is easier to tune than the baseline,
resulting in smoother, more robust performance.

</details>

### [144] [Neural Control Barrier Functions from Physics Informed Neural Networks](https://arxiv.org/abs/2504.11045)
*Shreenabh Agrawal,Manan Tayal,Aditya Singh,Shishir Kolathaya*

Main category: cs.RO

TLDR: 这篇论文提出了一种基于Zubov PDE的神经控制屏障函数方法，提高自主系统安全性，并在多个案例中验证。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统普及，确保安全性至关重要，传统CBFs设计困难，深度学习被用于合成神经CBFs。

Method: 引入物理启发神经网络框架，结合Zubov PDE和互惠CBFs，适用于高维系统，通过倒立摆、地面和空中导航案例验证。

Result: 方法可扩展，案例研究证明了其在不同系统中的有效性。

Conclusion: 提供了一种可扩展的神经CBFs合成方法，提升自主系统的安全性能。

Abstract: As autonomous systems become increasingly prevalent in daily life, ensuring
their safety is paramount. Control Barrier Functions (CBFs) have emerged as an
effective tool for guaranteeing safety; however, manually designing them for
specific applications remains a significant challenge. With the advent of deep
learning techniques, recent research has explored synthesizing CBFs using
neural networks-commonly referred to as neural CBFs. This paper introduces a
novel class of neural CBFs that leverages a physics-inspired neural network
framework by incorporating Zubov's Partial Differential Equation (PDE) within
the context of safety. This approach provides a scalable methodology for
synthesizing neural CBFs applicable to high-dimensional systems. Furthermore,
by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework
allows for the specification of flexible, user-defined safe regions. To
validate the effectiveness of the approach, we present case studies on three
different systems: an inverted pendulum, autonomous ground navigation, and
aerial navigation in obstacle-laden environments.

</details>

### [145] [HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems](https://arxiv.org/abs/2504.10699)
*Nan Wang,Ricardo G. Sanfelice*

Main category: cs.RO

TLDR: 本文提出HyRRT-Connect算法，用于混合系统的双向RRT运动规划，提高效率并处理不连续性。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地解决混合系统的运动规划问题，特别是处理不连续性和提高计算效率。

Method: 算法在混合时间内双向传播，检测重叠后通过反转和连接函数构建运动规划，并通过前向模拟重建后向部分消除不连续性。

Result: 应用于弹跳球和步行机器人示例，展示了计算效率的提升。

Conclusion: 算法有效消除不连续性，并改善混合系统运动规划的计算性能。

Abstract: This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring that the motion plan satisfies the given hybrid dynamics. To
address the potential discontinuity along the flow caused by tolerating some
distance between the forward and backward partial motion plans, we reconstruct
the backward partial motion plan by a forward-in-hybrid-time simulation from
the final state of the forward partial motion plan. effectively eliminating the
discontinuity. The proposed algorithm is applied to an actuated bouncing ball
system and a walking robot example to highlight its computational improvement.

</details>

### [146] [$π$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles](https://arxiv.org/abs/2504.10962)
*Edvin Martin Andrejev,Amith Manoharan,Karl-Eerik Unt,Arun Kumar Singh*

Main category: cs.RO

TLDR: 这篇论文引入π-MPPI方法，通过添加投影过滤器改善MPPI算法的控制平滑性，针对非线性系统如固定翼飞行器。


<details>
  <summary>Details</summary>
Motivation: MPPI算法的控制序列不平滑导致系统振荡，现有的平滑解决方案无法限制控制导数。

Method: 添加投影过滤器π最小化修正控制样本，确保控制幅度和导数的边界，并使用神经加速的自定义优化器。

Result: π-MPPI更容易调参，在固定翼飞行器上实现更平滑和鲁棒的性能。

Conclusion: π-MPPI提供简单方法实现任意平滑度的控制序列，可整合到任何MPPI管道中。

Abstract: Model Predictive Path Integral (MPPI) is a popular sampling-based Model
Predictive Control (MPC) algorithm for nonlinear systems. It optimizes
trajectories by sampling control sequences and averaging them. However, a key
issue with MPPI is the non-smoothness of the optimal control sequence, leading
to oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing
solutions use post-hoc smoothing, which fails to bound control derivatives.
This paper introduces a new approach: we add a projection filter $\pi$ to
minimally correct control samples, ensuring bounds on control magnitude and
higher-order derivatives. The filtered samples are then averaged using MPPI,
leading to our $\pi$-MPPI approach. We minimize computational overhead by using
a neural accelerated custom optimizer for the projection filter. $\pi$-MPPI
offers a simple way to achieve arbitrary smoothness in control sequences. While
we focus on FWVs, this projection filter can be integrated into any MPPI
pipeline. Applied to FWVs, $\pi$-MPPI is easier to tune than the baseline,
resulting in smoother, more robust performance.

</details>

### [147] [Neural Control Barrier Functions from Physics Informed Neural Networks](https://arxiv.org/abs/2504.11045)
*Shreenabh Agrawal,Manan Tayal,Aditya Singh,Shishir Kolathaya*

Main category: cs.RO

TLDR: 本文提出了一种新颖的神经控制屏障函数方法，使用Zubov's偏微分方程来提升自主系统的安全性，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在日常生活中的普及，确保其安全至关重要，手动设计控制屏障函数具有挑战性，因此采用深度学习技术合成神经CBF。

Method: 引入一种基于物理启发的神经网络框架，将Zubov's PDE融入安全性上下文中，使用互惠CBF定义灵活的安全区域，并适用于高维系统。

Result: 通过倒立摆、自主地面导航和空中障碍环境导航的案例研究，验证了方法的有效性和可扩展性。

Conclusion: 该方法提供了一种可扩展的神经CBF合成框架，能够指定用户自定义的安全区域，提高了自主系统的安全性。

Abstract: As autonomous systems become increasingly prevalent in daily life, ensuring
their safety is paramount. Control Barrier Functions (CBFs) have emerged as an
effective tool for guaranteeing safety; however, manually designing them for
specific applications remains a significant challenge. With the advent of deep
learning techniques, recent research has explored synthesizing CBFs using
neural networks-commonly referred to as neural CBFs. This paper introduces a
novel class of neural CBFs that leverages a physics-inspired neural network
framework by incorporating Zubov's Partial Differential Equation (PDE) within
the context of safety. This approach provides a scalable methodology for
synthesizing neural CBFs applicable to high-dimensional systems. Furthermore,
by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework
allows for the specification of flexible, user-defined safe regions. To
validate the effectiveness of the approach, we present case studies on three
different systems: an inverted pendulum, autonomous ground navigation, and
aerial navigation in obstacle-laden environments.

</details>

### [148] [Communication-aware Hierarchical Map Compression of Time-Varying Environments for Mobile Robots](https://arxiv.org/abs/2504.10751)
*Daniel T. Larsson,Dipankar Maity*

Main category: cs.RO

TLDR: 本论文开发了一个框架，用于动态概率占用网格的时间序列压缩，平衡压缩质量与资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了在有限的通信带宽或存储资源下，高效压缩占用地图，同时不需知道地图动态。

Method: 利用信号压缩理论，制定优化问题，开发算法求解多分辨率分层编码器。

Result: 在模拟中验证了框架在静态和动态占用地图上的有效性。

Conclusion: 框架能可靠压缩地图，满足资源约束，并证明其实用性。

Abstract: In this paper, we develop a systematic framework for the time-sequential
compression of dynamic probabilistic occupancy grids. Our approach leverages
ideas from signal compression theory to formulate an optimization problem that
searches for a multi-resolution hierarchical encoder that balances the quality
of the compressed map (distortion) with its description size, the latter of
which relates to the bandwidth required to reliably transmit the map to other
agents or to store map estimates in on-board memory. The resulting optimization
problem allows for multi-resolution map compressions to be obtained that
satisfy available communication or memory resources, and does not require
knowledge of the occupancy map dynamics. We develop an algorithm to solve our
problem, and demonstrate the utility of the proposed framework in simulation on
both static (i.e., non-time varying) and dynamic (time-varying) occupancy maps.

</details>

### [149] [ATLASv2: LLM-Guided Adaptive Landmark Acquisition and Navigation on the Edge](https://arxiv.org/abs/2504.10784)
*Mikolaj Walczak,Uttej Kallakuri,Tinoosh Mohsenin*

Main category: cs.RO

TLDR: ATLASv2 是一个在 Jetson Nano 边缘设备上运行的系统，集成了 TinyLLM、实时物体检测和路径规划，实现了高效的自主导航和操作。


<details>
  <summary>Details</summary>
Motivation: 解决自主系统在边缘设备上的资源约束、实时处理需求和动态环境适应挑战。

Method: 引入 ATLASv2 系统，通过微调 TinyLLM、实时物体检测和高效路径规划，在边缘设备上实现分层多任务导航和操作。

Result: 实世界评估显示高成功率，优化了资源利用，减少了延迟和功耗。

Conclusion: 桥接了模拟环境与真实应用之间的差距，通过全 onboard 的生成式 AI 框架。

Abstract: Autonomous systems deployed on edge devices face significant challenges,
including resource constraints, real-time processing demands, and adapting to
dynamic environments. This work introduces ATLASv2, a novel system that
integrates a fine-tuned TinyLLM, real-time object detection, and efficient path
planning to enable hierarchical, multi-task navigation and manipulation all on
the edge device, Jetson Nano. ATLASv2 dynamically expands its navigable
landmarks by detecting and localizing objects in the environment which are
saved to its internal knowledge base to be used for future task execution. We
evaluate ATLASv2 in real-world environments, including a handcrafted home and
office setting constructed with diverse objects and landmarks. Results show
that ATLASv2 effectively interprets natural language instructions, decomposes
them into low-level actions, and executes tasks with high success rates. By
leveraging generative AI in a fully on-board framework, ATLASv2 achieves
optimized resource utilization with minimal prompting latency and power
consumption, bridging the gap between simulated environments and real-world
applications.

</details>

### [150] [E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking](https://arxiv.org/abs/2504.10812)
*Kejia Gao,Liguo Zhou,Mingjun Liu,Alois Knoll*

Main category: cs.RO

TLDR: 本文创建并开源了一个端到端自动泊车数据集，以解决数据集缺失问题，并使用原有模型实现了85.16%的成功率和低误差。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开可用的数据集限制了再现性和基准测试。

Method: 基于原有模型和数据生成、训练、闭环测试的管道。

Result: 实现了85.16%的整体成功率，平均位置误差0.24米，方向误差0.34度。

Conclusion: 这个数据集填补了空白，促进了自动泊车研究的再现性和基准测试。

Abstract: End-to-end learning has shown great potential in autonomous parking, yet the
lack of publicly available datasets limits reproducibility and benchmarking.
While prior work introduced a visual-based parking model and a pipeline for
data generation, training, and close-loop test, the dataset itself was not
released. To bridge this gap, we create and open-source a high-quality dataset
for end-to-end autonomous parking. Using the original model, we achieve an
overall success rate of 85.16% with lower average position and orientation
errors (0.24 meters and 0.34 degrees).

</details>

### [151] [A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space](https://arxiv.org/abs/2504.11170)
*Taewook Kang,Bum-Jae You,Juyoun Park,Yisoo Lee*

Main category: cs.RO

TLDR: 本论文提出了一种集成Masked Autoregressive Flow和Sparse autoencoder的Adversarial AutoEncoders模型，用于机器人操作中的实时异常检测，提高了性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在多样环境中有效操作，但深度学习模型面临训练数据有限和噪声特征大的挑战。

Method: 提出Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders模型，将Masked Autoregressive Flow集成到Adversarial AutoEncoders中，并使用Sparse autoencoder关注重要特征。

Result: 实验显示，ROC曲线面积提高了4.96%至9.75%，碰撞场景性能提升19.67%，推理时间小于1毫秒。

Conclusion: 该模型适用于动态环境下的机器人安全系统，代码将公开。

Abstract: The growing demand for robots to operate effectively in diverse environments
necessitates the need for robust real-time anomaly detection techniques during
robotic operations. However, deep learning-based models in robotics face
significant challenges due to limited training data and highly noisy signal
features. In this paper, we present Sparse Masked Autoregressive Flow-based
Adversarial AutoEncoders model to address these problems. This approach
integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to
construct a flexible latent space and utilize Sparse autoencoder to efficiently
focus on important features, even in scenarios with limited feature space. Our
experiments demonstrate that the proposed model achieves a 4.96% to 9.75%
higher area under the receiver operating characteristic curve for
pick-and-place robotic operations with randomly placed cans, compared to
existing state-of-the-art methods. Notably, it showed up to 19.67% better
performance in scenarios involving collisions with lightweight objects.
Additionally, unlike the existing state-of-the-art model, our model performs
inferences within 1 millisecond, ensuring real-time anomaly detection. These
capabilities make our model highly applicable to machine learning-based robotic
safety systems in dynamic environments. The code will be made publicly
available after acceptance.

</details>

### [152] [Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks](https://arxiv.org/abs/2504.11247)
*Fikrican Özgür,René Zurbrügg,Suryansh Kumar*

Main category: cs.RO

TLDR: 本文引入'Next-Future'重放策略，改进HER在多目标强化学习中的样本效率和准确性，特别适用于机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: HER依赖启发式重放方法缺乏原则框架，因此需要一个新策略来提升学习效率。

Method: 提出'Next-Future'策略，专注于奖励单步转换，以提高多目标MDP的学习。

Result: 在八个机器人操作任务中，七个任务样本效率提升，六个任务成功率更高；实世实验验证了策略的可行性。

Conclusion: 'Next-Future'策略在复杂机器人臂任务中显示潜力，提高了价值逼近和性能。

Abstract: Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art
algorithm for achieving sample-efficient multi-goal reinforcement learning (RL)
in robotic manipulation tasks with binary rewards. HER facilitates learning
from failed attempts by replaying trajectories with redefined goals. However,
it relies on a heuristic-based replay method that lacks a principled framework.
To address this limitation, we introduce a novel replay strategy,
"Next-Future", which focuses on rewarding single-step transitions. This
approach significantly enhances sample efficiency and accuracy in learning
multi-goal Markov decision processes (MDPs), particularly under stringent
accuracy requirements -- a critical aspect for performing complex and precise
robotic-arm tasks. We demonstrate the efficacy of our method by highlighting
how single-step learning enables improved value approximation within the
multi-goal RL framework. The performance of the proposed replay strategy is
evaluated across eight challenging robotic manipulation tasks, using ten random
seeds for training. Our results indicate substantial improvements in sample
efficiency for seven out of eight tasks and higher success rates in six tasks.
Furthermore, real-world experiments validate the practical feasibility of the
learned policies, demonstrating the potential of "Next-Future" in solving
complex robotic-arm tasks.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [153] [Exploring Generative AI Techniques in Government: A Case Study](https://arxiv.org/abs/2504.10497)
*Sunyi Liu,Mengzhe Geng,Rebecca Hart*

Main category: cs.IR

TLDR: 这篇论文介绍了加拿大国家研究委员会 (NRC) 开发的智能代理 Pubbie，用于自动化性能测量、数据管理和报告，采用生成式 AI 技术减少手动工作。


<details>
  <summary>Details</summary>
Motivation: 认识到生成式人工智能的快速发展和变革潜力，NRC 启动试点项目，以探索将其整合到日常操作中提升性能。

Method: 使用 LLM 编排、RoBERTa 语义嵌入、战略微调和少样本学习等技术，设计用户友好的自然语言查询接口，支持文件上传/下载。

Result: Pubbie 减少了手动努力和可访问性障碍，提高了数据管理和报告的效率。

Conclusion: 本研究展示了生成式 AI 在组织性能优化中的应用潜力，为类似集成提供可行案例。

Abstract: The swift progress of Generative Artificial intelligence (GenAI), notably
Large Language Models (LLMs), is reshaping the digital landscape. Recognizing
this transformative potential, the National Research Council of Canada (NRC)
launched a pilot initiative to explore the integration of GenAI techniques into
its daily operation for performance excellence, where 22 projects were launched
in May 2024. Within these projects, this paper presents the development of the
intelligent agent Pubbie as a case study, targeting the automation of
performance measurement, data management and insight reporting at the NRC.
Cutting-edge techniques are explored, including LLM orchestration and semantic
embedding via RoBERTa, while strategic fine-tuning and few-shot learning
approaches are incorporated to infuse domain knowledge at an affordable cost.
The user-friendly interface of Pubbie allows general government users to input
queries in natural language and easily upload or download files with a simple
button click, greatly reducing manual efforts and accessibility barriers.

</details>

### [154] [ArxivBench: Can LLMs Assist Researchers in Conducting Research?](https://arxiv.org/abs/2504.10496)
*Ning Li,Jingran Zhang,Justin Cui*

Main category: cs.IR

TLDR: 本研究评估大型语言模型在生成arXiv相关论文和链接的准确性，引入arXivBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成事实错误内容的挑战，提升在学术环境中的可靠性。

Method: 使用arXivBench基准测试各种LLMs在八个arXiv主题和计算机科学五个子领域的性能。

Result: 准确性因主题而异，Claude-3.5-Sonnet表现最佳，人工智能子领域准确性较高。

Conclusion: 提供标准化工具，促进LLM在研究中的可靠使用，并开源代码和数据集。

Abstract: Large language models (LLMs) have demonstrated remarkable effectiveness in
completing various tasks such as reasoning, translation, and question
answering. However the issue of factual incorrect content in LLM-generated
responses remains a persistent challenge. In this study, we evaluate both
proprietary and open-source LLMs on their ability to respond with relevant
research papers and accurate links to articles hosted on the arXiv platform,
based on high level prompts. To facilitate this evaluation, we introduce
arXivBench, a benchmark specifically designed to assess LLM performance across
eight major subject categories on arXiv and five subfields within computer
science, one of the most popular categories among them. Our findings reveal a
concerning accuracy of LLM-generated responses depending on the subject, with
some subjects experiencing significantly lower accuracy than others. Notably,
Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant
and accurate responses. And interestingly, most LLMs achieve a much higher
accuracy in the Artificial Intelligence sub-field than other sub-fields. This
benchmark provides a standardized tool for evaluating the reliability of
LLM-generated scientific responses, promoting more dependable use of LLMs in
academic and research environments. Our code is open-sourced at
https://github.com/arxivBenchLLM/arXivBench and our dataset is available on
huggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.

</details>

### [155] [CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models](https://arxiv.org/abs/2504.10498)
*Jianling Lu,Mingqi Lv*

Main category: cs.IR

TLDR: 这篇论文提出CCSK方法，通过动态平衡大语言模型的自有知识和外部检索，改善Q&A任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前阈值-based方法静态单一，难以平衡LLM自有知识与外部IR，导致查询困难时IR决策可能无关。

Method: 提出CCSK，使用Siamese Network计算查询历史相似性，Response Quality Model基于LightGBM评估响应，并通过多头注意力机制融合特征进行动态联合决策。

Result: 在真实数据集上的广泛实验显示，CCSK显著提升了模型的信息检索有效性。

Conclusion: CCSK通过动态机制缓解了IR决策无关问题，证明了其在提升LLM Q&A性能方面的有效性。

Abstract: The performance of large language models (LLMs) in Q&A task increased
substantially through Retrieval-Augmented Generation (RAG) which brings in
external knowledge. However, the main difficulty lies in balancing the inherent
self-knowledge of LLMs with external information retrieval (IR). The current
threshold-based methods apply one-dimensional static mechanisms with single
criterion. As a result, their IR decisions might be irrelevant to the LLMs'
response under difficult queries. To alleviate this problem, we propose
Cognitive Convection of Self-Knowledge (CCSK). Different from traditional
methods that maintain single fixed IR activation criteria, CCSK implements a
dynamic joint decision process via a Siamese Network module and a Response
Quality Model. The Siamese Network calculates the cosine similarity between the
current query and the historical queries. The Response Quality Model evaluates
the responses of LLMs through LightGBM. The final decision of the CCSK is
derived from the outputs of the two modules, as well as text features fused
using a multi-head attention mechanism. Extensive experiments on real-world
datasets show that CCSK significantly enhances the model's effectiveness in
information retrieval.

</details>

### [156] [Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems](https://arxiv.org/abs/2504.10500)
*Eya Mhedhbi,Youssef Mourchid,Alice Othmani*

Main category: cs.IR

TLDR: 本论文提出了一种结合生成式自监督学习和残差图变换器的创新方法来提升推荐系统性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了通过更好的数据增强和用户物品交互学习来改进推荐系统。

Method: 整合生成式自监督学习与残差图变换器，使用理性aware SSL进行自动化预训练任务，结合拓扑aware 变换器和残差连接进行图表示学习，并通过自动蒸馏过程提炼自监督信号。

Result: 在多个数据集上的实验评估中，该方法 consistently 优于基线方法。

Conclusion: 该方法有效提升了推荐系统的性能，并展示了自监督学习在该领域的潜力。

Abstract: This paper introduces a cutting-edge method for enhancing recommender systems
through the integration of generative self-supervised learning (SSL) with a
Residual Graph Transformer. Our approach emphasizes the importance of superior
data enhancement through the use of pertinent pretext tasks, automated through
rationale-aware SSL to distill clear ways of how users and items interact. The
Residual Graph Transformer incorporates a topology-aware transformer for global
context and employs residual connections to improve graph representation
learning. Additionally, an auto-distillation process refines self-supervised
signals to uncover consistent collaborative rationales. Experimental
evaluations on multiple datasets demonstrate that our approach consistently
outperforms baseline methods.

</details>

### [157] [Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents](https://arxiv.org/abs/2504.10508)
*João Alberto de Oliveira Lima*

Main category: cs.IR

TLDR: 这篇论文提出Poly-Vector Retrieval方法，通过为法律条款分配多个嵌入向量来提升RAG在处理标签和交叉引用时的性能。


<details>
  <summary>Details</summary>
Motivation: 法律领域的RAG系统难以处理用户基于标签的查询和文本中的显式交叉引用。

Method: Poly-Vector Retrieval方法，为每个法律条款创建多个嵌入：一个捕捉内容，一个捕捉标签，并可选地捕捉其他名称。

Result: 在巴西联邦宪法实验中，该方法显著提高了标签导向查询的检索准确性，同时不影响纯语义查询的性能。

Conclusion: 讨论了分离引用和内容的哲学及实际含义，并提出未来应用于更广泛数据集的潜力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for
generating contextually accurate answers by integrating Large Language Models
(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently
reference norms by their labels or nicknames (e.g., Article 5 of the
Constitution or Consumer Defense Code (CDC)), rather than by their content,
posing challenges for traditional RAG approaches that rely solely on semantic
embeddings of text. Furthermore, legal texts themselves heavily rely on
explicit cross-references (e.g., "pursuant to Article 34") that function as
pointers. Both scenarios pose challenges for traditional RAG approaches that
rely solely on semantic embeddings of text, often failing to retrieve the
necessary referenced content. This paper introduces Poly-Vector Retrieval, a
method assigning multiple distinct embeddings to each legal provision: one
embedding captures the content (the full text), another captures the label (the
identifier or proper name), and optionally additional embeddings capture
alternative denominations. Inspired by Frege's distinction between Sense and
Reference, this poly-vector retrieval approach treats labels, identifiers and
reference markers as rigid designators and content embeddings as carriers of
semantic substance. Experiments on the Brazilian Federal Constitution
demonstrate that Poly-Vector Retrieval significantly improves retrieval
accuracy for label-centric queries and potential to resolve internal and
external cross-references, without compromising performance on purely semantic
queries. The study discusses philosophical and practical implications of
explicitly separating reference from content in vector embeddings and proposes
future research directions for applying this approach to broader legal datasets
and other domains characterized by explicit reference identifiers.

</details>

### [158] [Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion](https://arxiv.org/abs/2504.10509)
*Jakub Podolak,Leon Peric,Mina Janicijevic,Roxana Petcu*

Main category: cs.IR

TLDR: 本研究再现并扩展Setwise提示方法，用于LLM的零样本排名，引入Setwise Insertion以提高效率，实验显示查询时间减少31%、模型推理减少23%，并略微提升效果。


<details>
  <summary>Details</summary>
Motivation: 动机是验证Setwise方法的有效性和效率，并通过新方法减少计算开销以平衡性能。

Method: 方法包括再现原Setwise方法、提出Setwise Insertion（利用初始排名作为先验知识）、并在Flan-T5、Vicuna和Llama模型上进行实验。

Result: 结果显示Setwise Insertion比原方法查询时间减少31%、模型推理减少23%，并略微提高了重新排名的有效性。

Conclusion: 结论是，融入先验排名知识能提升Setwise方法的实用性，使零样本文档排名更高效准确。

Abstract: This study presents a comprehensive reproducibility and extension analysis of
the Setwise prompting methodology for zero-shot ranking with Large Language
Models (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and
efficiency compared to traditional Pointwise, Pairwise, and Listwise approaches
in document ranking tasks. Our reproduction confirms the findings of Zhuang et
al., highlighting the trade-offs between computational efficiency and ranking
effectiveness in Setwise methods. Building on these insights, we introduce
Setwise Insertion, a novel approach that leverages the initial document ranking
as prior knowledge, reducing unnecessary comparisons and uncertainty by
focusing on candidates more likely to improve the ranking results. Experimental
results across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show
that Setwise Insertion yields a 31% reduction in query time, a 23% reduction in
model inferences, and a slight improvement in reranking effectiveness compared
to the original Setwise method. These findings highlight the practical
advantage of incorporating prior ranking knowledge into Setwise prompting for
efficient and accurate zero-shot document reranking.

</details>

### [159] [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
*Minh-Anh Nguyen,Dung D. Le*

Main category: cs.IR

TLDR: 提出JEPA4Rec框架，通过结合联合嵌入预测架构和语言建模，提高序列推荐性能，解决数据稀疏和常识偏好问题。


<details>
  <summary>Details</summary>
Motivation: 语言表示学习在序列推荐中虽有优势，但面临数据稀疏和对用户常识偏好的有限理解。

Method: JEPA4Rec将项目表示为文本句子，使用双向Transformer编码器、掩码预测策略，并采用两阶段自监督学习。

Result: 在六个真实数据集上实验，JEPA4Rec在跨域、跨平台和低资源场景中优于最先进方法。

Conclusion: 该框架提升推荐性能，减少对大规模预训练数据的依赖。

Abstract: Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.

</details>

### [160] [Exploring Generative AI Techniques in Government: A Case Study](https://arxiv.org/abs/2504.10497)
*Sunyi Liu,Mengzhe Geng,Rebecca Hart*

Main category: cs.IR

TLDR: 這篇論文介紹了Pubbie智能代理的開發，使用生成式AI技術自動化加拿大國家研究委員會的性能測量、數據管理和洞察報告，提高效率。


<details>
  <summary>Details</summary>
Motivation: 由於生成式AI和大型語言模型的快速發展，加拿大國家研究委員會啟動試點項目，探索將GenAI整合到日常運營中以提升性能。

Method: 探索LLM編排、RoBERTa語義嵌入、策略微調和少樣本學習，以低成本注入領域知識。提供用戶友好界面，支持自然語言查詢和簡單文件上傳下載。

Result: 減少了手動努力和可訪問性障礙，提高了效率和自動化水平。

Conclusion: Pubbie的開發展示了GenAI在政府機構中的實際應用潛力，有助於提升性能和操作簡便性。

Abstract: The swift progress of Generative Artificial intelligence (GenAI), notably
Large Language Models (LLMs), is reshaping the digital landscape. Recognizing
this transformative potential, the National Research Council of Canada (NRC)
launched a pilot initiative to explore the integration of GenAI techniques into
its daily operation for performance excellence, where 22 projects were launched
in May 2024. Within these projects, this paper presents the development of the
intelligent agent Pubbie as a case study, targeting the automation of
performance measurement, data management and insight reporting at the NRC.
Cutting-edge techniques are explored, including LLM orchestration and semantic
embedding via RoBERTa, while strategic fine-tuning and few-shot learning
approaches are incorporated to infuse domain knowledge at an affordable cost.
The user-friendly interface of Pubbie allows general government users to input
queries in natural language and easily upload or download files with a simple
button click, greatly reducing manual efforts and accessibility barriers.

</details>

### [161] [ArxivBench: Can LLMs Assist Researchers in Conducting Research?](https://arxiv.org/abs/2504.10496)
*Ning Li,Jingran Zhang,Justin Cui*

Main category: cs.IR

TLDR: 本研究评估大型语言模型在提供arXiv相关论文和准确链接方面的性能，引入arXivBench基准，发现模型准确性因主题而异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成响应中存在事实错误问题，需要评估其在学术环境中的可靠性。

Method: 引入arXivBench基准，评估专有和开源LLM在八个arXiv主要主题类别和计算机科学五个子领域中的性能。

Result: 模型准确性因主题不同而变化，Claude-3.5-Sonnet表现最佳，在人工智能子领域准确性更高。

Conclusion: arXivBench提供标准化工具，提升LLM在研究环境中的可靠性，并开源代码和数据集。

Abstract: Large language models (LLMs) have demonstrated remarkable effectiveness in
completing various tasks such as reasoning, translation, and question
answering. However the issue of factual incorrect content in LLM-generated
responses remains a persistent challenge. In this study, we evaluate both
proprietary and open-source LLMs on their ability to respond with relevant
research papers and accurate links to articles hosted on the arXiv platform,
based on high level prompts. To facilitate this evaluation, we introduce
arXivBench, a benchmark specifically designed to assess LLM performance across
eight major subject categories on arXiv and five subfields within computer
science, one of the most popular categories among them. Our findings reveal a
concerning accuracy of LLM-generated responses depending on the subject, with
some subjects experiencing significantly lower accuracy than others. Notably,
Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant
and accurate responses. And interestingly, most LLMs achieve a much higher
accuracy in the Artificial Intelligence sub-field than other sub-fields. This
benchmark provides a standardized tool for evaluating the reliability of
LLM-generated scientific responses, promoting more dependable use of LLMs in
academic and research environments. Our code is open-sourced at
https://github.com/arxivBenchLLM/arXivBench and our dataset is available on
huggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.

</details>

### [162] [CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models](https://arxiv.org/abs/2504.10498)
*Jianling Lu,Mingqi Lv*

Main category: cs.IR

TLDR: 本文提出CCSK方法，通过动态决策改善LLM在Q&A任务中的外部知识检索，实验显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在平衡LLM自有知识和外部检索时存在困难，阈值-based方法可能导致无关决策。

Method: 提出CCSK，使用Siamese Network计算查询相似度，Response Quality Model基于LightGBM评估响应，并用多头注意力机制融合特征进行动态决策。

Result: 在真实数据集上的广泛实验显示CCSK显著提升信息检索的有效性。

Conclusion: CCSK方法有效地缓解了RAG中的问题，提高了模型性能。

Abstract: The performance of large language models (LLMs) in Q&A task increased
substantially through Retrieval-Augmented Generation (RAG) which brings in
external knowledge. However, the main difficulty lies in balancing the inherent
self-knowledge of LLMs with external information retrieval (IR). The current
threshold-based methods apply one-dimensional static mechanisms with single
criterion. As a result, their IR decisions might be irrelevant to the LLMs'
response under difficult queries. To alleviate this problem, we propose
Cognitive Convection of Self-Knowledge (CCSK). Different from traditional
methods that maintain single fixed IR activation criteria, CCSK implements a
dynamic joint decision process via a Siamese Network module and a Response
Quality Model. The Siamese Network calculates the cosine similarity between the
current query and the historical queries. The Response Quality Model evaluates
the responses of LLMs through LightGBM. The final decision of the CCSK is
derived from the outputs of the two modules, as well as text features fused
using a multi-head attention mechanism. Extensive experiments on real-world
datasets show that CCSK significantly enhances the model's effectiveness in
information retrieval.

</details>

### [163] [Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems](https://arxiv.org/abs/2504.10500)
*Eya Mhedhbi,Youssef Mourchid,Alice Othmani*

Main category: cs.IR

TLDR: 本文提出了一种结合生成式自监督学习和残差图变换器的创新方法来提升推荐系统性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 强调通过合理的自监督学习改善数据增强，以更好地理解用户与物品的交互。

Method: 将生成式自监督学习与残差图变换器整合，包含拓扑感知变换器、残差连接和自动蒸馏过程来提炼自监督信号。

Result: 在多个数据集上的实验评估中， consistently 优于基线方法。

Conclusion: 该方法通过先进的图表示学习技术有效地提升了推荐系统的性能。

Abstract: This paper introduces a cutting-edge method for enhancing recommender systems
through the integration of generative self-supervised learning (SSL) with a
Residual Graph Transformer. Our approach emphasizes the importance of superior
data enhancement through the use of pertinent pretext tasks, automated through
rationale-aware SSL to distill clear ways of how users and items interact. The
Residual Graph Transformer incorporates a topology-aware transformer for global
context and employs residual connections to improve graph representation
learning. Additionally, an auto-distillation process refines self-supervised
signals to uncover consistent collaborative rationales. Experimental
evaluations on multiple datasets demonstrate that our approach consistently
outperforms baseline methods.

</details>

### [164] [Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents](https://arxiv.org/abs/2504.10508)
*João Alberto de Oliveira Lima*

Main category: cs.IR

TLDR: 本文提出Poly-Vector Retrieval方法，通过为法律条款分配多个嵌入（内容、标签等），改善RAG系统在法律上下文中的检索准确性。


<details>
  <summary>Details</summary>
Motivation: 法律查询中，用户常使用标签或昵称引用规范，传统RAG依赖语义嵌入，无法有效处理交叉引用。

Method: 引入Poly-Vector Retrieval，为每个法律条款分配不同嵌入：一个捕获内容，一个捕获标签，可选捕获替代名称，受Frege的Sense and Reference启发。

Result: 实验在巴西联邦宪法上显示，显著提高了标签中心查询的检索准确性，并能处理交叉引用，而不影响纯语义查询。

Conclusion: 讨论了分离参考和内容的哲学及实际含义，并提出未来应用于更广泛法律数据集和其他领域的方向。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for
generating contextually accurate answers by integrating Large Language Models
(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently
reference norms by their labels or nicknames (e.g., Article 5 of the
Constitution or Consumer Defense Code (CDC)), rather than by their content,
posing challenges for traditional RAG approaches that rely solely on semantic
embeddings of text. Furthermore, legal texts themselves heavily rely on
explicit cross-references (e.g., "pursuant to Article 34") that function as
pointers. Both scenarios pose challenges for traditional RAG approaches that
rely solely on semantic embeddings of text, often failing to retrieve the
necessary referenced content. This paper introduces Poly-Vector Retrieval, a
method assigning multiple distinct embeddings to each legal provision: one
embedding captures the content (the full text), another captures the label (the
identifier or proper name), and optionally additional embeddings capture
alternative denominations. Inspired by Frege's distinction between Sense and
Reference, this poly-vector retrieval approach treats labels, identifiers and
reference markers as rigid designators and content embeddings as carriers of
semantic substance. Experiments on the Brazilian Federal Constitution
demonstrate that Poly-Vector Retrieval significantly improves retrieval
accuracy for label-centric queries and potential to resolve internal and
external cross-references, without compromising performance on purely semantic
queries. The study discusses philosophical and practical implications of
explicitly separating reference from content in vector embeddings and proposes
future research directions for applying this approach to broader legal datasets
and other domains characterized by explicit reference identifiers.

</details>

### [165] [Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion](https://arxiv.org/abs/2504.10509)
*Jakub Podolak,Leon Peric,Mina Janicijevic,Roxana Petcu*

Main category: cs.IR

TLDR: 本研究复现并扩展Setwise提示方法，用于LLM的零样本排名，引入Setwise Insertion减少计算并略微提升效果。


<details>
  <summary>Details</summary>
Motivation: 复现Zhuang et al.的Setwise方法，并通过新方法提高效率和排名效果，以解决传统方法的计算权衡问题。

Method: 复现原方法，引入Setwise Insertion，利用初始排名作为先验知识减少比较。

Result: 实验显示Setwise Insertion减少31%查询时间、23%模型推理，并略微提高重新排名效果。

Conclusion: 强调整合先验知识的实际优势，提高零样本文档排名的效率和准确性。

Abstract: This study presents a comprehensive reproducibility and extension analysis of
the Setwise prompting methodology for zero-shot ranking with Large Language
Models (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and
efficiency compared to traditional Pointwise, Pairwise, and Listwise approaches
in document ranking tasks. Our reproduction confirms the findings of Zhuang et
al., highlighting the trade-offs between computational efficiency and ranking
effectiveness in Setwise methods. Building on these insights, we introduce
Setwise Insertion, a novel approach that leverages the initial document ranking
as prior knowledge, reducing unnecessary comparisons and uncertainty by
focusing on candidates more likely to improve the ranking results. Experimental
results across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show
that Setwise Insertion yields a 31% reduction in query time, a 23% reduction in
model inferences, and a slight improvement in reranking effectiveness compared
to the original Setwise method. These findings highlight the practical
advantage of incorporating prior ranking knowledge into Setwise prompting for
efficient and accurate zero-shot document reranking.

</details>

### [166] [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
*Minh-Anh Nguyen,Dung D. Le*

Main category: cs.IR

TLDR: JEPA4Rec 是一种结合联合嵌入预测架构和语言建模的框架，用于解决序列推荐中的数据稀疏性和用户偏好问题，提高性能并减少对大规模数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 语言表示学习在数据稀疏性和对常识用户偏好的有限理解上存在挑战，因此提出 JEPA4Rec 来提升泛化能力和推荐效果。

Method: JEPA4Rec 将物品表示为文本句子，使用双向 Transformer 编码器、掩码策略和两阶段自监督学习训练，以学习可转移的物品嵌入。

Result: 在六个真实数据集上的实验显示，JEPA4Rec 优于最先进方法，尤其在跨域、跨平台和低资源场景中。

Conclusion: JEPA4Rec 通过捕获语义丰富的表示，改善推荐性能，并减少对大规模预训练数据的依赖，具有良好的泛化能力。

Abstract: Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.

</details>

### [167] [HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations](https://arxiv.org/abs/2504.10529)
*Peiru Yang,Xintian Li,Zhiyang Hu,Jiapeng Wang,Jinhua Yin,Huili Wang,Lizhi He,Shuai Yang,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.IR

TLDR: 本文提出了一种异构RAG框架，通过为检索和生成使用不同的知识块表示来提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法使用相同的知识块表示，导致性能不佳，因为检索需要全面信息，而生成过程会因冗余信息而效率低下。

Method: 提出\myname框架，使用短知识块适应生成步骤，并采用多粒度视图的上下文知识块提升检索准确性；同时引入自适应提示调优方法。

Result: 大量实验显示，\myname相较基线方法有显著改进。

Conclusion: 该框架通过解耦检索和生成表示，提升了LLM的有效性和效率。

Abstract: Retrieval-augmented generation (RAG) methods can enhance the performance of
LLMs by incorporating retrieved knowledge chunks into the generation process.
In general, the retrieval and generation steps usually have different
requirements for these knowledge chunks. The retrieval step benefits from
comprehensive information to improve retrieval accuracy, whereas excessively
long chunks may introduce redundant contextual information, thereby diminishing
both the effectiveness and efficiency of the generation process. However,
existing RAG methods typically employ identical representations of knowledge
chunks for both retrieval and generation, resulting in suboptimal performance.
In this paper, we propose a heterogeneous RAG framework (\myname) that
decouples the representations of knowledge chunks for retrieval and generation,
thereby enhancing the LLMs in both effectiveness and efficiency. Specifically,
we utilize short chunks to represent knowledge to adapt the generation step and
utilize the corresponding chunk with its contextual information from
multi-granular views to enhance retrieval accuracy. We further introduce an
adaptive prompt tuning method for the retrieval model to adapt the
heterogeneous retrieval augmented generation process. Extensive experiments
demonstrate that \myname achieves significant improvements compared to
baselines.

</details>

### [168] [Distilling Transitional Pattern to Large Language Models for Multimodal Session-based Recommendation](https://arxiv.org/abs/2504.10538)
*Jiajie Su,Qiyong Zhong,Yunshan Ma,Weiming Liu,Chaochao Chen,Xiaolin Zheng,Jianwei Yin,Tat-Seng Chua*

Main category: cs.IR

TLDR: 本论文提出TPAD框架，使用多模态LLM增强会话推荐系统，解决数据稀疏和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 为了缓解传统会话推荐中的数据稀疏和冷启动问题，并克服现有多模态方法在语义丰富性上的局限性，利用LLM的语义推理能力。

Method: 提出TPAD框架，包括Knowledge-MLLM和Transfer-MLLM，以及使用互信息估计的过渡模式对齐模块，来解耦和对齐过渡模式，减少分布差异。

Result: 在真实数据集上的广泛实验证明了框架的有效性。

Conclusion: TPAD框架通过LLM增强多模态会话推荐，提高了推荐性能和表示效用。

Abstract: Session-based recommendation (SBR) predicts the next item based on anonymous
sessions. Traditional SBR explores user intents based on ID collaborations or
auxiliary content. To further alleviate data sparsity and cold-start issues,
recent Multimodal SBR (MSBR) methods utilize simplistic pre-trained models for
modality learning but have limitations in semantic richness. Considering
semantic reasoning abilities of Large Language Models (LLM), we focus on the
LLM-enhanced MSBR scenario in this paper, which leverages LLM cognition for
comprehensive multimodal representation generation, to enhance downstream MSBR.
Tackling this problem faces two challenges: i) how to obtain LLM cognition on
both transitional patterns and inherent multimodal knowledge, ii) how to align
both features into one unified LLM, minimize discrepancy while maximizing
representation utility. To this end, we propose a multimodal LLM-enhanced
framework TPAD, which extends a distillation paradigm to decouple and align
transitional patterns for promoting MSBR. TPAD establishes parallel
Knowledge-MLLM and Transfer-MLLM, where the former interprets item
knowledge-reflected features and the latter extracts transition-aware features
underneath sessions. A transitional pattern alignment module harnessing mutual
information estimation theory unites two MLLMs, alleviating distribution
discrepancy and distilling transitional patterns into modal representations.
Extensive experiments on real-world datasets demonstrate the effectiveness of
our framework.

</details>

### [169] [Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation](https://arxiv.org/abs/2504.10541)
*Xu Guo,Tong Zhang,Yuanzhi Wang,Chenxu Wang,Fuyun Wang,Xudong Wang,Xiaoya Zhang,Xin Liu,Zhen Cui*

Main category: cs.IR

TLDR: 本论文提出HeLLM框架，通过融合超图结构和LLM提升多模态推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based方法未能充分探索推荐场景中的多视图图结构相关性，因此需捕捉更高阶语义相关性。

Method: 提出HeLLM框架，包括用户和物品超图、超图卷积、对比学习，以及LLM微调阶段注入图结构嵌入和序列特征。

Result: 实验证明HeLLM优于现有基线，展示了融合超图上下文和序列行为的优势。

Conclusion: HeLLM增强LLM对复杂关系和多模态信息的处理，提高了推荐性能。

Abstract: The burgeoning presence of Large Language Models (LLM) is propelling the
development of personalized recommender systems. Most existing LLM-based
methods fail to sufficiently explore the multi-view graph structure
correlations inherent in recommendation scenarios. To this end, we propose a
novel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation
(HeLLM), designed to equip LLMs with the capability to capture intricate
higher-order semantic correlations by fusing graph-level contextual signals
with sequence-level behavioral patterns. In the recommender pre-training phase,
we design a user hypergraph to uncover shared interest preferences among users
and an item hypergraph to capture correlations within multimodal similarities
among items. The hypergraph convolution and synergistic contrastive learning
mechanism are introduced to enhance the distinguishability of learned
representations. In the LLM fine-tuning phase, we inject the learned
graph-structured embeddings directly into the LLM's architecture and integrate
sequential features capturing each user's chronological behavior. This process
enables hypergraphs to leverage graph-structured information as global context,
enhancing the LLM's ability to perceive complex relational patterns and
integrate multimodal information, while also modeling local temporal dynamics.
Extensive experiments demonstrate the superiority of our proposed method over
state-of-the-art baselines, confirming the advantages of fusing
hypergraph-based context with sequential user behavior in LLMs for
recommendation.

</details>

### [170] [Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning](https://arxiv.org/abs/2504.10753)
*Radin Cheraghi,Amir Mohammad Mahfoozi,Sepehr Zolfaghari,Mohammadshayan Shabani,Maryam Ramezani,Hamid R. Rabiee*

Main category: cs.IR

TLDR: 这篇论文提出了一种新的Bayesian Deep Ensemble Collaborative Filtering方法（BDECF），以解决推荐系统中的过拟合和不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的推荐模型在处理显式反馈和稀疏数据时容易过拟合，且未能纳入认识论不确定性。

Method: 提出BDECF方法，使用Bayesian Neural Networks处理不确定性，引入基于注意力机制的可解释非线性匹配方法，并采用集成模型提高预测鲁棒性。

Result: 通过在真实世界数据集上的实验和消融研究，证实了方法的有效性和组件的重要性。

Conclusion: 该方法提高了模型的泛化能力和预测质量。

Abstract: Recommending items to users has long been a fundamental task, and studies
have tried to improve it ever since. Most well-known models commonly employ
representation learning to map users and items into a unified embedding space
for matching assessment. These approaches have primary limitations, especially
when dealing with explicit feedback and sparse data contexts. Two primary
limitations are their proneness to overfitting and failure to incorporate
epistemic uncertainty in predictions. To address these problems, we propose a
novel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To
improve model generalization and quality, we utilize Bayesian Neural Networks,
which incorporate uncertainty within their weight parameters. In addition, we
introduce a new interpretable non-linear matching approach for the user and
item embeddings, leveraging the advantages of the attention mechanism.
Furthermore, we endorse the implementation of an ensemble-based supermodel to
generate more robust and reliable predictions, resulting in a more complete
model. Empirical evaluation through extensive experiments and ablation studies
across a range of publicly accessible real-world datasets with differing
sparsity characteristics confirms our proposed method's effectiveness and the
importance of its components.

</details>

### [171] [PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems](https://arxiv.org/abs/2504.10507)
*Anirudhan Badrinath,Prabhat Agarwal,Laksh Bhasin,Jaewon Yang,Jiajing Xu,Charles Rosenberg*

Main category: cs.IR

TLDR: 这篇论文介绍了PinRec，一种用于Pinterest的生成式检索模型，通过结果条件生成和多标记生成，提高了可伸缩性、灵活性和指标平衡。


<details>
  <summary>Details</summary>
Motivation: 当前生成式检索方法缺乏工业级可伸缩性和灵活性，无法满足现代推荐系统的多指标需求。

Method: 引入PinRec，使用结果条件生成来平衡指标（如保存和点击次数），并采用多标记生成优化输出多样性。

Result: 实验证明PinRec在性能、多样性和效率间取得平衡，对用户产生积极影响，是首个在Pinterest规模上实施生成式检索的严谨研究。

Conclusion: 这标志着生成式检索领域的一个重要里程碑，展示了其在工业应用中的潜力。

Abstract: Generative retrieval methods utilize generative sequential modeling
techniques, such as transformers, to generate candidate items for recommender
systems. These methods have demonstrated promising results in academic
benchmarks, surpassing traditional retrieval models like two-tower
architectures. However, current generative retrieval methods lack the
scalability required for industrial recommender systems, and they are
insufficiently flexible to satisfy the multiple metric requirements of modern
systems.
  This paper introduces PinRec, a novel generative retrieval model developed
for applications at Pinterest. PinRec utilizes outcome-conditioned generation,
enabling modelers to specify how to balance various outcome metrics, such as
the number of saves and clicks, to effectively align with business goals and
user exploration. Additionally, PinRec incorporates multi-token generation to
enhance output diversity while optimizing generation. Our experiments
demonstrate that PinRec can successfully balance performance, diversity, and
efficiency, delivering a significant positive impact to users using generative
models. This paper marks a significant milestone in generative retrieval, as it
presents, to our knowledge, the first rigorous study on implementing generative
retrieval at the scale of Pinterest.

</details>

### [172] [Integrating Textual Embeddings from Contrastive Learning with Generative Recommender for Enhanced Personalization](https://arxiv.org/abs/2504.10545)
*Yijun Liu*

Main category: cs.IR

TLDR: 本论文提出了一种结合HSTU生成推荐器和BLaIR对比文本嵌入模型的混合框架，提高了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 最近推荐系统的进展突出了生成模型和预训练语言模型的互补优势，因此本研究旨在整合这些优势以提升推荐效果。

Method: 方法是将BLaIR对比文本嵌入模型增强到HSTU中，并在Amazon Reviews 2023数据集的两个领域上评估，与原HSTU和OpenAI的text-embedding-3-large模型比较。

Result: 结果显示，BLaIR增强的方法在计算效率高的设置中取得了比原HSTU和OpenAI模型更好的性能，尽管BLaIR是基于领域特定数据预训练的轻量级模型。

Conclusion: 这突出了对比文本嵌入在计算资源有限环境中的有效性。

Abstract: Recent advances in recommender systems have highlighted the complementary
strengths of generative modeling and pretrained language models. We propose a
hybrid framework that augments the Hierarchical Sequential Transduction Unit
(HSTU) generative recommender with BLaIR -- a contrastive text embedding model.
This integration enriches item representations with semantic signals from
textual metadata while preserving HSTU's powerful sequence modeling
capabilities.
  We evaluate our method on two domains from the Amazon Reviews 2023 dataset,
comparing it against the original HSTU and a variant that incorporates
embeddings from OpenAI's state-of-the-art text-embedding-3-large model. While
the OpenAI embedding model is likely trained on a substantially larger corpus
with significantly more parameters, our lightweight BLaIR-enhanced approach --
pretrained on domain-specific data -- consistently achieves better performance,
highlighting the effectiveness of contrastive text embeddings in
compute-efficient settings.

</details>

### [173] [Document Quality Scoring for Web Crawling](https://arxiv.org/abs/2504.11011)
*Francesca Pezzuti,Ariane Mueller,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TLDR: 本论文扩展神经质量评分方法应用于网络爬虫优先级设置，提高搜索效果，并提供Docker容器工具。


<details>
  <summary>Details</summary>
Motivation: 互联网存在大量低质量内容，浪费搜索引擎资源，因此需要高效的质量估计方法来优化检索和爬取过程。

Method: 基于Chang et al.的工作，扩展神经质量估计器用于爬虫优先级任务中评估网页语义质量。

Result: 实验显示，优先处理高质量页面可提升搜索效果；贡献了一个计算网页质量得分的Docker容器。

Conclusion: 质量评分方法可显著改善搜索引擎的索引修剪、层级化和爬取等过程。

Abstract: The internet contains large amounts of low-quality content, yet users expect
web search engines to deliver high-quality, relevant results. The abundant
presence of low-quality pages can negatively impact retrieval and crawling
processes by wasting resources on these documents. Therefore, search engines
can greatly benefit from techniques that leverage efficient quality estimation
methods to mitigate these negative impacts. Quality scoring methods for web
pages are useful for many processes typical for web search systems, including
static index pruning, index tiering, and crawling. Building on work by Chang et
al.~\cite{chang2024neural}, who proposed using neural estimators of semantic
quality for static index pruning, we extend their approach and apply their
neural quality scorers to assess the semantic quality of web pages in crawling
prioritisation tasks. In our experimental analysis, we found that prioritising
semantically high-quality pages over low-quality ones can improve downstream
search effectiveness. Our software contribution consists of a Docker container
that computes an effective quality score for a given web page, allowing the
quality scorer to be easily included and used in other components of web search
systems.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [174] [Progressive Rock Music Classification](https://arxiv.org/abs/2504.10821)
*Arpan Nagar,Joseph Bensabat,Jokent Gaza,Moinak Dey*

Main category: cs.SD

TLDR: 本研究使用各种机器学习技术对前卫摇滚音乐进行分类，最高准确率达76.38%。


<details>
  <summary>Details</summary>
Motivation: 解决音乐信息检索中前卫摇滚音乐分类的挑战，该流派具有复杂作曲和多样乐器。

Method: 提取音频特征（如频谱图、MFCC等），采用投票策略和机器学习方法（集成如Bagging、Boosting和深度学习如1D CNN、AST），并用PCA降维。

Result: Extra Trees等模型测试准确率达76.38%，其他模型性能各异。

Conclusion: 为前卫摇滚音乐分类任务提供机器学习方法的应用洞察和性能比较。

Abstract: This study investigates the classification of progressive rock music, a genre
characterized by complex compositions and diverse instrumentation, distinct
from other musical styles. Addressing this Music Information Retrieval (MIR)
task, we extracted comprehensive audio features, including spectrograms,
Mel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions
from song snippets using the Librosa library. A winner-take-all voting strategy
was employed to aggregate snippet-level predictions into final song
classifications. We conducted a comparative analysis of various machine
learning techniques. Ensemble methods, encompassing Bagging (Random Forest,
ExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were
explored, utilizing Principal Component Analysis (PCA) for dimensionality
reduction to manage computational constraints with high-dimensional feature
sets. Additionally, deep learning approaches were investigated, including the
development of custom 1D Convolutional Neural Network (1D CNN) architectures
(named "Zuck" and "Satya") featuring specific layer configurations,
normalization, and activation functions. Furthermore, we fine-tuned a
state-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its
attention-based mechanisms for audio classification. Performance evaluation on
validation and test sets revealed varying effectiveness across models, with
ensemble methods like Extra Trees achieving test accuracies up to 76.38%. This
research provides insights into the application and relative performance of
diverse machine learning paradigms for the nuanced task of progressive rock
genre classification.

</details>

### [175] [SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures](https://arxiv.org/abs/2504.10793)
*Kuang Yuan,Yifeng Wang,Xiyuxing Zhang,Chengyi Shen,Swarun Kumar,Justin Chan*

Main category: cs.SD

TLDR: SonicSieve是一种智能定向语音提取系统，使用生物启发声学微结构和神经网络，在智能手机上实时提升音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决在嘈杂环境（如餐厅或礼堂）中清晰捕获特定方向声音的需求。

Method: 采用被动设计的声学微结构嵌入定向线索，并结合端到端神经网络处理原始音频混合，仅需两个麦克风。

Result: 在30度角区域信号质量改善5.0 dB，且性能优于传统五麦克风阵列。

Conclusion: SonicSieve提供高效、低成本的语音提取方案，适用于移动设备场景。

Abstract: Imagine placing your smartphone on a table in a noisy restaurant and clearly
capturing the voices of friends seated around you, or recording a lecturer's
voice with clarity in a reverberant auditorium. We introduce SonicSieve, the
first intelligent directional speech extraction system for smartphones using a
bio-inspired acoustic microstructure. Our passive design embeds directional
cues onto incoming speech without any additional electronics. It attaches to
the in-line mic of low-cost wired earphones which can be attached to
smartphones. We present an end-to-end neural network that processes the raw
audio mixtures in real-time on mobile devices. Our results show that SonicSieve
achieves a signal quality improvement of 5.0 dB when focusing on a 30{\deg}
angular region. Additionally, the performance of our system based on only two
microphones exceeds that of conventional 5-microphone arrays.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [176] [Focal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition](https://arxiv.org/abs/1906.05682)
*Suraj Tripathi,Abhay Kumar,Abhiram Ramesh,Chirag Singh,Promod Yenigalla*

Main category: eess.AS

TLDR: 本论文提出使用ResNet神经网络结合语音特征和Focal Loss来识别语音情感。


<details>
  <summary>Details</summary>
Motivation: 语音特征如Spectrogram和MFCCs比纯文本更能表征情感，Focal Loss帮助训练关注难例。

Method: 基于ResNet构建模型，使用语音特征和Focal Loss训练。

Result: 改进了情感识别性能，特别是对难例的处理。

Conclusion: 这种方法提升了语音情感识别的准确性和鲁棒性。

Abstract: This paper proposes a Residual Convolutional Neural Network (ResNet) based on
speech features and trained under Focal Loss to recognize emotion in speech.
Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients
(MFCCs) have shown the ability to characterize emotion better than just plain
text. Further Focal Loss, first used in One-Stage Object Detectors, has shown
the ability to focus the training process more towards hard-examples and
down-weight the loss assigned to well-classified examples, thus preventing the
model from being overwhelmed by easily classifiable examples.

</details>

### [177] [Focal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition](https://arxiv.org/abs/1906.05682)
*Suraj Tripathi,Abhay Kumar,Abhiram Ramesh,Chirag Singh,Promod Yenigalla*

Main category: eess.AS

TLDR: 本论文提出了一种基于ResNet的语音情感识别方法，使用语音特征（如Spectrogram和MFCC）并采用Focal Loss训练。


<details>
  <summary>Details</summary>
Motivation: 语音特征比纯文本更好地表征情感，Focal Loss能关注难例样本，防止模型被易分类样本主导。

Method: 提出ResNet模型，使用Spectrogram和MFCC特征，训练时应用Focal Loss。

Result: 改进了情感识别性能，特别是对难例的处理。

Conclusion: 这种方法提升了语音情感识别的准确性和鲁棒性。

Abstract: This paper proposes a Residual Convolutional Neural Network (ResNet) based on
speech features and trained under Focal Loss to recognize emotion in speech.
Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients
(MFCCs) have shown the ability to characterize emotion better than just plain
text. Further Focal Loss, first used in One-Stage Object Detectors, has shown
the ability to focus the training process more towards hard-examples and
down-weight the loss assigned to well-classified examples, thus preventing the
model from being overwhelmed by easily classifiable examples.

</details>

### [178] [Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning](https://arxiv.org/abs/2504.11246)
*Davoud Shariat Panah,Alessandro N Franciosi,Cormac McCarthy,Andrew Hines*

Main category: eess.AS

TLDR: 本研究使用wav2vec 2.0模型改进吸入器声音分类，提高哮喘药物依从性监测，准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 哮喘患者吸入器使用依从性低，现有的分类模型无法泛化到不同吸入器类型。

Method: 通过预训练和微调wav2vec 2.0模型来处理吸入器声音数据。

Result: 模型在干粉吸入器数据集上达到98%的平衡准确率，并证明了使用少量数据重新微调的适应性。

Conclusion: 智能手表结合机器学习有潜力用于个性化吸入器依从性监测，这是该领域的首次研究。

Abstract: Asthma is a chronic respiratory condition that affects millions of people
worldwide. While this condition can be managed by administering controller
medications through handheld inhalers, clinical studies have shown low
adherence to the correct inhaler usage technique. Consequently, many patients
may not receive the full benefit of their medication. Automated classification
of inhaler sounds has recently been studied to assess medication adherence.
However, the existing classification models were typically trained using data
from specific inhaler types, and their ability to generalize to sounds from
different inhalers remains unexplored. In this study, we adapted the wav2vec
2.0 self-supervised learning model for inhaler sound classification by
pre-training and fine-tuning this model on inhaler sounds. The proposed model
shows a balanced accuracy of 98% on a dataset collected using a dry powder
inhaler and smartwatch device. The results also demonstrate that re-finetuning
this model on minimal data from a target inhaler is a promising approach to
adapting a generic inhaler sound classification model to a different inhaler
device and audio capture hardware. This is the first study in the field to
demonstrate the potential of smartwatches as assistive technologies for the
personalized monitoring of inhaler adherence using machine learning models.

</details>

<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [179] [Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance](https://arxiv.org/abs/2504.11299)
*Peter Matthew Jacobs,Foad Namjoo,Jeff M. Phillips*

Main category: stat.CO

TLDR: 本论文重新审视多维Kolmogorov-Smirnov距离的扩展，提出新公式并证明其收敛性、计算效率和假设检验。


<details>
  <summary>Details</summary>
Motivation: 探讨正确扩展Kolmogorov-Smirnov距离到多维空间的适当方法。

Method: 提出最大化正交主导矩形范围差的积分概率度量，证明距离收敛、界定收敛率、在4维内实现高效计算，并导出一个假设检验。

Result: 证明分布与样本距离随样本大小收敛到0，给出收敛率界；实现了高效计算和假设检验；其他变体不具备这些性质。

Conclusion: 该方法提供了一个合适的多维扩展，具有良好的度量和近似性能。

Abstract: We revisit extending the Kolmogorov-Smirnov distance between probability
distributions to the multidimensional setting and make new arguments about the
proper way to approach this generalization. Our proposed formulation maximizes
the difference over orthogonal dominating rectangular ranges (d-sided
rectangles in R^d), and is an integral probability metric. We also prove that
the distance between a distribution and a sample from the distribution
converges to 0 as the sample size grows, and bound this rate. Moreover, we show
that one can, up to this same approximation error, compute the distance
efficiently in 4 or fewer dimensions; specifically the runtime is near-linear
in the size of the sample needed for that error. With this, we derive a
delta-precision two-sample hypothesis test using this distance. Finally, we
show these metric and approximation properties do not hold for other popular
variants.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [180] [EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)](https://arxiv.org/abs/2504.09861)
*Luyao Zhang*

Main category: cs.CY

TLDR: EthosGPT是一个开源框架，通过映射和评估大型语言模型（LLM）在全球人类价值观中的表现，应对LLM导致价值同质化的风险，促进价值多样性。


<details>
  <summary>Details</summary>
Motivation: 论文动机是解决LLM可能使人类价值观同质化的风险，类似于生物多样性丧失对生态韧性的威胁，强调价值多样性对社会信任、机构合法性和长期繁荣的重要性，并借鉴古希腊以太概念和联合国可持续发展目标。

Method: 使用国际调查数据、基于提示的评估和比较统计分析来映射和评估LLM在不同地区和文化中的表现。

Result: 揭示LLM的适应性和偏差，提供行动洞见，如多样化训练数据和保护濒危文化遗产，以确保AI系统的代表性。

Conclusion: 通过跨学科合作，促进技术稳健和伦理包容的AI系统，推进价值多元性，以实现可持续和公平的未来，并与联合国可持续发展目标（如减少不平等、文化遗产保护与和平正义）保持一致。

Abstract: Large language models (LLMs) are transforming global decision-making and
societal systems by processing diverse data at unprecedented scales. However,
their potential to homogenize human values poses critical risks, similar to
biodiversity loss undermining ecological resilience. Rooted in the ancient
Greek concept of ethos, meaning both individual character and the shared moral
fabric of communities, EthosGPT draws on a tradition that spans from
Aristotle's virtue ethics to Adam Smith's moral sentiments as the ethical
foundation of economic cooperation. These traditions underscore the vital role
of value diversity in fostering social trust, institutional legitimacy, and
long-term prosperity. EthosGPT addresses the challenge of value homogenization
by introducing an open-source framework for mapping and evaluating LLMs within
a global scale of human values. Using international survey data on cultural
indices, prompt-based assessments, and comparative statistical analyses,
EthosGPT reveals both the adaptability and biases of LLMs across regions and
cultures. It offers actionable insights for developing inclusive LLMs, such as
diversifying training data and preserving endangered cultural heritage to
ensure representation in AI systems. These contributions align with the United
Nations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced
Inequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace,
Justice and Strong Institutions). Through interdisciplinary collaboration,
EthosGPT promotes AI systems that are both technically robust and ethically
inclusive, advancing value plurality as a cornerstone for sustainable and
equitable futures.

</details>

### [181] [EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)](https://arxiv.org/abs/2504.09861)
*Luyao Zhang*

Main category: cs.CY

TLDR: EthosGPT 是一个开源框架，用于评估大型语言模型（LLMs）在全球价值观中的表现，防止价值同质化，促进文化多样性。


<details>
  <summary>Details</summary>
Motivation: LLMs 可能导致人类价值观同质化，类似于生物多样性丧失的风险，论文借鉴历史伦理传统，强调价值多样性对社会信任和繁荣的重要性。

Method: 使用国际调查数据、基于提示的评估和比较统计分析，映射和评估 LLMs 在不同地区和文化中的适应性和偏差。

Result: 揭示 LLMs 的适应性和偏差，提供见解如多样化训练数据和保护文化遗产，以确保 AI 系统的包容性。

Conclusion: 通过跨学科合作，EthosGPT 推动伦理包容的 AI 系统，推进价值多元化，支持联合国可持续发展目标，实现可持续公平未来。

Abstract: Large language models (LLMs) are transforming global decision-making and
societal systems by processing diverse data at unprecedented scales. However,
their potential to homogenize human values poses critical risks, similar to
biodiversity loss undermining ecological resilience. Rooted in the ancient
Greek concept of ethos, meaning both individual character and the shared moral
fabric of communities, EthosGPT draws on a tradition that spans from
Aristotle's virtue ethics to Adam Smith's moral sentiments as the ethical
foundation of economic cooperation. These traditions underscore the vital role
of value diversity in fostering social trust, institutional legitimacy, and
long-term prosperity. EthosGPT addresses the challenge of value homogenization
by introducing an open-source framework for mapping and evaluating LLMs within
a global scale of human values. Using international survey data on cultural
indices, prompt-based assessments, and comparative statistical analyses,
EthosGPT reveals both the adaptability and biases of LLMs across regions and
cultures. It offers actionable insights for developing inclusive LLMs, such as
diversifying training data and preserving endangered cultural heritage to
ensure representation in AI systems. These contributions align with the United
Nations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced
Inequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace,
Justice and Strong Institutions). Through interdisciplinary collaboration,
EthosGPT promotes AI systems that are both technically robust and ethically
inclusive, advancing value plurality as a cornerstone for sustainable and
equitable futures.

</details>

### [182] [Will AI shape the way we speak? The emerging sociolinguistic influence of synthetic voices](https://arxiv.org/abs/2504.10650)
*Éva Székely,Jūra Miniota,Míša,Hejná*

Main category: cs.CY

TLDR: 语音AI的互动特性可能显著影响人类沟通和社会身份，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨语音接口对社会身份和语言模式的影响，特别是在互动场景下。

Method: 建议使用跨学科方法研究语音适应和语言顺应现象。

Result: 论证了语音AI可能 subtlely 塑造公众感知和社会身份。

Conclusion: 呼吁将AI语音的社会指数影响作为研究重点。

Abstract: The growing prevalence of conversational voice interfaces, powered by
developments in both speech and language technologies, raises important
questions about their influence on human communication. While written
communication can signal identity through lexical and stylistic choices,
voice-based interactions inherently amplify socioindexical elements - such as
accent, intonation, and speech style - which more prominently convey social
identity and group affiliation. There is evidence that even passive media such
as television is likely to influence the audience's linguistic patterns. Unlike
passive media, conversational AI is interactive, creating a more immersive and
reciprocal dynamic that holds a greater potential to impact how individuals
speak in everyday interactions. Such heightened influence can be expected to
arise from phenomena such as acoustic-prosodic entrainment and linguistic
accommodation, which occur naturally during interaction and enable users to
adapt their speech patterns in response to the system. While this phenomenon is
still emerging, its potential societal impact could provide organisations,
movements, and brands with a subtle yet powerful avenue for shaping and
controlling public perception and social identity. We argue that the
socioindexical influence of AI-generated speech warrants attention and should
become a focus of interdisciplinary research, leveraging new and existing
methodologies and technologies to better understand its implications.

</details>

### [183] [Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment](https://arxiv.org/abs/2504.10886)
*Jiseon Kim,Jea Kwon,Luiz Felipe Vecchietti,Alice Oh,Meeyoung Cha*

Main category: cs.CY

TLDR: 本研究调查LLM在道德困境中的决策与人类判断的契合度，发现LLM决策受角色影响较大，并在关键任务中变化更剧烈；还观察到政治角色主导决策方向，并讨论部署风险。


<details>
  <summary>Details</summary>
Motivation: 部署具有代理能力的LLM到现实应用中引发了对其行为，尤其是道德决策的担忧。

Method: 使用道德机器实验的各种情境和不同社会人口统计学特征的角色来检查LLM决策与人类判断的契合度。

Result: LLM的道德决策根据角色有显著变化，在关键任务中变化大于人类；存在党派分类现象，其中政治角色主导决策的方向和程度。

Conclusion: 讨论了在涉及道德决策的应用中部署LLM的伦理含义和风险。

Abstract: Deploying large language models (LLMs) with agency in real-world applications
raises critical questions about how these models will behave. In particular,
how will their decisions align with humans when faced with moral dilemmas? This
study examines the alignment between LLM-driven decisions and human judgment in
various contexts of the moral machine experiment, including personas reflecting
different sociodemographics. We find that the moral decisions of LLMs vary
substantially by persona, showing greater shifts in moral decisions for
critical tasks than humans. Our data also indicate an interesting partisan
sorting phenomenon, where political persona predominates the direction and
degree of LLM decisions. We discuss the ethical implications and risks
associated with deploying these models in applications that involve moral
decisions.

</details>

<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [184] [Limits of Discrete Energy of Families of Increasing Sets](https://arxiv.org/abs/2504.11302)
*Hari Nathan*

Main category: math.CA

TLDR: 这篇论文探讨使用Riesz能量和点序列填充来估计集合的Hausdorff维数，并应用于数据科学和Erdős/Falconer问题。


<details>
  <summary>Details</summary>
Motivation: 动机是利用Riesz能量检测Hausdorff维数，并研究离散模拟在填充集合时的界定潜力。

Method: 方法包括分析点序列填充集合E，并使用离散Riesz能量的模拟来调查维数界定。

Result: 结果显示这种方法可用于界定Hausdorff维数，并扩展到数据科学和相关数学问题。

Conclusion: 结论是离散Riesz能量在维数估计中具有实际应用价值。

Abstract: The Hausdorff dimension of a set can be detected using the Riesz energy.
Here, we consider situations where a sequence of points, $\{x_n\}$, ``fills
in'' a set $E \subset \mathbb{R}^d$ in an appropriate sense and investigate the
degree to which the discrete analog to the Riesz energy of these sets can be
used to bound the Hausdorff dimension of $E$. We also discuss applications to
data science and Erd\H{o}s/Falconer type problems.

</details>

<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [185] [LCDC: Bridging Science and Machine Learning for Light Curve Analysis](https://arxiv.org/abs/2504.10550)
*Daniel Kyselica,Tomáš Hrobár,Jiří Šilha,Roman Ďurikovič,Marek Šuppa*

Main category: astro-ph.IM

TLDR: 这篇论文介绍了Light Curve Dataset Creator (LCDC)，一个Python工具包，用于处理空间物体光变曲线数据，包括预处理、分析和机器学习应用，并创建了RoBo6数据集用于火箭体分类。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解人工空间物体的物理和旋转特性，并解决光变曲线分析研究中再现性和可比性不足的问题。

Method: 开发LCDC工具包，支持数据过滤、转换、特征提取，并与公开数据集集成；创建RoBo6标准化数据集，并使用机器学习模型进行基准测试。

Result: 成功创建RoBo6数据集，训练并评估了多个机器学习模型；对Atlas 2AS Centaur和Delta 4火箭体进行了表面特征和旋转动力学分析。

Conclusion: LCDC工具包推进了空间碎片表征研究，促进可持续空间探索，并支持空间碎片社区的AI研究。

Abstract: The characterization and analysis of light curves are vital for understanding
the physical and rotational properties of artificial space objects such as
satellites, rocket stages, and space debris. This paper introduces the Light
Curve Dataset Creator (LCDC), a Python-based toolkit designed to facilitate the
preprocessing, analysis, and machine learning applications of light curve data.
LCDC enables seamless integration with publicly available datasets, such as the
newly introduced Mini Mega Tortora (MMT) database. Moreover, it offers data
filtering, transformation, as well as feature extraction tooling. To
demonstrate the toolkit's capabilities, we created the first standardized
dataset for rocket body classification, RoBo6, which was used to train and
evaluate several benchmark machine learning models, addressing the lack of
reproducibility and comparability in recent studies. Furthermore, the toolkit
enables advanced scientific analyses, such as surface characterization of the
Atlas 2AS Centaur and the rotational dynamics of the Delta 4 rocket body, by
streamlining data preprocessing, feature extraction, and visualization. These
use cases highlight LCDC's potential to advance space debris characterization
and promote sustainable space exploration. Additionally, they highlight the
toolkit's ability to enable AI-focused research within the space debris
community.

</details>

### [186] [Inferring the Hubble Constant Using Simulated Strongly Lensed Supernovae and Neural Network Ensembles](https://arxiv.org/abs/2504.10553)
*Gonçalo Gonçalves,Nikki Arendse,Doogesh Kodi Ramanah,Radosław Wojtak*

Main category: astro-ph.IM

TLDR: 本研究使用模拟的重力透镜Ia型超新星和机器学习训练管道，以实现哈勃常数H0的精确估计；使用100个系统达到4.4%的精度，与真实值一致。


<details>
  <summary>Details</summary>
Motivation: 为了获得哈勃常数H0的独立测量，使用强重力透镜超新星作为新探针。

Method: 模拟重力透镜Ia型超新星的图像时间序列，使用Nancy Grace Roman Space望远镜数据训练五个卷积神经网络的集成，并结合基于模拟的推理框架量化不确定性和推断H0后验分布。

Result: 使用100个模拟系统，获得与真实值一致的H0估计，精度为4.4%。

Conclusion: 突显了机器学习与重力透镜超新星结合的潜力，可实现快速和自动化的H0测量。

Abstract: Strongly lensed supernovae are a promising new probe to obtain independent
measurements of the Hubble constant (${H_0}$). In this work, we employ
simulated gravitationally lensed Type Ia supernovae (glSNe Ia) to train our
machine learning (ML) pipeline to constrain $H_0$. We simulate image
time-series of glSNIa, as observed with the upcoming Nancy Grace Roman Space
Telescope, that we employ for training an ensemble of five convolutional neural
networks (CNNs). The outputs of this ensemble network are combined with a
simulation-based inference (SBI) framework to quantify the uncertainties on the
network predictions and infer full posteriors for the $H_0$ estimates. We
illustrate that the combination of multiple glSN systems enhances constraint
precision, providing a $4.4\%$ estimate of $H_0$ based on 100 simulated
systems, which is in agreement with the ground truth. This research highlights
the potential of leveraging the capabilities of ML with glSNe systems to obtain
a pipeline capable of fast and automated $H_0$ measurements.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [187] [FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation](https://arxiv.org/abs/2504.10564)
*Julian Cremer,Ross Irwin,Alessandro Tibot,Jon Paul Janet,Simon Olsson,Djork-Arné Clevert*

Main category: q-bio.QM

TLDR: FLOWR 是一种新型框架，用于三维配体的生成和优化，集成了流匹配、最优传输和蛋白质口袋条件，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据质量问题，并改进配体生成方法的效率和准确性。

Method: FLOWR 整合连续和分类流匹配、等变最优传输，并使用高效蛋白质口袋条件；引入 SPINDR 数据集作为训练基础。

Result: 在有效性、位姿准确性和交互恢复方面优于现有方法，推理速度提高 70 倍；FLOWR.multi 支持无须重新训练的针对性配体采样。

Conclusion: FLOWR 提供了一个更快、更准确的多功能框架，提升了配体设计应用。

Abstract: We introduce FLOWR, a novel structure-based framework for the generation and
optimization of three-dimensional ligands. FLOWR integrates continuous and
categorical flow matching with equivariant optimal transport, enhanced by an
efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a
thoroughly curated dataset comprising ligand-pocket co-crystal complexes
specifically designed to address existing data quality issues. Empirical
evaluations demonstrate that FLOWR surpasses current state-of-the-art
diffusion- and flow-based methods in terms of PoseBusters-validity, pose
accuracy, and interaction recovery, while offering a significant inference
speedup, achieving up to 70-fold faster performance. In addition, we introduce
FLOWR.multi, a highly accurate multi-purpose model allowing for the targeted
sampling of novel ligands that adhere to predefined interaction profiles and
chemical substructures for fragment-based design without the need of
re-training or any re-sampling strategies

</details>

### [188] [Cryo-em images are intrinsically low dimensional](https://arxiv.org/abs/2504.11249)
*Luke Evans,Octavian-Vlad Murad,Lars Dingeldein,Pilar Cossio,Roberto Covino,Marina Meila*

Main category: q-bio.QM

TLDR: 本研究通过流形学习分析CryoSBI的潜在表示，发现其为低维平滑流形，与物理参数相关联，验证并改进冷冻电子显微镜推断方法。


<details>
  <summary>Details</summary>
Motivation: CryoSBI潜在空间包含物理系统和推断过程的宝贵信息，需要理解其几何结构以充分发挥潜力。

Method: 应用流形学习技术（如Diffusion Maps）到CryoSBI的血凝素模拟和实验数据上，表征流形几何并识别主变化轴。

Result: 数据填充低维平滑流形，模拟数据覆盖实验数据，并建立潜在结构与关键物理参数的直接联系。

Conclusion: 发现的低维性和可解释性验证了CryoSBI方法，并为从数据结构中学习和优化未来推断策略提供机会。

Abstract: Simulation-based inference provides a powerful framework for cryo-electron
microscopy, employing neural networks in methods like CryoSBI to infer
biomolecular conformations via learned latent representations. This latent
space represents a rich opportunity, encoding valuable information about the
physical system and the inference process. Harnessing this potential hinges on
understanding the underlying geometric structure of these representations. We
investigate this structure by applying manifold learning techniques to CryoSBI
representations of hemagglutinin (simulated and experimental). We reveal that
these high-dimensional data inherently populate low-dimensional, smooth
manifolds, with simulated data effectively covering the experimental
counterpart. By characterizing the manifold's geometry using Diffusion Maps and
identifying its principal axes of variation via coordinate interpretation
methods, we establish a direct link between the latent structure and key
physical parameters. Discovering this intrinsic low-dimensionality and
interpretable geometric organization not only validates the CryoSBI approach
but enables us to learn more from the data structure and provides opportunities
for improving future inference strategies by exploiting this revealed manifold
geometry.

</details>

<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [189] [Adaptive Synaptogenesis Implemented on a Nanomagnetic Platform](https://arxiv.org/abs/2504.10767)
*Faiyaz Elahi Mullick,Supriyo Bandyopadhyay,Rob Baxter,Tony J. Ragucci,Avik W. Ghosh*

Main category: cond-mat.dis-nn

TLDR: 本论文探讨人类大脑的adaptive synaptogenesis算法，用于AI避免遗忘和促进终身学习，通过模拟和硬件实现。


<details>
  <summary>Details</summary>
Motivation: 人类大脑与人工神经网络不同，具有adaptive synaptogenesis，能动态调整突触以避免灾难性遗忘和支持终身学习。

Method: 采用supervised Hebbian学习和海马体控制机制，通过模拟演示和设计纳米磁硬件加速器。

Result: 通过模拟验证了算法功能，并设计了针对边缘计算的硬件加速器。

Conclusion: adaptive synaptogenesis算法可提升AI性能，适用于边缘计算场景。

Abstract: The human brain functions very differently from artificial neural networks
(ANN) and possesses unique features that are absent in ANN. An important one
among them is "adaptive synaptogenesis" that modifies synaptic weights when
needed to avoid catastrophic forgetting and promote lifelong learning. The key
aspect of this algorithm is supervised Hebbian learning, where weight
modifications in the neocortex driven by temporal coincidence are further
accepted or vetoed by an added control mechanism from the hippocampus during
the training cycle, to make distant synaptic connections highly sparse and
strategic. In this work, we discuss various algorithmic aspects of adaptive
synaptogenesis tailored to edge computing, demonstrate its function using
simulations, and design nanomagnetic hardware accelerators for specific
functions of synaptogenesis.

</details>

### [190] [Adaptive Synaptogenesis Implemented on a Nanomagnetic Platform](https://arxiv.org/abs/2504.10767)
*Faiyaz Elahi Mullick,Supriyo Bandyopadhyay,Rob Baxter,Tony J. Ragucci,Avik W. Ghosh*

Main category: cond-mat.dis-nn

TLDR: 本文探讨大脑的适应性突触发生机制及其在AI中的应用，包括算法讨论、模拟演示和硬件设计。


<details>
  <summary>Details</summary>
Motivation: 为了解决人工神经网络的灾难性遗忘问题，促进终身学习，模仿大脑的独特功能如适应性突触发生。

Method: 通过讨论算法方面、使用模拟演示功能，并设计纳米磁硬件加速器来实现适应性突触发生。

Result: 模拟展示了机制的有效性，并设计了硬件加速器以支持稀疏和战略性的突触连接。

Conclusion: 这种方法可提升AI系统的学习能力，避免遗忘，并适用于边缘计算场景。

Abstract: The human brain functions very differently from artificial neural networks
(ANN) and possesses unique features that are absent in ANN. An important one
among them is "adaptive synaptogenesis" that modifies synaptic weights when
needed to avoid catastrophic forgetting and promote lifelong learning. The key
aspect of this algorithm is supervised Hebbian learning, where weight
modifications in the neocortex driven by temporal coincidence are further
accepted or vetoed by an added control mechanism from the hippocampus during
the training cycle, to make distant synaptic connections highly sparse and
strategic. In this work, we discuss various algorithmic aspects of adaptive
synaptogenesis tailored to edge computing, demonstrate its function using
simulations, and design nanomagnetic hardware accelerators for specific
functions of synaptogenesis.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [191] [Optimizing Data Distribution and Kernel Performance for Efficient Training of Chemistry Foundation Models: A Case Study with MACE](https://arxiv.org/abs/2504.10700)
*Jesun Firoz,Franco Pellegrini,Mario Geiger,Darren Hsu,Jenna A. Bilbrey,Han-Yi Chou,Maximilian Stadler,Markus Hoehnerbach,Tingyu Wang,Dejun Lin,Emine Kucukbenli,Henry W. Sprueill,Ilyes Batatia,Sotiris S. Xantheas,MalSoon Lee,Chris Mundy,Gabor Csanyi,Justin S. Smith,Ponnuswamy Sadayappan,Sutanay Choudhury*

Main category: cs.DC

TLDR: 这篇论文优化了化学基础模型MACE的训练，通过数据分布负载均衡和内核优化，将训练时间从12分钟缩短到2分钟。


<details>
  <summary>Details</summary>
Motivation: 化学基础模型使用GNN处理3D分子图，需要针对处理大量大小不一的几何图的特性进行优化，以提升效率。

Method: 将负载均衡问题表述为多目标bin packing问题，提出迭代算法用于数据分布；优化MACE中的对称张量收缩内核。

Result: 实验结果显示，在740个GPU和2.6M样本数据集上，训练每个epoch的时间从12分钟减少到2分钟。

Conclusion: 结合平衡数据分布和内核优化显著提升了MACE的训练效率。

Abstract: Chemistry Foundation Models (CFMs) that leverage Graph Neural Networks (GNNs)
operating on 3D molecular graph structures are becoming indispensable tools for
computational chemists and materials scientists. These models facilitate the
understanding of matter and the discovery of new molecules and materials. In
contrast to GNNs operating on a large homogeneous graphs, GNNs used by CFMs
process a large number of geometric graphs of varying sizes, requiring
different optimization strategies than those developed for large homogeneous
GNNs. This paper presents optimizations for two critical phases of CFM
training: data distribution and model training, targeting MACE - a
state-of-the-art CFM. We address the challenge of load balancing in data
distribution by formulating it as a multi-objective bin packing problem. We
propose an iterative algorithm that provides a highly effective, fast, and
practical solution, ensuring efficient data distribution. For the training
phase, we identify symmetric tensor contraction as the key computational kernel
in MACE and optimize this kernel to improve the overall performance. Our
combined approach of balanced data distribution and kernel optimization
significantly enhances the training process of MACE. Experimental results
demonstrate a substantial speedup, reducing per-epoch execution time for
training from 12 to 2 minutes on 740 GPUs with a 2.6M sample dataset.

</details>

### [192] [Transformer-Based Model for Cold Start Mitigation in FaaS Architecture](https://arxiv.org/abs/2504.11338)
*Alexandre Savi Fayam Mbala Mouen,Jerry Lacmou Zeutouo,Vianney Kengne Tchendji*

Main category: cs.DC

TLDR: 这篇论文提出使用Transformer模型缓解FaaS中的冷启动问题，通过实验证明可减少高达79%的启动时间。


<details>
  <summary>Details</summary>
Motivation: 解决服务器less架构中冷启动问题导致的延迟和用户体验下降，以及现有解决方案在调用模式泛化和实现复杂度上的局限性。

Method: 提出一种利用Transformer模型建模函数初始化延迟并优化服务器less系统性能的创新方法。

Result: 使用Azure公共数据集实验评估，显示冷启动时间比传统方法减少高达79%。

Conclusion: 该方法显著降低了冷启动影响，提高了FaaS架构的性能。

Abstract: Serverless architectures, particularly the Function as a Service (FaaS)
model, have become a cornerstone of modern cloud computing due to their ability
to simplify resource management and enhance application deployment agility.
However, a significant challenge remains: the cold start problem. This
phenomenon occurs when an idle FaaS function is invoked, requiring a full
initialization process, which increases latency and degrades user experience.
Existing solutions for cold start mitigation are limited in terms of invocation
pattern generalization and implementation complexity. In this study, we propose
an innovative approach leveraging Transformer models to mitigate the impact of
cold starts in FaaS architectures. Our solution excels in accurately modeling
function initialization delays and optimizing serverless system performance.
Experimental evaluation using a public dataset provided by Azure demonstrates a
significant reduction in cold start times, reaching up to 79\% compared to
conventional methods.

</details>

<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [193] [Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails](https://arxiv.org/abs/2504.11168)
*William Hackett,Lewis Birch,Stefan Trawicki,Neeraj Suri,Peter Garraghan*

Main category: cs.CR

TLDR: 这篇论文展示了如何通过字符注入和对抗机器学习技术绕过LLM防护系统，实现了高达100%的规避成功率，并强调了需要更强大的防护机制。


<details>
  <summary>Details</summary>
Motivation: 揭示当前LLM防护机制在面对提示注入和越狱攻击时的漏洞，以突出其不足。

Method: 使用传统字符注入方法和算法对抗机器学习(AML)规避技术，对包括Microsoft的Azure Prompt Shield和Meta的Prompt Guard在内的六种主要防护系统进行测试。

Result: 在某些情况下实现了100%的规避成功率，并通过利用白盒模型计算的单词重要性排名来提高对黑盒目标的攻击成功率。

Conclusion: 突显了当前LLM防护机制的脆弱性，并指出需要开发更稳健的防护系统。

Abstract: Large Language Models (LLMs) guardrail systems are designed to protect
against prompt injection and jailbreak attacks. However, they remain vulnerable
to evasion techniques. We demonstrate two approaches for bypassing LLM prompt
injection and jailbreak detection systems via traditional character injection
methods and algorithmic Adversarial Machine Learning (AML) evasion techniques.
Through testing against six prominent protection systems, including Microsoft's
Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be
used to evade detection while maintaining adversarial utility achieving in some
instances up to 100% evasion success. Furthermore, we demonstrate that
adversaries can enhance Attack Success Rates (ASR) against black-box targets by
leveraging word importance ranking computed by offline white-box models. Our
findings reveal vulnerabilities within current LLM protection mechanisms and
highlight the need for more robust guardrail systems.

</details>

### [194] [Exploring Backdoor Attack and Defense for LLM-empowered Recommendations](https://arxiv.org/abs/2504.11182)
*Liangbo Ning,Wenqi Fan,Qing Li*

Main category: cs.CR

TLDR: 本文探讨LLM-based推荐系统的后门攻击漏洞，并提出BadRec攻击框架和P-Scanner防御策略。


<details>
  <summary>Details</summary>
Motivation: 动机是揭示LLM-based推荐系统对后门攻击的易感性，这方面研究较少。

Method: 方法包括BadRec框架，通过扰动物品标题和假用户交互注入后门；P-Scanner使用LLM-based扫描器和触发器增强代理检测中毒物品。

Result: 结果显示，毒害1%的训练数据即可成功注入后门；P-Scanner在三个真实数据集上有效。

Conclusion: 结论是证明了系统漏洞并验证了防御策略的有效性。

Abstract: The fusion of Large Language Models (LLMs) with recommender systems (RecSys)
has dramatically advanced personalized recommendations and drawn extensive
attention. Despite the impressive progress, the safety of LLM-based RecSys
against backdoor attacks remains largely under-explored. In this paper, we
raise a new problem: Can a backdoor with a specific trigger be injected into
LLM-based Recsys, leading to the manipulation of the recommendation responses
when the backdoor trigger is appended to an item's title? To investigate the
vulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new
attack framework termed Backdoor Injection Poisoning for RecSys (BadRec).
BadRec perturbs the items' titles with triggers and employs several fake users
to interact with these items, effectively poisoning the training set and
injecting backdoors into LLM-based RecSys. Comprehensive experiments reveal
that poisoning just 1% of the training data with adversarial examples is
sufficient to successfully implant backdoors, enabling manipulation of
recommendations. To further mitigate such a security threat, we propose a
universal defense strategy called Poison Scanner (P-Scanner). Specifically, we
introduce an LLM-based poison scanner to detect the poisoned items by
leveraging the powerful language understanding and rich knowledge of LLMs. A
trigger augmentation agent is employed to generate diverse synthetic triggers
to guide the poison scanner in learning domain-specific knowledge of the
poisoned item detection task. Extensive experiments on three real-world
datasets validate the effectiveness of the proposed P-Scanner.

</details>

### [195] [DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks](https://arxiv.org/abs/2504.11358)
*Yupei Liu,Yuqi Jia,Jinyuan Jia,Dawn Song,Neil Zhenqiang Gong*

Main category: cs.CR

TLDR: 本文提出DataSentinel，一种基于游戏理论的方法，用于检测LLM中的提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM集成应用易受提示注入攻击，现有的检测方法对先进和适应性攻击无效。

Method: 提出DataSentinel，通过微调LLM并使用minimax优化和梯度方法来检测适应性攻击。

Result: 在多个基准数据集和LLM上，DataSentinel有效检测现有和适应性提示注入攻击。

Conclusion: DataSentinel为提示注入攻击检测提供了一个鲁棒的解决方案。

Abstract: LLM-integrated applications and agents are vulnerable to prompt injection
attacks, where an attacker injects prompts into their inputs to induce
attacker-desired outputs. A detection method aims to determine whether a given
input is contaminated by an injected prompt. However, existing detection
methods have limited effectiveness against state-of-the-art attacks, let alone
adaptive ones. In this work, we propose DataSentinel, a game-theoretic method
to detect prompt injection attacks. Specifically, DataSentinel fine-tunes an
LLM to detect inputs contaminated with injected prompts that are strategically
adapted to evade detection. We formulate this as a minimax optimization
problem, with the objective of fine-tuning the LLM to detect strong adaptive
attacks. Furthermore, we propose a gradient-based method to solve the minimax
optimization problem by alternating between the inner max and outer min
problems. Our evaluation results on multiple benchmark datasets and LLMs show
that DataSentinel effectively detects both existing and adaptive prompt
injection attacks.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [196] [Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach](https://arxiv.org/abs/2504.10639)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于Koopman的安全终端电压估计方案，使用两阶段错误补偿来应对云BMS中的恶意传感器攻击。


<details>
  <summary>Details</summary>
Motivation: 为了在恶意攻击下确保电池终端电压数据的准确性，防止电池过充或欠充。

Method: 采用Koopman运算符的线性逼近，并通过两阶段错误补偿：第一阶段估计并补偿Koopman预测错误，第二阶段使用经验修正（如开路电压到荷态映射）或Gaussian过程回归数据驱动方法。

Result: 通过实验证明了使用经验和数据驱动修正的安全估计器的有效性。

Conclusion: 该方案能够有效恢复被攻击的电压数据，确保BMS的安全性。

Abstract: Cloud-based battery management system (BMS) requires accurate terminal
voltage measurement data to ensure optimal and safe charging of Lithium-ion
batteries. Unfortunately, an adversary can corrupt the battery terminal voltage
data as it passes from the local-BMS to the cloud-BMS through the communication
network, with the objective of under- or over-charging the battery. To ensure
accurate terminal voltage data under such malicious sensor attacks, this paper
investigates a Koopman-based secure terminal voltage estimation scheme using a
two-stage error-compensated self-learning feedback. During the first stage of
error correction, the potential Koopman prediction error is estimated to
compensate for the error accumulation due to the linear approximation of
Koopman operator. The second stage of error compensation aims to recover the
error amassing from the higher-order dynamics of the Lithium-ion batteries
missed by the self-learning strategy. Specifically, we have proposed two
different methods for this second stage error compensation. First, an
interpretable empirical correction strategy has been obtained using the open
circuit voltage to state-of-charge mapping for the battery. Second, a Gaussian
process regression-based data-driven method has been explored. Finally, we
demonstrate the efficacy of the proposed secure estimator using both empirical
and data-driven corrections.

</details>

### [197] [Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs](https://arxiv.org/abs/2504.10658)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 这篇论文提出了一种使用可适应XgBoost模型检测电动汽车充电中传感器网络攻击的方法，并通过模拟实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于对手可能篡改电压传感器数据导致充电中断，因此需要实时检测攻击以确保充电安全，并适应不同电池配置。

Method: 提出对XgBoost-based的电池单元级模型进行可适应微调，使用有限的电池组级数据进行电压预测和残差生成，并利用PyBaMM和liionpack软件包的实验数据训练和测试。

Result: 算法在传感器交换和重放攻击下，对两个大容量电池组进行了评估，模拟结果突出了其可适应性和有效性。

Conclusion: 该检测算法在实时确保电动汽车充电安全方面是有效的，且具有良好的适应性。

Abstract: Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor
measurements from the battery pack to the cloud-controller of the smart
charging station. However, an adversary could corrupt the voltage sensor data
during transmission, potentially causing local to wide-scale disruptions.
Therefore, it is essential to detect sensor cyberattacks in real-time to ensure
secure EV charging, and the developed algorithms must be readily adaptable to
variations, including pack configurations. To tackle these challenges, we
propose adaptable fine-tuning of an XgBoost-based cell-level model using
limited pack-level data to use for voltage prediction and residual generation.
We used battery cell and pack data from high-fidelity charging experiments in
PyBaMM and `liionpack' package to train and test the detection algorithm. The
algorithm's performance has been evaluated for two large-format battery packs
under sensor swapping and replay attacks. The simulation results also highlight
the adaptability and efficacy of our proposed detection algorithm.

</details>

### [198] [Spectrum Sharing in STAR-RIS-assisted UAV with NOMA for Cognitive Radio Networks](https://arxiv.org/abs/2504.10691)
*Ali Nazari,Ali Olfat*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于STAR-RIS辅助UAV的认知无线电网络优化方案，以提高频谱效率和缓解干扰。


<details>
  <summary>Details</summary>
Motivation: 为了解决认知无线电网络中动态信道挑战，并通过STAR-RIS和UAV技术改善主用户和次用户频谱效率。

Method: 提出通过非正交多址接入最大化总速率，联合优化UAV轨迹、传输-反射波束形成和功率分配，并开发了替代优化算法。

Result: 模拟结果研究了重要参数的影响、不同智能表面模式性能、联合轨迹和波束形成设计，以及STAR-RIS在缓解干扰方面的能力。

Conclusion: 该方法有效地提升了频谱效率，并展示了STAR-RIS在动态环境中的优势。

Abstract: As an emerging technology, the simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) can improve the spectrum
efficiency (SE) of primary users (PUs) and secondary users (SUs) in cognitive
radio (CR) networks by mitigating the interference of the incident signals. The
STAR-RIS-assisted unmanned aerial vehicle (UAV) can fully cover the dynamic
environment through high mobility and fast deployment. According to the dynamic
air-to-ground channels, the STAR-RIS-assisted UAV may face a challenge
configuring their elements' coefficients (i.e., reflecting and transmitting the
amplitude and phases). Hence, to meet the requirements of dynamic channel
determination with the SE approach, this paper proposes the sum rate
maximization of both PUs and SUs through non-orthogonal multiple access in CR
network to jointly optimize the trajectory and transmission-reflection
beamforming design of the STAR-RIS-assisted UAV, and power allocation. Since
the non-convex joint optimization problem includes coupled optimization
variables, we develop an alternative optimization algorithm. Simulation results
study the impact of: 1) the significant parameters, 2) the performance of
different intelligence surface modes and STAR-RIS operating protocols, 3) the
joint trajectory and beamforming design with fixed and mobile users, and 4)
STAR-RIS capabilities such as mitigating the interference, and how variations
in the roles of elements dynamically.

</details>

### [199] [Vehicle Dynamics Control for Simultaneous Optimization of Tire Emissions and Performance in EVs](https://arxiv.org/abs/2504.10709)
*Chi-Bach Pham,Homayoun Hamedmoghadam Rafati,Robert Noel Shorten*

Main category: eess.SY

TLDR: 这篇论文提出一种控制方案，通过使用不同轮胎配置文件和优化扭矩分配，减少电动汽车轮胎排放，模拟结果显示有效。


<details>
  <summary>Details</summary>
Motivation: 电动汽车重量增加和牵引力导致轮胎排放上升，对健康和环境有害。

Method: 提出控制方案，利用低磨损低牵引力和高磨损高牵引力轮胎，并优化扭矩分布。

Result: 数值模拟显示显著减少轮胎排放，同时保持车辆性能。

Conclusion: 该方法有效减轻轮胎磨损并维持稳定性。

Abstract: In recent years, Electric Vehicles (EVs) have seen widespread public
adoption. While EVs produce zero tailpipe emissions, they contribute to an
increase in another type of vehicular emission: tire emissions.
Battery-operated EVs are generally heavier than their combustion-engine
counterparts and require greater acceleration forces, which their high-torque
electric motors provide. This combination of increased weight and traction
forces leads to higher tire emissions, which possess various adverse health and
environmental effects. Here, we propose a control solution with promising
results in mitigating tire wear in all-wheel-drive EVs. The idea is to utilize
different tire profiles on each drive axis: a low-wear, low-traction axis and a
high-wear, high-traction axis. Derived from detailed mathematical analyses, we
propose a simple control scheme to counteract the performance difference from
using the low-traction tires. The proposed control mechanism then distributes
torque optimally between the two axes, maximizing usage from the low-wear axis
and simultaneously maintaining stability and performance by leveraging
high-traction tires. Through detailed numerical simulations, we demonstrate
that the developed model significantly reduces tire emissions and maintains
vehicle drivability and performance.

</details>

### [200] [Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks](https://arxiv.org/abs/2504.10855)
*Yu Kawano,Zhiyong Sun*

Main category: eess.SY

TLDR: 这篇论文使用对角占优结构稳定未知非线性时延网络，通过分布式自适应控制，并在SIS网络疫情传播控制中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 推广虚拟收缩分析到时延系统，以稳定未知非线性时延网络，特别是处理传输延迟的问题。

Method: 利用输入矩阵的广义对角占优特性，通过对角高增益和分布式自适应调谐规则实现稳定。

Result: 证明了时延网络可以通过对角高增益稳定，并通过SIS网络疫情传播控制的案例研究验证了效果。

Conclusion: 提出的去中心化自适应控制确保闭环轨迹收敛到原点，在实际应用中表现出色。

Abstract: In this paper, we utilize a diagonally dominant structure for the
decentralized stabilization of unknown nonlinear time-delayed networks.
Generalizing the idea of virtual contraction analysis to time-delayed systems,
we demonstrate that nonlinear time-delayed networks can be stabilized by
diagonal high-gains if the input matrices possess certain generalized
(column/row) diagonally dominant properties. To achieve stabilization of
unknown networks, we further propose a distributed adaptive tuning rule for
each individual gain function, ensuring that all closed-loop trajectories
converge to the origin. The effectiveness of the proposed decentralized
adaptive control is verified in a case study on epidemic spreading control in
SIS networks with transmission delays.

</details>

### [201] [Offset-free Nonlinear MPC with Koopman-based Surrogate Models](https://arxiv.org/abs/2504.10954)
*Irene Schimperna,Lea Bold,Karl Worthmann*

Main category: eess.SY

TLDR: 本篇论文设计了基于扩展动态模式分解(EDMD)的偏移自由非线性模型预测控制(MPC)，用于代理模型，确保在建模误差下实现跟踪，通过数值模拟验证。


<details>
  <summary>Details</summary>
Motivation: 动机是处理建模误差并在系统平衡点信息不完整时实现偏移自由跟踪。

Method: 方法包括使用EDMD构建代理模型，增加干扰项通过观察器估计，并引入参考计算器计算MPC参考状态和输入。

Result: 结果通过van-der-Pol振荡器和四水箱过程的数值模拟展示了方法的有效性。

Conclusion: 结论是，在建模误差渐进常数假设下，算法保证了输出跟踪无偏移。

Abstract: In this paper, we design offset-free nonlinear Model Predictive Control (MPC)
for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The
model used for prediction in MPC is augmented with a disturbance term, that is
estimated by an observer. If the full information about the equilibrium of the
real system is not available, a reference calculator is introduced in the
algorithm to compute the MPC state and input references. The control algorithm
guarantees offset-free tracking of the controlled output under the assumption
that the modeling errors are asymptotically constant. The effectiveness of the
proposed approach is showcased with numerical simulations for two popular
benchmark systems: the van-der-Pol oscillator and the four-tanks process.

</details>

### [202] [A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks](https://arxiv.org/abs/2504.10960)
*Evagoras Makridis,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 这篇论文提出了一种名为RPPAC的线性分布式算法，用于处理多智能体系统中可能不平衡和有延迟的方向网络的平均共识问题。


<details>
  <summary>Details</summary>
Motivation: 动机是解决方向网络中不平衡和延迟问题下的平均共识挑战。

Method: 方法是提出RPPAC算法，利用剩余共识机制和链路信息，处理异步更新和时变延迟，通过时变矩阵的后向乘积分析收敛性。

Result: 结果是算法保证了状态平均，尽管存在不平衡和延迟。

Conclusion: 结论是RPPAC算法在方向网络中实现了可靠的平均共识。

Abstract: In this paper, we address the average consensus problem of multi-agent
systems for possibly unbalanced and delay-prone networks with directional
information flow. We propose a linear distributed algorithm (referred to as
RPPAC) that handles asynchronous updates and time-varying heterogeneous
information delays. Our proposed distributed algorithm utilizes a
surplus-consensus mechanism and information regarding the number of incoming
and outgoing links to guarantee state averaging, despite the imbalanced and
delayed information flow in directional networks. The convergence of the RPPAC
algorithm is examined using key properties of the backward product of
time-varying matrices that correspond to different snapshots of the directional
augmented network.

</details>

### [203] [Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks](https://arxiv.org/abs/2504.10964)
*Evagoras Makridis,Gabriele Oliva,Kasagatta Ramesh Narahari,Mohammadreza Doostmohammadian,Usman A. Khan,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 本论文提出R-ADD-OPT算法，处理单向网络中异质有界延迟的分布式优化问题，并保证适当步长下收敛到最优解。


<details>
  <summary>Details</summary>
Motivation: 解决单向网络中可能存在异质但有界传输延迟的分布式优化问题。

Method: 提出Robustified ADD-OPT (R-ADD-OPT)算法的修改版本，以适应传输延迟。

Result: 梯度步长在特定范围内（取决于最大延迟）可保证节点收敛到最优解，且范围可预先计算。

Conclusion: R-ADD-OPT算法在有延迟网络中实现分布式优化的可靠收敛。

Abstract: In this paper, we address the distributed optimization problem over
unidirectional networks with possibly time-invariant heterogeneous bounded
transmission delays. In particular, we propose a modified version of the
Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein
called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed
optimization problem, even when the communication links suffer from
heterogeneous but bounded transmission delays. We show that if the gradient
step-size of the R-ADD-OPT algorithm is within a certain range, which also
depends on the maximum time delay in the network, then the nodes are guaranteed
to converge to the optimal solution of the distributed optimization problem.
The range of the gradient step-size that guarantees convergence can be computed
a priori based on the maximum time delay in the network.

</details>

### [204] [Steering Feedback in Dynamic Driving Simulators: Road-Induced and Non-Road-Induced Harshness](https://arxiv.org/abs/2504.11097)
*Maximilian Böhle,Bernhard Schick,Steffen Müller*

Main category: eess.SY

TLDR: 本论文研究了30-100 Hz频率范围内转向轮和车身激励对动态驾驶模拟器主观转向反馈的影响，通过42名参与者的对照研究，发现非道路诱发激励影响显著，而道路诱发激励影响较小。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨更高频率（30-100 Hz）转向反馈对主观评估的影响，因为传统方法主要关注低于30 Hz的频率。

Method: 方法包括使用动态驾驶模拟器进行单盲内部受试者设计的研究，比较参考车辆和四种模拟器变体，采用半经验和基于物理的轮胎模型，并添加发动机和车轮阶次激励。

Result: 结果显示，非道路诱发激励对主观转向反馈有强烈影响，而道路诱发激励的影响较不显著。

Conclusion: 结论强调非道路诱发激励在主观评估中的重要性，建议在驾驶模拟器开发中优先考虑这些因素。

Abstract: Steering feedback plays a substantial role in the validity of driving
simulators for the virtual development of modern vehicles. Established
objective steering characteristics typically assess the feedback behavior in
the frequency range of up to 30 Hz while factors such as steering wheel and
vehicle body vibrations at higher frequencies are mainly approached as comfort
issues. This work investigates the influence of steering wheel and vehicle body
excitations in the frequency range between 30 and 100 Hz on the subjective
evaluation of steering feedback in a dynamic driving simulator. A controlled
subject study with 42 participants was performed to compare a reference vehicle
with an electrical power steering system to four variants of its virtual
representation on a dynamic driving simulator. The effects of road-induced
excitations were investigated by comparing a semi-empirical and a physics-based
tire model, while the influence of non-road-induced excitations was
investigated by implementing engine and wheel orders. The simulator variants
were evaluated in comparison to the reference vehicle during closed-loop
driving on a country road in a single-blind within-subjects design. The
subjective evaluation focused on the perception of road feedback compared to
the reference vehicle. The statistical analysis of subjective results shows
that there is a strong effect of non-road-induced steering and vehicle body
excitations, while the effect of road-induced excitations is considerably less
pronounced.

</details>

### [205] [A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances](https://arxiv.org/abs/2504.11125)
*Dieter Teichrib,Moritz Schulze Darup*

Main category: eess.SY

TLDR: 本文提出了一种方法，通过混合整数线性约束表示分段仿射系统及其神经网络控制器的闭环动力学，并通过求解混合整数线性规划计算鲁棒正不变集，以验证稳定性和约束满足。


<details>
  <summary>Details</summary>
Motivation: 为了处理带有边界扰动和基于神经网络控制器的分段仿射系统，实现鲁棒稳定性和约束满足的认证。

Method: 将系统动力学表示为混合整数线性约束，并通过求解混合整数线性程序计算鲁棒正不变集。

Result: 能够计算鲁棒正不变集，认证稳定性和约束满足，并通过分段仿射逼近处理非线性系统及其误差边界。

Conclusion: 该方法为指定系统类提供了计算鲁棒正不变集的框架，从而确保稳定性和约束满足。

Abstract: We present a method for representing the closed-loop dynamics of piecewise
affine (PWA) systems with bounded additive disturbances and neural
network-based controllers as mixed-integer (MI) linear constraints. We show
that such representations enable the computation of robustly positively
invariant (RPI) sets for the specified system class by solving MI linear
programs. These RPI sets can subsequently be used to certify stability and
constraint satisfaction. Furthermore, the approach allows to handle non-linear
systems based on suitable PWA approximations and corresponding error bounds,
which can be interpreted as the bounded disturbances from above.

</details>

### [206] [Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning](https://arxiv.org/abs/2504.11261)
*Hannes Petrenz,Johannes Köhler,Francesco Borrelli*

Main category: eess.SY

TLDR: 本文提出了一种鲁棒自适应学习MPC框架，用于处理线性系统中的参数不确定性和扰动，通过迭代学习提升性能和安全性，仿真结果显示改进。


<details>
  <summary>Details</summary>
Motivation: 针对线性系统在迭代任务中参数不确定性和加性扰动的问题，需要改进控制性能和确保安全性。

Method: 使用集合成员估计迭代改进参数估计，从数据学习终端成本和终端集，确保递归可行性和约束满足。

Result: 保证递归可行性、约束满足和闭环成本鲁棒界限；仿真显示计算效率和控制性能优于现有方法。

Conclusion: 该方法有效提升了控制性能和效率，适用于类似系统。

Abstract: This paper presents a robust adaptive learning Model Predictive Control (MPC)
framework for linear systems with parametric uncertainties and additive
disturbances performing iterative tasks. The approach iteratively refines the
parameter estimates using set membership estimation. Performance enhancement
over iterations is achieved by learning the terminal cost from data. Safety is
enforced using a terminal set, which is also learned iteratively. The proposed
method guarantees recursive feasibility, constraint satisfaction, and a robust
bound on the closed-loop cost. Numerical simulations on a mass-spring-damper
system demonstrate improved computational efficiency and control performance
compared to an existing robust adaptive MPC approach.

</details>

### [207] [Balancing hydrogen delivery in national energy systems: impact of the temporal flexibility of hydrogen delivery on export prices](https://arxiv.org/abs/2504.11285)
*Hazem Abdel-Khalek,Eddy Jalbout,Caspar Schauß,Benjamin Pfluger*

Main category: eess.SY

TLDR: 本篇论文研究平衡成本如何影响氢气价格，针对不同交付时间表，在巴西、摩洛哥和土耳其发现价格上涨最高可达47%。


<details>
  <summary>Details</summary>
Motivation: 氢气在能源转型中扮演关键角色，现有的分析通常忽略交付时间，因此需要量化平衡成本对价格的影响。

Method: 通过模拟从完全灵活到完全稳定的氢气交付时间表，在巴西、摩洛哥和土耳其三个国家，以及10、50和200 TWh三种出口量下计算价格差异。

Result: 价格差异在巴西最高达36%，摩洛哥47%，土耳其18%，跨不同出口量。

Conclusion: 更严格的平衡约束显著增加氢气价格，强调在能源转型分析中必须考虑交付时间表。

Abstract: Hydrogen is expected to play a key role in the energy transition. Analyses
exploring the price of hydrogen usually calculate average or marginal
production costs regardless of the time of delivery. A key factor that affects
the price of hydrogen is the balancing costs, which we define as the expense of
ensuring a steady schedule of hydrogen delivery. We explore the effect of
delivering hydrogen to the export ports at different schedules, ranging from
fully flexible to moderately stable with a daily and weekly buffer, to fully
stable. We quantify the rise in hydrogen price with strict balancing constraint
in three countries: Brazil, Morocco and Turkey, and three export volumes: 10,
50 and 200 TWh. The price difference between the flexible and stable schedules
was found to reach a maximum of 36% in Brazil, 47% in Morocco and 18% in Turkey
across the different export volumes.

</details>

### [208] [Sensitivity Analysis of State Space Models for Scrap Composition Estimation in EAF and BOF](https://arxiv.org/abs/2504.11319)
*Yiqing Zhou,Karsten Naert,Dirk Nuyens*

Main category: eess.SY

TLDR: 本研究开发线性与非线性状态空间模型，估计炼钢中废钢元素组成，并评估对测量噪声的敏感性。


<details>
  <summary>Details</summary>
Motivation: 动机是提高电弧炉和碱性氧气炉中废钢成分估计的准确性，以支持工业应用。

Method: 方法包括使用质量平衡方程和卡尔曼滤波器（线性用修改版，非线性用UKF），以Cu和Cr元素测试噪声敏感性。

Result: 结果显示模型对大多数变量噪声鲁棒，但对炉渣质量噪声敏感。

Conclusion: 结论突出了该模型在实时估计中的实用性和限制。

Abstract: This study develops and analyzes linear and nonlinear state space models for
estimating the elemental composition of scrap steel used in steelmaking, with
applications to Electric Arc Furnace (EAF) and Basic Oxygen Furnace (BOF)
processes. The models incorporate mass balance equations and are fitted using a
modified Kalman filter for linear cases and the Unscented Kalman Filter (UKF)
for nonlinear cases. Using Cu and Cr as representative elements, we assess the
sensitivity of model predictions to measurement noise in key process variables,
including steel mass, steel composition, scrap input mass, slag mass, and iron
oxide fraction in slag. Results show that the models are robust to moderate
noise levels in most variables, particularly when errors are below $10\%$.
However, accuracy significantly deteriorates with noise in slag mass
estimation. These findings highlight the practical feasibility and limitations
of applying state space models for real-time scrap composition estimation in
industrial settings.

</details>

### [209] [Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes](https://arxiv.org/abs/2504.11355)
*Alberto Castillo,Elliot Pryor,Anas El Fathi,Boris Kovatchev,Marc Breton*

Main category: eess.SY

TLDR: 这篇论文提出优化采样数据集（OSDs）来训练神经网络模仿模型预测控制（MPC），在计算资源受限设备上实现高效控制，并通过胰岛素递送应用证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是虽然神经网络可以以更低计算成本复制MPC算法，但训练数据的组成对准确性至关重要，目前缺乏系统优化方法。

Method: 方法是引入OSDs，这是一个参数化的数据子集，能够保留MPC信息、避免重复状态并实现饱和，并提供高效生成算法。

Result: 结果是通过训练神经网络复制特定MPC算法，实现了准确性的四倍改进，并有基于OSDs的神经网络获得临床测试监管批准。

Conclusion: 结论是这种方法为在资源受限平台上部署复杂算法打开新途径，可能彻底改变优化实现方式。

Abstract: Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)
algorithms is an effective way to implement them in constrained embedded
devices. By collecting large amounts of input-output data, where inputs
represent system states and outputs are MPC-generated control actions, NNs can
be trained to replicate MPC behavior at a fraction of the computational cost.
However, although the composition of the training data critically influences
the final NN accuracy, methods for systematically optimizing it remain
underexplored. In this paper, we introduce the concept of Optimally-Sampled
Datasets (OSDs) as ideal training sets and present an efficient algorithm for
generating them. An OSD is a parametrized subset of all the available data that
(i) preserves existing MPC information up to a certain numerical resolution,
(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or
complete. We demonstrate the effectiveness of OSDs by training NNs to replicate
the University of Virginia's MPC algorithm for automated insulin delivery in
Type-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,
two OSD-trained NNs received regulatory clearance for clinical testing as the
first NN-based control algorithm for direct human insulin dosing. This
methodology opens new pathways for implementing advanced optimizations on
resource-constrained embedded platforms, potentially revolutionizing how
complex algorithms are deployed.

</details>

### [210] [A Winner-Takes-All Mechanism for Event Generation](https://arxiv.org/abs/2504.11374)
*Yongkang Huo,Fuvio Forni,Rodolphe Sepulchre*

Main category: eess.SY

TLDR: 本文提出一种新框架，用于中心模式发生器设计，结合神经元反弹兴奋性和赢者通吃计算，统一决策和节律模式生成。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一个简单、强大且鲁棒的网络架构，以统一决策和节律模式生成，解决相关领域的挑战。

Method: 方法使用全互抑连接并增强可设计兴奋交互的网络架构，结合神经元的内在反弹兴奋性和赢者通吃计算。

Result: 结果通过环形振荡器模型展示了自适应相位和频率调制，证明了框架在易实现、适应性和鲁棒性方面的优势。

Conclusion: 结论是该框架对神经形态系统和机器人应用具有重要前景。

Abstract: We present a novel framework for central pattern generator design that
leverages the intrinsic rebound excitability of neurons in combination with
winner-takes-all computation. Our approach unifies decision-making and rhythmic
pattern generation within a simple yet powerful network architecture that
employs all-to-all inhibitory connections enhanced by designable excitatory
interactions. This design offers significant advantages regarding ease of
implementation, adaptability, and robustness. We demonstrate its efficacy
through a ring oscillator model, which exhibits adaptive phase and frequency
modulation, making the framework particularly promising for applications in
neuromorphic systems and robotics.

</details>

### [211] [eXplainable AI for data driven control: an inverse optimal control approach](https://arxiv.org/abs/2504.11446)
*Federico Porcari,Donatello Materassi,Simone Formentin*

Main category: eess.SY

TLDR: 本论文提出XAI方法基于逆最优控制，解释黑箱控制器的行为，通过提取成本函数权重提供本地透明解释，与LIME相关联。


<details>
  <summary>Details</summary>
Motivation: 理解黑箱数据驱动控制器的行为是现代控制设计的关键挑战。

Method: 使用Inverse Optimal Control (IOC)方法，求解逆Linear Quadratic (LQ)问题，提取跟踪误差和控制努力的权重。

Result: 数值例子显示，推断的成本函数能更深入理解控制器的决策过程，揭示反直觉现象。

Conclusion: 该方法提供结构化、控制相关的解释，帮助阐明黑箱控制器的潜在目标。

Abstract: Understanding the behavior of black-box data-driven controllers is a key
challenge in modern control design. In this work, we propose an eXplainable AI
(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local
explanations for the behavior of a controller operating around a given region.
Specifically, we extract the weights assigned to tracking errors and control
effort in the implicit cost function that a black-box controller is optimizing,
offering a more transparent and interpretable representation of the
controller's underlying objectives. This approach presents connections with
well-established XAI techniques, such as Local Interpretable Model-agnostic
Explanations (LIME) since it is still based on a local approximation of the
control policy. However, rather being limited to a standard sensitivity
analysis, the explanation provided by our method relies on the solution of an
inverse Linear Quadratic (LQ) problem, offering a structured and more
control-relevant perspective. Numerical examples demonstrate that the inferred
cost function consistently provides a deeper understanding of the controller's
decision-making process, shedding light on otherwise counterintuitive or
unexpected phenomena.

</details>

### [212] [Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach](https://arxiv.org/abs/2504.10639)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于Koopman的secure terminal voltage estimation方案，用于cloud-based BMS，以对抗恶意传感器攻击，确保电池充电安全。


<details>
  <summary>Details</summary>
Motivation: 动机是防止adversary篡改电池终端电压数据，导致电池充电不当，从而确保数据准确性。

Method: 方法包括Koopman-based secure estimation，使用两阶段错误补偿：第一阶段补偿Koopman预测错误，第二阶段使用经验修正或Gaussian process regression。

Result: 结果展示了使用经验和数据驱动修正的方案的有效性。

Conclusion: 结论是所提出的secure estimator在对抗攻击时有效。

Abstract: Cloud-based battery management system (BMS) requires accurate terminal
voltage measurement data to ensure optimal and safe charging of Lithium-ion
batteries. Unfortunately, an adversary can corrupt the battery terminal voltage
data as it passes from the local-BMS to the cloud-BMS through the communication
network, with the objective of under- or over-charging the battery. To ensure
accurate terminal voltage data under such malicious sensor attacks, this paper
investigates a Koopman-based secure terminal voltage estimation scheme using a
two-stage error-compensated self-learning feedback. During the first stage of
error correction, the potential Koopman prediction error is estimated to
compensate for the error accumulation due to the linear approximation of
Koopman operator. The second stage of error compensation aims to recover the
error amassing from the higher-order dynamics of the Lithium-ion batteries
missed by the self-learning strategy. Specifically, we have proposed two
different methods for this second stage error compensation. First, an
interpretable empirical correction strategy has been obtained using the open
circuit voltage to state-of-charge mapping for the battery. Second, a Gaussian
process regression-based data-driven method has been explored. Finally, we
demonstrate the efficacy of the proposed secure estimator using both empirical
and data-driven corrections.

</details>

### [213] [Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs](https://arxiv.org/abs/2504.10658)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 本篇论文提出了一种基于XgBoost的算法，通过有限的数据微调来检测电动汽车充电中的传感器网络攻击，并在模拟中显示出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 为了检测和缓解可能破坏电动汽车充电的传感器网络攻击，确保实时安全并适应不同的电池配置。

Method: 提出通过使用PyBaMM和liionpack包的pack-level数据对XgBoost-based cell-level模型进行可适应微调，用于电压预测和残差生成。

Result: 算法在两个大容量电池组上针对传感器交换和重放攻击的测试中表现出色，突显其适应性和有效性。

Conclusion: 所提出的检测算法在确保电动汽车充电安全方面有效且可适应。

Abstract: Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor
measurements from the battery pack to the cloud-controller of the smart
charging station. However, an adversary could corrupt the voltage sensor data
during transmission, potentially causing local to wide-scale disruptions.
Therefore, it is essential to detect sensor cyberattacks in real-time to ensure
secure EV charging, and the developed algorithms must be readily adaptable to
variations, including pack configurations. To tackle these challenges, we
propose adaptable fine-tuning of an XgBoost-based cell-level model using
limited pack-level data to use for voltage prediction and residual generation.
We used battery cell and pack data from high-fidelity charging experiments in
PyBaMM and `liionpack' package to train and test the detection algorithm. The
algorithm's performance has been evaluated for two large-format battery packs
under sensor swapping and replay attacks. The simulation results also highlight
the adaptability and efficacy of our proposed detection algorithm.

</details>

### [214] [Spectrum Sharing in STAR-RIS-assisted UAV with NOMA for Cognitive Radio Networks](https://arxiv.org/abs/2504.10691)
*Ali Nazari,Ali Olfat*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于STAR-RIS辅助UAV的优化方法，以最大化认知无线电网络中主用户和次要用户的总速率。


<details>
  <summary>Details</summary>
Motivation: 为了提高频谱效率，缓解干扰，并适应动态环境中的挑战。

Method: 通过联合优化UAV轨迹、传输-反射波束形成和功率分配，使用替代优化算法解决非凸优化问题。

Result: 模拟结果分析了关键参数的影响、不同智能表面模式和协议的性能、轨迹设计以及STAR-RIS的干扰缓解能力。

Conclusion: STAR-RIS能够有效提升系统性能，并在动态环境中动态调整元素角色。

Abstract: As an emerging technology, the simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) can improve the spectrum
efficiency (SE) of primary users (PUs) and secondary users (SUs) in cognitive
radio (CR) networks by mitigating the interference of the incident signals. The
STAR-RIS-assisted unmanned aerial vehicle (UAV) can fully cover the dynamic
environment through high mobility and fast deployment. According to the dynamic
air-to-ground channels, the STAR-RIS-assisted UAV may face a challenge
configuring their elements' coefficients (i.e., reflecting and transmitting the
amplitude and phases). Hence, to meet the requirements of dynamic channel
determination with the SE approach, this paper proposes the sum rate
maximization of both PUs and SUs through non-orthogonal multiple access in CR
network to jointly optimize the trajectory and transmission-reflection
beamforming design of the STAR-RIS-assisted UAV, and power allocation. Since
the non-convex joint optimization problem includes coupled optimization
variables, we develop an alternative optimization algorithm. Simulation results
study the impact of: 1) the significant parameters, 2) the performance of
different intelligence surface modes and STAR-RIS operating protocols, 3) the
joint trajectory and beamforming design with fixed and mobile users, and 4)
STAR-RIS capabilities such as mitigating the interference, and how variations
in the roles of elements dynamically.

</details>

### [215] [Vehicle Dynamics Control for Simultaneous Optimization of Tire Emissions and Performance in EVs](https://arxiv.org/abs/2504.10709)
*Chi-Bach Pham,Homayoun Hamedmoghadam Rafati,Robert Noel Shorten*

Main category: eess.SY

TLDR: 本文提出控制方案，使用不同轮胎配置减少电动汽车轮胎排放，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 电动汽车重量增加和牵引力导致轮胎排放上升，对健康和环境有害，需要减轻轮胎磨损。

Method: 采用低磨损低牵引力和高磨损高牵引力轮胎组合，并优化扭矩分布控制方案。

Result: 数值模拟显示，该模型显著降低轮胎排放，同时维持车辆可驾驶性和性能。

Conclusion: 该控制机制有效减少轮胎排放，并保持车辆稳定性和性能，具有应用潜力。

Abstract: In recent years, Electric Vehicles (EVs) have seen widespread public
adoption. While EVs produce zero tailpipe emissions, they contribute to an
increase in another type of vehicular emission: tire emissions.
Battery-operated EVs are generally heavier than their combustion-engine
counterparts and require greater acceleration forces, which their high-torque
electric motors provide. This combination of increased weight and traction
forces leads to higher tire emissions, which possess various adverse health and
environmental effects. Here, we propose a control solution with promising
results in mitigating tire wear in all-wheel-drive EVs. The idea is to utilize
different tire profiles on each drive axis: a low-wear, low-traction axis and a
high-wear, high-traction axis. Derived from detailed mathematical analyses, we
propose a simple control scheme to counteract the performance difference from
using the low-traction tires. The proposed control mechanism then distributes
torque optimally between the two axes, maximizing usage from the low-wear axis
and simultaneously maintaining stability and performance by leveraging
high-traction tires. Through detailed numerical simulations, we demonstrate
that the developed model significantly reduces tire emissions and maintains
vehicle drivability and performance.

</details>

### [216] [Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks](https://arxiv.org/abs/2504.10855)
*Yu Kawano,Zhiyong Sun*

Main category: eess.SY

TLDR: 本文利用对角占优结构实现未知非线性时延网络的去中心化稳定化，并通过自适应控制和疫情传播案例验证。


<details>
  <summary>Details</summary>
Motivation: 针对未知非线性时延网络稳定化的需求，推广虚拟收缩分析方法以处理时延问题。

Method: 采用对角高增益和具有广义对角占优属性的输入矩阵，以及分布式自适应调谐规则来稳定网络。

Result: 实现了网络稳定化，所有闭环轨迹收敛到原点，并在SIS网络疫情传播控制案例中验证有效。

Conclusion: 提出的去中心化自适应控制方法对时延网络稳定化具有实际可行性和有效性。

Abstract: In this paper, we utilize a diagonally dominant structure for the
decentralized stabilization of unknown nonlinear time-delayed networks.
Generalizing the idea of virtual contraction analysis to time-delayed systems,
we demonstrate that nonlinear time-delayed networks can be stabilized by
diagonal high-gains if the input matrices possess certain generalized
(column/row) diagonally dominant properties. To achieve stabilization of
unknown networks, we further propose a distributed adaptive tuning rule for
each individual gain function, ensuring that all closed-loop trajectories
converge to the origin. The effectiveness of the proposed decentralized
adaptive control is verified in a case study on epidemic spreading control in
SIS networks with transmission delays.

</details>

### [217] [Offset-free Nonlinear MPC with Koopman-based Surrogate Models](https://arxiv.org/abs/2504.10954)
*Irene Schimperna,Lea Bold,Karl Worthmann*

Main category: eess.SY

TLDR: 本文设计了基于EDMD的代理模型的偏移自由非线性MPC，通过干扰估计和参考计算保证跟踪精度，并在基准系统中验证。


<details>
  <summary>Details</summary>
Motivation: 处理MPC建模误差，确保在平衡信息未知时实现偏移自由跟踪。

Method: 使用EDMD构建代理模型，添加干扰项由观察器估计；引入参考计算器计算MPC参考状态和输入。

Result: 算法在建模误差恒定时保证偏移自由跟踪；通过van-der-Pol振荡器和四水箱过程的数值模拟验证有效性。

Conclusion: 提出的方法有效处理非线性系统的建模不确定性。

Abstract: In this paper, we design offset-free nonlinear Model Predictive Control (MPC)
for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The
model used for prediction in MPC is augmented with a disturbance term, that is
estimated by an observer. If the full information about the equilibrium of the
real system is not available, a reference calculator is introduced in the
algorithm to compute the MPC state and input references. The control algorithm
guarantees offset-free tracking of the controlled output under the assumption
that the modeling errors are asymptotically constant. The effectiveness of the
proposed approach is showcased with numerical simulations for two popular
benchmark systems: the van-der-Pol oscillator and the four-tanks process.

</details>

### [218] [A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks](https://arxiv.org/abs/2504.10960)
*Evagoras Makridis,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 本篇论文提出RPPAC算法，解决多代理系统在不平衡和延迟网络中的平均共识问题。


<details>
  <summary>Details</summary>
Motivation: 针对多代理系统在可能不平衡、易受延迟的定向网络中平均共识问题的挑战。

Method: 提出线性分布式RPPAC算法，利用剩余共识机制和链路信息，处理异步更新和时变延迟。

Result: 算法保证状态平均，并通过时变矩阵后向乘积分析收敛性。

Conclusion: RPPAC算法在定向网络中实现平均共识，尽管信息流不平衡和有延迟。

Abstract: In this paper, we address the average consensus problem of multi-agent
systems for possibly unbalanced and delay-prone networks with directional
information flow. We propose a linear distributed algorithm (referred to as
RPPAC) that handles asynchronous updates and time-varying heterogeneous
information delays. Our proposed distributed algorithm utilizes a
surplus-consensus mechanism and information regarding the number of incoming
and outgoing links to guarantee state averaging, despite the imbalanced and
delayed information flow in directional networks. The convergence of the RPPAC
algorithm is examined using key properties of the backward product of
time-varying matrices that correspond to different snapshots of the directional
augmented network.

</details>

### [219] [Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks](https://arxiv.org/abs/2504.10964)
*Evagoras Makridis,Gabriele Oliva,Kasagatta Ramesh Narahari,Mohammadreza Doostmohammadian,Usman A. Khan,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 本文提出R-ADD-OPT算法，解决单向网络中带异构延迟的分布式优化问题，并保证收敛。


<details>
  <summary>Details</summary>
Motivation: 解决单向网络中存在异构但有界传输延迟的分布式优化问题。

Method: 提出R-ADD-OPT算法的修改版，以处理异构有界传输延迟。

Result: 在特定梯度步长范围内（取决于最大延迟），保证节点收敛到最优解，且步长范围可预先计算。

Conclusion: 算法在给定条件下确保收敛到分布式优化问题的最优解。

Abstract: In this paper, we address the distributed optimization problem over
unidirectional networks with possibly time-invariant heterogeneous bounded
transmission delays. In particular, we propose a modified version of the
Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein
called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed
optimization problem, even when the communication links suffer from
heterogeneous but bounded transmission delays. We show that if the gradient
step-size of the R-ADD-OPT algorithm is within a certain range, which also
depends on the maximum time delay in the network, then the nodes are guaranteed
to converge to the optimal solution of the distributed optimization problem.
The range of the gradient step-size that guarantees convergence can be computed
a priori based on the maximum time delay in the network.

</details>

### [220] [Steering Feedback in Dynamic Driving Simulators: Road-Induced and Non-Road-Induced Harshness](https://arxiv.org/abs/2504.11097)
*Maximilian Böhle,Bernhard Schick,Steffen Müller*

Main category: eess.SY

TLDR: 本研究考察了30-100 Hz频率振动对驾驶模拟器转向反馈主观评价的影响，结果显示非道路诱发振动的影响更大。


<details>
  <summary>Details</summary>
Motivation: 转向反馈对驾驶模拟器的有效性很重要，但高频振动常被忽视或视为舒适性问题，本文旨在探讨其对主观评价的影响。

Method: 通过一项涉及42名参与者的控制性研究，比较参考车辆与动态驾驶模拟器的四个变体，使用半经验和基于物理的轮胎模型，以及添加发动机和车轮阶次的非道路诱发激发，采用单盲内组设计在乡村道路上进行闭环驾驶评价。

Result: 统计分析表明，非道路诱发转向和车身激发对主观评价有显著影响，而道路诱发激发的效果较弱。

Conclusion: 这强调了在驾驶模拟器设计中，应更重视非道路诱发振动的角色，以提高模拟的真实性和有效性。

Abstract: Steering feedback plays a substantial role in the validity of driving
simulators for the virtual development of modern vehicles. Established
objective steering characteristics typically assess the feedback behavior in
the frequency range of up to 30 Hz while factors such as steering wheel and
vehicle body vibrations at higher frequencies are mainly approached as comfort
issues. This work investigates the influence of steering wheel and vehicle body
excitations in the frequency range between 30 and 100 Hz on the subjective
evaluation of steering feedback in a dynamic driving simulator. A controlled
subject study with 42 participants was performed to compare a reference vehicle
with an electrical power steering system to four variants of its virtual
representation on a dynamic driving simulator. The effects of road-induced
excitations were investigated by comparing a semi-empirical and a physics-based
tire model, while the influence of non-road-induced excitations was
investigated by implementing engine and wheel orders. The simulator variants
were evaluated in comparison to the reference vehicle during closed-loop
driving on a country road in a single-blind within-subjects design. The
subjective evaluation focused on the perception of road feedback compared to
the reference vehicle. The statistical analysis of subjective results shows
that there is a strong effect of non-road-induced steering and vehicle body
excitations, while the effect of road-induced excitations is considerably less
pronounced.

</details>

### [221] [A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances](https://arxiv.org/abs/2504.11125)
*Dieter Teichrib,Moritz Schulze Darup*

Main category: eess.SY

TLDR: 这篇论文提出一种方法，将分段仿射系统和神经网络控制器的闭环动力学表示为混合整数线性约束，从而计算鲁棒正不变集，以证明稳定性并满足约束。


<details>
  <summary>Details</summary>
Motivation: 为了处理带有边界扰动和神经网络控制器的系统，证明其稳定性并确保约束满足。

Method: 通过将系统动力学表示为混合整数线性约束，并通过求解混合整数线性规划来计算鲁棒正不变集。

Result: 成功计算了鲁棒正不变集，能够证明稳定性、约束满足，并通过PWA逼近处理非线性系统。

Conclusion: 该方法有效适用于指定系统类，并通过误差边界扩展到非线性系统。

Abstract: We present a method for representing the closed-loop dynamics of piecewise
affine (PWA) systems with bounded additive disturbances and neural
network-based controllers as mixed-integer (MI) linear constraints. We show
that such representations enable the computation of robustly positively
invariant (RPI) sets for the specified system class by solving MI linear
programs. These RPI sets can subsequently be used to certify stability and
constraint satisfaction. Furthermore, the approach allows to handle non-linear
systems based on suitable PWA approximations and corresponding error bounds,
which can be interpreted as the bounded disturbances from above.

</details>

### [222] [Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning](https://arxiv.org/abs/2504.11261)
*Hannes Petrenz,Johannes Köhler,Francesco Borrelli*

Main category: eess.SY

TLDR: 这篇论文提出了一种鲁棒自适应学习MPC框架，用于处理线性系统的参数不确定性和干扰，通过迭代学习提高性能和安全。


<details>
  <summary>Details</summary>
Motivation: 动机是针对迭代任务中不确定性的挑战，提高控制性能和计算效率。

Method: 方法使用集合成员估计迭代改进参数估计，从数据学习终端成本，并迭代学习终端集。

Result: 结果保证递归可行性、约束满足和闭环成本的鲁棒界；模拟显示比现有方法更好的计算效率和控制性能。

Conclusion: 结论是提出的方法在安全性和性能上优于现有鲁棒自适应MPC方法。

Abstract: This paper presents a robust adaptive learning Model Predictive Control (MPC)
framework for linear systems with parametric uncertainties and additive
disturbances performing iterative tasks. The approach iteratively refines the
parameter estimates using set membership estimation. Performance enhancement
over iterations is achieved by learning the terminal cost from data. Safety is
enforced using a terminal set, which is also learned iteratively. The proposed
method guarantees recursive feasibility, constraint satisfaction, and a robust
bound on the closed-loop cost. Numerical simulations on a mass-spring-damper
system demonstrate improved computational efficiency and control performance
compared to an existing robust adaptive MPC approach.

</details>

### [223] [Balancing hydrogen delivery in national energy systems: impact of the temporal flexibility of hydrogen delivery on export prices](https://arxiv.org/abs/2504.11285)
*Hazem Abdel-Khalek,Eddy Jalbout,Caspar Schauß,Benjamin Pfluger*

Main category: eess.SY

TLDR: 本论文探讨氢气交付时间表对价格的影响，强调平衡成本导致的价格上升，最高可达47%。


<details>
  <summary>Details</summary>
Motivation: 氢气在能源转型中关键，但现有分析忽略交付时间，故研究平衡成本的影响。

Method: 模拟不同交付时间表（灵活到稳定），在巴西、摩洛哥、土耳其三国，出口量10、50、200 TWh下量化价格变化。

Result: 价格差异最大为巴西36%、摩洛哥47%、土耳其18%，随国家与出口量变化。

Conclusion: 严格平衡约束显著提高氢气价格，强调交付时间表在规划中的重要性。

Abstract: Hydrogen is expected to play a key role in the energy transition. Analyses
exploring the price of hydrogen usually calculate average or marginal
production costs regardless of the time of delivery. A key factor that affects
the price of hydrogen is the balancing costs, which we define as the expense of
ensuring a steady schedule of hydrogen delivery. We explore the effect of
delivering hydrogen to the export ports at different schedules, ranging from
fully flexible to moderately stable with a daily and weekly buffer, to fully
stable. We quantify the rise in hydrogen price with strict balancing constraint
in three countries: Brazil, Morocco and Turkey, and three export volumes: 10,
50 and 200 TWh. The price difference between the flexible and stable schedules
was found to reach a maximum of 36% in Brazil, 47% in Morocco and 18% in Turkey
across the different export volumes.

</details>

### [224] [Sensitivity Analysis of State Space Models for Scrap Composition Estimation in EAF and BOF](https://arxiv.org/abs/2504.11319)
*Yiqing Zhou,Karsten Naert,Dirk Nuyens*

Main category: eess.SY

TLDR: 本研究开发线性与非线性状态空间模型，估计炼钢废钢元素组成，应用于电弧炉和碱性氧气炉，使用卡尔曼滤波器拟合，结果显示对大多数变量噪声鲁棒，但对炉渣质量噪声敏感。


<details>
  <summary>Details</summary>
Motivation: 动机是提高炼钢过程中废钢成分估计准确性，以优化电弧炉和碱性氧气炉的操作效率。

Method: 方法包括构建状态空间模型结合质量平衡方程，使用改进卡尔曼滤波器和无迹卡尔曼滤波器（UKF）拟合，并以Cu和Cr元素评估噪声敏感性。

Result: 结果显示模型对大多数变量噪声水平低于10%时鲁棒，但对炉渣质量噪声敏感，准确性显著下降。

Conclusion: 结论强调了状态空间模型在工业实时估计废钢成分的可行性及其局限性。

Abstract: This study develops and analyzes linear and nonlinear state space models for
estimating the elemental composition of scrap steel used in steelmaking, with
applications to Electric Arc Furnace (EAF) and Basic Oxygen Furnace (BOF)
processes. The models incorporate mass balance equations and are fitted using a
modified Kalman filter for linear cases and the Unscented Kalman Filter (UKF)
for nonlinear cases. Using Cu and Cr as representative elements, we assess the
sensitivity of model predictions to measurement noise in key process variables,
including steel mass, steel composition, scrap input mass, slag mass, and iron
oxide fraction in slag. Results show that the models are robust to moderate
noise levels in most variables, particularly when errors are below $10\%$.
However, accuracy significantly deteriorates with noise in slag mass
estimation. These findings highlight the practical feasibility and limitations
of applying state space models for real-time scrap composition estimation in
industrial settings.

</details>

### [225] [Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes](https://arxiv.org/abs/2504.11355)
*Alberto Castillo,Elliot Pryor,Anas El Fathi,Boris Kovatchev,Marc Breton*

Main category: eess.SY

TLDR: 这篇论文介绍了使用优化采样数据集（OSDs）训练神经网络模仿模型预测控制（MPC），以在资源受限设备上高效实现，并展示了显著准确性提升和临床应用。


<details>
  <summary>Details</summary>
Motivation: 训练数据组成对神经网络准确性至关重要，但优化方法尚未充分探索；需要高效地在约束嵌入式设备上实现MPC。

Method: 引入OSDs概念和生成算法，OSDs是参数化数据子集，保留MPC信息、避免重复状态，并实现数据饱和。

Result: 训练NN复制特定MPC算法，准确性提高了四倍；两个OSDs训练的NN获得临床测试监管批准，作为首个直接用于人类胰岛素剂量的NN控制算法。

Conclusion: 这种方法为在资源受限平台上部署高级优化算法开辟新途径，可能彻底改变复杂算法的部署方式。

Abstract: Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)
algorithms is an effective way to implement them in constrained embedded
devices. By collecting large amounts of input-output data, where inputs
represent system states and outputs are MPC-generated control actions, NNs can
be trained to replicate MPC behavior at a fraction of the computational cost.
However, although the composition of the training data critically influences
the final NN accuracy, methods for systematically optimizing it remain
underexplored. In this paper, we introduce the concept of Optimally-Sampled
Datasets (OSDs) as ideal training sets and present an efficient algorithm for
generating them. An OSD is a parametrized subset of all the available data that
(i) preserves existing MPC information up to a certain numerical resolution,
(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or
complete. We demonstrate the effectiveness of OSDs by training NNs to replicate
the University of Virginia's MPC algorithm for automated insulin delivery in
Type-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,
two OSD-trained NNs received regulatory clearance for clinical testing as the
first NN-based control algorithm for direct human insulin dosing. This
methodology opens new pathways for implementing advanced optimizations on
resource-constrained embedded platforms, potentially revolutionizing how
complex algorithms are deployed.

</details>

### [226] [A Winner-Takes-All Mechanism for Event Generation](https://arxiv.org/abs/2504.11374)
*Yongkang Huo,Fuvio Forni,Rodolphe Sepulchre*

Main category: eess.SY

TLDR: 本论文提出了一种新框架，用于中心模式发生器设计，结合神经元反弹兴奋性和赢家通吃计算，统一决策和节律模式生成，适用于神经形态系统和机器人。


<details>
  <summary>Details</summary>
Motivation: 动机是利用神经元固有特性创建简单、易实现、可适应和鲁棒的网络架构，以解决决策和节律模式生成问题。

Method: 方法包括使用全互抑连接并添加可设计兴奋交互的网络架构，并通过环形振荡器模型进行演示。

Result: 结果显示了自适应相位和频率调制，证明了框架在易用性和鲁棒性方面的优势。

Conclusion: 结论是该框架在神经形态系统和机器人应用中具有显著潜力。

Abstract: We present a novel framework for central pattern generator design that
leverages the intrinsic rebound excitability of neurons in combination with
winner-takes-all computation. Our approach unifies decision-making and rhythmic
pattern generation within a simple yet powerful network architecture that
employs all-to-all inhibitory connections enhanced by designable excitatory
interactions. This design offers significant advantages regarding ease of
implementation, adaptability, and robustness. We demonstrate its efficacy
through a ring oscillator model, which exhibits adaptive phase and frequency
modulation, making the framework particularly promising for applications in
neuromorphic systems and robotics.

</details>

### [227] [eXplainable AI for data driven control: an inverse optimal control approach](https://arxiv.org/abs/2504.11446)
*Federico Porcari,Donatello Materassi,Simone Formentin*

Main category: eess.SY

TLDR: 本文提出基于逆最优控制的XAI方法，解释黑箱控制器的行为，通过提取成本函数权重提供局部解释。


<details>
  <summary>Details</summary>
Motivation: 理解黑箱数据驱动控制器的行为是现代控制设计的关键挑战。

Method: 使用逆线性二次问题求解，提取隐式成本函数中跟踪误差和控制努力的权重，并与LIME等技术关联。

Result: 数值例子显示，推断的成本函数能更深入理解控制器的决策过程，解释反直觉现象。

Conclusion: 该方法提供更透明、可解释的控制器目标表示，提升了对黑箱系统的理解。

Abstract: Understanding the behavior of black-box data-driven controllers is a key
challenge in modern control design. In this work, we propose an eXplainable AI
(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local
explanations for the behavior of a controller operating around a given region.
Specifically, we extract the weights assigned to tracking errors and control
effort in the implicit cost function that a black-box controller is optimizing,
offering a more transparent and interpretable representation of the
controller's underlying objectives. This approach presents connections with
well-established XAI techniques, such as Local Interpretable Model-agnostic
Explanations (LIME) since it is still based on a local approximation of the
control policy. However, rather being limited to a standard sensitivity
analysis, the explanation provided by our method relies on the solution of an
inverse Linear Quadratic (LQ) problem, offering a structured and more
control-relevant perspective. Numerical examples demonstrate that the inferred
cost function consistently provides a deeper understanding of the controller's
decision-making process, shedding light on otherwise counterintuitive or
unexpected phenomena.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [228] [A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field](https://arxiv.org/abs/2504.11064)
*Bo Ma,Yi Ji,Liyong Fang*

Main category: cs.MA

TLDR: 本文提出DSA-AAPF算法，改进传统APF方法，解决UAV障碍避免中的吸引力和局部极小值问题，模拟结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统APF方法存在吸引力分布不当和易陷局部极小值的局限性，导致UAV在复杂环境中可能碰撞障碍或无法达标。

Method: 提出DSA-AAPF算法，结合改进模拟退火和增强APF，融入领导者-跟随者策略、自适应重力增益和方向偏转机制，以平滑轨迹和逃离局部极小值。

Result: 模拟结果证明DSA-AAPF在编队重构、复杂障碍避免和逃离陷阱中的可行性、稳健性和优越性。

Conclusion: DSA-AAPF算法有效提升UAV障碍避免性能，解决了传统方法的缺陷。

Abstract: The traditional Artificial Potential Field (APF) method exhibits limitations
in its force distribution: excessive attraction when UAVs are far from the
target may cause collisions with obstacles, while insufficient attraction near
the goal often results in failure to reach the target. Furthermore, APF is
highly susceptible to local minima, compromising motion reliability in complex
environments. To address these challenges, this paper presents a novel hybrid
obstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial
Potential Field (DSA-AAPF)-which combines an improved simulated annealing
mechanism with an enhanced APF model. The proposed approach integrates a
Leader-Follower distributed formation strategy with the APF framework, where
the resultant force formulation is redefined to smooth UAV trajectories. An
adaptive gravitational gain function is introduced to dynamically adjust UAV
velocity based on environmental context, and a fast-converging controller
ensures accurate and efficient convergence to the target. Moreover, a
directional deflection mechanism is embedded within the simulated annealing
process, enabling UAVs to escape local minima caused by semi-enclosed obstacles
through continuous rotational motion. The simulation results, covering
formation reconfiguration, complex obstacle avoidance, and entrapment escape,
demonstrate the feasibility, robustness, and superiority of the proposed
DSA-AAPF algorithm.

</details>

### [229] [A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field](https://arxiv.org/abs/2504.11064)
*Bo Ma,Yi Ji,Liyong Fang*

Main category: cs.MA

TLDR: 本文提出DSA-AAPF混合算法，改善UAV障碍避免，解决传统APF的局限性，如局部最小值问题。


<details>
  <summary>Details</summary>
Motivation: 传统APF方法力分布不当，可能导致碰撞或无法达标，且易陷局部最小值，影响复杂环境下的可靠性。

Method: 结合改进模拟退火和增强APF，融入Leader-Follower策略、自适应重力增益、快速收敛控制器和方向偏转机制，以平滑轨迹并逃离局部最小值。

Result: 模拟结果证明DSA-AAPF在编队重构、复杂障碍避免和逃脱陷阱方面具有可行性、鲁棒性和优越性。

Conclusion: DSA-AAPF算法有效提升UAV运动可靠性和目标到达率，适用于复杂环境。

Abstract: The traditional Artificial Potential Field (APF) method exhibits limitations
in its force distribution: excessive attraction when UAVs are far from the
target may cause collisions with obstacles, while insufficient attraction near
the goal often results in failure to reach the target. Furthermore, APF is
highly susceptible to local minima, compromising motion reliability in complex
environments. To address these challenges, this paper presents a novel hybrid
obstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial
Potential Field (DSA-AAPF)-which combines an improved simulated annealing
mechanism with an enhanced APF model. The proposed approach integrates a
Leader-Follower distributed formation strategy with the APF framework, where
the resultant force formulation is redefined to smooth UAV trajectories. An
adaptive gravitational gain function is introduced to dynamically adjust UAV
velocity based on environmental context, and a fast-converging controller
ensures accurate and efficient convergence to the target. Moreover, a
directional deflection mechanism is embedded within the simulated annealing
process, enabling UAVs to escape local minima caused by semi-enclosed obstacles
through continuous rotational motion. The simulation results, covering
formation reconfiguration, complex obstacle avoidance, and entrapment escape,
demonstrate the feasibility, robustness, and superiority of the proposed
DSA-AAPF algorithm.

</details>

### [230] [LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems](https://arxiv.org/abs/2504.10915)
*Rajesh Ranjan,Shailja Gupta,Surya Narayan Singh*

Main category: cs.MA

TLDR: LOKA 协议是一种新的架构，用于构建道德治理的 AI 代理生态系统，解决了身份、责任和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着自主 AI 代理的兴起，数字生态系统中存在身份、责任和伦理一致性的基础差距。

Method: 提出 LOKA 协议，包括通用代理身份层、意图中心通信协议和去中心化伦理共识协议，使用去中心化标识符、可验证凭证和后量子密码学。

Result: 提供了一个可扩展的、面向未来的 AI 治理蓝图。

Conclusion: 通过将身份、信任和伦理嵌入协议层，奠定了负责、透明和自主 AI 生态系统的基础。

Abstract: The rise of autonomous AI agents, capable of perceiving, reasoning, and
acting independently, signals a profound shift in how digital ecosystems
operate, govern, and evolve. As these agents proliferate beyond centralized
infrastructures, they expose foundational gaps in identity, accountability, and
ethical alignment. Three critical questions emerge: Identity: Who or what is
the agent? Accountability: Can its actions be verified, audited, and trusted?
Ethical Consensus: Can autonomous systems reliably align with human values and
prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered
Orchestration for Knowledgeful Agents), a unified, systems-level architecture
for building ethically governed, interoperable AI agent ecosystems. LOKA
introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized,
verifiable identity; intent-centric communication protocols for semantic
coordination across diverse agents; and a Decentralized Ethical Consensus
Protocol (DECP) that enables agents to make context-aware decisions grounded in
shared ethical baselines. Anchored in emerging standards such as Decentralized
Identifiers (DIDs), Verifiable Credentials (VCs), and post-quantum
cryptography, LOKA offers a scalable, future-resilient blueprint for
multi-agent AI governance. By embedding identity, trust, and ethics into the
protocol layer itself, LOKA establishes the foundation for a new era of
responsible, transparent, and autonomous AI ecosystems operating across digital
and physical domains.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [231] [Automated Testing of COBOL to Java Transformation](https://arxiv.org/abs/2504.10548)
*Sandeep Hans,Atul Kumar,Toshikai Yasue,Kouichi Ono,Saravanan Krishnan,Devika Sondhi,Fumiko Satoh,Gerald Mitchell,Sachin Kumar,Diptikalyan Saha*

Main category: cs.SE

TLDR: 本论文分享了开发IBM WCA4Z测试框架的经验，用于自动化验证COBOL到Java翻译的函数等价性。


<details>
  <summary>Details</summary>
Motivation: LLM-based代码翻译虽有进展，但不可靠，需要手动验证，耗时费力，因此开发自动化测试框架。

Method: 使用符号执行生成COBOL单元测试，模拟外部调用，并转换为JUnit测试以验证Java代码的语义等价性。

Result: 框架有助于识别和修复翻译差异，并为AI模型提供改进反馈。

Conclusion: 该框架在工业环境中有效提高了翻译验证的效率和准确性。

Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques
have made it feasible to translate enterprise-level code from legacy languages
such as COBOL to modern languages such as Java or Python. While the results of
LLM-based automatic transformation are encouraging, the resulting code cannot
be trusted to correctly translate the original code, making manual validation
of translated Java code from COBOL a necessary but time-consuming and
labor-intensive process. In this paper, we share our experience of developing a
testing framework for IBM Watsonx Code Assistant for Z (WCA4Z) [5], an
industrial tool designed for COBOL to Java translation. The framework automates
the process of testing the functional equivalence of the translated Java code
against the original COBOL programs in an industry context. Our framework uses
symbolic execution to generate unit tests for COBOL, mocking external calls and
transforming them into JUnit tests to validate semantic equivalence with
translated Java. The results not only help identify and repair any detected
discrepancies but also provide feedback to improve the AI model.

</details>

### [232] [The Code Barrier: What LLMs Actually Understand?](https://arxiv.org/abs/2504.10557)
*Serge Lionel Nikiema,Jordan Samhi,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TLDR: 本研究使用代码混淆测试框架评估LLM的代码语义理解能力，发现通用模型比代码专用模型更具韧性，但整体能力有限。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLM是否具有真正的语义理解，而非仅限于标记识别。

Method: 通过对源代码施加受控混淆，并通过描述混淆代码和去混淆任务评估13个模型（包括代码专用和通用模型），使用CodeNet基准的250个Java问题。

Result: 混淆复杂度增加时性能显著下降；通用模型显示意外韧性；模型能识别混淆技术，但重建程序逻辑能力受限。

Conclusion: 引入新评估方法，为逆向工程和对抗代码分析等安全关键应用建立经验基准。

Abstract: Understanding code represents a core ability needed for automating software
development tasks. While foundation models like LLMs show impressive results
across many software engineering challenges, the extent of their true semantic
understanding beyond simple token recognition remains unclear. This research
uses code obfuscation as a structured testing framework to evaluate LLMs'
semantic understanding capabilities. We methodically apply controlled
obfuscation changes to source code and measure comprehension through two
complementary tasks: generating accurate descriptions of obfuscated code and
performing deobfuscation, a skill with important implications for reverse
engineering applications.
  Our testing approach includes 13 cutting-edge models, covering both
code-specialized (e.g., StarCoder2) and general-purpose (e.g., GPT-4o)
architectures, evaluated on a benchmark created from CodeNet and consisting of
filtered 250 Java programming problems and their solutions. Findings show a
statistically significant performance decline as obfuscation complexity
increases, with unexpected resilience shown by general-purpose models compared
to their code-focused counterparts. While some models successfully identify
obfuscation techniques, their ability to reconstruct the underlying program
logic remains constrained, suggesting limitations in their semantic
representation mechanisms. This research introduces a new evaluation approach
for assessing code comprehension in language models and establishes empirical
baselines for advancing research in security-critical code analysis
applications such as reverse engineering and adversarial code analysis.

</details>

### [233] [QualiTagger: Automating software quality detection in issue trackers](https://arxiv.org/abs/2504.11053)
*Karthik Shivashankar,Rafael Capilla,Maren Maritsdatter Kruke,Mili Orucevic,Antonio Martini*

Main category: cs.SE

TLDR: 本研究使用Transformer模型和GitHub数据集识别软件质量属性文本，并评估其实际应用。


<details>
  <summary>Details</summary>
Motivation: 软件质量下降可能导致技术债务等问题，现有方法不实用且数据集有限。

Method: 采用Transformer等机器学习模型，从GitHub挖掘大数据集，识别自然语言中质量属性文本，并通过学生和工业场景评估。

Result: 研究了质量属性在问题跟踪器中的分布，并验证了方法在识别安全标签等方面的有效性。

Conclusion: 改进了质量属性识别技术，使其更适用于实际软件开发。

Abstract: A systems quality is a major concern for development teams when it evolve.
Understanding the effects of a loss of quality in the codebase is crucial to
avoid side effects like the appearance of technical debt. Although the
identification of these qualities in software requirements described in natural
language has been investigated, most of the results are often not applicable in
practice, and rely on having been validated on small datasets and limited
amount of projects. For many years, machine learning (ML) techniques have been
proved as a valid technique to identify and tag terms described in natural
language. In order to advance previous works, in this research we use cutting
edge models like Transformers, together with a vast dataset mined and curated
from GitHub, to identify what text is usually associated with different quality
properties. We also study the distribution of such qualities in issue trackers
from openly accessible software repositories, and we evaluate our approach both
with students from a software engineering course and with its application to
recognize security labels in industry.

</details>

### [234] [Scalability and Maintainability Challenges and Solutions in Machine Learning: Systematic Literature Review](https://arxiv.org/abs/2504.11079)
*Karthik Shivashankar,Ghadi S. Al Hajj,Antonio Martini*

Main category: cs.SE

TLDR: 本篇系统文献综述探讨了机器学习系统中可扩展性和可维护性的挑战与解决方案，分析了124篇论文。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用的复杂性和广泛应用不断增加，平衡系统可扩展性和长期可维护性的需求变得日益重要。

Method: 通过系统文献综述，分析了124篇论文，识别并分类了41个可维护性挑战和13个可扩展性挑战，并围绕六个主要研究问题构建结构。

Result: 发现可扩展性和可维护性之间存在复杂的相互依赖关系，并提供了挑战和解决方案的分类。

Conclusion: 为研究者和从业者提供宝贵见解，指导未来研究方向、告知最佳实践，并促进更稳健、高效和可持续的机器学习应用开发。

Abstract: This systematic literature review examines the critical challenges and
solutions related to scalability and maintainability in Machine Learning (ML)
systems. As ML applications become increasingly complex and widespread across
industries, the need to balance system scalability with long-term
maintainability has emerged as a significant concern. This review synthesizes
current research and practices addressing these dual challenges across the
entire ML life-cycle, from data engineering to model deployment in production.
We analyzed 124 papers to identify and categorize 41 maintainability challenges
and 13 scalability challenges, along with their corresponding solutions. Our
findings reveal intricate inter dependencies between scalability and
maintainability, where improvements in one often impact the other.
  The review is structured around six primary research questions, examining
maintainability and scalability challenges in data engineering, model
engineering, and ML system development. We explore how these challenges
manifest differently across various stages of the ML life-cycle.
  This comprehensive overview offers valuable insights for both researchers and
practitioners in the field of ML systems. It aims to guide future research
directions, inform best practices, and contribute to the development of more
robust, efficient, and sustainable ML applications across various domains.

</details>

### [235] [TD-Suite: All Batteries Included Framework for Technical Debt Classification](https://arxiv.org/abs/2504.11085)
*Karthik Shivashankar,Antonio Martini*

Main category: cs.SE

TLDR: TD-Suite 是一个软件框架，使用 transformer 模型自动分类软件项目中的技术债务，包括二元分类和类别分类，并具有鲁棒训练和可持续性跟踪功能。


<details>
  <summary>Details</summary>
Motivation: 解决软件项目中技术债务的持久挑战，通过提供自动分类工具。

Method: 利用 transformer 模型进行自然语言处理，分析文本工件；端到端管道包括数据摄取、预处理、模型训练（使用 k 折交叉验证、提前停止、类权重调整）、评估和推理；并提供用户友好的 Gradio web 接口。

Result: 实现技术债务类别的准确分类，处理不平衡数据集，并跟踪训练过程中的碳排放。

Conclusion: TD-Suite 提供了一个全面、易用的技术债务管理解决方案，提升软件开发实践。

Abstract: Recognizing that technical debt is a persistent and significant challenge
requiring sophisticated management tools, TD-Suite offers a comprehensive
software framework specifically engineered to automate the complex task of its
classification within software projects. It leverages the advanced natural
language understanding of state-of-the-art transformer models to analyze
textual artifacts, such as developer discussions in issue reports, where subtle
indicators of debt often lie hidden.
  TD-Suite provides a seamless end-to-end pipeline, managing everything from
initial data ingestion and rigorous preprocessing to model training, thorough
evaluation, and final inference. This allows it to support both straightforward
binary classification (debt or no debt) and more valuable, identifying specific
categories like code, design, or documentation debt, thus enabling more
targeted management strategies.
  To ensure the generated models are robust and perform reliably on real-world,
often imbalanced, datasets, TD-Suite incorporates critical training
methodologies: k-fold cross-validation assesses generalization capability,
early stopping mechanisms prevent overfitting to the training data, and class
weighting strategies effectively address skewed data distributions. Beyond core
functionality, and acknowledging the growing importance of sustainability, the
framework integrates tracking and reporting of carbon emissions associated with
the computationally intensive model training process.
  It also features a user-friendly Gradio web interface in a Docker container
setup, simplifying model interaction, evaluation, and inference.

</details>

### [236] [Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java](https://arxiv.org/abs/2504.11335)
*Gopichand Bandarupalli*

Main category: cs.SE

TLDR: 本研究使用AI将遗留COBOL代码现代化为Java，实现93%准确率和显著效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决老化软件系统中的关键挑战，特别是将COBOL代码升级为Java，对银行和保险等行业至关重要。

Method: 利用Legacy COBOL 2024 Corpus（50,000个COBOL文件），通过Java解析代码、AI建议升级、React可视化收益。

Result: 实现93%准确率，复杂度降低35%（从18到11.7），耦合度降低33%（从8到5.4），优于手动方法（75%）和基于规则工具（82%）。

Conclusion: 提供可扩展路径来振兴COBOL系统，助力行业现代化。

Abstract: This study investigates AI-driven modernization of legacy COBOL code into
Java, addressing a critical challenge in aging software systems. Leveraging the
Legacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise
sources -- Java parses the code, AI suggests upgrades, and React visualizes
gains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and
coupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based
tools (82%). The approach offers a scalable path to rejuvenate COBOL systems,
vital for industries like banking and insurance.

</details>

<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [237] [Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets](https://arxiv.org/abs/2504.11258)
*Liam Welsh,Udit Grover,Sebastian Jaimungal*

Main category: q-fin.MF

TLDR: 本论文研究了通过抵消信用市场控制温室气体排放的Nash均衡，使用强化学习方法Nash-DQN估计，并通过数值实验证明了其有效性和财务节约潜力。


<details>
  <summary>Details</summary>
Motivation: 气候变化威胁人类未来，人为温室气体排放加剧，政府可通过排放限额和惩罚措施控制排放，鼓励企业投资碳减排项目以生成抵消信用。

Method: 使用强化学习技术Nash-DQN高效估计有限代理Nash均衡。

Result: 数值实验证明了强化学习方法在气候金融市场的有效性，以及企业遵守Nash均衡时可实现的显著财务节约。

Conclusion: 强化学习方法可有效应用于气候主题金融市场，帮助企业实现财务利益。

Abstract: Climate change is a major threat to the future of humanity, and its impacts
are being intensified by excess man-made greenhouse gas emissions. One method
governments can employ to control these emissions is to provide firms with
emission limits and penalize any excess emissions above the limit. Excess
emissions may also be offset by firms who choose to invest in carbon reducing
and capturing projects. These projects generate offset credits which can be
submitted to a regulating agency to offset a firm's excess emissions, or they
can be traded with other firms. In this work, we characterize the finite-agent
Nash equilibrium for offset credit markets. As computing Nash equilibria is an
NP-hard problem, we utilize the modern reinforcement learning technique
Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate
not only the validity of employing reinforcement learning methods applied to
climate themed financial markets, but also the significant financial savings
emitting firms may achieve when abiding by the Nash equilibria through
numerical experiments.

</details>

<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [238] [Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning](https://arxiv.org/abs/2504.10707)
*Haoyu Ji,Yalan Song,Tadd Bindas,Chaopeng Shen,Yuan Yang,Ming Pan,Jiangtao Liu,Farshid Rahmani,Ather Abbas,Hylke Beck,Yoshihide Wada,Kathryn Lawson*

Main category: physics.geo-ph

TLDR: 本论文引入一个高分辨率、物理嵌入式的大数据训练模型，用于捕获水文响应模式及其变化，揭示全球水资源分配的重大转变，并提供更准确的模拟结果。


<details>
  <summary>Details</summary>
Motivation: 全球水模型需要真实表示水文系统的响应模式，但受限于从数据中学习的能力，无法有效跟踪快速变化。

Method: 引入一个高分辨率、物理嵌入式的大数据训练模型。

Result: 模型揭示了全球基础流比和绿蓝水分配变化（高达20%），导致洪水风险增加、水供给压力升高，并提供比当前系统更准确的月度和日尺度模拟。

Conclusion: 这种模型使全球规模模型能够提供可靠的本地相关水管理洞察，帮助预测季节性水可用性和突出管理挑战区域。

Abstract: To track rapid changes within our water sector, Global Water Models (GWMs)
need to realistically represent hydrologic systems' response patterns - such as
baseflow fraction - but are hindered by their limited ability to learn from
data. Here we introduce a high-resolution physics-embedded big-data-trained
model as a breakthrough in reliably capturing characteristic hydrologic
response patterns ('signatures') and their shifts. By realistically
representing the long-term water balance, the model revealed widespread shifts
- up to ~20% over 20 years - in fundamental green-blue-water partitioning and
baseflow ratios worldwide. Shifts in these response patterns, previously
considered static, contributed to increasing flood risks in northern
mid-latitudes, heightening water supply stresses in southern subtropical
regions, and declining freshwater inputs to many European estuaries, all with
ecological implications. With more accurate simulations at monthly and daily
scales than current operational systems, this next-generation model resolves
large, nonlinear seasonal runoff responses to rainfall ('elasticity') and
streamflow flashiness in semi-arid and arid regions. These metrics highlight
regions with management challenges due to large water supply variability and
high climate sensitivity, but also provide tools to forecast seasonal water
availability. This capability newly enables global-scale models to deliver
reliable and locally relevant insights for water management.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [239] [BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs](https://arxiv.org/abs/2504.10948)
*Mohammad Matin Najafi,Xianju Zhu,Chrysanthi Kosyfaki,Laks V. S. Lakshmanan,Reynold Cheng*

Main category: cs.DS

TLDR: 这篇论文引入BEACON基准，用于评估子图计数中的算法和机器学习方法，揭示其优缺点，促进研究统一。


<details>
  <summary>Details</summary>
Motivation: 子图计数任务缺乏统一框架、标准化数据集和ground truth，阻碍了系统分析和公平比较。

Method: 引入BEACON，提供标准化数据集、评估环境和排行榜，评估AL和ML方法。

Result: 实验显示AL方法高效处理大型图但复杂模式困难，ML方法能处理大模式但需大量数据，在小密集图上准确性不足。

Conclusion: BEACON推动子图计数研究统一，突出AL和ML权衡，鼓励未来创新。

Abstract: Subgraph counting the task of determining the number of instances of a query
pattern within a large graph lies at the heart of many critical applications,
from analyzing financial networks and transportation systems to understanding
biological interactions. Despite decades of work yielding efficient algorithmic
(AL) solutions and, more recently, machine learning (ML) approaches, a clear
comparative understanding is elusive. This gap stems from the absence of a
unified evaluation framework, standardized datasets, and accessible ground
truths, all of which hinder systematic analysis and fair benchmarking. To
overcome these barriers, we introduce BEACON: a comprehensive benchmark
designed to rigorously evaluate both AL and ML-based subgraph counting methods.
BEACON provides a standardized dataset with verified ground truths, an
integrated evaluation environment, and a public leaderboard, enabling
reproducible and transparent comparisons across diverse approaches. Our
extensive experiments reveal that while AL methods excel in efficiently
counting subgraphs on very large graphs, they struggle with complex patterns
(e.g., those exceeding six nodes). In contrast, ML methods are capable of
handling larger patterns but demand massive graph data inputs and often yield
suboptimal accuracy on small, dense graphs. These insights not only highlight
the unique strengths and limitations of each approach but also pave the way for
future advancements in subgraph counting techniques. Overall, BEACON represents
a significant step towards unifying and accelerating research in subgraph
counting, encouraging innovative solutions and fostering a deeper understanding
of the trade-offs between algorithmic and machine learning paradigms.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [240] [SDFs from Unoriented Point Clouds using Neural Variational Heat Distances](https://arxiv.org/abs/2504.11212)
*Samuel Weidemaier,Florine Hartwig,Josua Sassen,Sergio Conti,Mirela Ben-Chen,Martin Rumpf*

Main category: math.NA

TLDR: 本论文提出一种新变分方法，使用热方法代替eikonal方程，通过神经网络从无向点云计算符号距离场（SDF），并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是将热方法从离散表面扩展到神经领域，以改进从点云计算SDF的准确性和一致性。

Method: 方法包括使用神经网络解决两个凸优化问题：先通过热流近似无符号距离场的梯度，然后计算SDF。

Result: 结果显示该方法实现了最先进的表面重建、一致的SDF梯度，并可用于零水平集上求解PDE。

Conclusion: 结论是变分问题良定义，通过数值实验验证了方法的有效性。

Abstract: We propose a novel variational approach for computing neural Signed Distance
Fields (SDF) from unoriented point clouds. To this end, we replace the commonly
used eikonal equation with the heat method, carrying over to the neural domain
what has long been standard practice for computing distances on discrete
surfaces. This yields two convex optimization problems for whose solution we
employ neural networks: We first compute a neural approximation of the
gradients of the unsigned distance field through a small time step of heat flow
with weighted point cloud densities as initial data. Then we use it to compute
a neural approximation of the SDF. We prove that the underlying variational
problems are well-posed. Through numerical experiments, we demonstrate that our
method provides state-of-the-art surface reconstruction and consistent SDF
gradients. Furthermore, we show in a proof-of-concept that it is accurate
enough for solving a PDE on the zero-level set.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [241] [Wasserstein Distributionally Regret Optimization](https://arxiv.org/abs/2504.10796)
*Lukas-Benedikt Fiechtner,Jose Blanchet*

Main category: math.OC

TLDR: 本论文提出DRRO框架以减少DRO的保守性，通过Wasserstein方法分析，提供计算和松弛方案。


<details>
  <summary>Details</summary>
Motivation: DRO的对抗性可能导致过于保守的解决方案，因此引入DRRO优化遗憾。

Method: 系统分析Wasserstein DRRO，包括与ERM比较、newsvendor问题求解、NP-hard证明和凸松弛方法。

Result: 证明DRRO在平滑条件下与ERM一致，regret计算方法，NP-hard结果，凸松弛性能良好，并给出优化间隙上界。

Conclusion: 提出的松弛方法有效，并改善现有方法的上界。

Abstract: Distributionally Robust Optimization (DRO) is a popular framework for
decision-making under uncertainty, but its adversarial nature can lead to
overly conservative solutions. To address this, we study ex-ante
Distributionally Robust Regret Optimization (DRRO), focusing on
Wasserstein-based ambiguity sets which are popular due to their links to
regularization and machine learning. We provide a systematic analysis of
Wasserstein DRRO, paralleling known results for Wasserstein DRO. Under
smoothness and regularity conditions, we show that Wasserstein DRRO coincides
with Empirical Risk Minimization (ERM) up to first-order terms, and exactly so
in convex quadratic settings. We revisit the Wasserstein DRRO newsvendor
problem, where the loss is the maximum of two linear functions of demand and
decision. Extending [25], we show that the regret can be computed by maximizing
two one-dimensional concave functions. For more general loss functions
involving the maximum of multiple linear terms in multivariate random variables
and decision vectors, we prove that computing the regret and thus also the DRRO
policy is NP-hard. We then propose a convex relaxation for these more general
Wasserstein DRRO problems and demonstrate its strong empirical performance.
Finally, we provide an upper bound on the optimality gap of our relaxation and
show it improves over recent alternatives.

</details>

### [242] [Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on Numerical Black-box Optimization Problems](https://arxiv.org/abs/2504.11440)
*Lennart Schäpermeier*

Main category: math.OC

TLDR: 本论文提出了一种贪婪重启调度方法，通过迭代选择最佳算法，显著提升优化性能，并在BBOB测试台上接近虚拟最佳求解器。


<details>
  <summary>Details</summary>
Motivation: 优化领域存在多种求解器，各有优劣，元算法方法旨在通过算法选择和调度提升性能，但数据驱动的重启调度研究不足。

Method: 提出简单调度方法，迭代选择在未解决问题分布上表现最好的算法，形成问题无关的贪婪重启调度。

Result: 在BBOB测试台上，使用数值黑箱优化求解器，显著缩小单一和虚拟最佳求解器差距，在多种评估协议下表现良好。

Conclusion: 该方法为更复杂的动态算法选择模型提供强大基线。

Abstract: In many optimization domains, there are multiple different solvers that
contribute to the overall state-of-the-art, each performing better on some, and
worse on other types of problem instances. Meta-algorithmic approaches, such as
instance-based algorithm selection, configuration and scheduling, aim to close
this gap by extracting the most performance possible from a set of
(configurable) optimizers. In this context, the best performing individual
algorithms are often hand-crafted hybrid heuristics which perform many restarts
of fast local optimization approaches. However, data-driven techniques to
create optimized restart schedules have not yet been extensively studied.
  Here, we present a simple scheduling approach that iteratively selects the
algorithm performing best on the distribution of unsolved training problems at
time of selection, resulting in a problem-independent solver schedule. We
demonstrate our approach using well-known optimizers from numerical black-box
optimization on the BBOB testbed, bridging much of the gap between single and
virtual best solver from the original portfolio across various evaluation
protocols. Our greedy restart schedule presents a powerful baseline for more
complex dynamic algorithm selection models.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [243] [SPreV](https://arxiv.org/abs/2504.10620)
*Srivathsan Amruth*

Main category: cs.GR

TLDR: SPREV 是一种新型降维技术，用于处理小类大小、高维度和低样本量的标记数据集，并可视化隐藏模式。


<details>
  <summary>Details</summary>
Motivation: 解决降维和可视化挑战，特别是针对小类大小、高维度和低样本量的数据集。

Method: 整合几何原则，并适应离散计算环境。

Result: 用户能够识别趋势、提取洞见，并高效导航复杂数据。

Conclusion: SPREV 是现代数据科学工具包中不可或缺的工具。

Abstract: SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for
Visualisation, is a novel dimensionality reduction technique developed to
address the challenges of reducing dimensions and visualizing labeled datasets
that exhibit a unique combination of three characteristics: small class size,
high dimensionality, and low sample size. SPREV is designed not only to uncover
but also to visually represent hidden patterns within such datasets. Its
distinctive integration of geometric principles, adapted for discrete
computational environments, makes it an indispensable tool in the modern data
science toolkit, enabling users to identify trends, extract insights, and
navigate complex data efficiently and effectively.

</details>

### [244] [VideoPanda: Video Panoramic Diffusion with Multi-view Attention](https://arxiv.org/abs/2504.11389)
*Kevin Xie,Amirmojtaba Sabour,Jiahui Huang,Despoina Paschalidou,Greg Klar,Umar Iqbal,Sanja Fidler,Xiaohui Zeng*

Main category: cs.GR

TLDR: 本文提出VideoPanda方法，通过文本或单视图视频合成高质量360度全景视频。


<details>
  <summary>Details</summary>
Motivation: 高分辨率全景视频采集困难，需要专用设备，本文旨在简化合成过程。

Method: 使用多视图注意力层增强视频扩散模型，联合训练文本和单视图条件，支持自回归长视频生成，并通过随机采样减少计算负担。

Result: 实验显示生成更真实、更连贯的360度全景视频，优于现有方法。

Conclusion: VideoPanda有效提升了360度视频合成的质量和效率。

Abstract: High resolution panoramic video content is paramount for immersive
experiences in Virtual Reality, but is non-trivial to collect as it requires
specialized equipment and intricate camera setups. In this work, we introduce
VideoPanda, a novel approach for synthesizing 360$^\circ$ videos conditioned on
text or single-view video data. VideoPanda leverages multi-view attention
layers to augment a video diffusion model, enabling it to generate consistent
multi-view videos that can be combined into immersive panoramic content.
VideoPanda is trained jointly using two conditions: text-only and single-view
video, and supports autoregressive generation of long-videos. To overcome the
computational burden of multi-view video generation, we randomly subsample the
duration and camera views used during training and show that the model is able
to gracefully generalize to generating more frames during inference. Extensive
evaluations on both real-world and synthetic video datasets demonstrate that
VideoPanda generates more realistic and coherent 360$^\circ$ panoramas across
all input conditions compared to existing methods. Visit the project website at
https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [245] [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
*Yijun Liang,Ming Li,Chenrui Fan,Ziyue Li,Dang Nguyen,Kwesi Cobbina,Shweta Bhardwaj,Jiuhai Chen,Fuxiao Liu,Tianyi Zhou*

Main category: cs.CV

TLDR: 这篇论文引入ColorBench基准来评估视觉语言模型（VLMs）的颜色理解能力，并揭示了关键发现。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs是否和如何像人类一样感知和利用颜色，因为颜色在视觉推理中很重要。

Method: 引入ColorBench，这是一个精心设计的基准，包含各种测试场景，用于评估VLMs的颜色感知、推理和鲁棒性。

Result: 评估32个VLMs后发现：缩放定律成立，语言模型比视觉编码器更重要；性能差距小，表明颜色理解被忽略；CoT推理改善准确性和鲁棒性；颜色线索可能误导模型。

Conclusion: 当前VLMs在颜色理解方面有局限性，需要改进，ColorBench可作为推进多模态AI研究的基础工具。

Abstract: Color plays an important role in human perception and usually provides
critical clues in visual reasoning. However, it is unclear whether and how
vision-language models (VLMs) can perceive, understand, and leverage color as
humans. This paper introduces ColorBench, an innovative benchmark meticulously
crafted to assess the capabilities of VLMs in color understanding, including
color perception, reasoning, and robustness. By curating a suite of diverse
test scenarios, with grounding in real applications, ColorBench evaluates how
these models perceive colors, infer meanings from color-based cues, and
maintain consistent performance under varying color transformations. Through an
extensive evaluation of 32 VLMs with varying language models and vision
encoders, our paper reveals some undiscovered findings: (i) The scaling law
(larger models are better) still holds on ColorBench, while the language model
plays a more important role than the vision encoder. (ii) However, the
performance gaps across models are relatively small, indicating that color
understanding has been largely neglected by existing VLMs. (iii) CoT reasoning
improves color understanding accuracies and robustness, though they are
vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on
ColorBench but they can also mislead models in some tasks. These findings
highlight the critical limitations of current VLMs and underscore the need to
enhance color comprehension. Our ColorBenchcan serve as a foundational tool for
advancing the study of human-level color understanding of multimodal AI.

</details>

### [246] [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
*Yijun Liang,Ming Li,Chenrui Fan,Ziyue Li,Dang Nguyen,Kwesi Cobbina,Shweta Bhardwaj,Jiuhai Chen,Fuxiao Liu,Tianyi Zhou*

Main category: cs.CV

TLDR: 这篇论文引入ColorBench基准，用于评估视觉语言模型（VLMs）在颜色理解方面的能力，包括感知、推理和鲁棒性，并揭示关键发现。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs是否和如何像人类一样感知和利用颜色，揭示当前AI在颜色理解方面的不足。

Method: 开发ColorBench基准，通过多样化的测试场景评估32个VLMs在颜色感知、推理和鲁棒性方面的表现。

Result: 发现规模定律依然适用，语言模型比视觉编码器更重要；模型间性能差距小；CoT推理改善准确性和鲁棒性；颜色线索可能误导模型。

Conclusion: 当前VLMs在颜色理解方面存在局限性，需要改进，ColorBench可作为推进多模态AI颜色理解的工具。

Abstract: Color plays an important role in human perception and usually provides
critical clues in visual reasoning. However, it is unclear whether and how
vision-language models (VLMs) can perceive, understand, and leverage color as
humans. This paper introduces ColorBench, an innovative benchmark meticulously
crafted to assess the capabilities of VLMs in color understanding, including
color perception, reasoning, and robustness. By curating a suite of diverse
test scenarios, with grounding in real applications, ColorBench evaluates how
these models perceive colors, infer meanings from color-based cues, and
maintain consistent performance under varying color transformations. Through an
extensive evaluation of 32 VLMs with varying language models and vision
encoders, our paper reveals some undiscovered findings: (i) The scaling law
(larger models are better) still holds on ColorBench, while the language model
plays a more important role than the vision encoder. (ii) However, the
performance gaps across models are relatively small, indicating that color
understanding has been largely neglected by existing VLMs. (iii) CoT reasoning
improves color understanding accuracies and robustness, though they are
vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on
ColorBench but they can also mislead models in some tasks. These findings
highlight the critical limitations of current VLMs and underscore the need to
enhance color comprehension. Our ColorBenchcan serve as a foundational tool for
advancing the study of human-level color understanding of multimodal AI.

</details>

### [247] [NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results](https://arxiv.org/abs/2504.10685)
*Yuqian Fu,Xingyu Qiu,Bin Ren,Yanwei Fu,Radu Timofte,Nicu Sebe,Ming-Hsuan Yang,Luc Van Gool,Kaijin Zhang,Qingpeng Nong,Xiugang Dong,Hong Gao,Xiangsheng Zhou,Jiancheng Pan,Yanxing Liu,Xiao He,Jiahao Li,Yuze Sun,Xiaomeng Huang,Zhenyu Zhang,Ran Ma,Yuhan Liu,Zijian Zhuang,Shuai Yi,Yixiong Zou,Lingyi Hong,Mingxi Chen,Runze Li,Xingdong Sheng,Wenqiang Zhang,Weisen Chen,Yongxin Yan,Xinguo Chen,Yuanjie Shao,Zhengrong Zuo,Nong Sang,Hao Wu,Haoran Sun,Shuming Hu,Yan Zhang,Zhiguang Shi,Yu Zhang,Chao Chen,Tao Wang,Da Feng,Linhai Zhuo,Ziming Lin,Yali Huang,Jie Me,Yiming Yang,Mi Guo,Mingyuan Jiu,Mingliang Xu,Maomao Xiong,Qunshu Zhang,Xinyu Cao,Yuqing Yang,Dianmo Sheng,Xuanpu Zhao,Zhiyu Li,Xuyang Ding,Wenqian Li*

Main category: cs.CV

TLDR: 这篇论文概述了第1届NTIRE 2025跨域少样本物体检测挑战赛，旨在提升检测器在新域上的性能，并总结了参与者的成果。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本物体检测的挑战，并通过挑战赛推进检测器在新域有限数据下的性能。

Method: 组织挑战赛，吸引参与者提出新模型，在开源和闭源设置下优化检测方法。

Result: 152人注册，42个团队提交，13个团队有效最终提交，并实现了新的最先进结果。

Conclusion: 挑战赛成功展示了创新解决方案，并对参与者结果进行了总结。

Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges
to existing object detection and few-shot detection models when applied across
domains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD
Challenge, aiming to advance the performance of current object detectors on
entirely novel target domains with only limited labeled data. The challenge
attracted 152 registered participants, received submissions from 42 teams, and
concluded with 13 teams making valid final submissions. Participants approached
the task from diverse perspectives, proposing novel models that achieved new
state-of-the-art (SOTA) results under both open-source and closed-source
settings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD
Challenge, highlighting the proposed solutions and summarizing the results
submitted by the participants.

</details>

### [248] [CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates](https://arxiv.org/abs/2504.10738)
*Ankit Kumar Shaw,Kun Jiang,Tuopu Wen,Chandan Kumar Sah,Yining Shi,Mengmeng Yang,Diange Yang,Xiaoli Lian*

Main category: cs.CV

TLDR: CleanMAP 是一种基于多模态大语言模型的框架，用于过滤众包数据以实现高精度高清地图更新。


<details>
  <summary>Details</summary>
Motivation: 智能网联汽车和车辆-道路-云系统快速发展，对实时高清地图更新需求增加，但众包数据因运动模糊、光照变化、恶劣天气和车道标记退化等因素存在不一致性问题。

Method: CleanMAP 利用 MLLM 驱动的车道可见性评分模型量化视觉参数并分配置信度分数（0-10），结合动态分段置信度函数和基于置信度的局部地图融合策略优化数据。

Result: 实验结果显示，融合前三张局部地图的平均更新错误为 0.28m，比基线（0.37m）低，并与人类评估一致性达 84.88%。

Conclusion: CleanMAP 被确立为可扩展的解决方案，确保自动导航更精确和可靠。

Abstract: The rapid growth of intelligent connected vehicles (ICVs) and integrated
vehicle-road-cloud systems has increased the demand for accurate, real-time HD
map updates. However, ensuring map reliability remains challenging due to
inconsistencies in crowdsourced data, which suffer from motion blur, lighting
variations, adverse weather, and lane marking degradation. This paper
introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based
distillation framework designed to filter and refine crowdsourced data for
high-confidence HD map updates. CleanMAP leverages an MLLM-driven lane
visibility scoring model that systematically quantifies key visual parameters,
assigning confidence scores (0-10) based on their impact on lane detection. A
novel dynamic piecewise confidence-scoring function adapts scores based on lane
visibility, ensuring strong alignment with human evaluations while effectively
filtering unreliable data. To further optimize map accuracy, a
confidence-driven local map fusion strategy ranks and selects the top-k
highest-scoring local maps within an optimal confidence range (best score minus
10%), striking a balance between data quality and quantity. Experimental
evaluations on a real-world autonomous vehicle dataset validate CleanMAP's
effectiveness, demonstrating that fusing the top three local maps achieves the
lowest mean map update error of 0.28m, outperforming the baseline (0.37m) and
meeting stringent accuracy thresholds (<= 0.32m). Further validation with
real-vehicle data confirms 84.88% alignment with human evaluators, reinforcing
the model's robustness and reliability. This work establishes CleanMAP as a
scalable and deployable solution for crowdsourced HD map updates, ensuring more
precise and reliable autonomous navigation. The code will be available at
https://Ankit-Zefan.github.io/CleanMap/

</details>

### [249] [Hearing Anywhere in Any Environment](https://arxiv.org/abs/2504.10746)
*Xiulong Liu,Anurag Kumar,Paul Calamia,Sebastia V. Amengual,Calvin Murdock,Ishwarya Ananthabhotla,Philip Robinson,Eli Shlizerman,Vamsi Krishna Ithapu,Ruohan Gao*

Main category: cs.CV

TLDR: 本论文提出xRIR框架，用于跨房间房间脉冲响应（RIR）预测，以提升混合现实中的声学沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单一环境，无法泛化到不同几何和材料的新房间，而声学体验对混合现实沉浸感至关重要。

Method: 结合几何特征提取器（从全景深度图像捕获空间上下文）和RIR编码器（从少量参考RIR样本提取声学特征），构建通用模型。

Result: 实验显示方法优于基线，并在四个真实环境中实现模拟到真实转移，证明了泛化性和数据集的真实性。

Conclusion: 展示了xRIR框架的有效性和泛化能力，推动了空间声学重建的进展。

Abstract: In mixed reality applications, a realistic acoustic experience in spatial
environments is as crucial as the visual experience for achieving true
immersion. Despite recent advances in neural approaches for Room Impulse
Response (RIR) estimation, most existing methods are limited to the single
environment on which they are trained, lacking the ability to generalize to new
rooms with different geometries and surface materials. We aim to develop a
unified model capable of reconstructing the spatial acoustic experience of any
environment with minimum additional measurements. To this end, we present xRIR,
a framework for cross-room RIR prediction. The core of our generalizable
approach lies in combining a geometric feature extractor, which captures
spatial context from panorama depth images, with a RIR encoder that extracts
detailed acoustic features from only a few reference RIR samples. To evaluate
our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity
simulation of over 300,000 RIRs from 260 rooms. Experiments show that our
method strongly outperforms a series of baselines. Furthermore, we successfully
perform sim-to-real transfer by evaluating our model on four real-world
environments, demonstrating the generalizability of our approach and the
realism of our dataset.

</details>

### [250] [Visual Language Models show widespread visual deficits on neuropsychological tests](https://arxiv.org/abs/2504.10786)
*Gene Tangtartharakul,Katherine R. Storrs*

Main category: cs.CV

TLDR: 视觉语言模型在高级视觉任务中表现出色，但基础视觉概念存在缺陷，与人类视觉有显著差距。


<details>
  <summary>Details</summary>
Motivation: 最近报告显示VLMs在元素视觉概念上挣扎，揭示了人类与VLM视觉的潜在差距，需要系统评估。

Method: 使用51个测试，从六个临床和实验电池中，评估三个最先进的VLMs。

Result: 模型在物体识别任务中表现出色，但低级和中级视觉能力存在广泛缺陷，在人类中被视为临床意义。

Conclusion: 人工系统可实现复杂物体识别，而无需人类自然发展的基础视觉概念。

Abstract: Visual Language Models (VLMs) show remarkable performance in visual reasoning
tasks, successfully tackling college-level challenges that require high-level
understanding of images. However, some recent reports of VLMs struggling to
reason about elemental visual concepts like orientation, position, continuity,
and occlusion suggest a potential gulf between human and VLM vision. Here we
use the toolkit of neuropsychology to systematically assess the capabilities of
three state-of-the-art VLMs across visual domains. Using 51 tests drawn from
six clinical and experimental batteries, we characterise the visual abilities
of leading VLMs relative to normative performance in healthy adults. While the
models excel in straightforward object recognition tasks, we find widespread
deficits in low- and mid-level visual abilities that would be considered
clinically significant in humans. These selective deficits, profiled through
validated test batteries, suggest that an artificial system can achieve complex
object recognition without developing foundational visual concepts that in
humans require no explicit training.

</details>

### [251] [PatrolVision: Automated License Plate Recognition in the wild](https://arxiv.org/abs/2504.10810)
*Anmol Singhal Navya Singhal*

Main category: cs.CV

TLDR: 本文提出一个基于YOLO的自动车牌识别(ALPR)系统，用于城市巡逻车辆，针对新加坡车牌的非约束场景，实现了86%的检测精度和67%的字符识别准确率。


<details>
  <summary>Details</summary>
Motivation: AI在公共服务中的采用率低，由于准确性和速度问题；计算机视觉在交通监控中未普及；缺乏端到端ALPR解决方案。

Method: 使用RFB-Net检测并校正车牌，然后采用自定义YOLO网络进行字符识别，针对单双行新加坡车牌的扭曲场景。

Result: 检测精度86%，字符识别准确率67%，部分匹配89%，延迟64FPS on Tesla P4 GPU。

Conclusion: 系统展示了在实际城市环境中的可行性，提供了一个高效的端到端ALPR解决方案。

Abstract: Adoption of AI driven techniques in public services remains low due to
challenges related to accuracy and speed of information at population scale.
Computer vision techniques for traffic monitoring have not gained much
popularity despite their relative strength in areas such as autonomous driving.
Despite large number of academic methods for Automatic License Plate
Recognition (ALPR) systems, very few provide an end to end solution for
patrolling in the city. This paper presents a novel prototype for a low power
GPU based patrolling system to be deployed in an urban environment on
surveillance vehicles for automated vehicle detection, recognition and
tracking. In this work, we propose a complete ALPR system for Singapore license
plates having both single and double line creating our own YOLO based network.
We focus on unconstrained capture scenarios as would be the case in real world
application, where the license plate (LP) might be considerably distorted due
to oblique views. In this work, we first detect the license plate from the full
image using RFB-Net and rectify multiple distorted license plates in a single
image. After that, the detected license plate image is fed to our network for
character recognition. We evaluate the performance of our proposed system on a
newly built dataset covering more than 16,000 images. The system was able to
correctly detect license plates with 86\% precision and recognize characters of
a license plate in 67\% of the test set, and 89\% accuracy with one incorrect
character (partial match). We also test latency of our system and achieve 64FPS
on Tesla P4 GPU

</details>

### [252] [Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2504.10873)
*Tonko E. W. Bossen,Andreas Møgelmose,Ross Greer*

Main category: cs.CV

TLDR: 本研究评估了视觉语言模型在零样本交通手势解释中的能力，使用自定义数据集测试其在自动驾驶中的表现。


<details>
  <summary>Details</summary>
Motivation: 正确解释交通手势对于自动驾驶的安全和舒适至关重要，包括权威指令或行人信号。

Method: 创建了两个数据集（ATG和ITGI），使用专家生成的标题作为基准，通过标题相似度、手势分类和姿势序列重建相似度三种方法评估模型。

Result: 结果显示当前VLMs在手势理解上表现不佳，句子相似度平均低于0.59，分类F1分数仅为0.14-0.39，远低于专家基准0.70；姿势重建有潜力但需更多数据和改进。

Conclusion: 结论是当前SOTA VLMs无法准确可靠地解释交通手势，需要进一步研究以提升其鲁棒性。

Abstract: In autonomous driving, it is crucial to correctly interpret traffic gestures
(TGs), such as those of an authority figure providing orders or instructions,
or a pedestrian signaling the driver, to ensure a safe and pleasant traffic
environment for all road users. This study investigates the capabilities of
state-of-the-art vision-language models (VLMs) in zero-shot interpretation,
focusing on their ability to caption and classify human gestures in traffic
contexts. We create and publicly share two custom datasets with varying formal
and informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are
"Acted TG (ATG)" and "Instructive TG In-The-Wild (ITGI)". They are annotated
with natural language, describing the pedestrian's body position and gesture.
We evaluate models using three methods utilizing expert-generated captions as
baseline and control: (1) caption similarity, (2) gesture classification, and
(3) pose sequence reconstruction similarity. Results show that current VLMs
struggle with gesture understanding: sentence similarity averages below 0.59,
and classification F1 scores reach only 0.14-0.39, well below the expert
baseline of 0.70. While pose reconstruction shows potential, it requires more
data and refined metrics to be reliable. Our findings reveal that although some
SOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and
robust enough to be trustworthy, emphasizing the need for further research in
this domain.

</details>

### [253] [Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content](https://arxiv.org/abs/2504.10878)
*Yilang Peng,Sijia Qian,Yingdan Lu,Cuihua Shen*

Main category: cs.CV

TLDR: 本研究提出使用LLM预测视觉内容可信度并解释原因的框架，优于基线模型，帮助对抗错误信息。


<details>
  <summary>Details</summary>
Motivation: 在视觉主导的社交媒体中，反对错误信息至关重要，但视觉特征的多样性使预测可信度面临挑战。

Method: 引入LLM指导的特征发现框架，利用多模态LLM如GPT-4o通过针对性提示提取量化特征，并整合到机器学习模型中评估可信度。

Result: 方法在R2上比零样本GPT预测高13%，识别出关键特征如信息具体性和图像格式，在4191个帖子和5355个 crowdfunded 评分中验证。

Conclusion: 讨论了该方法对减少错误信息、提升视觉可信度以及LLM在社会科学中作用的含义。

Abstract: In today's visually dominated social media landscape, predicting the
perceived credibility of visual content and understanding what drives human
judgment are crucial for countering misinformation. However, these tasks are
challenging due to the diversity and richness of visual features. We introduce
a Large Language Model (LLM)-informed feature discovery framework that
leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and
explain its reasoning. We extract and quantify interpretable features using
targeted prompts and integrate them into machine learning models to improve
credibility predictions. We tested this approach on 4,191 visual social media
posts across eight topics in science, health, and politics, using credibility
ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot
GPT-based predictions by 13 percent in R2, and revealed key features like
information concreteness and image format. We discuss the implications for
misinformation mitigation, visual credibility, and the role of LLMs in social
science.

</details>

### [254] [Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models](https://arxiv.org/abs/2504.10883)
*Karan Jain,Mohammad Nayeem Teli*

Main category: cs.CV

TLDR: 本论文提出了一种基于可逆UNet和注意力模块的扩散模型架构，实现了单GPU高效训练高维医疗图像，内存消耗减少15%，图像质量与SOTA相当。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中性能出色，但计算资源需求高，尤其在3D医疗图像上，因此需要内存高效的训练方法。

Method: 使用可逆UNet架构和可逆注意力模块，使扩散模型的内存使用独立于数据维度。

Result: 在BraTS2020数据集上，峰值内存消耗减少15%，结果与最先进方法相当，图像质量保持一致。

Conclusion: 该模型可应用于多种图像生成任务，显著提高了内存和能源效率。

Abstract: Diffusion models have recently gained state of the art performance on many
image generation tasks. However, most models require significant computational
resources to achieve this. This becomes apparent in the application of medical
image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,
electron microscope, etc. In this paper we propose a novel architecture for a
single GPU memory-efficient training for diffusion models for high dimensional
medical datasets. The proposed model is built by using an invertible UNet
architecture with invertible attention modules. This leads to the following two
contributions: 1. denoising diffusion models and thus enabling memory usage to
be independent of the dimensionality of the dataset, and 2. reducing the energy
usage during training. While this new model can be applied to a multitude of
image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020
dataset leading to up to 15\% decrease in peak memory consumption during
training with comparable results to SOTA while maintaining the image quality.

</details>

### [255] [PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving](https://arxiv.org/abs/2504.10885)
*Zeyu Zhang,Zijian Chen,Zicheng Zhang,Yuze Sun,Yuan Tian,Ziheng Jia,Chunyi Li,Xiaohong Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TLDR: 本文提出OVPG框架，用于动态生成多模态模型评估数据，解决现有基准的静态和污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准静态、易受数据污染，手动标注耗时费力且易产生偏见和一致性问题。

Method: 提出OVPG框架，包括原材料采样、视觉内容生成和谜题规则设计模块，并构建PuzzleBench基准，包含11840个VQA样本和六种谜题任务。

Result: 创建了动态、可扩展的PuzzleBench基准，针对视觉识别、逻辑推理和上下文理解能力。

Conclusion: PuzzleBench通过持续更新和开放设计，适应大型多模态模型的演进。

Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities
across a wide range of multimodal tasks, achieving ever-increasing performance
on various evaluation benchmarks. However, existing benchmarks are typically
static and often overlap with pre-training datasets, leading to fixed
complexity constraints and substantial data contamination issues. Meanwhile,
manually annotated datasets are labor-intensive, time-consuming, and subject to
human bias and inconsistency, leading to reliability and reproducibility
issues. To address these problems, we propose a fully dynamic multimodal
evaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which
aims to generate fresh, diverse, and verifiable evaluation data automatically
in puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw
material sampling module, a visual content generation module, and a puzzle rule
design module, which ensures that each evaluation instance is primitive, highly
randomized, and uniquely solvable, enabling continual adaptation to the
evolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a
dynamic and scalable benchmark comprising 11,840 VQA samples. It features six
carefully designed puzzle tasks targeting three core LMM competencies, visual
recognition, logical reasoning, and context understanding. PuzzleBench differs
from static benchmarks that quickly become outdated. It enables ongoing dataset
refreshing through OVPG and a rich set of open-ended puzzle designs, allowing
seamless adaptation to the evolving capabilities of LMMs.

</details>

### [256] [CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors](https://arxiv.org/abs/2504.10888)
*Jiahuan Long,Wen Yao,Tingsong Jiang,Chao Ma*

Main category: cs.CV

TLDR: 这篇论文提出CDUPatch，一种针对可见光-红外双模态物体检测器的通用跨模态对抗补丁攻击方法，提升了在不同规模、视角和场景下的攻击有效性。


<details>
  <summary>Details</summary>
Motivation: 现有双模态对抗补丁攻击在多样物理场景下的效果有限，需要更鲁棒的攻击方法。

Method: 提出RGB-to-infrared适配器，通过学习最优颜色分布操纵热响应，结合多尺度裁剪策略和新建MSDrone数据集增强鲁棒性。

Result: 在DroneVehicle、LLVIP、VisDrone和MSDrone等数据集的实验中，CDUPatch在数字域和物理测试中均优于现有攻击，显示出强转移性。

Conclusion: CDUPatch在不同规模、视角和场景下具有高鲁棒性和有效性，证实了其在真实世界中的适用性。

Abstract: Adversarial patches are widely used to evaluate the robustness of object
detection systems in real-world scenarios. These patches were initially
designed to deceive single-modal detectors (e.g., visible or infrared) and have
recently been extended to target visible-infrared dual-modal detectors.
However, existing dual-modal adversarial patch attacks have limited attack
effectiveness across diverse physical scenarios. To address this, we propose
CDUPatch, a universal cross-modal patch attack against visible-infrared object
detectors across scales, views, and scenarios. Specifically, we observe that
color variations lead to different levels of thermal absorption, resulting in
temperature differences in infrared imaging. Leveraging this property, we
propose an RGB-to-infrared adapter that maps RGB patches to infrared patches,
enabling unified optimization of cross-modal patches. By learning an optimal
color distribution on the adversarial patch, we can manipulate its thermal
response and generate an adversarial infrared texture. Additionally, we
introduce a multi-scale clipping strategy and construct a new visible-infrared
dataset, MSDrone, which contains aerial vehicle images in varying scales and
perspectives. These data augmentation strategies enhance the robustness of our
patch in real-world conditions. Experiments on four benchmark datasets (e.g.,
DroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms
existing patch attacks in the digital domain. Extensive physical tests further
confirm strong transferability across scales, views, and scenarios.

</details>

### [257] [Perturbed State Space Feature Encoders for Optical Flow with Event Cameras](https://arxiv.org/abs/2504.10669)
*Gokul Raju Govinda Raju,Nikola Zubić,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TLDR: 这篇论文提出P-SSE方法，用于事件相机光学流估计，通过扰动技术改善时空推理，在基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前神经网络在事件相机光学流估计中的时空推理限制。

Method: 提出扰动状态空间特征编码器(P-SSE)，包括对状态动态矩阵的扰动技术，并整合双向流和循环连接框架。

Result: 在DSEC-Flow和MVSEC数据集上，EPE性能分别提高8.48%和11.86%。

Conclusion: P-SSE在事件相机光学流估计中实现最先进性能。

Abstract: With their motion-responsive nature, event-based cameras offer significant
advantages over traditional cameras for optical flow estimation. While deep
learning has improved upon traditional methods, current neural networks adopted
for event-based optical flow still face temporal and spatial reasoning
limitations. We propose Perturbed State Space Feature Encoders (P-SSE) for
multi-frame optical flow with event cameras to address these challenges. P-SSE
adaptively processes spatiotemporal features with a large receptive field akin
to Transformer-based methods, while maintaining the linear computational
complexity characteristic of SSMs. However, the key innovation that enables the
state-of-the-art performance of our model lies in our perturbation technique
applied to the state dynamics matrix governing the SSM system. This approach
significantly improves the stability and performance of our model. We integrate
P-SSE into a framework that leverages bi-directional flows and recurrent
connections, expanding the temporal context of flow prediction. Evaluations on
DSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and
11.86% improvements in EPE performance, respectively.

</details>

### [258] [TMCIR: Token Merge Benefits Composed Image Retrieval](https://arxiv.org/abs/2504.10995)
*Chaoyang Wang,Zeyu Zhang,Long Teng,Zijun Li,Shichao Kan*

Main category: cs.CV

TLDR: 本文提出TMCIR框架，通过改进视觉和文本信息融合，提升组合图像检索性能，在数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前组合图像检索中跨模态融合方法存在的偏差问题，这些方法无法准确捕捉用户意图。

Method: 提出TMCIR框架，包括意图感知跨模态对齐（使用扩散模型合成伪目标图像微调CLIP编码器）和自适应标记融合（对比学习动态平衡视觉和文本表示）。

Result: 在Fashion-IQ和CIRR数据集上实验表明，TMCIR显著优于最先进方法，尤其在捕捉细微用户意图方面。

Conclusion: TMCIR框架有效平衡视觉和文本表示，提高了检索准确性。

Abstract: Composed Image Retrieval (CIR) retrieves target images using a multi-modal
query that combines a reference image with text describing desired
modifications. The primary challenge is effectively fusing this visual and
textual information. Current cross-modal feature fusion approaches for CIR
exhibit an inherent bias in intention interpretation. These methods tend to
disproportionately emphasize either the reference image features
(visual-dominant fusion) or the textual modification intent (text-dominant
fusion through image-to-text conversion). Such an imbalanced representation
often fails to accurately capture and reflect the actual search intent of the
user in the retrieval results. To address this challenge, we propose TMCIR, a
novel framework that advances composed image retrieval through two key
innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP
encoders contrastively using intent-reflecting pseudo-target images,
synthesized from reference images and textual descriptions via a diffusion
model. This step enhances the encoder ability of text to capture nuanced
intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune
all encoders contrastively by comparing adaptive token-fusion features with the
target image. This mechanism dynamically balances visual and textual
representations within the contrastive learning pipeline, optimizing the
composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR
datasets demonstrate that TMCIR significantly outperforms state-of-the-art
methods, particularly in capturing nuanced user intent.

</details>

### [259] [MediSee: Reasoning-based Pixel-level Perception in Medical Images](https://arxiv.org/abs/2504.11008)
*Qinyue Tong,Ziqian Lu,Jun Liu,Yangming Zheng,Zheming Lu*

Main category: cs.CV

TLDR: 本文引入医疗推理分割和检测任务，使用自然语言查询，提出数据集和基线模型，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法任务特定或依赖边界框/文本标签，医疗知识门槛高，用户更倾向逻辑推理的口头查询。

Method: 引入MedSD任务，构建MLMR-SD数据集，并提出MediSee基线模型。

Result: 实验结果显示，该方法有效处理隐式查询，并优于传统医疗分割方法。

Conclusion: 本文方法提升了医疗图像处理的通用性和易用性。

Abstract: Despite remarkable advancements in pixel-level medical image perception,
existing methods are either limited to specific tasks or heavily rely on
accurate bounding boxes or text labels as input prompts. However, the medical
knowledge required for input is a huge obstacle for general public, which
greatly reduces the universality of these methods. Compared with these
domain-specialized auxiliary information, general users tend to rely on oral
queries that require logical reasoning. In this paper, we introduce a novel
medical vision task: Medical Reasoning Segmentation and Detection (MedSD),
which aims to comprehend implicit queries about medical images and generate the
corresponding segmentation mask and bounding box for the target object. To
accomplish this task, we first introduce a Multi-perspective, Logic-driven
Medical Reasoning Segmentation and Detection (MLMR-SD) dataset, which
encompasses a substantial collection of medical entity targets along with their
corresponding reasoning. Furthermore, we propose MediSee, an effective baseline
model designed for medical reasoning segmentation and detection. The
experimental results indicate that the proposed method can effectively address
MedSD with implicit colloquial queries and outperform traditional medical
referring segmentation methods.

</details>

### [260] [ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models](https://arxiv.org/abs/2504.10757)
*Amirhosein Chahe,Lifeng Zhou*

Main category: cs.CV

TLDR: 显式推理微调提升了视觉语言模型在自动驾驶决策中的性能，使用GPT-4o生成推理链，并证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自动驾驶中缺乏透明推理能力，这对安全至关重要，因此需要增强推理建模。

Method: 使用GPT-4o生成结构化推理链，通过类别特定提示策略在DriveLM基准上微调Llama 3.2、Llava 1.5和Qwen 2.5VL模型，比较推理-based和仅答案微调。

Result: 推理-based微调 consistently 优于其他方法，Llama3.2-11B-reason 表现最佳，显著提高了准确性和文本生成质量。

Conclusion: 强调透明决策过程在安全关键领域的重要性，并为开发更可解释的自动驾驶系统提供了有前景的方向。

Abstract: Vision-language models (VLMs) show promise for autonomous driving but often
lack transparent reasoning capabilities that are critical for safety. We
investigate whether explicitly modeling reasoning during fine-tuning enhances
VLM performance on driving decision tasks. Using GPT-4o, we generate structured
reasoning chains for driving scenarios from the DriveLM benchmark with
category-specific prompting strategies. We compare reasoning-based fine-tuning,
answer-only fine-tuning, and baseline instruction-tuned models across multiple
small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results
demonstrate that reasoning-based fine-tuning consistently outperforms
alternatives, with Llama3.2-11B-reason achieving the highest performance.
Models fine-tuned with reasoning show substantial improvements in accuracy and
text generation quality, suggesting explicit reasoning enhances internal
representations for driving decisions. These findings highlight the importance
of transparent decision processes in safety-critical domains and offer a
promising direction for developing more interpretable autonomous driving
systems.

</details>

### [261] [GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*](https://arxiv.org/abs/2504.11014)
*Eunsoo Im,Jung Kwon Lee,Changhyun Jee*

Main category: cs.CV

TLDR: 本文引入GATE3D框架，通过弱监督学习实现单目3D物体检测的泛化，使用2D和3D预测一致性损失桥接领域差距，在KITTI和室内数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 动机是解决计算机视觉中通用模型的多领域训练挑战，特别是单目3D物体检测数据集稀缺和偏置问题，尤其在非道路环境。

Method: 方法是提出GATE3D框架，利用伪标签和2D-3D预测一致性损失来提升泛化能力。

Result: 结果显示GATE3D在KITTI基准和室内数据集上取得竞争性性能，并加速了从有限标注数据中学习。

Conclusion: 结论是GATE3D有效解决了检测挑战，具有在机器人、增强现实和虚拟现实等领域的广泛潜力。

Abstract: The emerging trend in computer vision emphasizes developing universal models
capable of simultaneously addressing multiple diverse tasks. Such universality
typically requires joint training across multi-domain datasets to ensure
effective generalization. However, monocular 3D object detection presents
unique challenges in multi-domain training due to the scarcity of datasets
annotated with accurate 3D ground-truth labels, especially beyond typical
road-based autonomous driving contexts. To address this challenge, we introduce
a novel weakly supervised framework leveraging pseudo-labels. Current
pretrained models often struggle to accurately detect pedestrians in non-road
environments due to inherent dataset biases. Unlike generalized image-based 2D
object detection models, achieving similar generalization in monocular 3D
detection remains largely unexplored. In this paper, we propose GATE3D, a novel
framework designed specifically for generalized monocular 3D object detection
via weak supervision. GATE3D effectively bridges domain gaps by employing
consistency losses between 2D and 3D predictions. Remarkably, our model
achieves competitive performance on the KITTI benchmark as well as on an
indoor-office dataset collected by us to evaluate the generalization
capabilities of our framework. Our results demonstrate that GATE3D
significantly accelerates learning from limited annotated data through
effective pre-training strategies, highlighting substantial potential for
broader impacts in robotics, augmented reality, and virtual reality
applications. Project page: https://ies0411.github.io/GATE3D/

</details>

### [262] [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://arxiv.org/abs/2504.11038)
*Yudong Zhang,Ruobing Xie,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Yu Wang*

Main category: cs.CV

TLDR: 本论文引入QAVA，一种针对视觉语言模型的查询无关视觉攻击方法，能够在未知问题下生成错误响应。


<details>
  <summary>Details</summary>
Motivation: 动机是传统攻击针对特定图像和问题，而实际中一个图像可能关联多个问题，模型可能仍能正确回答其他问题，因此需要开发一种对未知问题的鲁棒攻击来揭示模型的漏洞。

Method: 方法是提出QAVA，这是一种查询无关的视觉攻击，旨在创建对抗样本，使模型对未指定的未知问题产生错误响应。

Result: 结果显示QAVA显著提高了攻击的有效性和效率，与针对已知目标问题的攻击性能相当。

Conclusion: 结论是这项研究扩展了视觉对抗攻击的范围，揭示了在实际场景中模型的先前未被注意的漏洞。

Abstract: In typical multimodal tasks, such as Visual Question Answering (VQA),
adversarial attacks targeting a specific image and question can lead large
vision-language models (LVLMs) to provide incorrect answers. However, it is
common for a single image to be associated with multiple questions, and LVLMs
may still answer other questions correctly even for an adversarial image
attacked by a specific question. To address this, we introduce the
query-agnostic visual attack (QAVA), which aims to create robust adversarial
examples that generate incorrect responses to unspecified and unknown
questions. Compared to traditional adversarial attacks focused on specific
images and questions, QAVA significantly enhances the effectiveness and
efficiency of attacks on images when the question is unknown, achieving
performance comparable to attacks on known target questions. Our research
broadens the scope of visual adversarial attacks on LVLMs in practical
settings, uncovering previously overlooked vulnerabilities, particularly in the
context of visual adversarial threats. The code is available at
https://github.com/btzyd/qava.

</details>

### [263] [Tabular foundation model to detect empathy from visual cues](https://arxiv.org/abs/2504.10808)
*Md Rakibul Hasan,Shafin Rahman,Md Zakir Hossain,Aneesh Krishna,Tom Gedeon*

Main category: cs.CV

TLDR: This paper uses tabular foundation models to improve empathy detection from video features, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Motivated by the success of textual foundation models and the common release of video data as tabular features due to privacy concerns.

Method: Experimented with TabPFN v2 and TabICL using in-context learning and fine-tuning on a human-robot interaction benchmark.

Result: Improved cross-subject empathy detection accuracy from 0.590 to 0.730 and AUC from 0.564 to 0.669, with enhanced generalization.

Conclusion: Findings are applicable to future empathy detection datasets released as tabular data due to privacy constraints.

Abstract: Detecting empathy from video interactions is an emerging area of research.
Video datasets, however, are often released as extracted features (i.e.,
tabular data) rather than raw footage due to privacy and ethical concerns.
Prior research on such tabular datasets established tree-based classical
machine learning approaches as the best-performing models. Motivated by the
recent success of textual foundation models (i.e., large language models), we
explore the use of tabular foundation models in empathy detection from tabular
visual features. We experiment with two recent tabular foundation models $-$
TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups.
Our experiments on a public human-robot interaction benchmark demonstrate a
significant boost in cross-subject empathy detection accuracy over several
strong baselines (accuracy: $0.590 \rightarrow 0.730$; AUC: $0.564 \rightarrow
0.669$). In addition to performance improvement, we contribute novel insights
and an evaluation setup to ensure generalisation on unseen subjects in this
public benchmark. As the practice of releasing video features as tabular
datasets is likely to persist due to privacy constraints, our findings will be
widely applicable to future empathy detection video datasets as well.

</details>

### [264] [Recognition of Geometrical Shapes by Dictionary Learning](https://arxiv.org/abs/2504.10958)
*Alexander Köhler,Michael Breuß*

Main category: cs.CV

TLDR: 本文首次将字典学习应用于几何形状识别，实验显示优化方法对识别质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 扩展字典学习从图像重建到形状识别任务的应用。

Method: 提出一种基于字典学习的形状识别方法，并探讨不同优化方法的 impact。

Result: 实验结果证实字典学习对形状识别有效，且优化方法选择至关重要。

Conclusion: 字典学习可能是一种有趣的形状识别方法。

Abstract: Dictionary learning is a versatile method to produce an overcomplete set of
vectors, called atoms, to represent a given input with only a few atoms. In the
literature, it has been used primarily for tasks that explore its powerful
representation capabilities, such as for image reconstruction. In this work, we
present a first approach to make dictionary learning work for shape
recognition, considering specifically geometrical shapes. As we demonstrate,
the choice of the underlying optimization method has a significant impact on
recognition quality. Experimental results confirm that dictionary learning may
be an interesting method for shape recognition tasks.

</details>

### [265] [DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention](https://arxiv.org/abs/2504.11160)
*Haohan Chen,Hongjia Liu,Shiyong Lan,Wenwu Wang,Yixin Qiao,Yao Li,Guonan Deng*

Main category: cs.CV

TLDR: 本文提出DMAGaze框架，通过解耦和注意力机制改善注视估计，关注相关特征，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决面部图像中复杂不相关信息干扰的挑战。

Method: 设计连续掩码解耦器分离注视相关信息，引入多尺度全局局部注意力模块，并结合头部姿势和局部眼睛特征进行高精度估计。

Result: 在两个主流公共数据集上验证，取得最先进性能。

Conclusion: DMAGaze框架有效提升了注视估计的准确性。

Abstract: Gaze estimation, which predicts gaze direction, commonly faces the challenge
of interference from complex gaze-irrelevant information in face images. In
this work, we propose DMAGaze, a novel gaze estimation framework that exploits
information from facial images in three aspects: gaze-relevant global features
(disentangled from facial image), local eye features (extracted from cropped
eye patch), and head pose estimation features, to improve overall performance.
Firstly, we design a new continuous mask-based Disentangler to accurately
disentangle gaze-relevant and gaze-irrelevant information in facial images by
achieving the dual-branch disentanglement goal through separately
reconstructing the eye and non-eye regions. Furthermore, we introduce a new
cascaded attention module named Multi-Scale Global Local Attention Module
(MS-GLAM). Through a customized cascaded attention structure, it effectively
focuses on global and local information at multiple scales, further enhancing
the information from the Disentangler. Finally, the global gaze-relevant
features disentangled by the upper face branch, combined with head pose and
local eye features, are passed through the detection head for high-precision
gaze estimation. Our proposed DMAGaze has been extensively validated on two
mainstream public datasets, achieving state-of-the-art performance.

</details>

### [266] [TerraMind: Large-Scale Generative Multimodality for Earth Observation](https://arxiv.org/abs/2504.11171)
*Johannes Jakubik,Felix Yang,Benedikt Blumenstiel,Erik Scheurer,Rocco Sedona,Stefano Maurogiovanni,Jente Bosmans,Nikolaos Dionelis,Valerio Marsocci,Niklas Kopp,Rahul Ramachandran,Paolo Fraccaro,Thomas Brunschwiler,Gabriele Cavallaro,Juan Bernabe-Moreno,Nicolas Longépé*

Main category: cs.CV

TLDR: TerraMind 是第一个用于地球观测的任意到任意生成式多模态基础模型，通过双尺度表示结合 token 级和 pixel 级数据，实现了超越现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有多模态模型在处理跨模态关系和空间细微差别方面的不足，提升地球观测任务的生成能力。

Method: 方法包括在全球大规模数据集的九种地理空间模态上预训练 TerraMind，使用双尺度早融合方法和 'Thinking-in-Modalities' (TiM) 机制生成额外数据。

Result: 结果显示 TerraMind 实现了零样本和少样本应用，支持 TiM 机制，并在 PANGAEA 等基准上取得了超越现有技术的性能。

Conclusion: 结论是 TerraMind 的双尺度方法有效，并通过开源预训练数据集、模型权重和代码促进了进一步研究。

Abstract: We present TerraMind, the first any-to-any generative, multimodal foundation
model for Earth observation (EO). Unlike other multimodal models, TerraMind is
pretrained on dual-scale representations combining both token-level and
pixel-level data across modalities. On a token level, TerraMind encodes
high-level contextual information to learn cross-modal relationships, while on
a pixel level, TerraMind leverages fine-grained representations to capture
critical spatial nuances. We pretrained TerraMind on nine geospatial modalities
of a global, large-scale dataset. In this paper, we demonstrate that (i)
TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and
few-shot applications for Earth observation, (ii) TerraMind introduces
"Thinking-in-Modalities" (TiM) -- the capability of generating additional
artificial data during finetuning and inference to improve the model output --
and (iii) TerraMind achieves beyond state-of-the-art performance in
community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the
model weights, and our code is open-sourced under a permissive license.

</details>

### [267] [K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery](https://arxiv.org/abs/2504.11128)
*P. Tomkiewicz,J. Jaworski,P. Zielonka,A. Wilinski*

Main category: cs.CV

TLDR: 本论文提出了一种使用多模态卫星图像进行密度梯度分析的新方法，用于评估城市指标和公共交通系统。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种成本效益高、全球适用的工具，使用免费卫星数据来初步评估公共交通和城市结构。

Method: 结合光学和SAR数据，分割城市区域，识别中心，量化密度梯度，计算α和LD指标，并使用K-means聚类分析密度梯度图。

Result: 通过比较单中心和多中心城市，建立密度梯度与公共交通网络关系的证据，证明指标作为筛选工具的有效性。

Conclusion: 这种方法为城市规划者提供经济有效的开源工具，支持全球公共交通评估。

Abstract: This paper presents a novel computational approach for evaluating urban
metrics through density gradient analysis using multi-modal satellite imagery,
with applications including public transport and other urban systems. By
combining optical and Synthetic Aperture Radar (SAR) data, we develop a method
to segment urban areas, identify urban centers, and quantify density gradients.
Our approach calculates two key metrics: the density gradient coefficient
($\alpha$) and the minimum effective distance (LD) at which density reaches a
target threshold. We further employ machine learning techniques, specifically
K-means clustering, to objectively identify uniform and high-variability
regions within density gradient plots. We demonstrate that these metrics
provide an effective screening tool for public transport analyses by revealing
the underlying urban structure. Through comparative analysis of two
representative cities with contrasting urban morphologies (monocentric vs
polycentric), we establish relationships between density gradient
characteristics and public transport network topologies. Cities with clear
density peaks in their gradient plots indicate distinct urban centers requiring
different transport strategies than those with more uniform density
distributions. This methodology offers urban planners a cost-effective,
globally applicable approach to preliminary public transport assessment using
freely available satellite data. The complete implementation, with additional
examples and documentation, is available in an open-source repository under the
MIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.

</details>

### [268] [GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention](https://arxiv.org/abs/2504.11150)
*Mahir Gulzar,Yar Muhammad,Naveed Muhammad*

Main category: cs.CV

TLDR: 这篇论文提出了一种基于车道图的运动预测模型，使用编码器-交互器-解码器架构，在nuScenes数据集上达到了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 为了提高车辆轨迹预测的准确性和鲁棒性，模型需有效利用静态（如车道）和动态（如交通参与者）上下文信息。

Method: 模型采用encoder-interactor-decoder架构：encoder用轻量级门控循环单元编码场景上下文，interactor通过交叉注意力融合特征和图-based目标提案，decoder用拉普拉斯混合密度网络回归多模态轨迹。

Result: 在nuScenes运动预测数据集上取得了最先进的结果。

Conclusion: 通过交叉注意力机制，模型学会关注未来目标相关的场景元素，提升了轨迹估计的鲁棒性，并验证了方法的有效性。

Abstract: Predicting future trajectories of surrounding vehicles heavily relies on what
contextual information is given to a motion prediction model. The context
itself can be static (lanes, regulatory elements, etc) or dynamic (traffic
participants). This paper presents a lane graph-based motion prediction model
that first predicts graph-based goal proposals and later fuses them with cross
attention over multiple contextual elements. We follow the famous
encoder-interactor-decoder architecture where the encoder encodes scene context
using lightweight Gated Recurrent Units, the interactor applies cross-context
attention over encoded scene features and graph goal proposals, and the decoder
regresses multimodal trajectories via Laplacian Mixture Density Network from
the aggregated encodings. Using cross-attention over graph-based goal proposals
gives robust trajectory estimates since the model learns to attend to future
goal-relevant scene elements for the intended agent. We evaluate our work on
nuScenes motion prediction dataset, achieving state-of-the-art results.

</details>

### [269] [Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning](https://arxiv.org/abs/2504.11268)
*Juan Garcia Giraldo,Nikolaos Dimitriadis,Ke Wang,Pascal Frossard*

Main category: cs.CV

TLDR: 本论文探讨在单输入多输出（SIMO）多任务设置下改进模型合并方法，通过简单修复解决表示不对齐问题，并展示了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法仅关注单输入单输出设置，忽略SIMO场景导致性能下降，因此需要针对任务特定解码器和损失目标进行优化。

Method: 提出两种简单高效的修复方法，用于合并后重新对齐特征表示，以适应SIMO多任务环境。

Result: 实验在NYUv2、Cityscapes和Taskonomy数据集上显示，任务算术结合表示对齐可实现多任务能力，与传统多任务学习性能相当，但使用更少样本和训练步骤。

Conclusion: 模型合并在SIMO设置下通过表示对齐可高效实现多任务学习，揭示任务关系并减少计算资源需求。

Abstract: Model merging is a flexible and computationally tractable approach to merge
single-task checkpoints into a multi-task model. Prior work has solely focused
on constrained multi-task settings where there is a one-to-one mapping between
a sample and a task, overlooking the paradigm where multiple tasks may operate
on the same sample, e.g., scene understanding. In this paper, we focus on the
multi-task setting with single-input-multiple-outputs (SIMO) and show that it
qualitatively differs from the single-input-single-output model merging
settings studied in the literature due to the existence of task-specific
decoders and diverse loss objectives. We identify that existing model merging
methods lead to significant performance degradation, primarily due to
representation misalignment between the merged encoder and task-specific
decoders. We propose two simple and efficient fixes for the SIMO setting to
re-align the feature representation after merging. Compared to joint
fine-tuning, our approach is computationally effective and flexible, and sheds
light into identifying task relationships in an offline manner. Experiments on
NYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task
arithmetic suffices to enable multi-task capabilities; however, the
representations generated by the merged encoder has to be re-aligned with the
task-specific heads; (2) the proposed architecture rivals traditional
multi-task learning in performance but requires fewer samples and training
steps by leveraging the existence of task-specific models.

</details>

### [270] [CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection](https://arxiv.org/abs/2504.11305)
*Jincheng Kang,Yi Cen,Yigang Cen,Ke Wang,Yuhan Liu*

Main category: cs.CV

TLDR: 本论文提出CFIS-YOLO模型，用于木头缺陷检测，优化了边缘设备上的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法成本高、主观且劳动密集，以及深度学习模型在边缘部署时准确性和计算效率平衡问题。

Method: 提出CFIS-YOLO模型，包括增强C2f结构、动态特征重组模块和新型损失函数，提高多尺度特征融合和小目标定位。

Result: 在公共数据集上mAP@0.5达77.5%，比YOLOv10s高4%；在SOPHON BM1684X边缘设备上帧率135 FPS，功耗降至17.3%，mAP仅下降0.5%。

Conclusion: CFIS-YOLO是资源受限环境下木头缺陷检测的实用有效解决方案。

Abstract: Wood defect detection is critical for ensuring quality control in the wood
processing industry. However, current industrial applications face two major
challenges: traditional methods are costly, subjective, and labor-intensive,
while mainstream deep learning models often struggle to balance detection
accuracy and computational efficiency for edge deployment. To address these
issues, this study proposes CFIS-YOLO, a lightweight object detection model
optimized for edge devices. The model introduces an enhanced C2f structure, a
dynamic feature recombination module, and a novel loss function that
incorporates auxiliary bounding boxes and angular constraints. These
innovations improve multi-scale feature fusion and small object localization
while significantly reducing computational overhead. Evaluated on a public wood
defect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of
77.5\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON
BM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to
17.3\% of the original implementation, and incurs only a 0.5 percentage point
drop in mAP. These results demonstrate that CFIS-YOLO is a practical and
effective solution for real-world wood defect detection in resource-constrained
environments.

</details>

### [271] [Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review](https://arxiv.org/abs/2504.11349)
*Yuezhe Yang,Boyu Yang,Yaqian Wang,Yang He,Xingbo Dong,Zhe Jin*

Main category: cs.CV

TLDR: 这篇综述探讨了基于AI的放射学成像3D重建算法，将其分类为显式和隐式方法，并讨论了评估指标、数据集、当前状态、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 高品质医疗成像的需求推动AI用于提高重建准确性、减少时间和辐射暴露。

Method: 分类AI方法为显式（点、体积、Gaussian表示）和隐式（隐式先验嵌入、神经辐射场），并审查评估指标和基准数据集。

Result: 讨论了当前发展状态、关键挑战和未来研究方向。

Conclusion: AI在医疗成像中具有潜力，项目代码可在GitHub上获取。

Abstract: The demand for high-quality medical imaging in clinical practice and assisted
diagnosis has made 3D reconstruction in radiological imaging a key research
focus. Artificial intelligence (AI) has emerged as a promising approach to
enhancing reconstruction accuracy while reducing acquisition and processing
time, thereby minimizing patient radiation exposure and discomfort and
ultimately benefiting clinical diagnosis. This review explores state-of-the-art
AI-based 3D reconstruction algorithms in radiological imaging, categorizing
them into explicit and implicit approaches based on their underlying
principles. Explicit methods include point-based, volume-based, and Gaussian
representations, while implicit methods encompass implicit prior embedding and
neural radiance fields. Additionally, we examine commonly used evaluation
metrics and benchmark datasets. Finally, we discuss the current state of
development, key challenges, and future research directions in this evolving
field. Our project available on: https://github.com/Bean-Young/AI4Med.

</details>

### [272] [Robustness and sex differences in skin cancer detection: logistic regression vs CNNs](https://arxiv.org/abs/2504.11415)
*Nikolette Pedersen,Regitze Sydendal,Andreas Wulff,Ralf Raumanns,Eike Petersen,Veronika Cheplygina*

Main category: cs.CV

TLDR: 本研究复制了[28]研究，探讨皮肤癌检测中性别偏置，发现CNN在男性患者上表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤癌检测中性能高，但存在可重复性和偏置问题，因此复制[28]研究以评估性别偏置。

Method: 使用PAD-UFES-20数据集，比较逻辑回归（LR）基于手工特征（ABCDE和7点检查表）和ResNet-50 CNN模型，在不同性别分布下评估鲁棒性。

Result: LR和CNN对性别分布鲁棒，但CNN在男性患者的准确性和AUROC显著高于女性患者。

Conclusion: 希望为调查医疗机器学习偏置贡献力量，并提供了可重复的数据和脚本。

Abstract: Deep learning has been reported to achieve high performances in the detection
of skin cancer, yet many challenges regarding the reproducibility of results
and biases remain. This study is a replication (different data, same analysis)
of a study on Alzheimer's disease [28] which studied robustness of logistic
regression (LR) and convolutional neural networks (CNN) across patient sexes.
We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset
with LR trained on handcrafted features reflecting dermatological guidelines
(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We
evaluate these models in alignment with [28]: across multiple training datasets
with varied sex composition to determine their robustness. Our results show
that both the LR and the CNN were robust to the sex distributions, but the
results also revealed that the CNN had a significantly higher accuracy (ACC)
and area under the receiver operating characteristics (AUROC) for male patients
than for female patients. We hope these findings to contribute to the growing
field of investigating potential bias in popular medical machine learning
methods. The data and relevant scripts to reproduce our results can be found in
our Github.

</details>

### [273] [Multi-level Cellular Automata for FLIM networks](https://arxiv.org/abs/2504.11406)
*Felipe Crispim Salvagnini,Jancarlo F. Gomes,Cid A. N. Santos,Silvio Jamil F. Guimarães,Alexandre X. Falcão*

Main category: cs.CV

TLDR: 这篇论文提出了一种结合FLIM和CA的方法，用于在资源有限的医疗应用中进行显著对象检测，减少了对标注数据和复杂网络的需求。


<details>
  <summary>Details</summary>
Motivation: 深层学习显著对象检测需要大量标注数据和复杂网络架构，尤其在计算资源有限的发展中国家医疗应用中，这是一个重大挑战。结合现代和经典技术可以实现高性能和实用性。

Method: 提出使用FLIM网络初始化CA状态，通过从FLIM网络不同层解码特征来同时初始化多个CA，形成多层框架，并合并多个显著性图生成最终输出。

Result: 在两个具有挑战性的医疗数据集上的基准测试显示，该多层CA方法与现有深层SOD模型相比具有竞争力，使用更少参数且无需反向传播。

Conclusion: 该方法证明了在数据稀缺场景下，使用专家知识初始化CA可以获得高质量的显著对象检测结果，并提供了一个轻量级、可实践的解决方案。

Abstract: The necessity of abundant annotated data and complex network architectures
presents a significant challenge in deep-learning Salient Object Detection
(deep SOD) and across the broader deep-learning landscape. This challenge is
particularly acute in medical applications in developing countries with limited
computational resources. Combining modern and classical techniques offers a
path to maintaining competitive performance while enabling practical
applications. Feature Learning from Image Markers (FLIM) methodology empowers
experts to design convolutional encoders through user-drawn markers, with
filters learned directly from these annotations. Recent findings demonstrate
that coupling a FLIM encoder with an adaptive decoder creates a flyweight
network suitable for SOD, requiring significantly fewer parameters than
lightweight models and eliminating the need for backpropagation. Cellular
Automata (CA) methods have proven successful in data-scarce scenarios but
require proper initialization -- typically through user input, priors, or
randomness. We propose a practical intersection of these approaches: using FLIM
networks to initialize CA states with expert knowledge without requiring user
interaction for each image. By decoding features from each level of a FLIM
network, we can initialize multiple CAs simultaneously, creating a multi-level
framework. Our method leverages the hierarchical knowledge encoded across
different network layers, merging multiple saliency maps into a high-quality
final output that functions as a CA ensemble. Benchmarks across two challenging
medical datasets demonstrate the competitiveness of our multi-level CA approach
compared to established models in the deep SOD literature.

</details>

### [274] [ADT: Tuning Diffusion Models with Adversarial Supervision](https://arxiv.org/abs/2504.11423)
*Dazhong Shen,Guanglu Song,Yi Zhang,Bingqi Ma,Lujundong Li,Dongzhi Jiang,Zhuofan Zong,Yu Liu*

Main category: cs.CV

TLDR: 本论文提出Adversarial Diffusion Tuning (ADT)框架，通过对抗训练细调扩散模型，改善训练和推理分布对齐问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练和推理过程分歧导致预测偏差和累积错误，阻碍数据分布对齐。

Method: 提出ADT框架，使用siamese-network鉴别器、image-to-image采样策略、保留原始diffusion loss，并约束梯度回传路径。

Result: 在Stable Diffusion模型（v1.5、XL、v3）上的实验显示ADT显著提升分布对齐和图像质量。

Conclusion: ADT通过刺激推理过程和对抗监督有效改善扩散模型的性能。

Abstract: Diffusion models have achieved outstanding image generation by reversing a
forward noising process to approximate true data distributions. During
training, these models predict diffusion scores from noised versions of true
samples in a single forward pass, while inference requires iterative denoising
starting from white noise. This training-inference divergences hinder the
alignment between inference and training data distributions, due to potential
prediction biases and cumulative error accumulation. To address this problem,
we propose an intuitive but effective fine-tuning framework, called Adversarial
Diffusion Tuning (ADT), by stimulating the inference process during
optimization and aligning the final outputs with training data by adversarial
supervision. Specifically, to achieve robust adversarial training, ADT features
a siamese-network discriminator with a fixed pre-trained backbone and
lightweight trainable parameters, incorporates an image-to-image sampling
strategy to smooth discriminative difficulties, and preserves the original
diffusion loss to prevent discriminator hacking. In addition, we carefully
constrain the backward-flowing path for back-propagating gradients along the
inference path without incurring memory overload or gradient explosion.
Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),
demonstrate that ADT significantly improves both distribution alignment and
image quality.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [275] [Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version](https://arxiv.org/abs/2504.10973)
*Tomasz M. Rutkowski,Stanisław Narębski,Mihoko Otake-Matsuura,Tomasz Komendziński*

Main category: q-bio.NC

TLDR: 本文提出使用轻量级CNN从EEG数据中推断认知障碍的新方法，采用被动FPVS范式，实现无行为响应的客观评估。


<details>
  <summary>Details</summary>
Motivation: 早期痴呆诊断需要敏感的功能脑变化生物标志物，但当前行为评估易受努力、练习效应和教育背景影响，导致早期检测困难。

Method: 使用轻量级卷积神经网络分析EEG数据，结合被动快速周期视觉刺激范式，不依赖参与者的行为响应或任务理解。

Result: 该方法提供独立于混杂因素的客观工作记忆功能测量，有望实现早期认知衰退的无偏检测。

Conclusion: 被动方法为早期痴呆诊断开辟新途径，提供可靠的认知功能评估。

Abstract: Early dementia diagnosis requires biomarkers sensitive to both structural and
functional brain changes. While structural neuroimaging biomarkers have
progressed significantly, objective functional biomarkers of early cognitive
decline remain a critical unmet need. Current cognitive assessments often rely
on behavioral responses, making them susceptible to factors like effort,
practice effects, and educational background, thereby hindering early and
accurate detection. This work introduces a novel approach, leveraging a
lightweight convolutional neural network (CNN) to infer cognitive impairment
levels directly from electroencephalography (EEG) data. Critically, this method
employs a passive fast periodic visual stimulation (FPVS) paradigm, eliminating
the need for explicit behavioral responses or task comprehension from the
participant. This passive approach provides an objective measure of working
memory function, independent of confounding factors inherent in active
cognitive tasks, and offers a promising new avenue for early and unbiased
detection of cognitive decline.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [276] [MatterTune: An Integrated, User-Friendly Platform for Fine-Tuning Atomistic Foundation Models to Accelerate Materials Simulation and Discovery](https://arxiv.org/abs/2504.10655)
*Lingyu Kong,Nima Shoghi,Guoxiang Hu,Pan Li,Victor Fung*

Main category: cond-mat.mtrl-sci

TLDR: 这篇论文介绍了MatterTune框架，用于微调原子基础模型以提升材料科学应用。


<details>
  <summary>Details</summary>
Motivation: 解决几何机器学习模型数据需求高的问题，并利用预训练模型处理数据稀疏场景。

Method: 开发MatterTune模块化框架，支持微调如ORB、MatterSim等模型，并提供分布式细调和任务集成功能。

Result: MatterTune提升了模型泛化性和易用性，促进材料信息学和模拟应用。

Conclusion: MatterTune降低了原子基础模型采用门槛，推动材料科学多样应用。

Abstract: Geometric machine learning models such as graph neural networks have achieved
remarkable success in recent years in chemical and materials science research
for applications such as high-throughput virtual screening and atomistic
simulations. The success of these models can be attributed to their ability to
effectively learn latent representations of atomic structures directly from the
training data. Conversely, this also results in high data requirements for
these models, hindering their application to problems which are data sparse
which are common in this domain. To address this limitation, there is a growing
development in the area of pre-trained machine learning models which have
learned general, fundamental, geometric relationships in atomistic data, and
which can then be fine-tuned to much smaller application-specific datasets. In
particular, models which are pre-trained on diverse, large-scale atomistic
datasets have shown impressive generalizability and flexibility to downstream
applications, and are increasingly referred to as atomistic foundation models.
To leverage the untapped potential of these foundation models, we introduce
MatterTune, a modular and extensible framework that provides advanced
fine-tuning capabilities and seamless integration of atomistic foundation
models into downstream materials informatics and simulation workflows, thereby
lowering the barriers to adoption and facilitating diverse applications in
materials science. In its current state, MatterTune supports a number of
state-of-the-art foundation models such as ORB, MatterSim, JMP, and
EquformerV2, and hosts a wide range of features including a modular and
flexible design, distributed and customizable fine-tuning, broad support for
downstream informatics tasks, and more.

</details>

<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [277] [Molecular Learning Dynamics](https://arxiv.org/abs/2504.10560)
*Yaroslav Gusev,Vitaly Vanchurin*

Main category: physics.chem-ph

TLDR: 本文引入物理-学习二元性应用于分子系统，通过学习方法模拟水分子，实现高效且准确的模拟。


<details>
  <summary>Details</summary>
Motivation: 动机是结合物理描述与学习框架，提高分子模拟的计算效率和准确性。

Method: 方法包括从CP2K物理模拟数据推断氧和氢的损失函数，并使用这些函数进行基于学习的模拟。

Result: 结果显示，学习-based模拟在准确性上与物理模拟相当，但计算效率显著更高。

Conclusion: 结论是这种物理-学习二元性方法在分子模拟中具有潜力，可用于更有效的应用。

Abstract: We apply the physics-learning duality to molecular systems by complementing
the physical description of interacting particles with a dual learning
description, where each particle is modeled as an agent minimizing a loss
function. In the traditional physics framework, the equations of motion are
derived from the Lagrangian function, while in the learning framework, the same
equations emerge from learning dynamics driven by the agent loss function. The
loss function depends on scalar quantities that describe invariant properties
of all other agents or particles. To demonstrate this approach, we first infer
the loss functions of oxygen and hydrogen directly from a dataset generated by
the CP2K physics-based simulation of water molecules. We then employ the loss
functions to develop a learning-based simulation of water molecules, which
achieves comparable accuracy while being significantly more computationally
efficient than standard physics-based simulations.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [278] [AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse](https://arxiv.org/abs/2504.10540)
*Zichao Yu,Zhen Zou,Guojiang Shao,Chengwei Zhang,Shengze Xu,Jie Huang,Feng Zhao,Xiaodong Cun,Wenyi Zhang*

Main category: stat.ML

TLDR: 本文通过理论分析提出了一种新的扩散模型加速方法，使用高阶Adams-Bashforth方法实现近3倍加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但迭代去噪过程导致推理缓慢，现有的加速方法缺乏理论基础且可能性能下降。

Method: 通过二阶Adams-Bashforth方法分析去噪过程，揭示相邻步骤的线性关系，并提出高阶缓存加速方法，截断误差为O(h^k)。

Result: 在多种图像和视频扩散模型上验证，实现了近3倍加速而不降低性能。

Conclusion: 提供了一个实用的实时加速解决方案。

Abstract: Diffusion models have demonstrated remarkable success in generative tasks,
yet their iterative denoising process results in slow inference, limiting their
practicality. While existing acceleration methods exploit the well-known
U-shaped similarity pattern between adjacent steps through caching mechanisms,
they lack theoretical foundation and rely on simplistic computation reuse,
often leading to performance degradation. In this work, we provide a
theoretical understanding by analyzing the denoising process through the
second-order Adams-Bashforth method, revealing a linear relationship between
the outputs of consecutive steps. This analysis explains why the outputs of
adjacent steps exhibit a U-shaped pattern. Furthermore, extending
Adams-Bashforth method to higher order, we propose a novel caching-based
acceleration approach for diffusion models, instead of directly reusing cached
results, with a truncation error bound of only \(O(h^k)\) where $h$ is the step
size. Extensive validation across diverse image and video diffusion models
(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates
our method's effectiveness in achieving nearly $3\times$ speedup while
maintaining original performance levels, offering a practical real-time
solution without compromising generation quality.

</details>

### [279] [Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed Benchmarks](https://arxiv.org/abs/2504.10598)
*Omar Montasser,Abhishek Shetty,Nikita Zhivotovskiy*

Main category: stat.ML

TLDR: 这篇论文重新审视在线二元分类，将遗憾度竞争从最佳二元损失转向松弛基准，如鲁棒于小扰动或Gaussian平滑，实现了更好的遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 动机是避免标准方法导致的与Littlestone维相关的糟糕界限，通过竞争平滑的最优性基准来改善遗憾保证。

Method: 方法包括设计算法，利用对抗鲁棒性和平滑在线学习的思想，比较鲁棒预测器和高边距预测器。

Result: 结果是遗憾保证仅依赖VC维和实例空间复杂性，对广义边距γ有O(log(1/γ))的对数依赖，并有匹配的下界。

Conclusion: 结论是这种方法提供了比现有方法更好的遗憾界限，突出了松弛基准的优势。

Abstract: We revisit online binary classification by shifting the focus from competing
with the best-in-class binary loss to competing against relaxed benchmarks that
capture smoothed notions of optimality. Instead of measuring regret relative to
the exact minimal binary error -- a standard approach that leads to worst-case
bounds tied to the Littlestone dimension -- we consider comparing with
predictors that are robust to small input perturbations, perform well under
Gaussian smoothing, or maintain a prescribed output margin. Previous examples
of this were primarily limited to the hinge loss. Our algorithms achieve regret
guarantees that depend only on the VC dimension and the complexity of the
instance space (e.g., metric entropy), and notably, they incur only an
$O(\log(1/\gamma))$ dependence on the generalized margin $\gamma$. This stands
in contrast to most existing regret bounds, which typically exhibit a
polynomial dependence on $1/\gamma$. We complement this with matching lower
bounds. Our analysis connects recent ideas from adversarial robustness and
smoothed online learning.

</details>

### [280] [Differentially Private Geodesic and Linear Regression](https://arxiv.org/abs/2504.11304)
*Aditya Kulkarni,Carlos Soto*

Main category: stat.ML

TLDR: 这篇论文扩展了微分隐私到黎曼流形上的测地回归，提供敏感度界和应用示范。


<details>
  <summary>Details</summary>
Motivation: 动机是处理非线性空间数据时需要隐私保护，经典线性回归不适用。

Method: 方法是通过K-范数梯度机制在黎曼流形上发布DP参数，并推导与雅可比场相关的敏感度界。

Result: 结果：导出了敏感度理论界，并在球面S^2和欧几里得空间上验证了有效性。

Conclusion: 结论：该方法通用于任何黎曼流形，适用于医疗成像和计算机视觉等领域。

Abstract: In statistical applications it has become increasingly common to encounter
data structures that live on non-linear spaces such as manifolds. Classical
linear regression, one of the most fundamental methodologies of statistical
learning, captures the relationship between an independent variable and a
response variable which both are assumed to live in Euclidean space. Thus,
geodesic regression emerged as an extension where the response variable lives
on a Riemannian manifold. The parameters of geodesic regression, as with linear
regression, capture the relationship of sensitive data and hence one should
consider the privacy protection practices of said parameters. We consider
releasing Differentially Private (DP) parameters of geodesic regression via the
K-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical
bounds for the sensitivity of the parameters showing they are tied to their
respective Jacobi fields and hence the curvature of the space. This
corroborates recent findings of differential privacy for the Fr\'echet mean. We
demonstrate the efficacy of our methodology on the sphere,
$\mbS^2\subset\mbR^3$ and, since it is general to Riemannian manifolds, the
manifold of Euclidean space which simplifies geodesic regression to a case of
linear regression. Our methodology is general to any Riemannian manifold and
thus it is suitable for data in domains such as medical imaging and computer
vision.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [281] [HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs](https://arxiv.org/abs/2504.11421)
*Mahdi Hasanzadeh,Kasem Khalil,Cynthia Sturton,Ahmad Patooghy*

Main category: cs.AR

TLDR: 本文提出了一种轻量级多处理器片上系统（MPSoC）热攻击监测机制，实现了高效异常检测和低硬件开销。


<details>
  <summary>Details</summary>
Motivation: MPSoC易受热攻击影响，需要对抗动态热管理系统的操纵，以提升系统安全性。

Method: 采用自适应实时监测机制，通过加权移动平均和位移操作，在网络路由器中实现异常检测模块，并进行设计空间探索。

Result: 将温度波动从3.00°C降低到1.9°C，异常检测准确率达82%，硬件使用减少高达75%。

Conclusion: 提供实用、低成本解决方案，适用于资源受限环境，确保抵抗热攻击并维持性能。

Abstract: Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal
attacks that manipulate dynamic thermal management systems. To counter this, we
propose an adaptive real-time monitoring mechanism that detects abnormal
thermal patterns in chip tiles. Our design space exploration helped identify
key thermal features for an efficient anomaly detection module to be
implemented at routers of network-enabled MPSoCs. To minimize hardware
overhead, we employ weighted moving average (WMA) calculations and bit-shift
operations, ensuring a lightweight yet effective implementation. By defining a
spectrum of abnormal behaviors, our system successfully detects and mitigates
malicious temperature fluctuations, reducing severe cases from 3.00{\deg}C to
1.9{\deg}C. The anomaly detection module achieves up to 82% of accuracy in
detecting thermal attacks, which is only 10-15% less than top-performing
machine learning (ML) models like Random Forest. However, our approach reduces
hardware usage by up to 75% for logic resources and 100% for specialized
resources, making it significantly more efficient than ML-based solutions. This
method provides a practical, low-cost solution for resource-constrained
environments, ensuring resilience against thermal attacks while maintaining
system performance.

</details>

### [282] [HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs](https://arxiv.org/abs/2504.11421)
*Mahdi Hasanzadeh,Kasem Khalil,Cynthia Sturton,Ahmad Patooghy*

Main category: cs.AR

TLDR: 本论文提出了一种轻量级实时监控机制，用于检测MPSoC中的热攻击，提高效率并减少硬件开销。


<details>
  <summary>Details</summary>
Motivation: MPSoC易受热攻击影响，需要高效机制对抗动态热管理系统的操纵。

Method: 使用设计空间探索识别热特征，在路由器实现异常检测模块，采用加权移动平均和位移操作最小化硬件开销。

Result: 将温度波动从3.00°C减至1.9°C，检测准确率达82%，硬件使用减少75%逻辑资源和100%专用资源，比ML模型高效。

Conclusion: 提供实用、低成本解决方案，适用于资源受限环境，确保热攻击抵抗力和系统性能。

Abstract: Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal
attacks that manipulate dynamic thermal management systems. To counter this, we
propose an adaptive real-time monitoring mechanism that detects abnormal
thermal patterns in chip tiles. Our design space exploration helped identify
key thermal features for an efficient anomaly detection module to be
implemented at routers of network-enabled MPSoCs. To minimize hardware
overhead, we employ weighted moving average (WMA) calculations and bit-shift
operations, ensuring a lightweight yet effective implementation. By defining a
spectrum of abnormal behaviors, our system successfully detects and mitigates
malicious temperature fluctuations, reducing severe cases from 3.00{\deg}C to
1.9{\deg}C. The anomaly detection module achieves up to 82% of accuracy in
detecting thermal attacks, which is only 10-15% less than top-performing
machine learning (ML) models like Random Forest. However, our approach reduces
hardware usage by up to 75% for logic resources and 100% for specialized
resources, making it significantly more efficient than ML-based solutions. This
method provides a practical, low-cost solution for resource-constrained
environments, ensuring resilience against thermal attacks while maintaining
system performance.

</details>

### [283] [VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers](https://arxiv.org/abs/2504.11227)
*Run Wang,Gamze Islamoglu,Andrea Belano,Viviane Potocnik,Francesco Conti,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TLDR: 这篇论文通过自定义硬件加速Transformer中的Softmax函数，使用近似算法实现显著的性能和能量效率提升。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中，性能瓶颈从浮点矩阵乘法转移到非线性函数如Softmax的指数运算，需要高效加速。

Method: 设计Bfloat16指数运算自定义算术块，基于Schraudolph方法近似，集成到RISC-V核心FPU中，并通过自定义ISA扩展和软件优化。

Result: Softmax延迟减少162.7倍、能量消耗减少74.3倍；FlashAttention-2性能提升8.2倍、能量效率提高4.1倍；端到端推理延迟和能量消耗减少高达5.8倍和3.6倍。

Conclusion: 该方法无需重新训练，仅微小精度损失，即可高效执行Transformer模型推理。

Abstract: While Transformers are dominated by Floating-Point (FP)
Matrix-Multiplications, their aggressive acceleration through dedicated
hardware or many-core programmable systems has shifted the performance
bottleneck to non-linear functions like Softmax. Accelerating Softmax is
challenging due to its non-pointwise, non-linear nature, with exponentiation as
the most demanding step. To address this, we design a custom arithmetic block
for Bfloat16 exponentiation leveraging a novel approximation algorithm based on
Schraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of
the RISC-V cores of a compute cluster, through custom Instruction Set
Architecture (ISA) extensions, with a negligible area overhead of 1\%. By
optimizing the software kernels to leverage the extension, we execute Softmax
with 162.7$\times$ less latency and 74.3$\times$ less energy compared to the
baseline cluster, achieving an 8.2$\times$ performance improvement and
4.1$\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2
configuration. Moreover, the proposed approach enables a multi-cluster system
to efficiently execute end-to-end inference of pre-trained Transformer models,
such as GPT-2, GPT-3 and ViT, achieving up to 5.8$\times$ and 3.6$\times$
reduction in latency and energy consumption, respectively, without requiring
re-training and with negligible accuracy loss.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [284] [Uplink Assisted Joint Channel Estimation and CSI Feedback: An Approach Based on Deep Joint Source-Channel Coding](https://arxiv.org/abs/2504.10836)
*Yiran Guo,Wei Chen,Bo Ai*

Main category: eess.SP

TLDR: 这篇论文提出了一种基于深度学习的联合信道估计和CSI反馈方法，用于FDD MIMO系统，通过利用上行链路CSI改善下行链路CSI获取，而不增加额外开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统模块化通信框架中模块分离设计导致的性能次优和分布偏差问题，以最大化空间资源利用和提高系统频谱效率。

Method: 提出上行辅助的联合CE和CSI反馈方法，使用深度联合源-信道编码（DJSCC）架构和端到端多模块联合训练，借助上行链路CSI作为辅助信息。

Result: 通过消融和可伸缩性实验验证了上行CSI作为辅助信息的有效性和端到端联合训练架构的必要性，提高了CSI重建准确性。

Conclusion: 该方法减轻了传统框架下的性能退化，利用部分信道互易性改善了系统性能。

Abstract: In frequency division duplex (FDD) multiple-input multiple-output (MIMO)
wireless communication systems, the acquisition of downlink channel state
information (CSI) is essential for maximizing spatial resource utilization and
improving system spectral efficiency. The separate design of modules in
AI-based CSI feedback architectures under traditional modular communication
frameworks, including channel estimation (CE), CSI compression and feedback,
leads to sub-optimal performance. In this paper, we propose an uplink assisted
joint CE and and CSI feedback approach via deep learning for downlink CSI
acquisition, which mitigates performance degradation caused by distribution
bias across separately trained modules in traditional modular communication
frameworks. The proposed network adopts a deep joint source-channel coding
(DJSCC) architecture to mitigate the cliff effect encountered in the
conventional separate source-channel coding. Furthermore, we exploit the uplink
CSI as auxiliary information to enhance CSI reconstruction accuracy by
leveraging the partial reciprocity between the uplink and downlink channels in
FDD systems, without introducing additional overhead. The effectiveness of
uplink CSI as assisted information and the necessity of an end-toend
multi-module joint training architecture is validated through comprehensive
ablation and scalability experiments.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [285] [AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification](https://arxiv.org/abs/2504.11091)
*Maximilian G. Schuh,Joshua Hesse,Stephan A. Sieber*

Main category: q-bio.BM

TLDR: 本研究开发AI指导的抗生素发现管道，从目标识别到化合物实现，评估多种生成模型，突出DeepBlock和TamGen的性能。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性是全球健康危机，需要新疗法；AI和机器学习进展提供机会，但整合指导有限。

Method: 开发端到端AI管道；使用结构-based聚类识别目标；评估六种3D结构aware生成模型（包括扩散、自回归、图神经网络和语言模型）；后处理过滤和商业类似物搜索。

Result: DeepBlock和TamGen表现最佳；揭示模型复杂性、可用性和输出质量之间的权衡。

Conclusion: 提供比较基准和蓝图，用于AI在早期抗生素开发中的部署。

Abstract: Antibiotic resistance presents a growing global health crisis, demanding new
therapeutic strategies that target novel bacterial mechanisms. Recent advances
in protein structure prediction and machine learning-driven molecule generation
offer a promising opportunity to accelerate drug discovery. However, practical
guidance on selecting and integrating these models into real-world pipelines
remains limited. In this study, we develop an end-to-end, artificial
intelligence-guided antibiotic discovery pipeline that spans target
identification to compound realization. We leverage structure-based clustering
across predicted proteomes of multiple pathogens to identify conserved,
essential, and non-human-homologous targets. We then systematically evaluate
six leading 3D-structure-aware generative models$\unicode{x2014}$spanning
diffusion, autoregressive, graph neural network, and language model
architectures$\unicode{x2014}$on their usability, chemical validity, and
biological relevance. Rigorous post-processing filters and commercial analogue
searches reduce over 100 000 generated compounds to a focused, synthesizable
set. Our results highlight DeepBlock and TamGen as top performers across
diverse criteria, while also revealing critical trade-offs between model
complexity, usability, and output quality. This work provides a comparative
benchmark and blueprint for deploying artificial intelligence in early-stage
antibiotic development.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [286] [Neural Network Emulation of the Classical Limit in Quantum Systems via Learned Observable Mappings](https://arxiv.org/abs/2504.10781)
*Kamran Majid*

Main category: quant-ph

TLDR: 这篇论文使用神经网络计算模拟量子谐振子从量子到经典的过渡，当ħ趋近于零时。


<details>
  <summary>Details</summary>
Motivation: 探讨量子力学的经典极限，这是物理哲学中的一个深刻领域，使用计算方法如机器学习来获得洞见。

Method: 开发并训练神经网络，学习从初始期望值和ħ到位置期望值时间演化的映射。

Result: 网络预测在不同ħ条件下的分析，提供量子-经典过渡的计算洞见，并展示机器学习在这一领域的潜力。

Conclusion: 证明机器学习可以作为探索量子力学基础问题和经典极限的补充工具。

Abstract: The classical limit of quantum mechanics, formally investigated through
frameworks like strict deformation quantization, remains a profound area of
inquiry in the philosophy of physics. This paper explores a computational
approach employing a neural network to emulate the emergence of classical
behavior from the quantum harmonic oscillator as Planck's constant $\hbar$
approaches zero. We develop and train a neural network architecture to learn
the mapping from initial expectation values and $\hbar$ to the time evolution
of the expectation value of position. By analyzing the network's predictions
across different regimes of hbar, we aim to provide computational insights into
the nature of the quantum-classical transition. This work demonstrates the
potential of machine learning as a complementary tool for exploring
foundational questions in quantum mechanics and its classical limit.

</details>

### [287] [An Efficient Quantum Classifier Based on Hamiltonian Representations](https://arxiv.org/abs/2504.10542)
*Federico Tiblias,Anna Schroeder,Yue Zhang,Mariami Gachechiladze,Iryna Gurevych*

Main category: quant-ph

TLDR: 本文提出Hamiltonian classifier方法，解决量子机器学习的可扩展性问题，并在分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习研究依赖玩具数据集或特征减少，存在可扩展性担忧，且受硬件限制和数据编码成本影响。

Method: 提出Hamiltonian classifier，通过将输入映射到Pauli字符串并计算期望值，避免数据编码成本；引入两个分类器变体。

Result: 在文本和图像分类任务上，性能与经典和量子模型相当或更好；复杂度为对数级别，适合大规模应用。

Conclusion: 该方法有效且高效，并提供开源实现。

Abstract: Quantum machine learning (QML) is a discipline that seeks to transfer the
advantages of quantum computing to data-driven tasks. However, many studies
rely on toy datasets or heavy feature reduction, raising concerns about their
scalability. Progress is further hindered by hardware limitations and the
significant costs of encoding dense vector representations on quantum devices.
To address these challenges, we propose an efficient approach called
Hamiltonian classifier that circumvents the costs associated with data encoding
by mapping inputs to a finite set of Pauli strings and computing predictions as
their expectation values. In addition, we introduce two classifier variants
with different scaling in terms of parameters and sample complexity. We
evaluate our approach on text and image classification tasks, against
well-established classical and quantum models. The Hamiltonian classifier
delivers performance comparable to or better than these methods. Notably, our
method achieves logarithmic complexity in both qubits and quantum gates, making
it well-suited for large-scale, real-world applications. We make our
implementation available on GitHub.

</details>

### [288] [Cross-Problem Parameter Transfer in Quantum Approximate Optimization Algorithm: A Machine Learning Approach](https://arxiv.org/abs/2504.10733)
*Kien X. Nguyen,Bao Bach,Ilya Safro*

Main category: quant-ph

TLDR: 本研究探讨了从MaxCut到Maximum Independent Set (MIS)转移QAOA参数，实验显示这可减少优化迭代并保持可比近似比。


<details>
  <summary>Details</summary>
Motivation: QAOA参数优化因barren plateaus等因素困难，参数转移可降低优化开销，从已研究问题转移到新问题。

Method: 设计机器学习模型，筛选MaxCut优化的参数捐赠者，并将其应用于MIS问题。

Result: 实验结果表明，参数转移显著减少优化迭代次数，同时达到可比的近似比。

Conclusion: 参数转移从MaxCut到MIS有效，可减少优化开销并缓解性能问题。

Abstract: Quantum Approximate Optimization Algorithm (QAOA) is one of the most
promising candidates to achieve the quantum advantage in solving combinatorial
optimization problems. The process of finding a good set of variational
parameters in the QAOA circuit has proven to be challenging due to multiple
factors, such as barren plateaus. As a result, there is growing interest in
exploiting parameter transferability, where parameter sets optimized for one
problem instance are transferred to another that could be more complex either
to estimate the solution or to serve as a warm start for further optimization.
But can we transfer parameters from one class of problems to another?
Leveraging parameter sets learned from a well-studied class of problems could
help navigate the less studied one, reducing optimization overhead and
mitigating performance pitfalls. In this paper, we study whether pretrained
QAOA parameters of MaxCut can be used as is or to warm start the Maximum
Independent Set (MIS) circuits. Specifically, we design machine learning models
to find good donor candidates optimized on MaxCut and apply their parameters to
MIS acceptors. Our experimental results show that such parameter transfer can
significantly reduce the number of optimization iterations required while
achieving comparable approximation ratios.

</details>

### [289] [QAMA: Quantum annealing multi-head attention operator with classical deep learning framework](https://arxiv.org/abs/2504.11083)
*Peng Du,Shuolei Wang,Shicheng Li,Jinjing Shi*

Main category: quant-ph

TLDR: 本论文提出量子退火多头注意力机制QAMA，优化计算效率和能耗，与经典注意力兼容。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在大型语言模型中内存和能量消耗急剧增加，量子退火提供高效低耗解决方案。

Method: 使用QUBO建模和Ising模型优化注意力计算，实现从O(n^2)到线性的时空复杂度。

Result: 实验显示QAMA准确率与经典相当，推理时间减至毫秒级，能耗显著降低。

Conclusion: 开创量子计算与深度学习整合，可应用于任何注意力模型，推动AI计算范式创新。

Abstract: As large language models scale up, the conventional attention mechanism faces
critical challenges of exponential growth in memory consumption and energy
costs. Quantum annealing computing, with its inherent advantages in
computational efficiency and low energy consumption, offers an innovative
direction for constructing novel deep learning architectures. This study
proposes the first Quantum Annealing-based Multi-head Attention (QAMA)
mechanism, achieving seamless compatibility with classical attention
architectures through quadratic unconstrained binary optimization (QUBO)
modeling of forward propagation and energy-based backpropagation. The method
innovatively leverages the quantum bit interaction characteristics of Ising
models to optimize the conventional $O(n^2)$ spatiotemporal complexity into
linear resource consumption. Integrated with the optical computing advantages
of coherent Ising machines (CIM), the system maintains millisecond-level
real-time responsiveness while significantly reducing energy consumption. Our
key contributions include: Theoretical proofs establish QAMA mathematical
equivalence to classical attention mechanisms; Dual optimization of multi-head
specificity and long-range information capture via QUBO constraints; Explicit
gradient proofs for the Ising energy equation are utilized to implement
gradient conduction as the only path in the computational graph as a layer;
Proposed soft selection mechanism overcoming traditional binary attention
limitations to approximate continuous weights. Experiments on QBoson CPQC
quantum computer show QAMA achieves comparable accuracy to classical operators
while reducing inference time to millisecond level and improving solution
quality. This work pioneers architectural-level integration of quantum
computing and deep learning, applicable to any attention-based model, driving
paradigm innovation in AI foundational computing.

</details>

### [290] [Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation](https://arxiv.org/abs/2504.11109)
*Linus Jern,Valter Uotila,Cong Yu,Bo Zhao*

Main category: quant-ph

TLDR: 本论文通过微调LLM自动生成量子电路，覆盖优化问题，并显示出优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: LLM在数学、编码和科学报告分析中表现突出，但量子计算领域尤其是大规模自动生成量子电路的潜力尚未充分探索。

Method: 通过生成14,000个量子电路数据集，构建端到端管道微调预训练LLM，注入量子计算知识，生成符合OpenQASM 3.0的参数化电路。

Result: 微调LLM生成语法正确的电路，参数优于随机和现有模型，可作为进一步优化的起点。

Conclusion: 生成的电路和参数可作为量子机器学习模板、编译器基准和硬件测试的起点。

Abstract: Large language models (LLM) have achieved remarkable outcomes in addressing
complex problems, including math, coding, and analyzing large amounts of
scientific reports. Yet few works have explored the potential of LLM in quantum
computing. The most challenging problem is how to leverage LLMs to
automatically generate quantum circuits at a large scale. In this paper, we
address such a challenge by fine-tuning LLMs and injecting the domain-specific
knowledge of quantum computing. In particular, we investigate the mechanisms to
generate training data sets and construct the end-to-end pipeline to fine-tune
pre-trained LLMs that produce parameterized quantum circuits for optimization
problems. We have prepared 14,000 quantum circuits covering a substantial part
of the quantum optimization landscape: 12 optimization problem instances and
their optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can
construct syntactically correct parametrized quantum circuits in the most
recent OpenQASM 3.0. We have evaluated the quality of the parameters by
comparing them to the optimized expectation values and distributions. Our
evaluation shows that the fine-tuned LLM outperforms state-of-the-art models
and that the parameters are better than random. The LLM-generated parametrized
circuits and initial parameters can be used as a starting point for further
optimization, \emph{e.g.,} templates in quantum machine learning and the
benchmark for compilers and hardware.

</details>

### [291] [Mildly-Interacting Fermionic Unitaries are Efficiently Learnable](https://arxiv.org/abs/2504.11318)
*Vishnu Iyer*

Main category: quant-ph

TLDR: 论文开发算法学习近似高斯 fermionic 单元，时间复杂度多项式，并解决开放问题。


<details>
  <summary>Details</summary>
Motivation: 高斯单元学习已有算法，但近似高斯单元（如少量非高斯门准备）无算法，且在量子化学和多体物理中重要。

Method: 算法查询 n 模单元，返回 diamond 距离 ε 内近似电路，复杂度 poly(n, 2^t, 1/ε)，适用于 Gaussian dimension 至少 2n - O(t) 的单元。

Result: 首次给出学习算法，解决 Mele 和 Herasymenko 问题；提供多项式时间区分算法。

Conclusion: 算法更通用，涵盖更广单元类，并证明结构结果。

Abstract: Recent work has shown that one can efficiently learn fermionic Gaussian
unitaries, also commonly known as nearest-neighbor matchcircuits or
non-interacting fermionic unitaries. However, one could ask a similar question
about unitaries that are near Gaussian: for example, unitaries prepared with a
small number of non-Gaussian circuit elements. These operators find
significance in quantum chemistry and many-body physics, yet no algorithm
exists to learn them.
  We give the first such result by devising an algorithm which makes queries to
a $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates
and returns a circuit approximating $U$ to diamond distance $\varepsilon$ in
time $\textrm{poly}(n,2^t,1/\varepsilon)$. This resolves a central open
question of Mele and Herasymenko under the strongest distance metric. In fact,
our algorithm is much more general: we define a property of unitary Gaussianity
known as unitary Gaussian dimension and show that our algorithm can learn
$n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time
$\textrm{poly}(n,2^t,1/\varepsilon)$. Indeed, this class subsumes unitaries
prepared by at most $O(t)$ non-Gaussian gates but also includes several
unitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.
  In addition, we give a $\textrm{poly}(n,1/\varepsilon)$-time algorithm to
distinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$
or $\varepsilon$-far from all such unitaries in Frobenius distance, promised
that one is the case. Along the way, we prove structural results about
near-Gaussian fermionic unitaries that are likely to be of independent
interest.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [292] [Who is More Bayesian: Humans or ChatGPT?](https://arxiv.org/abs/2504.10636)
*Tianshi Mu,Pranjal Rawat,John Rust,Chengjun Zhang,Qixuan Zhong*

Main category: econ.GN

TLDR: 本论文比较人类和AI在二元分类任务中的表现，使用Bayes规则作为基准。发现人类和早期ChatGPT存在偏差，但最新ChatGPT已达到近乎完美水平。


<details>
  <summary>Details</summary>
Motivation: 动机是评估AI决策与人类决策的异同，特别是认知偏差和AI演变，以理解AI在决策任务中的进步。

Method: 方法包括重新分析人类实验数据（来自El-Gamal和Grether、Holt和Smith的实验室研究），并测试不同版本的ChatGPT模型。

Result: 结果显示人类决策存在异质性和偏差（如代表性启发式和保守主义）；ChatGPT也受偏差影响，但从ChatGPT 3.5的亚人类水平演变为ChatGPT 4o的超人类近乎完美表现。

Conclusion: 结论是AI模型如ChatGPT在决策任务中的快速演变，表明AI正朝着更优决策方向发展，可能超越人类。

Abstract: We compare the performance of human and artificially intelligent (AI)
decision makers in simple binary classification tasks where the optimal
decision rule is given by Bayes Rule. We reanalyze choices of human subjects
gathered from laboratory experiments conducted by El-Gamal and Grether and Holt
and Smith. We confirm that while overall, Bayes Rule represents the single best
model for predicting human choices, subjects are heterogeneous and a
significant share of them make suboptimal choices that reflect judgement biases
described by Kahneman and Tversky that include the ``representativeness
heuristic'' (excessive weight on the evidence from the sample relative to the
prior) and ``conservatism'' (excessive weight on the prior relative to the
sample). We compare the performance of AI subjects gathered from recent
versions of large language models (LLMs) including several versions of ChatGPT.
These general-purpose generative AI chatbots are not specifically trained to do
well in narrow decision making tasks, but are trained instead as ``language
predictors'' using a large corpus of textual data from the web. We show that
ChatGPT is also subject to biases that result in suboptimal decisions. However
we document a rapid evolution in the performance of ChatGPT from sub-human
performance for early versions (ChatGPT 3.5) to superhuman and nearly perfect
Bayesian classifications in the latest versions (ChatGPT 4o).

</details>

### [293] [Shifting Work Patterns with Generative AI](https://arxiv.org/abs/2504.11436)
*Eleanor Wiske Dillon,Sonia Jaffe,Nicole Immorlica,Christopher T. Stanton*

Main category: econ.GN

TLDR: 本研究通过随机实验发现，生成式AI工具减少了知识工作者在邮件和文档上的时间，但未显著影响会议时间。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何改变知识工作者的工作模式。

Method: 采用6个月跨行业随机场实验，涉及6000名工人，一半有AI工具访问，AI整合到邮件、文档和会议应用中。

Result: 使用AI工具的工人每周邮件时间减少1.4小时（25%），文档完成更快，但会议时间无显著变化。

Conclusion: AI主要影响独立可改变的行为，而非需要协调的行为。

Abstract: We present evidence on how generative AI changes the work patterns of
knowledge workers using data from a 6-month-long, cross-industry, randomized
field experiment. Half of the 6,000 workers in the study received access to a
generative AI tool integrated into the applications they already used for
emails, document creation, and meetings. We find that access to the AI tool
during the first year of its release primarily impacted behaviors that could be
changed independently and not behaviors that required coordination to change:
workers who used the tool spent 3 fewer hours, or 25% less time on email each
week (intent to treat estimate is 1.4 hours) and seemed to complete documents
moderately faster, but did not significantly change time spent in meetings.

</details>

### [294] [Early Impacts of M365 Copilot](https://arxiv.org/abs/2504.11443)
*Eleanor Wiske Dillon,Sonia Jaffe,Sida Peng,Alexia Cambon*

Main category: econ.GN

TLDR: 这项研究通过随机实验显示，生成式AI帮助知识工作者节省时间，提高效率。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI快速发展对人类工作方式的影响。

Method: 对56家公司的超过6000名员工进行大型真实世界随机实验分析。

Result: 使用AI后，员工每周阅读邮件节省30分钟，文档完成速度提高12%；近40%的员工在6个月内定期使用。

Conclusion: 生成式AI显著提升了工作效率，并显示出较高的采用率。

Abstract: Advances in generative AI have rapidly expanded the potential of computers to
perform or assist in a wide array of tasks traditionally performed by humans.
We analyze a large, real-world randomized experiment of over 6,000 workers at
56 firms to present some of the earliest evidence on how these technologies are
changing the way knowledge workers do their jobs. We find substantial time
savings on common core tasks across a wide range of industries and occupations:
workers who make use of this technology spent half an hour less reading email
each week and completed documents 12% faster. Despite the newness of the
technology, nearly 40% of workers who were given access to the tool used it
regularly in their work throughout the 6-month study.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [295] [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/abs/2504.10637)
*Afra Amini,Tim Vieira,Ryan Cotterell*

Main category: cs.CL

TLDR: 本文提出Rao-Blackwellized估计器来减少语言模型KL散度估计的方差，提高稳定性，并在经验研究中显示出优势。


<details>
  <summary>Details</summary>
Motivation: KL散度估计在RLHF、解释性和知识蒸馏等应用中重要，但标准Monte Carlo估计器方差高且可能给出负值，需要改进。

Method: 引入无偏的Rao-Blackwellized估计器及其KL散度梯度估计器，方差不大于Monte Carlo估计器。

Result: 经验研究显示，方差显著降低，估计更稳定，训练更可靠，模型更常位于奖励与KL的Pareto前沿。

Conclusion: 该估计器有效减少KL散度估计的方差，提高了语言模型训练的稳定性和性能。

Abstract: Estimating the Kullback--Leibler (KL) divergence between language models has
many applications, e.g., reinforcement learning from human feedback (RLHF),
interpretability, and knowledge distillation. However, computing the exact KL
divergence between two arbitrary language models is intractable. Thus,
practitioners often resort to the use of sampling-based estimators. While it is
easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased
estimate of the KL divergence between language models, this estimator
notoriously suffers from high variance, and can even result in a negative
estimate of the KL divergence, a non-negative quantity. In this paper, we
introduce a Rao--Blackwellized estimator that is also unbiased and provably has
variance less than or equal to that of the standard Monte Carlo estimator. In
an empirical study on sentiment-controlled fine-tuning, we show that our
estimator provides more stable KL estimates and reduces variance substantially
in practice. Additionally, we derive an analogous Rao--Blackwellized estimator
of the gradient of the KL divergence, which leads to more stable training and
produces models that more frequently appear on the Pareto frontier of reward
vs. KL compared to the ones trained with the MC estimator of the gradient.

</details>

### [296] [Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning](https://arxiv.org/abs/2504.10646)
*Saif Punjwani,Larry Heck*

Main category: cs.CL

TLDR: 本论文提出Weight-of-Thought (WoT)推理方法，通过分析神经网络权重提升大语言模型的推理性能和可解释性，在多种任务上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Chain-of-Thought关注输出层面，忽略内部权重动态，因此需要探索权重空间来改进推理能力。

Method: 引入WoT推理，利用图-based消息传递、多步推理和注意力机制，构建互联的推理节点图。

Result: 在逻辑、数学、代数、组合和几何任务的实验中，WoT显示出优越性能，尤其在复杂问题上，并提高了推理过程的可解释性。

Conclusion: 这种方法为增强大语言模型推理能力提供了有前景的方向。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities when prompted with strategies such as Chain-of-Thought (CoT).
However, these approaches focus on token-level output without considering
internal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a
novel approach that examines neural network weights before inference to
identify reasoning pathways. Unlike existing methods, WoT explores the weight
space through graph-based message passing, multi-step reasoning processes, and
attention mechanisms. Our implementation creates an interconnected graph of
reasoning nodes. Experiments on diverse reasoning tasks (syllogistic,
mathematical, algebraic, combinatorial, and geometric) demonstrate that WoT
achieves superior performance compared to traditional methods, particularly for
complex problems. This approach leads to both improved performance and greater
interpretability of the reasoning process, offering a promising direction for
enhancing LLM reasoning capabilities.

</details>

### [297] [LITERA: An LLM Based Approach to Latin-to-English Translation](https://arxiv.org/abs/2504.10660)
*Paul Rosu*

Main category: cs.CL

TLDR: 这篇论文介绍了LITERA，一个基于LLM的拉丁语到英语翻译平台，使用微调的GPT-4o-mini和GPT-4o，提高了翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 解决拉丁语文本翻译的挑战，并通过与杜克大学古典研究系合作，提供高质量的翻译工具以辅助研究。

Method: 采用多层翻译过程、微调GPT-4o-mini和GPT-4o、创建小型高质并行数据集，以及特定的提示策略。

Result: 显著提高了BLEU分数和BLEURT分数，尤其在古典拉丁语方面的准确性。

Conclusion: LITERA能够提供高精度的字面翻译，提升了古典研究中的辅助功能。

Abstract: This paper introduces an LLM-based Latin-to-English translation platform
designed to address the challenges of translating Latin texts. We named the
model LITERA, which stands for Latin Interpretation and Translations into
English for Research Assistance. Through a multi-layered translation process
utilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an
unprecedented level of accuracy, showcased by greatly improved BLEU scores,
particularly in classical Latin, along with improved BLEURT scores. The
development of LITERA involved close collaboration with Duke University's
Classical Studies Department, which was instrumental in creating a small,
high-quality parallel Latin-English dataset. This paper details the
architecture, fine-tuning methodology, and prompting strategies used in LITERA,
emphasizing its ability to produce literal translations.

</details>

### [298] [Characterizing Knowledge Manipulation in a Russian Wikipedia Fork](https://arxiv.org/abs/2504.10663)
*Mykola Trokhymovych,Oleksandr Kosovan,Nathan Forrester,Pablo Aragón,Diego Saez-Trumper,Ricardo Baeza-Yates*

Main category: cs.CL

TLDR: 这篇论文通过比较分析Ruwiki（俄罗斯维基百科的分支），识别知识操纵的实践和叙事，并提出一种可应用于其他维基分叉的分析方法。


<details>
  <summary>Details</summary>
Motivation: 为了识别与知识操纵相关的实践和叙事，特别是针对Ruwiki这种修改内容以符合俄罗斯法律的维基百科分支。

Method: 提出一种表征变化的方法ology；对超过190万篇文章进行全面比较分析，使用元信息、地理、时间、类别和文本特征；对知识操纵主题进行分类。

Result: 探索Ruwiki编辑所做的变化；呈现知识操纵主题的分类和数值估计。

Conclusion: 揭示Ruwiki中的重大变化，并提供一种可应用于分析其他维基百科分叉和类似协作项目的methodology。

Abstract: Wikipedia is powered by MediaWiki, a free and open-source software that is
also the infrastructure for many other wiki-based online encyclopedias. These
include the recently launched website Ruwiki, which has copied and modified the
original Russian Wikipedia content to conform to Russian law. To identify
practices and narratives that could be associated with different forms of
knowledge manipulation, this article presents an in-depth analysis of this
Russian Wikipedia fork. We propose a methodology to characterize the main
changes with respect to the original version. The foundation of this study is a
comprehensive comparative analysis of more than 1.9M articles from Russian
Wikipedia and its fork. Using meta-information and geographical, temporal,
categorical, and textual features, we explore the changes made by Ruwiki
editors. Furthermore, we present a classification of the main topics of
knowledge manipulation in this fork, including a numerical estimation of their
scope. This research not only sheds light on significant changes within Ruwiki,
but also provides a methodology that could be applied to analyze other
Wikipedia forks and similar collaborative projects.

</details>

### [299] [Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.10679)
*F. A. Rizvi,T. Navojith,A. M. N. H. Adhikari,W. P. U. Senevirathna,Dharshana Kasthurirathna,Lakmini Abeywardhana*

Main category: cs.CL

TLDR: 这篇论文提出一种混合NLP方法来提升银行领域代码混合内容的分析，包括关键词提取、内容过滤和基于方面的分类。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统NLP模型在处理代码混合和低资源语言（如Sinhala-English）时的误分类和忽略问题，以更好地捕捉领域特定知识。

Method: 方法使用混合方法：英语关键词提取结合微调SpaCy NER、FinBERT-based KeyBERT、YAKE和EmbedRank；代码混合和Sinhala关键词提取使用微调XLM-RoBERTa整合特定Sinhala金融词汇；过滤和分类使用BERT-base-uncased和XLM-RoBERTa。

Result: 结果显示，英语关键词提取准确率91.2%，Sinhala准确率87.4%；过滤准确率英语85.2%、Sinhala 88.1%；分类准确率英语87.4%、Sinhala 85.9%，均优于传统方法。

Conclusion: 结论是，该框架为代码混合和低资源银行环境提供准确、可扩展的品牌声誉监测解决方案。

Abstract: Brand reputation in the banking sector is maintained through insightful
analysis of customer opinion on code-mixed and multilingual content.
Conventional NLP models misclassify or ignore code-mixed text, when mix with
low resource languages such as Sinhala-English and fail to capture
domain-specific knowledge. This study introduces a hybrid NLP method to improve
keyword extraction, content filtering, and aspect-based classification of
banking content. Keyword extraction in English is performed with a hybrid
approach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT
embeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%.
Code-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa
model integrated with a domain-specific Sinhala financial vocabulary, and it
results in an accuracy of 87.4%. To ensure data quality, irrelevant comment
filtering was performed using several models, with the BERT-base-uncased model
achieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better
than GPT-4o, SVM, and keyword-based filtering. Aspect classification followed
the same pattern, with the BERT-base-uncased model achieving 87.4% for English
and XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based
approaches. These findings confirm that fine-tuned transformer models
outperform traditional methods in multilingual financial text analysis. The
present framework offers an accurate and scalable solution for brand reputation
monitoring in code-mixed and low-resource banking environments.

</details>

### [300] [The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks](https://arxiv.org/abs/2504.10768)
*Ralf Schmälzle,Sue Lim,Yuetong Du,Gary Bente*

Main category: cs.CL

TLDR: 这篇论文研究了在科学演讲中使用薄切片方法（基于少量信息进行判断），发现LLM能准确预测演讲质量，甚至短片段（少于10%）即可，并提出LLM框架作为反馈工具。


<details>
  <summary>Details</summary>
Motivation: 基于非语言沟通和个性心理学的研究，探讨薄切片方法在科学演讲中的应用，以证明简短片段能可靠预测整体质量。

Method: 使用一个包含一百多个真实科学演讲的语料库，采用LLM评估完整演讲和薄切片的转录文本，通过相关性分析确定所需信息量。

Result: LLM评估与人类评分高度一致，即使不到10%的演讲片段也能强烈预测整体评价；结果在不同LLM和提示策略下稳健。

Conclusion: 扩展薄切片研究到公开演讲，连接印象形成理论，讨论对沟通和社会认知研究的影响，并建议LLM-based薄切片框架作为可扩展的反馈工具。

Abstract: This paper examines the thin-slicing approach - the ability to make accurate
judgments based on minimal information - in the context of scientific
presentations. Drawing on research from nonverbal communication and personality
psychology, we show that brief excerpts (thin slices) reliably predict overall
presentation quality. Using a novel corpus of over one hundred real-life
science talks, we employ Large Language Models (LLMs) to evaluate transcripts
of full presentations and their thin slices. By correlating LLM-based
evaluations of short excerpts with full-talk assessments, we determine how much
information is needed for accurate predictions. Our results demonstrate that
LLM-based evaluations align closely with human ratings, proving their validity,
reliability, and efficiency. Critically, even very short excerpts (less than 10
percent of a talk) strongly predict overall evaluations. This suggests that the
first moments of a presentation convey relevant information that is used in
quality evaluations and can shape lasting impressions. The findings are robust
across different LLMs and prompting strategies. This work extends thin-slicing
research to public speaking and connects theories of impression formation to
LLMs and current research on AI communication. We discuss implications for
communication and social cognition research on message reception. Lastly, we
suggest an LLM-based thin-slicing framework as a scalable feedback tool to
enhance human communication.

</details>

### [301] [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://arxiv.org/abs/2410.10291)
*Xiangru Zhu,Penglei Sun,Yaoxian Song,Yanghua Xiao,Zhixu Li,Chengyu Wang,Jun Huang,Bei Yang,Xiaoxiao Xu*

Main category: cs.CL

TLDR: 本文提出SemVarEffect指标和SemVarBench基准，用于评估文本到图像合成中输入语义变化与输出因果关系。


<details>
  <summary>Details</summary>
Motivation: 当前模型难以处理词序变化带来的语义差异，现有的评估方法依赖间接指标，难以可靠评估复杂语言模式。

Method: 提出SemVarEffect指标和SemVarBench基准，通过语言置换实现语义变化，避免简单字面变化。

Result: CogView-3-Plus和Ideogram 2得分0.2/1；对象关系理解得分0.07/1，属性理解得分0.17-0.19/1；跨模态对齐在处理语义变化中至关重要。

Conclusion: 建立了有效的评估框架，推动文本到图像合成社区更好地理解人类指令。

Abstract: Accurate interpretation and visualization of human instructions are crucial
for text-to-image (T2I) synthesis. However, current models struggle to capture
semantic variations from word order changes, and existing evaluations, relying
on indirect metrics like text-image similarity, fail to reliably assess these
challenges. This often obscures poor performance on complex or uncommon
linguistic patterns by the focus on frequent word combinations. To address
these deficiencies, we propose a novel metric called SemVarEffect and a
benchmark named SemVarBench, designed to evaluate the causality between
semantic variations in inputs and outputs in T2I synthesis. Semantic variations
are achieved through two types of linguistic permutations, while avoiding
easily predictable literal variations. Experiments reveal that the
CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.
Semantic variations in object relations are less understood than attributes,
scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in
UNet or Transformers plays a crucial role in handling semantic variations, a
factor previously overlooked by a focus on textual encoders. Our work
establishes an effective evaluation framework that advances the T2I synthesis
community's exploration of human instruction understanding. Our benchmark and
code are available at https://github.com/zhuxiangru/SemVarBench .

</details>

### [302] [Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies](https://arxiv.org/abs/2504.10797)
*Annabella Sakunkoo,Jonathan Sakunkoo*

Main category: cs.CL

TLDR: This paper examines how large language models (LLMs) exhibit biases based on first and last names across ethnicities and genders, revealing complex status hierarchies and the impact of name choices.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs unfairly sort individuals into status positions based on names that signal gender, race, and status, given their widespread use.

Method: Large-scale analysis of name variations across 5 ethnicities to investigate inequality characteristics in LLMs.

Result: LLMs reflect status hierarchies; East and South Asian names sometimes rank higher; gender moderates biases, disadvantaging girls in some groups; adopting Western names improves perceived status for certain Asian individuals.

Conclusion: Emphasizes the need for intersectional understandings of race, gender, and identities in LLM evaluation to address biases.

Abstract: Across cultures, names tell a lot about their bearers as they carry deep
personal and cultural significance. Names also serve as powerful signals of
gender, race, and status in the social hierarchy - a pecking order in which
individual positions shape others' expectations on their perceived competence
and worth. With the widespread adoption of LLMs and as names are often an input
for LLMs, it is crucial to evaluate whether LLMs may sort people into status
positions based on first and last names and, if so, whether it is in an unfair,
biased fashion. While prior work has primarily investigated biases in first
names, little attention has been paid to last names and even less to the
combined effects of first and last names. In this study, we conduct a
large-scale analysis of name variations across 5 ethnicities to examine how AI
exhibits name biases. Our study investigates three key characteristics of
inequality and finds that LLMs reflect and reinforce status hierarchies based
on names that signal gender and ethnicity as they encode differential
expectations of competence, leadership, and economic potential. Contrary to the
common assumption that AI tends to favor Whites, we show that East and, in some
contexts, South Asian names receive higher rankings. We also disaggregate
Asians, a population projected to be the largest immigrant group in the U.S. by
2055. Our results challenge the monolithic Asian model minority assumption,
illustrating a more complex and stratified model of bias. Gender moderates
biases, with girls facing unfair disadvantages in certain racial groups.
Additionally, spanning cultural categories by adopting Western first names
improves AI-perceived status for East and Southeast Asian students,
particularly for girls. Our findings underscore the importance of
intersectional and more nuanced understandings of race, gender, and mixed
identities in the evaluation of LLMs.

</details>

### [303] [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
*Ayoung Lee,Ryan Sungmo Kwon,Peter Railton,Lu Wang*

Main category: cs.CL

TLDR: 本论文引入CLASH数据集评估LLM在高风险价值冲突中的推理能力，发现模型在模糊决策和价值转变方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 填补现有工作对LLM在高风险场景中推理能力的评估空白，特别是处理决策模糊性、心理不适和价值转变。

Method: 引入CLASH数据集（345个困境、3795个视角），并对10个LLM模型进行基准测试。

Result: 模型在模糊决策准确率不足50%；能预测心理不适但处理价值转变差；价值偏好与可操控性相关；第三方视角下可操控性更强。

Conclusion: LLM需改进复杂价值推理，不同视角可提升可操控性。

Abstract: Navigating high-stakes dilemmas involving conflicting values is challenging
even for humans, let alone for AI. Yet prior work in evaluating the reasoning
capabilities of large language models (LLMs) in such situations has been
limited to everyday scenarios. To close this gap, this work first introduces
CLASH (Character perspective-based LLM Assessments in Situations with
High-stakes), a meticulously curated dataset consisting of 345 high-impact
dilemmas along with 3,795 individual perspectives of diverse values. In
particular, we design CLASH in a way to support the study of critical aspects
of value-based decision-making processes which are missing from prior work,
including understanding decision ambivalence and psychological discomfort as
well as capturing the temporal shifts of values in characters' perspectives. By
benchmarking 10 open and closed frontier models, we uncover several key
findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,
achieve less than 50% accuracy in identifying situations where the decision
should be ambivalent, while they perform significantly better in clear-cut
scenarios. (2) While LLMs reasonably predict psychological discomfort as marked
by human, they inadequately comprehend perspectives involving value shifts,
indicating a need for LLMs to reason over complex values. (3) Our experiments
also reveal a significant correlation between LLMs' value preferences and their
steerability towards a given value. (4) Finally, LLMs exhibit greater
steerability when engaged in value reasoning from a third-party perspective,
compared to a first-person setup, though certain value pairs benefit uniquely
from the first-person framing.

</details>

### [304] [Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive Language Generators](https://arxiv.org/abs/2504.10845)
*Phill Kyu Rhee*

Main category: cs.CL

TLDR: This paper proposes a framework interpreting Large Language Models as probabilistic left context-sensitive language generators to better understand Transformer mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the poor understanding of the underlying mechanisms of Large Language Models.

Method: Decomposing Transformers into context windows, attention mechanisms, and autoregressive generation, interpreting next-token predictions as probabilistic CSL production rules.

Result: Transformers stochastically approximate context-sensitive languages, bridging Formal Language Theory and LLM generative capabilities.

Conclusion: Transformers model human-like intelligence through CSL approximation, fostering advancements in generative AI theory and applications.

Abstract: Large Language Models (LLMs), powered by Transformers, have demonstrated
human-like intelligence capabilities, yet their underlying mechanisms remain
poorly understood. This paper presents a novel framework for interpreting LLMs
as probabilistic left context-sensitive languages (CSLs) generators. We
hypothesize that Transformers can be effectively decomposed into three
fundamental components: context windows, attention mechanisms, and
autoregressive generation frameworks. This decomposition allows for the
development of more flexible and interpretable computational models, moving
beyond the traditional view of attention and autoregression as inseparable
processes. We argue that next-token predictions can be understood as
probabilistic, dynamic approximations of left CSL production rules, providing
an intuitive explanation for how simple token predictions can yield human-like
intelligence outputs. Given that all CSLs are left context-sensitive
(Penttonen, 1974), we conclude that Transformers stochastically approximate
CSLs, which are widely recognized as models of human-like intelligence. This
interpretation bridges the gap between Formal Language Theory and the observed
generative power of Transformers, laying a foundation for future advancements
in generative AI theory and applications. Our novel perspective on Transformer
architectures will foster a deeper understanding of LLMs and their future
potentials.

</details>

### [305] [Efficient Reasoning Models: A Survey](https://arxiv.org/abs/2504.10903)
*Sicheng Feng,Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CL

TLDR: 这篇论文概述了高效推理的最新进展，将工作分为缩短推理链、开发小型模型和加速解码三类。


<details>
  <summary>Details</summary>
Motivation: 推理模型生成长链式思考导致计算开销大，需要加速方法。

Method: 将现有工作分类为：(1) 压缩推理链；(2) 通过知识蒸馏等技术开发小型模型；(3) 设计高效解码策略。

Result: 提供了高效推理的全面概述和GitHub仓库资源。

Conclusion: 强调了高效推理的必要性，并总结了当前研究方向。

Abstract: Reasoning models have demonstrated remarkable progress in solving complex and
logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to
arriving at a final answer. Yet, the emergence of this "slow-thinking"
paradigm, with numerous tokens generated in sequence, inevitably introduces
substantial computational overhead. To this end, it highlights an urgent need
for effective acceleration. This survey aims to provide a comprehensive
overview of recent advances in efficient reasoning. It categorizes existing
works into three key directions: (1) shorter - compressing lengthy CoTs into
concise yet effective reasoning chains; (2) smaller - developing compact
language models with strong reasoning capabilities through techniques such as
knowledge distillation, other model compression techniques, and reinforcement
learning; and (3) faster - designing efficient decoding strategies to
accelerate inference. A curated collection of papers discussed in this survey
is available in our GitHub repository.

</details>

### [306] [Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
*Yingjian Chen,Feiyang Li,Xingyu Song,Tianxiao Li,Issey Sudeka,Irene Li*

Main category: cs.CL

TLDR: 本研究探索了基于知识图谱的RAG框架在日语医疗问答中使用小型开源LLM的效果，发现影响有限。


<details>
  <summary>Details</summary>
Motivation: 由于隐私限制，商业LLM如GPT-4无法在临床环境中使用，导致日语医疗QA效果有限，且RAG结合潜力未被充分探索。

Method: 首次开发并测试了基于知识图谱的RAG框架，应用于日语医疗QA的小型开源LLM。

Result: 实验结果显示KG-based RAG对日语医疗QA的影响有限，且其有效性依赖于检索内容的质量和相关性。

Conclusion: 为日语医疗QA中RAG的应用提供挑战和潜力的见解，并为其他低资源语言提供参考。

Abstract: Large language models (LLMs) perform well in medical QA, but their
effectiveness in Japanese contexts is limited due to privacy constraints that
prevent the use of commercial models like GPT-4 in clinical settings. As a
result, recent efforts focus on instruction-tuning open-source LLMs, though the
potential of combining them with retrieval-augmented generation (RAG) remains
underexplored. To bridge this gap, we are the first to explore a knowledge
graph-based (KG) RAG framework for Japanese medical QA small-scale open-source
LLMs. Experimental results show that KG-based RAG has only a limited impact on
Japanese medical QA using small-scale open-source LLMs. Further case studies
reveal that the effectiveness of the RAG is sensitive to the quality and
relevance of the external retrieved content. These findings offer valuable
insights into the challenges and potential of applying RAG in Japanese medical
QA, while also serving as a reference for other low-resource languages.

</details>

### [307] [HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving](https://arxiv.org/abs/2504.10724)
*Avinash Kumar,Shashank Nag,Jason Clemons,Lizy John,Poulami Das*

Main category: cs.CL

TLDR: HELIOS 通过动态管理早退和模型选择优化大语言模型部署，提高效率。


<details>
  <summary>Details</summary>
Motivation: 部署大语言模型面临延迟、准确性和吞吐量的权衡，早退LLM虽有效但加载保守且无法适应查询变化。

Method: 提出HELIOS框架：短列表候选模型并评估、基于早退数据贪婪加载层数、监控并切换模型以适应输入。

Result: HELIOS 实现1.48倍吞吐量、1.10倍能效、1.39倍响应时间降低、3.7倍推理批量大小改善。

Conclusion: HELIOS 有效解决部署挑战，提高性能指标。

Abstract: Deploying large language models (LLMs) presents critical challenges due to
the inherent trade-offs associated with key performance metrics, such as
latency, accuracy, and throughput. Typically, gains in one metric is
accompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently
navigate this trade-off space by skipping some of the later model layers when
it confidently finds an output token early, thus reducing latency without
impacting accuracy. However, as the early exits taken depend on the task and
are unknown apriori to request processing, EE-LLMs conservatively load the
entire model, limiting resource savings and throughput. Also, current
frameworks statically select a model for a user task, limiting our ability to
adapt to changing nature of the input queries.
  We propose HELIOS to address these challenges. First, HELIOS shortlists a set
of candidate LLMs, evaluates them using a subset of prompts, gathering
telemetry data in real-time. Second, HELIOS uses the early exit data from these
evaluations to greedily load the selected model only up to a limited number of
layers. This approach yields memory savings which enables us to process more
requests at the same time, thereby improving throughput. Third, HELIOS monitors
and periodically reassesses the performance of the candidate LLMs and if
needed, switches to another model that can service incoming queries more
efficiently (such as using fewer layers without lowering accuracy). Our
evaluations show that HELIOS achieves 1.48$\times$ throughput, 1.10$\times$
energy-efficiency, 1.39$\times$ lower response time, and 3.7$\times$
improvements in inference batch sizes compared to the baseline, when optimizing
for the respective service level objectives.

</details>

### [308] [Dynamic Compressing Prompts for Efficient Inference of Large Language Models](https://arxiv.org/abs/2504.11004)
*Jinwu Hu,Wei Zhang,Yufeng Wang,Yu Hu,Bin Xiao,Mingkui Tan,Qing Du*

Main category: cs.CL

TLDR: 论文提出了一种任务无关的提示压缩方法LLM-DCP，通过马尔可夫决策过程减少提示令牌，同时保持LLM性能。


<details>
  <summary>Details</summary>
Motivation: LLM提示技术需长提示，增加计算成本并受上下文窗口限制；现有压缩方法难保留关键信息、适应变化和跨任务有效。

Method: 提出LLM-DCP，将压缩建模为MDP，使用DCP-Agent移除冗余令牌，设计奖励函数和基于课程学习的HPC策略进行训练。

Result: 实验显示，该方法在高压缩率下优于现有技术。

Conclusion: 方法有效减少提示令牌而不显著降低性能，代码将在GitHub上公开。

Abstract: Large Language Models (LLMs) have shown outstanding performance across a
variety of tasks, partly due to advanced prompting techniques. However, these
techniques often require lengthy prompts, which increase computational costs
and can hinder performance because of the limited context windows of LLMs.
While prompt compression is a straightforward solution, existing methods
confront the challenges of retaining essential information, adapting to context
changes, and remaining effective across different tasks. To tackle these
issues, we propose a task-agnostic method called Dynamic Compressing Prompts
(LLM-DCP). Our method reduces the number of prompt tokens while aiming to
preserve the performance as much as possible. We model prompt compression as a
Markov Decision Process (MDP), enabling the DCP-Agent to sequentially remove
redundant tokens by adapting to dynamic contexts and retaining crucial content.
We develop a reward function for training the DCP-Agent that balances the
compression rate, the quality of the LLM output, and the retention of key
information. This allows for prompt token reduction without needing an external
black-box LLM. Inspired by the progressive difficulty adjustment in curriculum
learning, we introduce a Hierarchical Prompt Compression (HPC) training
strategy that gradually increases the compression difficulty, enabling the
DCP-Agent to learn an effective compression method that maintains information
integrity. Experiments demonstrate that our method outperforms state-of-the-art
techniques, especially at higher compression rates. The code for our approach
will be available at https://github.com/Fhujinwu/DCP.

</details>

### [309] [DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis](https://arxiv.org/abs/2504.11082)
*Efthymios Georgiou,Vassilis Katsouros,Yannis Avrithis,Alexandros Potamianos*

Main category: cs.CL

TLDR: 提出DeepMLF模型，改进了多模态情感分析中的融合深度和容量分配，实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 探讨融合深度和多模态容量分配的作用，这些方面在多模态情感分析中尚未充分研究。

Method: 引入DeepMLF，使用音视频编码器和预训练解码器语言模型，添加可学习标记进行深度融合，并结合特定损失函数训练。

Result: 在三个基准测试中达到最先进性能，最优融合深度为5-7，少量标记（约20个）即可最佳，并通过消融实验验证设计优势。

Conclusion: 更深融合带来更好性能，证明了所提融合设计和门控机制的优越性，并分析了可扩展性和训练目标的影响。

Abstract: While multimodal fusion has been extensively studied in Multimodal Sentiment
Analysis (MSA), the role of fusion depth and multimodal capacity allocation
remains underexplored. In this work, we position fusion depth, scalability, and
dedicated multimodal capacity as primary factors for effective fusion. We
introduce DeepMLF, a novel multimodal language model (LM) with learnable tokens
tailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a
pretrained decoder LM augmented with multimodal information across its layers.
We append learnable tokens to the LM that: 1) capture modality interactions in
a controlled fashion and 2) preserve independent information flow for each
modality. These fusion tokens gather linguistic information via causal
self-attention in LM Blocks and integrate with audiovisual information through
cross-attention MM Blocks. Serving as dedicated multimodal capacity, this
design enables progressive fusion across multiple layers, providing depth in
the fusion process. Our training recipe combines modality-specific losses and
language modelling loss, with the decoder LM tasked to predict ground truth
polarity. Across three MSA benchmarks with varying dataset characteristics,
DeepMLF achieves state-of-the-art performance. Our results confirm that deeper
fusion leads to better performance, with optimal fusion depths (5-7) exceeding
those of existing approaches. Additionally, our analysis on the number of
fusion tokens reveals that small token sets ($\sim$20) achieve optimal
performance. We examine the importance of representation learning order (fusion
curriculum) through audiovisual encoder initialization experiments. Our
ablation studies demonstrate the superiority of the proposed fusion design and
gating while providing a holistic examination of DeepMLF's scalability to LLMs,
and the impact of each training objective and embedding regularization.

</details>

### [310] [MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos](https://arxiv.org/abs/2504.11169)
*Laura De Grazia,Pol Pastells,Mauro Vázquez Chas,Desmond Elliott,Danae Sánchez Villegas,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TLDR: 本研究引入MuSeD数据集，提出注释框架，并评估LLM在多模态性别歧视检测中的表现，发现视觉信息重要，但隐性歧视检测困难。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频中性别歧视传播需要多模态分析，以应对文本、音频和视觉元素的结合。

Method: 引入约11小时的MuSeD西班牙语数据集，提出文本和多模态标签注释框架，并评估各种LLM和多模态LLM。

Result: 视觉信息在检测中起关键作用；模型对显性性别歧视检测较好，但对隐性歧视和刻板印象 struggled；注释者对隐性案例一致性低。

Conclusion: 识别隐性性别歧视因社会文化背景而具有挑战性，突显任务的复杂性。

Abstract: Sexism is generally defined as prejudice and discrimination based on sex or
gender, affecting every sector of society, from social institutions to
relationships and individual behavior. Social media platforms amplify the
impact of sexism by conveying discriminatory content not only through text but
also across multiple modalities, highlighting the critical need for a
multimodal approach to the analysis of sexism online. With the rise of social
media platforms where users share short videos, sexism is increasingly
spreading through video content. Automatically detecting sexism in videos is a
challenging task, as it requires analyzing the combination of verbal, audio,
and visual elements to identify sexist content. In this study, (1) we introduce
MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of
$\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose
an innovative annotation framework for analyzing the contribution of textual
and multimodal labels in the classification of sexist and non-sexist content;
and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs
on the task of sexism detection. We find that visual information plays a key
role in labeling sexist content for both humans and models. Models effectively
detect explicit sexism; however, they struggle with implicit cases, such as
stereotypes, instances where annotators also show low agreement. This
highlights the inherent difficulty of the task, as identifying implicit sexism
depends on the social and cultural context.

</details>

### [311] [Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items](https://arxiv.org/abs/2504.11186)
*Minjie Zou,Sahana Srinivasan,Thaddaeus Wai Soon Lo,Ke Zou,Gabriel Dawei Yang,Xuguang Ai,Hyunjae Kim,Maxwell Singer,Fares Antaki,Kelvin Li,Robert Chang,Marcus Tan,David Ziyou Chen,Dianbo Liu,Qingyu Chen,Yih Chung Tham*

Main category: cs.CL

TLDR: This study evaluates four reasoning-focused LLMs on ophthalmology exam questions, comparing accuracy, reasoning, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored performance of reasoning-focused LLMs in specialized medical domains like ophthalmology, despite advances in complex decision-making.

Method: Evaluated four LLMs using 5,888 multiple-choice questions from MedMCQA in zero-shot setting, with metrics including accuracy, Macro-F1, text-generation scores, inference time, and qualitative assessment by ophthalmologists.

Result: O1 and DeepSeek-R1 had highest accuracy; models varied in metrics like ROUGE-L and inference time; qualitative results showed differences in reasoning detail.

Conclusion: Reasoning-focused LLMs show promise in medical applications with trade-offs in accuracy, efficiency, and reasoning quality.

Abstract: Recent advances in reasoning-focused large language models (LLMs) mark a
shift from general LLMs toward models designed for complex decision-making, a
crucial aspect in medicine. However, their performance in specialized domains
like ophthalmology remains underexplored. This study comprehensively evaluated
and compared the accuracy and reasoning capabilities of four newly developed
reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0
Flash-Thinking. Each model was assessed using 5,888 multiple-choice
ophthalmology exam questions from the MedMCQA dataset in zero-shot setting.
Quantitative evaluation included accuracy, Macro-F1, and five text-generation
metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed
against ground-truth reasonings. Average inference time was recorded for a
subset of 100 randomly selected questions. Additionally, two board-certified
ophthalmologists qualitatively assessed clarity, completeness, and reasoning
structure of responses to differential diagnosis questions.O1 (0.902) and
DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in
Macro-F1 (0.900). The performance of models across the text-generation metrics
varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1
and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0
Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and
o1 (0.176) led AlignScore. Inference time across the models varied, with
DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest
(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0
Flash-Thinking tended to provide detailed and comprehensive intermediate
reasoning, whereas o1 and o3-mini displayed concise and summarized
justifications.

</details>

### [312] [A Dual-Space Framework for General Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2504.11426)
*Xue Zhang,Songming Zhang,Yunlong Liang,Fandong Meng,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TLDR: 本文提出双空间知识蒸馏框架DSKD，解决传统白盒KD在不同输出空间和词汇表下的限制，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前白盒KD框架存在输出空间不一致和无法处理不同词汇表的问题，根因是教师和学生模型使用不同预测头。

Method: 提出DSKD框架，使用两个投影器统一隐藏状态空间，并开发精确标记对齐算法ETA，支持任意LLM间的离线和在线知识蒸馏。

Result: 实验在指令遵循、数学推理和代码生成任务上显示，DSKD优于现有白盒KD方法，并在不同词汇表场景下超越其他跨标记器KD方法。

Conclusion: DSKD是一种通用框架，可处理任意词汇表和输出空间的LLM知识蒸馏，提升压缩效率和性能。

Abstract: Knowledge distillation (KD) is a promising solution to compress large
language models (LLMs) by transferring their knowledge to smaller models.
During this process, white-box KD methods usually minimize the distance between
the output distributions of the teacher model and the student model to transfer
more information. However, we reveal that the current white-box KD framework
exhibits two limitations: a) bridging probability distributions from different
output spaces will limit the similarity between the teacher model and the
student model; b) this framework cannot be applied to LLMs with different
vocabularies. One of the root causes for these limitations is that the
distributions from the teacher and the student for KD are output by different
prediction heads, which yield distributions in different output spaces and
dimensions. Therefore, in this paper, we propose a dual-space knowledge
distillation (DSKD) framework that unifies the prediction heads of the teacher
and the student models for KD. Specifically, we first introduce two projectors
with ideal initialization to project the teacher/student hidden states into the
student/teacher representation spaces. After this, the hidden states from
different models can share the same head and unify the output spaces of the
distributions. Furthermore, we develop an exact token alignment (ETA) algorithm
to align the same tokens in two differently-tokenized sequences. Based on the
above, our DSKD framework is a general KD framework that supports both
off-policy and on-policy KD, and KD between any two LLMs regardless of their
vocabularies. Extensive experiments on instruction-following, mathematical
reasoning, and code generation benchmarks show that DSKD significantly
outperforms existing methods based on the current white-box KD framework and
surpasses other cross-tokenizer KD methods for LLMs with different
vocabularies.

</details>

### [313] [Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models](https://arxiv.org/abs/2504.11431)
*Maria Teleki,Xiangjue Dong,Haoran Liu,James Caverlee*

Main category: cs.CL

TLDR: 这篇论文研究话语中的男性默认偏差，使用播客数据和LLM，发现性别偏差问题。


<details>
  <summary>Details</summary>
Motivation: 男性默认偏差被低估且研究不足，包括文化背景、男性特征和奖励，本文旨在揭示其影响。

Method: 提出GDCF框架通过LDA和BERTopic分析15,117个播客剧集发现性别化话语词，并使用D-WEAT测量LLM中的性别偏差。

Result: 在商业、技术/政治和视频游戏领域存在性别化话语；男性话语词在LLM中表示更稳定，导致男性在下游任务中表现更好。

Conclusion: 这种嵌入差异是一种表征性伤害，并体现了男性默认偏差。

Abstract: Masculine defaults are widely recognized as a significant type of gender
bias, but they are often unseen as they are under-researched. Masculine
defaults involve three key parts: (i) the cultural context, (ii) the masculine
characteristics or behaviors, and (iii) the reward for, or simply acceptance
of, those masculine characteristics or behaviors. In this work, we study
discourse-based masculine defaults, and propose a twofold framework for (i) the
large-scale discovery and analysis of gendered discourse words in spoken
content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the
measurement of the gender bias associated with these gendered discourse words
in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus
our study on podcasts, a popular and growing form of social media, analyzing
15,117 podcast episodes. We analyze correlations between gender and discourse
words -- discovered via LDA and BERTopic -- to automatically form gendered
discourse word lists. We then study the prevalence of these gendered discourse
words in domain-specific contexts, and find that gendered discourse-based
masculine defaults exist in the domains of business, technology/politics, and
video games. Next, we study the representation of these gendered discourse
words from a state-of-the-art LLM embedding model from OpenAI, and find that
the masculine discourse words have a more stable and robust representation than
the feminine discourse words, which may result in better system performance on
downstream tasks for men. Hence, men are rewarded for their discourse patterns
with better system performance by one of the state-of-the-art language models
-- and this embedding disparity is a representational harm and a masculine
default.

</details>

### [314] [OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution](https://arxiv.org/abs/2504.11369)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TLDR: 这篇论文提出OpenTuringBench基准，用于训练和评估检测开放大型语言模型(OLLMs)生成文本的模型，并引入OTBDetector框架，展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: OLLMs在生成AI应用中日益广泛使用，带来了检测其输出的新挑战。

Method: 提出基于OLLMs的OpenTuringBench基准和OTBDetector对比学习框架，用于图灵测试和作者归属问题。

Result: OTBDetector在各种任务中表现出色，优于大多数现有检测器，并突显基准任务的相关性和难度。

Conclusion: 证明了OpenTuringBench和OTBDetector的有效性，资源可在Hugging Face仓库获取。

Abstract: Open Large Language Models (OLLMs) are increasingly leveraged in generative
AI applications, posing new challenges for detecting their outputs. We propose
OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate
machine-generated text detectors on the Turing Test and Authorship Attribution
problems. OpenTuringBench focuses on a representative set of OLLMs, and
features a number of challenging evaluation tasks, including
human/machine-manipulated texts, out-of-domain texts, and texts from previously
unseen models. We also provide OTBDetector, a contrastive learning framework to
detect and attribute OLLM-based machine-generated texts. Results highlight the
relevance and varying degrees of difficulty of the OpenTuringBench tasks, with
our detector achieving remarkable capabilities across the various tasks and
outperforming most existing detectors. Resources are available on the
OpenTuringBench Hugging Face repository at
https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench

</details>

### [315] [TextArena](https://arxiv.org/abs/2504.11442)
*Leon Guertler,Bobby Cheng,Simon Yu,Bo Liu,Leshem Choshen,Cheston Tan*

Main category: cs.CL

TLDR: TextArena 是一个开源文本游戏集合，用于训练和评估大语言模型的代理行为，包括57+个环境，支持在线对战和TrueSkill评分。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试很少评估动态社交技能如谈判、理论心智和欺骗，因此需要TextArena来填补这一空白。

Method: 通过构建包含单人、双人、多人环境的游戏框架，支持在线对战系统、TrueSkill评分，并强调易扩展和社区使用。

Result: 开发了一个易于评估模型能力的平台，具有在线排行榜和详细文档，促进了LLM社交技能的测试。

Conclusion: TextArena 旨在支持研究和社区扩展，提供游戏环境、排行榜和示例的文档，可通过GitHub和网站访问。

Abstract: TextArena is an open-source collection of competitive text-based games for
training and evaluation of agentic behavior in Large Language Models (LLMs). It
spans 57+ unique environments (including single-player, two-player, and
multi-player setups) and allows for easy evaluation of model capabilities via
an online-play system (against humans and other submitted models) with
real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social
skills such as negotiation, theory of mind, and deception, creating a gap that
TextArena addresses. Designed with research, community and extensibility in
mind, TextArena emphasizes ease of adding new games, adapting the framework,
testing models, playing against the models, and training models. Detailed
documentation of environments, games, leaderboard, and examples are available
on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.

</details>

### [316] [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)
*Zhiwei He,Tian Liang,Jiahao Xu,Qiuzhi Liu,Xingyu Chen,Yue Wang,Linfeng Song,Dian Yu,Zhenwen Liang,Wenxuan Wang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: 本论文引入DeepMath-103K数据集，用于通过强化学习训练AI的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前，应用于大型语言模型的强化学习在数学推理方面进展缓慢，主要是由于缺乏大规模、具有挑战性、可验证答案格式且无评估基准污染的训练数据。

Method: 通过来源分析、严格去污染处理、高难度问题过滤（主要5-9级）、提供可验证答案和三种R1生成解决方案的管道来构建数据集。

Result: 在DeepMath-103K上训练的模型在数学基准测试中取得了显著改进。

Conclusion: 数据集公开发布，以促进社区构建更强AI推理系统。

Abstract: The capacity for complex mathematical reasoning is a key benchmark for
artificial intelligence. While reinforcement learning (RL) applied to LLMs
shows promise, progress is significantly hindered by the lack of large-scale
training data that is sufficiently challenging, possesses verifiable answer
formats suitable for RL, and is free from contamination with evaluation
benchmarks. To address these limitations, we introduce DeepMath-103K, a new,
large-scale dataset comprising approximately 103K mathematical problems,
specifically designed to train advanced reasoning models via RL. DeepMath-103K
is curated through a rigorous pipeline involving source analysis, stringent
decontamination against numerous benchmarks, and filtering for high difficulty
(primarily Levels 5-9), significantly exceeding existing open resources in
challenge. Each problem includes a verifiable final answer, enabling rule-based
RL, and three distinct R1-generated solutions suitable for diverse training
paradigms like supervised fine-tuning or distillation. Spanning a wide range of
mathematical topics, DeepMath-103K promotes the development of generalizable
reasoning. We demonstrate that models trained on DeepMath-103K achieve
significant improvements on challenging mathematical benchmarks, validating its
effectiveness. We release DeepMath-103K publicly to facilitate community
progress in building more capable AI reasoning systems:
https://github.com/zwhe99/DeepMath.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [317] [Physics-Informed Neural Networks for Enhanced Interface Preservation in Lattice Boltzmann Multiphase Simulations](https://arxiv.org/abs/2504.10539)
*Yue Li*

Main category: physics.flu-dyn

TLDR: 这篇论文提出了一种使用Physics-Informed Neural Networks (PINNs)改进的多相Lattice Boltzmann Method (LBM)，以保持接口清晰。


<details>
  <summary>Details</summary>
Motivation: 多相LBM中接口扩散问题常见，导致模拟精度降低，尤其在界面动态关键的场景。

Method: 提出耦合PINN-LBM框架，通过液滴模拟和指标（如接口宽度、最大梯度）进行验证。

Result: PINN-LBM在接口清晰度和对抗数值扩散方面优于标准LBM，定量和可视化结果显示性能提升。

Conclusion: 神经网络整合有效减少扩散，同时保持流体动力学物理一致性。

Abstract: This paper presents an improved approach for preserving sharp interfaces in
multiphase Lattice Boltzmann Method (LBM) simulations using Physics-Informed
Neural Networks (PINNs). Interface diffusion is a common challenge in
multiphase LBM, leading to reduced accuracy in simulating phenomena where
interfacial dynamics are critical. We propose a coupled PINN-LBM framework that
maintains interface sharpness while preserving the physical accuracy of the
simulation. Our approach is validated through droplet simulations, with
quantitative metrics measuring interface width, maximum gradient, phase
separation, effective interface width, and interface energy. The enhanced
visualization techniques employed in this work clearly demonstrate the superior
performance of PINN-LBM over standard LBM for multiphase simulations,
particularly in maintaining well-defined interfaces throughout the simulation.
We provide a comprehensive analysis of the results, showcasing how the neural
network integration effectively counteracts numerical diffusion, while
maintaining physical consistency with the underlying fluid dynamics.

</details>

### [318] [Visual anemometry of natural vegetation from their leaf motion](https://arxiv.org/abs/2504.10584)
*Roni H. Goldshmid,John O. Dabiri,John E. Sader*

Main category: physics.flu-dyn

TLDR: 本研究开发了一种基于叶片运动的远程定量风速测量方法，使用公式在多种植物上验证。


<details>
  <summary>Details</summary>
Motivation: 高分辨率风速数据对天气预测、野火控制和飞机安全至关重要，但现有方法成本高或复杂，研究旨在利用自然植被叶片运动实现廉价测量。

Method: 分析不同植被的测量数据，在低到中等风速下解耦叶片运动，开发公式U_wind≈740√(μU_leaf/ρD)，并通过实验室和现场测试验证。

Result: 公式被第一性原理模型证实，并在橡树、橄榄树等各种植物上有效验证。

Conclusion: 这项研究开辟了使用自然植被进行远程、快速、廉价风速测量的全新范式。

Abstract: High-resolution, near-ground wind-speed data are critical for improving the
accuracy of weather predictions and climate models,$^{1-3}$ supporting wildfire
control efforts,$^{4-7}$ and ensuring the safe passage of airplanes during
takeoff and landing maneouvers.$^{8,9}$ Quantitative wind speed anemometry
generally employs on-site instrumentation for accurate single-position data or
sophisticated remote techniques such as Doppler radar for quantitative field
measurements. It is widely recognized that the wind-induced motion of
vegetation depends in a complex manner on their structure and mechanical
properties, obviating their use in quantitative anemometry.$^{10-14}$ We
analyze measurements on a host of different vegetation showing that leaf motion
can be decoupled from the leaf's branch and support structure, at
low-to-moderate wind speed, $U_{wind}$. This wind speed range is characterized
by a leaf Reynolds number, enabling the development of a remote, quantitative
anemometry method based on the formula,
$U_{wind}\approx740\sqrt{{\mu}U_{leaf}/{\rho}D}$, that relies only on the leaf
size $D$, its measured fluctuating (RMS) speed $U_{leaf}$, the air viscosity
$\mu$, and its mass density $\rho$. This formula is corroborated by a
first-principles model and validated using a host of laboratory and field tests
on diverse vegetation types, ranging from oak, olive, and magnolia trees
through to camphor and bullgrass. The findings of this study open the door to a
new paradigm in anemometry, using natural vegetation to enable remote and rapid
quantitative field measurements at global locations with minimal cost.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [319] [Using Time Structure to Estimate Causal Effects](https://arxiv.org/abs/2504.11076)
*Tom Hochsprung,Jakob Runge,Andreas Gerhardus*

Main category: stat.ME

TLDR: 这篇论文提出了一种不依赖辅助变量的时间序列因果效应估计方法，基于结构向量自回归（SVAR）过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助变量如工具变量或控制变量，本文旨在提供一种不需要这些变量的替代方法。

Method: 假设时间序列为SVAR过程，通过求解线性方程组（使用协方差和模型参数）来估计直接因果效应，并给出图形和滞后-based 标准确保可识别性。

Result: 给出了充分条件确保方程组唯一可解和因果效应可识别，数值实验验证了方法的正确性和适用性。

Conclusion: 该方法在特定条件下有效，能够识别直接（和总）因果效应。

Abstract: There exist several approaches for estimating causal effects in time series
when latent confounding is present. Many of these approaches rely on additional
auxiliary observed variables or time series such as instruments, negative
controls or time series that satisfy the front- or backdoor criterion in
certain graphs. In this paper, we present a novel approach for estimating
direct (and via Wright's path rule total) causal effects in a time series setup
which does not rely on additional auxiliary observed variables or time series.
This approach assumes that the underlying time series is a Structural Vector
Autoregressive (SVAR) process and estimates direct causal effects by solving
certain linear equation systems made up of different covariances and model
parameters. We state sufficient graphical criteria in terms of the so-called
full time graph under which these linear equations systems are uniquely
solvable and under which their solutions contain the to-be-identified direct
causal effects as components. We also state sufficient lag-based criteria under
which the previously mentioned graphical conditions are satisfied and, thus,
under which direct causal effects are identifiable. Several numerical
experiments underline the correctness and applicability of our results.

</details>