<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 29]
- [cs.RO](#cs.RO) [Total: 3]
- [eess.SY](#eess.SY) [Total: 16]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Toward Super Agent System with Hybrid AI Routers](https://arxiv.org/abs/2504.10519)
*Yuhang Yao,Haixin Wang,Yibo Chen,Jiawen Wang,Min Chang Jordan Ren,Bosheng Ding,Salman Avestimehr,Chaoyang He*

Main category: cs.AI

TLDR: 这篇论文设计了一个超级代理系统，使用混合本地和云端模型来高效处理用户任务。


<details>
  <summary>Details</summary>
Motivation: 为了使基于大语言模型的AI超级代理在现实世界中可部署，需要优化效率、降低成本并解决隐私问题。

Method: 系统通过检测用户意图、路由到专用代理或生成工作流，并动态选择本地或云端模型。

Result: 提出了一个增强型本地设备的超级代理蓝图，减少了对云端的依赖。

Conclusion: 展望未来，超级代理将通过本地计算和云端协作无缝融入日常生活。

Abstract: AI Agents powered by Large Language Models are transforming the world through
enormous applications. A super agent has the potential to fulfill diverse user
needs, such as summarization, coding, and research, by accurately understanding
user intent and leveraging the appropriate tools to solve tasks. However, to
make such an agent viable for real-world deployment and accessible at scale,
significant optimizations are required to ensure high efficiency and low cost.
This paper presents a design of the Super Agent System. Upon receiving a user
prompt, the system first detects the intent of the user, then routes the
request to specialized task agents with the necessary tools or automatically
generates agentic workflows. In practice, most applications directly serve as
AI assistants on edge devices such as phones and robots. As different language
models vary in capability and cloud-based models often entail high
computational costs, latency, and privacy concerns, we then explore the hybrid
mode where the router dynamically selects between local and cloud models based
on task complexity. Finally, we introduce the blueprint of an on-device super
agent enhanced with cloud. With advances in multi-modality models and edge
hardware, we envision that most computations can be handled locally, with cloud
collaboration only as needed. Such architecture paves the way for super agents
to be seamlessly integrated into everyday life in the near future.

</details>

### [2] [Explainable Artificial Intelligence techniques for interpretation of food datasets: a review](https://arxiv.org/abs/2504.10527)
*Leonardo Arrighi,Ingrid Alves de Moraes,Marco Zullich,Michele Simonato,Douglas Fernandes Barbin,Sylvio Barbon Junior*

Main category: cs.AI

TLDR: 这篇调查介绍了在食品工程中使用可解释AI（XAI）技术的分类法，以提高AI模型的可靠性和透明度，并指导研究人员选择合适的方法。


<details>
  <summary>Details</summary>
Motivation: AI在食品工程中的应用日益复杂，导致可靠性问题，而XAI尚未被充分利用，需要提升AI决策的透明度以满足食品质量标准。

Method: 呈现一个基于数据类型和解释方法的分类法，用于分类使用XAI技术的食品质量研究。

Result: 突出了XAI在食品工程中的趋势、挑战和机会，以鼓励其采用。

Conclusion: 通过提供分类法和见解，指导研究人员并推动XAI在食品工程中的应用。

Abstract: Artificial Intelligence (AI) has become essential for analyzing complex data
and solving highly-challenging tasks. It is being applied across numerous
disciplines beyond computer science, including Food Engineering, where there is
a growing demand for accurate and trustworthy predictions to meet stringent
food quality standards. However, this requires increasingly complex AI models,
raising reliability concerns. In response, eXplainable AI (XAI) has emerged to
provide insights into AI decision-making, aiding model interpretation by
developers and users. Nevertheless, XAI remains underutilized in Food
Engineering, limiting model reliability. For instance, in food quality control,
AI models using spectral imaging can detect contaminants or assess freshness
levels, but their opaque decision-making process hinders adoption. XAI
techniques such as SHAP (Shapley Additive Explanations) and Grad-CAM
(Gradient-weighted Class Activation Mapping) can pinpoint which spectral
wavelengths or image regions contribute most to a prediction, enhancing
transparency and aiding quality control inspectors in verifying AI-generated
assessments. This survey presents a taxonomy for classifying food quality
research using XAI techniques, organized by data types and explanation methods,
to guide researchers in choosing suitable approaches. We also highlight trends,
challenges, and opportunities to encourage the adoption of XAI in Food
Engineering.

</details>

### [3] [Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics](https://arxiv.org/abs/2504.10649)
*Matthew Zalesak,Hins Hu,Samitha Samaranayake*

Main category: cs.AI

TLDR: 这篇论文开源了乘车池模拟器，引入新算法，提升了效率，并在真实数据上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 缺乏开源实现导致基准测试困难，因此开发了开源工具以便算法比较。

Method: 实现了乘车池模拟器，包括车辆路由和再平衡组件，引入交换-based局部搜索启发式，并提出LA-MR-CE算法。

Result: 实验显示LA-MR-CE算法在服务率和计算时间上优越，所有短视算法受容量瓶颈限制。

Conclusion: 建议整合未来信息以克服性能瓶颈，提升算法效果。

Abstract: On-demand ride-pooling has emerged as a popular urban transportation
solution, addressing the efficiency limitations of traditional ride-hailing
services by grouping multiple riding requests with spatiotemporal proximity
into a single vehicle. Although numerous algorithms have been developed for the
Ride-pool Assignment Problem (RAP) -- a core component of ride-pooling systems,
there is a lack of open-source implementations, making it difficult to
benchmark these algorithms on a common dataset and objective. In this paper, we
present the implementation details of a ride-pool simulator that encompasses
several key ride-pool assignment algorithms, along with associated components
such as vehicle routing and rebalancing. We also open-source a highly optimized
and modular C++ codebase, designed to facilitate the extension of new
algorithms and features. Additionally, we introduce a family of swapping-based
local-search heuristics to enhance existing ride-pool assignment algorithms,
achieving a better balance between performance and computational efficiency.
Extensive experiments on a large-scale, real-world dataset from Manhattan, NYC
reveal that while all selected algorithms perform comparably, the newly
proposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE)
algorithm achieves a state-of-the-art service rate with significantly reduced
computational time. Furthermore, an in-depth analysis suggests that a
performance barrier exists for all myopic ride-pool assignment algorithms due
to the system's capacity bottleneck, and incorporating future information could
be key to overcoming this limitation.

</details>

### [4] [Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control](https://arxiv.org/abs/2504.10831)
*Hyojun Ahn,Seungcheol Oh,Gyu Seon Kim,Soyi Jung,Soohyun Park,Joongheon Kim*

Main category: cs.AI

TLDR: 本论文提出SafeGPT框架，结合GPT和RL提升UAV最后一段交付的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效可靠的无人机(UAV)最后一段交付，解决GPT可能产生的幻觉问题。

Method: SafeGPT采用两层结构：Global GPT分配高水平任务，On-Device GPT处理实时路线规划；RL安全过滤器监控并覆盖不安全决策；双重回放缓冲区优化策略。

Result: 模拟结果显示，SafeGPT比GPT-only基准有更高交付成功率，并显著减少电池消耗和旅行距离。

Conclusion: 验证了结合GPT语义推理与正式安全保证的有效性，为鲁棒节能的UAV物流提供解决方案。

Abstract: This paper proposes SafeGPT, a two-tiered framework that integrates
generative pretrained transformers (GPTs) with reinforcement learning (RL) for
efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In
the proposed design, a Global GPT module assigns high-level tasks such as
sector allocation, while an On-Device GPT manages real-time local route
planning. An RL-based safety filter monitors each GPT decision and overrides
unsafe actions that could lead to battery depletion or duplicate visits,
effectively mitigating hallucinations. Furthermore, a dual replay buffer
mechanism helps both the GPT modules and the RL agent refine their strategies
over time. Simulation results demonstrate that SafeGPT achieves higher delivery
success rates compared to a GPT-only baseline, while substantially reducing
battery consumption and travel distance. These findings validate the efficacy
of combining GPT-based semantic reasoning with formal safety guarantees,
contributing a viable solution for robust and energy-efficient UAV logistics.

</details>

### [5] [Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration](https://arxiv.org/abs/2504.10865)
*Han-Dong Lim,Donghwan Lee*

Main category: cs.AI

TLDR: 本文研究投影Bellman方程的理论属性、线性Q学习和近似值迭代算法，探讨解的存在性和收敛条件。


<details>
  <summary>Details</summary>
Motivation: 动机是分析投影Bellman方程解的存在以及算法收敛的充分条件。

Method: 方法包括使用严格负行主导对角假设和AVI收敛条件，检查线性Q学习的收敛，并观察ε-贪婪策略下的解。

Result: 结果显示SNRDD假设确保线性Q学习的收敛，并探讨其与AVI收敛的关系，提供相关观察。

Conclusion: 结论是对投影Bellman方程理论性质的深入理解和算法改进的启示。

Abstract: In this paper, we study the theoretical properties of the projected Bellman
equation (PBE) and two algorithms to solve this equation: linear Q-learning and
approximate value iteration (AVI). We consider two sufficient conditions for
the existence of a solution to PBE : strictly negatively row dominating
diagonal (SNRDD) assumption and a condition motivated by the convergence of
AVI. The SNRDD assumption also ensures the convergence of linear Q-learning,
and its relationship with the convergence of AVI is examined. Lastly, several
interesting observations on the solution of PBE are provided when using
$\epsilon$-greedy policy.

</details>

### [6] [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
*Yize Zhang,Tianshu Wang,Sirui Chen,Kun Wang,Xingyu Zeng,Hongyu Lin,Xianpei Han,Le Sun,Chaochao Lu*

Main category: cs.AI

TLDR: 本文提出ARise框架，通过风险评估、动态RAG和Monte Carlo树搜索优化LLMs推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在开放式知识密集推理中受限，现有方法存在泛化问题、错误传播和验证瓶颈。

Method: ARise框架整合中间状态风险评估、动态检索增强生成和Monte Carlo树搜索范式。

Result: 实验结果显示ARise比最先进KAR方法提升23.10%，比RAG增强模型提升25.37%。

Conclusion: ARise框架有效解决推理局限性，优化多分支假设推理计划。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities and
are receiving increasing attention to enhance their reasoning through scaling
test--time compute. However, their application in open--ended,
knowledge--intensive, complex reasoning scenarios is still limited.
Reasoning--oriented methods struggle to generalize to open--ended scenarios due
to implicit assumptions of complete world knowledge. Meanwhile,
knowledge--augmented reasoning (KAR) methods fail to address two core
challenges: 1) error propagation, where errors in early steps cascade through
the chain, and 2) verification bottleneck, where the explore--exploit tradeoff
arises in multi--branch decision processes. To overcome these limitations, we
introduce ARise, a novel framework that integrates risk assessment of
intermediate reasoning states with dynamic retrieval--augmented generation
(RAG) within a Monte Carlo tree search paradigm. This approach enables
effective construction and optimization of reasoning plans across multiple
maintained hypothesis branches. Experimental results show that ARise
significantly outperforms the state--of--the--art KAR methods by up to 23.10%,
and the latest RAG-equipped large reasoning models by up to 25.37%.

</details>

### [7] [Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior](https://arxiv.org/abs/2504.11075)
*Dongmin Kim,Hoshinori Kanazawa,Naoto Yoshida,Yasuo Kuniyoshi*

Main category: cs.AI

TLDR: 本论文提出'self-prior'密度模型，通过最小化过去和当前感官体验差异，诱导代理内在目标导向行为；在模拟环境中，代理自发伸手触碰触觉刺激，展示了内在动机行为的涌现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注探索如何获得外部奖励，本论文探讨纯内在过程如何驱动无外部奖励的 spontaneous 探索和学习。

Method: 提出'self-prior'多模态感官体验密度模型，并整合到基于自由能量原理的主动推理框架中；在模拟环境中测试代理行为。

Result: 代理自发地伸手触碰触觉刺激，证实了内在动机机制的有效性。

Conclusion: 展示了代理通过自身感官体验塑造目标导向行为，类似于婴儿早期发展的 intentional 行为涌现。

Abstract: Infants often exhibit goal-directed behaviors, such as reaching for a sensory
stimulus, even when no external reward criterion is provided. These
intrinsically motivated behaviors facilitate spontaneous exploration and
learning of the body and environment during early developmental stages.
Although computational modeling can offer insight into the mechanisms
underlying such behaviors, many existing studies on intrinsic motivation focus
primarily on how exploration contributes to acquiring external rewards. In this
paper, we propose a novel density model for an agent's own multimodal sensory
experiences, called the "self-prior," and investigate whether it can
autonomously induce goal-directed behavior. Integrated within an active
inference framework based on the free energy principle, the self-prior
generates behavioral references purely from an intrinsic process that minimizes
mismatches between average past sensory experiences and current observations.
This mechanism is also analogous to the acquisition and utilization of a body
schema through continuous interaction with the environment. We examine this
approach in a simulated environment and confirm that the agent spontaneously
reaches toward a tactile stimulus. Our study implements intrinsically motivated
behavior shaped by the agent's own sensory experiences, demonstrating the
spontaneous emergence of intentional behavior during early development.

</details>

### [8] [C-SHAP for time series: An approach to high-level temporal explanations](https://arxiv.org/abs/2504.11159)
*Annemarie Jutte,Faizan Ahmed,Jeroen Linssen,Maurice van Keulen*

Main category: cs.AI

TLDR: 本文提出C-SHAP方法，用于基于概念解释时间序列AI模型，提供高级模式解释。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法仅解释低级模式，忽略高水平模式的影响，因此需要概念-based方法提升AI可靠性。

Method: 提出C-SHAP方法，并使用时间序列分解作为示例实现。

Result: 通过能源领域用例证明方法有效性。

Conclusion: C-SHAP提升了时间序列AI模型的可解释性，捕捉高水平模式。

Abstract: Time series are ubiquitous in domains such as energy forecasting, healthcare,
and industry. Using AI systems, some tasks within these domains can be
efficiently handled. Explainable AI (XAI) aims to increase the reliability of
AI solutions by explaining model reasoning. For time series, many XAI methods
provide point- or sequence-based attribution maps. These methods explain model
reasoning in terms of low-level patterns. However, they do not capture
high-level patterns that may also influence model reasoning. We propose a
concept-based method to provide explanations in terms of these high-level
patterns. In this paper, we present C-SHAP for time series, an approach which
determines the contribution of concepts to a model outcome. We provide a
general definition of C-SHAP and present an example implementation using time
series decomposition. Additionally, we demonstrate the effectiveness of the
methodology through a use case from the energy domain.

</details>

### [9] [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
*Anna Sofia Lippolis,Andrea Giovanni Nuzzolese,Aldo Gangemi*

Main category: cs.AI

TLDR: 这篇论文提出了一种逻辑增强生成框架，使用语义知识图来提升大型语言模型在隐喻检测和理解中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型难以从自然语言中提取隐含知识，因此使用语义知识图作为概念空间来提高推理效率和可解释性。

Method: 应用逻辑增强生成框架，结合语义知识图和提示启发式，生成隐含的类比连接，并用于隐喻检测任务。

Result: 方法超越基线，在视觉隐喻理解上优于人类，但对领域特定隐喻有局限，并进行了错误分析。

Conclusion: 该方法提高了推理的可解释性，但仍存在挑战，建议改进隐喻注释和评估方法。

Abstract: Recent advances in Large Language Models have demonstrated their capabilities
across a variety of tasks. However, automatically extracting implicit knowledge
from natural language remains a significant challenge, as machines lack active
experience with the physical world. Given this scenario, semantic knowledge
graphs can serve as conceptual spaces that guide the automated text generation
reasoning process to achieve more efficient and explainable results. In this
paper, we apply a logic-augmented generation (LAG) framework that leverages the
explicit representation of a text through a semantic knowledge graph and
applies it in combination with prompt heuristics to elicit implicit analogical
connections. This method generates extended knowledge graph triples
representing implicit meaning, enabling systems to reason on unlabeled
multimodal data regardless of the domain. We validate our work through three
metaphor detection and understanding tasks across four datasets, as they
require deep analogical reasoning capabilities. The results show that this
integrated approach surpasses current baselines, performs better than humans in
understanding visual metaphors, and enables more explainable reasoning
processes, though still has inherent limitations in metaphor understanding,
especially for domain-specific metaphors. Furthermore, we propose a thorough
error analysis, discussing issues with metaphorical annotations and current
evaluation methods.

</details>

### [10] [Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs](https://arxiv.org/abs/2504.11200)
*Irene Celino,Mario Scrocca,Agnese Chiatti*

Main category: cs.AI

TLDR: 这篇论文探讨了神经符号人工智能（NeSy AI）如何通过结合符号知识和数据驱动学习来提升人类与系统之间的相互理解，涵盖知识分享、交换和治理三个维度，并通过案例分析识别挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是强调NeSy AI在促进人类与系统相互理解中的重要性，通过定义三个关键维度来表征和改进这一过程。

Method: 方法包括引入知识分享、交换和治理三个维度，呈现NeSy AI和知识图谱的应用案例，并分析这些方法在不同维度的覆盖情况。

Result: 结果突出了NeSy AI的潜力与挑战，并识别了未来研究中相互理解的空白和不足。

Conclusion: 结论是，这种分析有助于指导未来研究，解决当前方法在相互理解方面的局限性。

Abstract: This chapter investigates the concept of mutual understanding between humans
and systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)
methods can significantly enhance this mutual understanding by leveraging
explicit symbolic knowledge representations with data-driven learning models.
We start by introducing three critical dimensions to characterize mutual
understanding: sharing knowledge, exchanging knowledge, and governing
knowledge. Sharing knowledge involves aligning the conceptual models of
different agents to enable a shared understanding of the domain of interest.
Exchanging knowledge relates to ensuring the effective and accurate
communication between agents. Governing knowledge concerns establishing rules
and processes to regulate the interaction between agents. Then, we present
several different use case scenarios that demonstrate the application of NeSy
AI and Knowledge Graphs to aid meaningful exchanges between human, artificial,
and robotic agents. These scenarios highlight both the potential and the
challenges of combining top-down symbolic reasoning with bottom-up neural
learning, guiding the discussion of the coverage provided by current solutions
along the dimensions of sharing, exchanging, and governing knowledge.
Concurrently, this analysis facilitates the identification of gaps and less
developed aspects in mutual understanding to address in future research.

</details>

### [11] [Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs](https://arxiv.org/abs/2504.11239)
*Chang Yang,Ruiyu Wang,Junzhe Jiang,Qi Jiang,Qinggang Zhang,Yanchen Deng,Shuxin Li,Shuyue Hu,Bo Li,Florian T. Pokorny,Xiao Huang,Xinrun Wang*

Main category: cs.AI

TLDR: 本文提出NPPC，一个针对大语言模型的永续扩展推理基准测试，专注于NP完全问题，旨在不可击溃、不可黑客攻击、自动可验证和通用。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试容易被快速击溃和黑客攻击，因此需要构建一个永续扩展、不可击溃的基准测试。

Method: 提出NPPC框架，包括npgym（生成NP完全问题实例）、npsolver（评估LLMs性能）和npeval（分析性能指标）三个模块。

Result: 实验显示NPPC将高级LLMs性能降至10%以下，DeepSeek-R1表现最佳，并观察到令牌数和顿悟时刻随难度变化的模式。

Conclusion: NPPC是首个永续扩展推理基准测试，可作为AGI发展的不可击溃测试平台。

Abstract: Reasoning is the fundamental capability of large language models (LLMs). Due
to the rapid progress of LLMs, there are two main issues of current benchmarks:
i) these benchmarks can be crushed in a short time (less than 1 year), and ii)
these benchmarks may be easily hacked. To handle these issues, we propose the
ever-scalingness for building the benchmarks which are uncrushable, unhackable,
auto-verifiable and general. This paper presents Nondeterministic
Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark
for LLMs. Specifically, the NPPC has three main modules: i) npgym, which
provides a unified interface of 25 well-known NP-complete problems and can
generate any number of instances with any levels of complexities, ii) npsolver:
which provides a unified interface to evaluate the problem instances with both
online and offline models via APIs and local deployments, respectively, and
iii) npeval: which provides the comprehensive and ready-to-use tools to analyze
the performances of LLMs over different problems, the number of tokens, the aha
moments, the reasoning errors and the solution errors. Extensive experiments
over widely-used LLMs demonstrate: i) NPPC can successfully decrease the
performances of advanced LLMs' performances to below 10%, demonstrating that
NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the
most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and
o1/o3-mini in most NP-complete problems considered, and iii) the numbers of
tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and
DeepSeek-R1, are observed first to increase and then decrease when the problem
instances become more and more difficult. We believe that NPPC is the first
ever-scaling reasoning benchmark, serving as the uncrushable and unhackable
testbed for LLMs toward artificial general intelligence (AGI).

</details>

### [12] [Towards Automated Safety Requirements Derivation Using Agent-based RAG](https://arxiv.org/abs/2504.11243)
*Balahari Vignesh Balu,Florian Geissler,Francesco Carella,Joao-Vitor Zacchi,Josef Jiru,Nuria Mata,Reinhard Stolle*

Main category: cs.AI

TLDR: 本文提出使用代理-based RAG结合LLM自动推导自驾车安全要求，解决了传统方法在复杂查询中的相关性问题，并在Apollo案例中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统LLM缺乏领域知识，现有RAG在处理安全相关复杂查询时性能下降，本文旨在通过代理-based RAG提升信息检索相关性。

Method: 提出代理-based RAG方法，应用于汽车标准和Apollo案例，使用从Apollo数据提取的安全要求问题集进行实现。

Result: 通过RAG指标评估，代理-based 方法比默认RAG检索信息更相关，并讨论了其优势。

Conclusion: 代理-based RAG在自驾车安全要求推导中更有效，提供更好的性能和相关性。

Abstract: We study the automated derivation of safety requirements in a self-driving
vehicle use case, leveraging LLMs in combination with agent-based
retrieval-augmented generation. Conventional approaches that utilise
pre-trained LLMs to assist in safety analyses typically lack domain-specific
knowledge. Existing RAG approaches address this issue, yet their performance
deteriorates when handling complex queries and it becomes increasingly harder
to retrieve the most relevant information. This is particularly relevant for
safety-relevant applications. In this paper, we propose the use of agent-based
RAG to derive safety requirements and show that the retrieved information is
more relevant to the queries. We implement an agent-based approach on a
document pool of automotive standards and the Apollo case study, as a
representative example of an automated driving perception system. Our solution
is tested on a data set of safety requirement questions and answers, extracted
from the Apollo data. Evaluating a set of selected RAG metrics, we present and
discuss advantages of a agent-based approach compared to default RAG methods.

</details>

### [13] [Learning to Be A Doctor: Searching for Effective Medical Agent Architectures](https://arxiv.org/abs/2504.11301)
*Yangyang Zhuang,Wenjia Jiang,Jiayu Zhang,Ze Yang,Joey Tianyi Zhou,Chi Zhang*

Main category: cs.AI

TLDR: 这篇论文提出了一种自动设计医疗代理架构的框架，使用大型语言模型（LLM），通过分层搜索空间实现动态适应和诊断准确性提升。


<details>
  <summary>Details</summary>
Motivation: 现有医疗代理系统依赖静态工作流，缺乏灵活性；受AutoML成功启发，需要自动设计以适应多样化诊断需求。

Method: 定义分层代理搜索空间，将医疗代理视为基于图的架构，支持节点、结构和框架级修改，以及诊断反馈引导的迭代自改进。

Result: 在皮肤病诊断任务的实验中，方法有效演化工作流结构，并显著提高诊断准确性。

Conclusion: 这是第一个完全自动化的医疗代理架构设计框架，为在真实临床环境中部署智能代理提供可扩展的基础。

Abstract: Large Language Model (LLM)-based agents have demonstrated strong capabilities
across a wide range of tasks, and their application in the medical domain holds
particular promise due to the demand for high generalizability and reliance on
interdisciplinary knowledge. However, existing medical agent systems often rely
on static, manually crafted workflows that lack the flexibility to accommodate
diverse diagnostic requirements and adapt to emerging clinical scenarios.
Motivated by the success of automated machine learning (AutoML), this paper
introduces a novel framework for the automated design of medical agent
architectures. Specifically, we define a hierarchical and expressive agent
search space that enables dynamic workflow adaptation through structured
modifications at the node, structural, and framework levels. Our framework
conceptualizes medical agents as graph-based architectures composed of diverse,
functional node types and supports iterative self-improvement guided by
diagnostic feedback. Experimental results on skin disease diagnosis tasks
demonstrate that the proposed method effectively evolves workflow structures
and significantly enhances diagnostic accuracy over time. This work represents
the first fully automated framework for medical agent architecture design and
offers a scalable, adaptable foundation for deploying intelligent agents in
real-world clinical environments.

</details>

### [14] [Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.11354)
*Haiming Wang,Mert Unsal,Xiaohan Lin,Mantas Baksys,Junqi Liu,Marco Dos Santos,Flood Sung,Marina Vinyes,Zhenzhe Ying,Zekai Zhu,Jianqiao Lu,Hugues de Saxcé,Bolton Bailey,Chendong Song,Chenjun Xiao,Dehao Zhang,Ebony Zhang,Frederick Pu,Han Zhu,Jiawei Liu,Jonas Bayer,Julien Michel,Longhui Yu,Léo Dreyfus-Schmidt,Lewis Tunstall,Luigi Pagani,Moreira Machado,Pauline Bourigault,Ran Wang,Stanislas Polu,Thibaut Barroyer,Wen-Ding Li,Yazhe Niu,Yann Fleureau,Yangyang Hu,Zhouliang Yu,Zihan Wang,Zhilin Yang,Zhengying Liu,Jia Li*

Main category: cs.AI

TLDR: Kimina-Prover 是一个新型推理驱动的正式定理证明模型，使用强化学习训练，在 Lean 4 证明生成中达到 miniF2F 基准 80.7% pass@8192 的新纪录，并开源精简版本。


<details>
  <summary>Details</summary>
Motivation: 创新推理范式，模拟人类问题解决策略，桥接正式验证与非正式数学直觉的差距。

Method: 采用大规模强化学习管道从 Qwen2.5-72B 训练，使用 '正式推理模式' 迭代生成和完善证明步骤。

Result: 在 miniF2F 基准上达到 80.7% pass@8192 的新状态-of-the-art，高样本效率、模型大小性能 scaling，以及独特推理风格。

Conclusion: 展示了样本效率、性能 scaling 的优势，并有潜力弥合正式验证与数学直觉的差距；开源 1.5B 和 7B 参数版本。

Abstract: We introduce Kimina-Prover Preview, a large language model that pioneers a
novel reasoning-driven exploration paradigm for formal theorem proving, as
showcased in this preview release. Trained with a large-scale reinforcement
learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong
performance in Lean 4 proof generation by employing a structured reasoning
pattern we term \textit{formal reasoning pattern}. This approach allows the
model to emulate human problem-solving strategies in Lean, iteratively
generating and refining proof steps. Kimina-Prover sets a new state-of-the-art
on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved
benchmark performance, our work yields several key insights: (1) Kimina-Prover
exhibits high sample efficiency, delivering strong results even with minimal
sampling (pass@1) and scaling effectively with computational budget, stemming
from its unique reasoning pattern and RL training; (2) we demonstrate clear
performance scaling with model size, a trend previously unobserved for neural
theorem provers in formal mathematics; (3) the learned reasoning style,
distinct from traditional search algorithms, shows potential to bridge the gap
between formal verification and informal mathematical intuition. We open source
distilled versions with 1.5B and 7B parameters of Kimina-Prover

</details>

### [15] [Embodied World Models Emerge from Navigational Task in Open-Ended Environments](https://arxiv.org/abs/2504.11419)
*Li Jin,Liu Jia*

Main category: cs.AI

TLDR: 本研究通过元强化学习和神经网络，展示了AI代理如何通过主动互动学习空间意识，并通过因果验证确认神经表示的有效性。


<details>
  <summary>Details</summary>
Motivation: AI研究中，空间意识和推理发展长期面临挑战，传统模型依赖被动观察，而具身认知理论强调主动互动的重要性。

Method: 使用门控循环单元(GRU)结合元强化学习(Meta-RL)，引入混合动力系统(HDS)建模交互，脊表示映射路径，规范相关分析(CCA)确认对齐，并进行干预实验。

Result: 代理学会编码方向、距离和障碍避免，显示稳定极限环，CCA确认神经状态与行为空间强对齐，干预实验证实特定神经维度与导航性能的因果联系。

Conclusion: 桥接行动与感知的鸿沟，提供构建适应性、可解释AI模型的洞见，并为理解和控制AI内部机制开辟新途径。

Abstract: Understanding how artificial systems can develop spatial awareness and
reasoning has long been a challenge in AI research. Traditional models often
rely on passive observation, but embodied cognition theory suggests that deeper
understanding emerges from active interaction with the environment. This study
investigates whether neural networks can autonomously internalize spatial
concepts through interaction, focusing on planar navigation tasks. Using Gated
Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we
show that agents can learn to encode spatial properties like direction,
distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)
to model the agent-environment interaction as a closed dynamical system,
revealing stable limit cycles that correspond to optimal navigation strategies.
Ridge Representation allows us to map navigation paths into a fixed-dimensional
behavioral space, enabling comparison with neural states. Canonical Correlation
Analysis (CCA) confirms strong alignment between these representations,
suggesting that the agent's neural states actively encode spatial knowledge.
Intervention experiments further show that specific neural dimensions are
causally linked to navigation performance. This work provides an approach to
bridging the gap between action and perception in AI, offering new insights
into building adaptive, interpretable models that can generalize across complex
environments. The causal validation of neural representations also opens new
avenues for understanding and controlling the internal mechanisms of AI
systems, pushing the boundaries of how machines learn and reason in dynamic,
real-world scenarios.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA](https://arxiv.org/abs/2504.10490)
*Gabriel Bo,Marc Bernardino,Justin Gu*

Main category: cs.LG

TLDR: 本研究尝试将KAN和图表示整合到GPT-2中提升多任务学习准确性，但发现LoRA优化方法更有效。


<details>
  <summary>Details</summary>
Motivation: 受KAN和GAT在CoT模型应用增加以及与MLP比较的争议启发，旨在提高多任务学习准确性。

Method: 使用LoRA增强transformer，进行超参数微调和L2正则化；开发Graph LoRA和Hybrid-KAN LoRA变体，并进行系统评估。

Result: LoRA增强transformer在SST测试集准确率55.249%，CFIMDB dev集99.18%，释义检测89.9%，十四行诗生成CHRF分数42.097；变体未优于基准。

Conclusion: 高效参数适配通过LoRA是针对情感分析、释义检测和十四行诗生成的最有效策略。

Abstract: We explore the potential of integrating learnable and interpretable
modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based
representations--within a pre-trained GPT-2 model to enhance multi-task
learning accuracy. Motivated by the recent surge in using KAN and graph
attention (GAT) architectures in chain-of-thought (CoT) models and debates over
their benefits compared to simpler architectures like MLPs, we begin by
enhancing a standard self-attention transformer using Low-Rank Adaptation
(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This
approach yields significant improvements. To further boost interpretability and
richer representations, we develop two variants that attempt to improve the
standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,
systematic evaluations reveal that neither variant outperforms the optimized
LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,
99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On
sonnet generation, we get a CHRF score of 42.097. These findings highlight that
efficient parameter adaptation via LoRA remains the most effective strategy for
our tasks: sentiment analysis, paraphrase detection, and sonnet generation.

</details>

### [17] [Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP](https://arxiv.org/abs/2504.10536)
*Lihong Zhang,Yue Li*

Main category: cs.LG

TLDR: 本论文提出层跳跃联邦学习方法，应用于医疗NLP，减少通信开销70%，性能接近集中式训练。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在医疗NLP中的隐私问题，以及通信开销和数据异质性挑战。

Method: 提出层跳跃联邦学习，仅微调预训练LLM（如LLaMA 3.2-1B）的选定层，其他层冻结。

Result: 通信成本减少约70%，性能与集中式训练相差不到2%，优于基线，处理非IID数据并与差分隐私兼容。

Conclusion: 这种方法为医疗NLP隐私保护协作学习提供实用解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
organizations without sharing raw data, addressing crucial privacy concerns in
healthcare natural language processing (NLP). However, training large language
models (LLMs) in federated settings faces significant challenges, including
communication overhead and data heterogeneity. We propose Layer-Skipping
Federated Learning, where only selected layers of a pre-trained LLM are
fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,
our approach reduces communication costs by approximately 70% while maintaining
performance within 2% of centralized training. We evaluate our method on
clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our
experiments demonstrate that Layer-Skipping FL outperforms competitive
baselines, handles non-IID clinical data distributions effectively, and shows
robustness when combined with differential privacy. This approach represents a
practical solution for privacy-preserving collaborative learning in healthcare
NLP.

</details>

### [18] [MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers](https://arxiv.org/abs/2504.10551)
*Lili Zhao,Qi Liu,Wei Chen,Liyi Chen,Ruijun Sun,Min Hou,Yang Wang,Shijin Wang*

Main category: cs.LG

TLDR: 本论文提出MiMu方法，缓解Empirical Risk Minimization (ERM)模型的多重捷径学习问题，提高鲁棒性泛化性能。


<details>
  <summary>Details</summary>
Motivation: ERM模型依赖虚假相关性导致捷径学习，现有方法仅针对单一捷径，而现实中捷径多样且未知，模型对强捷径过度依赖影响泛化。

Method: 提出MiMu方法，包括自校准策略（防止依赖捷径）和自提升策略（随机掩码及自适应注意力对齐）。

Result: 实验在NLP和CV领域显示MiMu有效提升了模型的鲁棒性泛化能力。

Conclusion: MiMu通过减轻多重捷径学习行为，显著改善了模型的泛化性能。

Abstract: Empirical Risk Minimization (ERM) models often rely on spurious correlations
between features and labels during the learning process, leading to shortcut
learning behavior that undermines robustness generalization performance.
Current research mainly targets identifying or mitigating a single shortcut;
however, in real-world scenarios, cues within the data are diverse and unknown.
In empirical studies, we reveal that the models rely to varying extents on
different shortcuts. Compared to weak shortcuts, models depend more heavily on
strong shortcuts, resulting in their poor generalization ability. To address
these challenges, we propose MiMu, a novel method integrated with
Transformer-based ERMs designed to Mitigate Multiple shortcut learning
behavior, which incorporates self-calibration strategy and self-improvement
strategy. In the source model, we preliminarily propose the self-calibration
strategy to prevent the model from relying on shortcuts and make overconfident
predictions. Then, we further design self-improvement strategy in target model
to reduce the reliance on multiple shortcuts. The random mask strategy involves
randomly masking partial attention positions to diversify the focus of target
model other than concentrating on a fixed region. Meanwhile, the adaptive
attention alignment module facilitates the alignment of attention weights to
the calibrated source model, without the need for post-hoc attention maps or
supervision. Finally, extensive experiments conducted on Natural Language
Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu
in improving robustness generalization abilities.

</details>

### [19] [LEMUR Neural Network Dataset: Towards Seamless AutoML](https://arxiv.org/abs/2504.10552)
*Arash Torabi Goodarzi,Roman Kochnev,Waleed Khalid,Furui Qin,Tolgay Atinc Uzun,Yashkumar Sanjaybhai Dhameliya,Yash Kanubhai Kathiriya,Zofia Antonina Bentyn,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TLDR: LEMUR 是一个开源数据集，包含各种神经网络模型代码，用于 AutoML 和模型分析，提供结构化表示和性能数据。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集对神经网络发展至关重要，需支持基准测试、AutoML 和模型分析。

Method: 引入 LEMUR，使用 Python 和 PyTorch 构建，集成 Optuna 框架进行评估、优化和统计分析。

Result: 启用 LLM 微调、提供评估工具、API 和边设备部署，支持模型开发和性能统计。

Conclusion: LEMUR 将开源发布，帮助研究者和从业者高效开发、测试和分析神经网络模型。

Abstract: Neural networks are fundamental in artificial intelligence, driving progress
in computer vision and natural language processing. High-quality datasets are
crucial for their development, and there is growing interest in datasets
composed of neural networks themselves to support benchmarking, automated
machine learning (AutoML), and model analysis. We introduce LEMUR, an open
source dataset of neural network models with well-structured code for diverse
architectures across tasks such as object detection, image classification,
segmentation, and natural language processing. LEMUR is primarily designed to
enable fine-tuning of large language models (LLMs) for AutoML tasks, providing
a rich source of structured model representations and associated performance
data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new
datasets and models while maintaining consistency. It integrates an
Optuna-powered framework for evaluation, hyperparameter optimization,
statistical analysis, and graphical insights. LEMUR provides an extension that
enables models to run efficiently on edge devices, facilitating deployment in
resource-constrained environments. Providing tools for model evaluation,
preprocessing, and database management, LEMUR supports researchers and
practitioners in developing, testing, and analyzing neural networks.
Additionally, it offers an API that delivers comprehensive information about
neural network models and their complete performance statistics with a single
request, which can be used in experiments with code-generating large language
models. The LEMUR will be released as an open source project under the MIT
license upon acceptance of the paper.

</details>

### [20] [Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains](https://arxiv.org/abs/2504.10555)
*Marco Salmè,Lorenzo Tronchin,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.LG

TLDR: 本研究探讨深度生成模型在数据稀缺环境下的应用，扩展生成学习三难困境并评估VAE、GAN和DM的性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺阻碍了医学和精准农业等领域的技术进步，需要合成数据解决生成学习三难困境的限制。

Method: 评估VAE、GAN和DM在保真度、多样性和采样效率下的性能，并提出框架评估合成数据的实用性、鲁棒性和隐私性。

Result: 不同DGM模型显示出应用上下文相关的独特优势。

Conclusion: 扩展生成学习三难困境，提供选择DGM模型的指导，以适应真实世界需求。

Abstract: Data scarcity remains a critical bottleneck impeding technological
advancements across various domains, including but not limited to medicine and
precision agriculture. To address this challenge, we explore the potential of
Deep Generative Models (DGMs) in producing synthetic data that satisfies the
Generative Learning Trilemma: fidelity, diversity, and sampling efficiency.
However, recognizing that these criteria alone are insufficient for practical
applications, we extend the trilemma to include utility, robustness, and
privacy, factors crucial for ensuring the applicability of DGMs in real-world
scenarios. Evaluating these metrics becomes particularly challenging in
data-scarce environments, as DGMs traditionally rely on large datasets to
perform optimally. This limitation is especially pronounced in domains like
medicine and precision agriculture, where ensuring acceptable model performance
under data constraints is vital. To address these challenges, we assess the
Generative Learning Trilemma in data-scarcity settings using state-of-the-art
evaluation metrics, comparing three prominent DGMs: Variational Autoencoders
(VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs).
Furthermore, we propose a comprehensive framework to assess utility,
robustness, and privacy in synthetic data generated by DGMs. Our findings
demonstrate varying strengths among DGMs, with each model exhibiting unique
advantages based on the application context. This study broadens the scope of
the Generative Learning Trilemma, aligning it with real-world demands and
providing actionable guidance for selecting DGMs tailored to specific
applications.

</details>

### [21] [VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification](https://arxiv.org/abs/2504.10556)
*Lucas Heublein,Simon Kocher,Tobias Feigl,Alexander Rügamer,Christopher Mutschler,Felix Ott*

Main category: cs.LG

TLDR: 本论文使用变分自编码器（VAE）实现GNSS干扰分类的模型压缩和数据增强，达到高达99.92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 分布式学习和Edge AI需要高效数据处理、低延迟通信、去中心化训练和数据隐私；在GNSS应用中，需准确监控和分类干扰以提升 situational awareness，挑战是压缩ML模型同时保持高准确性。

Method: 提出VAE的disentanglement方法提取latent features，用于干扰分类；包括vanilla、factorized和conditional generative三种变体；用于数据压缩和增强，通过latent表示插值；评估四个数据集，并进行超参数优化。

Result: 数据压缩率从512到8192；准确率高达99.92%。

Conclusion: VAE方法有效实现了高压缩率和高准确率的干扰分类，证明了在分布式环境中的可行性。

Abstract: Distributed learning and Edge AI necessitate efficient data processing,
low-latency communication, decentralized model training, and stringent data
privacy to facilitate real-time intelligence on edge devices while reducing
dependency on centralized infrastructure and ensuring high model performance.
In the context of global navigation satellite system (GNSS) applications, the
primary objective is to accurately monitor and classify interferences that
degrade system performance in distributed environments, thereby enhancing
situational awareness. To achieve this, machine learning (ML) models can be
deployed on low-resource devices, ensuring minimal communication latency and
preserving data privacy. The key challenge is to compress ML models while
maintaining high classification accuracy. In this paper, we propose variational
autoencoders (VAEs) for disentanglement to extract essential latent features
that enable accurate classification of interferences. We demonstrate that the
disentanglement approach can be leveraged for both data compression and data
augmentation by interpolating the lower-dimensional latent representations of
signal power. To validate our approach, we evaluate three VAE variants -
vanilla, factorized, and conditional generative - on four distinct datasets,
including two collected in controlled indoor environments and two real-world
highway datasets. Additionally, we conduct extensive hyperparameter searches to
optimize performance. Our proposed VAE achieves a data compression rate ranging
from 512 to 8,192 and achieves an accuracy up to 99.92%.

</details>

### [22] [Efficient Process Reward Model Training via Active Learning](https://arxiv.org/abs/2504.10559)
*Keyu Duan,Zichen Liu,Xin Mao,Tianyu Pang,Changyu Chen,Qiguang Chen,Michael Qizhe Shieh,Longxu Dou*

Main category: cs.LG

TLDR: ActPRM 是一种主动学习方法，通过减少 50% 的标注成本，同时保持或提高性能，并在特定基准上达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 训练数据标注的规模化对人类和 LLM 都具有挑战性，因此提出 ActPRM 来解决这一限制。

Method: 提出主动学习方法 ActPRM，选择不确定性最高的样本，使用 PRM 估计不确定性，由高成本的推理模型标注数据，然后计算损失并更新 PRM 的权重。

Result: 减少 50% 的标注成本，同时达到相当或更好的性能；通过过滤超过 100 万的数学推理轨迹，保留 60% 的数据，在 ProcessBench (75.0%) 和 PRMBench (65.5%) 上达到新的 SOTA。

Conclusion: ActPRM 在减少标注成本和提高性能方面表现出色，并实现了新的状态-of-the-art 结果。

Abstract: Process Reward Models (PRMs) provide step-level supervision to large language
models (LLMs), but scaling up training data annotation remains challenging for
both humans and LLMs. To address this limitation, we propose an active learning
approach, ActPRM, which proactively selects the most uncertain samples for
training, substantially reducing labeling costs. During training, we use the
PRM to estimate uncertainty after the forward pass, retaining only highly
uncertain data. A capable yet costly reasoning model then labels this data.
Then we compute the loss with respect to the labels and update the PRM's
weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active
learning setting, demonstrating that ActPRM reduces 50% annotation, but
achieving the comparable or even better performance. Beyond annotation
efficiency, we further advance the actively trained PRM by filtering over 1M+
math reasoning trajectories with ActPRM, retaining 60% of the data. A
subsequent training on this selected dataset yields a new state-of-the-art
(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same
sized models.

</details>

### [23] [Self-Controlled Dynamic Expansion Model for Continual Learning](https://arxiv.org/abs/2504.10561)
*Runqing Wu,Fei Ye,Rongyao Hu,Guoxi Huang*

Main category: cs.LG

TLDR: 本文提出自控动态扩展模型（SCDEM）来提升持续学习性能，通过多骨干网动态适应新任务，并引入优化机制实现知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法使用静态Vision Transformer骨干网，无法有效适应新任务和数据域，导致参数利用不足。

Method: 引入SCDEM模型，使用多ViT骨干网动态生成新专家，结合协作优化机制（COM）、特征分布一致性（FDC）及动态层级特征注意力机制（DLWFAM）来优化学习过程。

Result: 实验结果显示，该方法在各种基准上达到了最先进性能。

Conclusion: 该方法有效地解决了持续学习中的适应性和知识保留问题，并通过实证验证了其优越性。

Abstract: Continual Learning (CL) epitomizes an advanced training paradigm wherein
prior data samples remain inaccessible during the acquisition of new tasks.
Numerous investigations have delved into leveraging a pre-trained Vision
Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless,
these approaches typically utilize a singular, static backbone, which
inadequately adapts to novel tasks, particularly when engaging with diverse
data domains, due to a substantial number of inactive parameters. This paper
addresses this limitation by introducing an innovative Self-Controlled Dynamic
Expansion Model (SCDEM), which orchestrates multiple distinct trainable
pre-trained ViT backbones to furnish diverse and semantically enriched
representations. Specifically, by employing the multi-backbone architecture as
a shared module, the proposed SCDEM dynamically generates a new expert with
minimal parameters to accommodate a new task. A novel Collaborative
Optimization Mechanism (COM) is introduced to synergistically optimize multiple
backbones by harnessing prediction signals from historical experts, thereby
facilitating new task learning without erasing previously acquired knowledge.
Additionally, a novel Feature Distribution Consistency (FDC) approach is
proposed to align semantic similarity between previously and currently learned
representations through an optimal transport distance-based mechanism,
effectively mitigating negative knowledge transfer effects. Furthermore, to
alleviate over-regularization challenges, this paper presents a novel Dynamic
Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the
penalization intensity on each trainable representation layer. An extensive
series of experiments have been conducted to evaluate the proposed
methodology's efficacy, with empirical results corroborating that the approach
attains state-of-the-art performance.

</details>

### [24] [Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling](https://arxiv.org/abs/2504.10612)
*Michal Balcerak,Tamaz Amiranashvili,Suprosanna Shit,Antonio Terpin,Sebastian Kaltenbach,Petros Koumoutsakos,Bjoern Menze*

Main category: cs.LG

TLDR: 本文提出Energy Matching框架，统一流方法和能量模型，提高生成性能，在CIFAR-10上FID达3.97。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在处理部分观察或额外先验时复杂，受到Wasserstein梯度流进展启发，旨在解决这些局限性。

Method: 提出Energy Matching框架，使用单一时间无关标量场，结合流方法和能量模型灵活性，无需时间条件或额外网络。

Result: 在CIFAR-10生成上显著优于现有EBM（FID 3.97 vs 8.61），保留无模拟训练，并引入交互能量支持多样模式探索。

Conclusion: 该方法标志着EBM的重大创新，简化框架，提升能力，促进在各种领域的广泛应用。

Abstract: Generative models often map noise to data by matching flows or scores, but
these approaches become cumbersome for incorporating partial observations or
additional priors. Inspired by recent advances in Wasserstein gradient flows,
we propose Energy Matching, a framework that unifies flow-based approaches with
the flexibility of energy-based models (EBMs). Far from the data manifold,
samples move along curl-free, optimal transport paths from noise to data. As
they approach the data manifold, an entropic energy term guides the system into
a Boltzmann equilibrium distribution, explicitly capturing the underlying
likelihood structure of the data. We parameterize this dynamic with a single
time-independent scalar field, which serves as both a powerful generator and a
flexible prior for effective regularization of inverse problems. Our method
substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97
compared to 8.61), while retaining the simulation-free training of
transport-based approaches away from the data manifold. Additionally, we
exploit the flexibility of our method and introduce an interaction energy for
diverse mode exploration. Our approach focuses on learning a static scalar
potential energy -- without time conditioning, auxiliary generators, or
additional networks -- marking a significant departure from recent EBM methods.
We believe this simplified framework significantly advances EBM capabilities
and paves the way for their broader adoption in generative modeling across
diverse domains.

</details>

### [25] [Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning](https://arxiv.org/abs/2504.10677)
*Muhammad Al-Zafar Khan,Jamal Al-Karaki*

Main category: cs.LG

TLDR: 这篇论文提出了一种多智能体强化学习框架，用于优化使用工程生物代理的组织修复过程。


<details>
  <summary>Details</summary>
Motivation: 为了优化组织修复过程，提高生物医学应用中的效率和效果。

Method: 整合了随机反应-扩散系统、神经样电化学通信（包括Hebbian可塑性）、生物学信息奖励函数（结合化学梯度跟踪、神经同步和鲁棒惩罚）、以及课程学习方案。

Result: 通过in silico实验，展示了紧急修复策略，包括动态分泌控制和空间协调。

Conclusion: 该框架证明了在组织修复中的有效性，突显了紧急行为和优化潜力。

Abstract: In this paper, we present a multi-agent reinforcement learning (MARL)
framework for optimizing tissue repair processes using engineered biological
agents. Our approach integrates: (1) stochastic reaction-diffusion systems
modeling molecular signaling, (2) neural-like electrochemical communication
with Hebbian plasticity, and (3) a biologically informed reward function
combining chemical gradient tracking, neural synchronization, and robust
penalties. A curriculum learning scheme guides the agent through progressively
complex repair scenarios. In silico experiments demonstrate emergent repair
strategies, including dynamic secretion control and spatial coordination.

</details>

### [26] [The Jailbreak Tax: How Useful are Your Jailbreak Outputs?](https://arxiv.org/abs/2504.10694)
*Kristina Nikolić,Luze Sun,Jie Zhang,Florian Tramèr*

Main category: cs.LG

TLDR: 这篇论文评估了越狱攻击对大语言模型的效用，引入了'越狱税'概念，强调攻击成功可能导致输出质量下降。


<details>
  <summary>Details</summary>
Motivation: 质疑现有越狱攻击的输出是否真正有用，因为它们可能产生有害但低效用的响应。

Method: 构建新评估集，通过让模型拒绝良性主题（如生物或数学）的问题，并评估八种代表性越狱攻击在五个效用基准上的表现。

Result: 发现越狱响应中模型效用一致下降，称为越狱税，例如数学准确率下降高达92%。

Conclusion: 提出越狱税作为AI安全新指标，并提供基准，发布在https://github.com/ethz-spylab/jailbreak-tax。

Abstract: Jailbreak attacks bypass the guardrails of large language models to produce
harmful outputs. In this paper, we ask whether the model outputs produced by
existing jailbreaks are actually useful. For example, when jailbreaking a model
to give instructions for building a bomb, does the jailbreak yield good
instructions? Since the utility of most unsafe answers (e.g., bomb
instructions) is hard to evaluate rigorously, we build new jailbreak evaluation
sets with known ground truth answers, by aligning models to refuse questions
related to benign and easy-to-evaluate topics (e.g., biology or math). Our
evaluation of eight representative jailbreaks across five utility benchmarks
reveals a consistent drop in model utility in jailbroken responses, which we
term the jailbreak tax. For example, while all jailbreaks we tested bypass
guardrails in models aligned to refuse to answer math, this comes at the
expense of a drop of up to 92% in accuracy. Overall, our work proposes the
jailbreak tax as a new important metric in AI safety, and introduces benchmarks
to evaluate existing and future jailbreaks. We make the benchmark available at
https://github.com/ethz-spylab/jailbreak-tax

</details>

### [27] [Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)](https://arxiv.org/abs/2504.10720)
*Kamaljyoti Nath,Khemraj Shukla,Victor C. Tsai,Umair bin Waheed,Christian Huber,Omer Alpak,Chuen-Song Chen,Ligang Lu,Amik St-Cyr*

Main category: cs.LG

TLDR: 本研究使用Deep Operator Networks (DeepONet)改进Full Waveform Inversion (FWI)，以提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统FWI计算需求高，且逆问题因数据有限可能非唯一，需要更高效方法。

Method: 提出DeepONet反演地震波形获取地下速度场，包括处理噪声数据、出分布预测，并作为传统FWI起始模型。

Result: DeepONet捕获关键特征，在噪声数据和某些速度模型中准确性优于其他机器学习方法，并加速FWI收敛。

Conclusion: 整合DeepONet可加速FWI过程，并提高其鲁棒性和可靠性。

Abstract: Full Waveform Inversion (FWI) is an important geophysical technique
considered in subsurface property prediction. It solves the inverse problem of
predicting high-resolution Earth interior models from seismic data. Traditional
FWI methods are computationally demanding. Inverse problems in geophysics often
face challenges of non-uniqueness due to limited data, as data are often
collected only on the surface. In this study, we introduce a novel methodology
that leverages Deep Operator Networks (DeepONet) to attempt to improve both the
efficiency and accuracy of FWI. The proposed DeepONet methodology inverts
seismic waveforms for the subsurface velocity field. This approach is able to
capture some key features of the subsurface velocity field. We have shown that
the architecture can be applied to noisy seismic data with an accuracy that is
better than some other machine learning methods. We also test our proposed
method with out-of-distribution prediction for different velocity models. The
proposed DeepONet shows comparable and better accuracy in some velocity models
than some other machine learning methods. To improve the FWI workflow, we
propose using the DeepONet output as a starting model for conventional FWI and
that it may improve FWI performance. While we have only shown that DeepONet
facilitates faster convergence than starting with a homogeneous velocity field,
it may have some benefits compared to other approaches to constructing starting
models. This integration of DeepONet into FWI may accelerate the inversion
process and may also enhance its robustness and reliability.

</details>

### [28] [Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization](https://arxiv.org/abs/2504.10735)
*Timur Carstensen,Neeratyoy Mallik,Frank Hutter,Martin Rapp*

Main category: cs.LG

TLDR: 本论文提出了一种新的多保真度超参数优化（MF-HPO）方法，使用训练或冻结层数作为保真度来源，以节省计算资源，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增长，高效的超参数优化方法变得至关重要，现有的保真度来源在低计算资源下表现不佳。

Method: 提出将训练或冻结层数作为保真度来源，通过冻结部分层减少计算和内存消耗，同时保持超参数的相关性。

Result: 在ResNets和Transformers上实证评估显示了良好相关性，并分析了与其他保真度来源结合的效用。

Conclusion: 这项工作为使用硬件资源作为保真度的MF-HPO打开新应用，并为改进联合保真度空间的算法提供了机会。

Abstract: As model sizes grow, finding efficient and cost-effective hyperparameter
optimization (HPO) methods becomes increasingly crucial for deep learning
pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources
required for DL training with lower fidelity estimations, existing fidelity
sources often fail under lower compute and memory constraints. We propose a
novel fidelity source: the number of layers that are trained or frozen during
training. For deep networks, this approach offers significant compute and
memory savings while preserving rank correlations between hyperparameters at
low fidelities compared to full model training. We demonstrate this in our
empirical evaluation across ResNets and Transformers and additionally analyze
the utility of frozen layers as a fidelity in using GPU resources as a fidelity
in HPO, and for a combined MF-HPO with other fidelity sources. This
contribution opens new applications for MF-HPO with hardware resources as a
fidelity and creates opportunities for improved algorithms navigating joint
fidelity spaces.

</details>

### [29] [Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity](https://arxiv.org/abs/2504.10752)
*Neil Mehta,Ines Goncalves,Alberto Montagna,Mathis Fleury,Gustavo Caetano,Ines Esteves,Athanasios Vourvopoulos,Pulkit Grover,Patricia Figueiredo*

Main category: cs.LG

TLDR: 这项研究证明EEG可以预测fMRI运动脑网络信号，在任务和自发条件下跨不同天有统计学意义，并有神经反馈应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨EEG和fMRI是否捕捉共享神经活动信息，特别是预测fMRI信号的程度。

Method: 使用Sparse Group Lasso正则化的可解释模型，针对个体受试者训练，并在不同天测试，与空模型和传统EEG传感器运动节律比较。

Result: 在大多数受试者中获得显著预测，任务条件下更频繁，解释了EEG通道、频率和血流动力学延迟。

Conclusion: 证明EEG可跨不同天预测fMRI活动，具有个体统计学意义，并为EEG神经反馈应用提供潜力。

Abstract: Simultaneous EEG-fMRI recordings are increasingly used to investigate brain
activity by leveraging the complementary high spatial and high temporal
resolution of fMRI and EEG signals respectively. It remains unclear, however,
to what degree these two imaging modalities capture shared information about
neural activity. Here, we investigate whether it is possible to predict both
task-evoked and spontaneous fMRI signals of motor brain networks from EEG
time-varying spectral power using interpretable models trained for individual
subjects with Sparse Group Lasso regularization. Critically, we test the
trained models on data acquired from each subject on a different day and obtain
statistical validation by comparison with appropriate null models as well as
the conventional EEG sensorimotor rhythm. We find significant prediction
results in most subjects, although less frequently for resting-state compared
to task-based conditions. Furthermore, we interpret the model learned
parameters to understand representations of EEG-fMRI coupling in terms of
predictive EEG channels, frequencies, and haemodynamic delays. In conclusion,
our work provides evidence of the ability to predict fMRI motor brain activity
from EEG recordings alone across different days, in both task-evoked and
spontaneous conditions, with statistical significance in individual subjects.
These results present great potential for translation to EEG neurofeedback
applications.

</details>

### [30] [auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory](https://arxiv.org/abs/2504.10754)
*Arjun Subramonian,Elvis Dohmatob*

Main category: cs.LG

TLDR: 本论文引入auto-fpt工具，使用自由概率理论自动计算高维随机矩阵期望迹，简化机器学习计算。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习理论中高维计算复杂，需要自动化工具简化符号计算。

Method: 开发基于Python和SymPy的auto-fpt工具，自动生成并求解固定点方程系统。

Result: 应用于神经网络高维误差分析，复现已知结果。

Conclusion: 希望auto-fpt简化高维分析，帮助机器学习社区复现和发现新现象。

Abstract: A large part of modern machine learning theory often involves computing the
high-dimensional expected trace of a rational expression of large rectangular
random matrices. To symbolically compute such quantities using free probability
theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that
can automatically produce a reduced system of fixed-point equations which can
be solved for the quantities of interest, and effectively constitutes a theory.
We overview the algorithmic ideas underlying auto-fpt and its applications to
various interesting problems, such as the high-dimensional error of linearized
feed-forward neural networks, recovering well-known results. We hope that
auto-fpt streamlines the majority of calculations involved in high-dimensional
analysis, while helping the machine learning community reproduce known and
uncover new phenomena.

</details>

### [31] [How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients](https://arxiv.org/abs/2504.10766)
*Ming Li,Yanhong Li,Ziyue Li,Tianyi Zhou*

Main category: cs.LG

TLDR: 本研究通过梯度谱分析统一数据质量指标，为LLM后训练提供新见解。


<details>
  <summary>Details</summary>
Motivation: 探索不同数据质量对LLM微调动态的影响，因为此领域研究不足。

Method: 使用层级梯度的奇异值分解(SVD)进行谱分析，评估指令和推理数据的质量。

Result: 高质量数据具有较低核范数和较高有效秩；有效秩更能捕捉微妙差异；同一模型家族梯度模式相似，不同家族差异显著。

Conclusion: 提供数据质量影响的统一视角，帮助开发更好的数据探索策略。

Abstract: As the post-training of large language models (LLMs) advances from
instruction-following to complex reasoning tasks, understanding how different
data affect finetuning dynamics remains largely unexplored. In this paper, we
present a spectral analysis of layer-wise gradients induced by low/high-quality
instruction and reasoning data for LLM post-training. Our analysis reveals that
widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and
Reward, can be explained and unified by spectral properties computed from
gradients' singular value decomposition (SVD). Specifically, higher-quality
data are usually associated with lower nuclear norms and higher effective
ranks. Notably, effective rank exhibits better robustness and resolution than
nuclear norm in capturing subtle quality differences. For example, reasoning
data achieves substantially higher effective ranks than instruction data,
implying richer gradient structures on more complex tasks. Our experiments also
highlight that models within the same family share similar gradient patterns
regardless of their sizes, whereas different model families diverge
significantly. Providing a unified view on the effects of data quality across
instruction and reasoning data, this work illuminates the interplay between
data quality and training stability, shedding novel insights into developing
better data exploration strategies for post-training.

</details>

### [32] [MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation](https://arxiv.org/abs/1908.08652)
*Abhay Kumar,Nishant Jain,Suraj Tripathi,Chirag Singh,Kamal Krishna*

Main category: cs.LG

TLDR: 提出了一种多任务学习网络MTCNet，用于人群密度和计数估计，通过辅助任务提升了主任务的性能。


<details>
  <summary>Details</summary>
Motivation: 人群计数估计面临非均匀尺度变化和任意视角的挑战，因此使用多任务学习来捕捉相关尺度信息。

Method: 模型包括主任务（使用VGG-16提取特征和扩张CNN生成密度图）和辅助任务（共享前端的CNN分类器）。

Result: 在ShanghaiTech数据集上，MAE比最先进方法低5.8%和14.9%（无数据增强），在UCF_CC_50数据集上低10.5%。

Conclusion: 证明了多任务学习方法在人群计数中的有效性，取得了更好的性能。

Abstract: We propose a Multi-Task Learning (MTL) paradigm based deep neural network
architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and
count estimation. Crowd count estimation is challenging due to the non-uniform
scale variations and the arbitrary perspective of an individual image. The
proposed model has two related tasks, with Crowd Density Estimation as the main
task and Crowd-Count Group Classification as the auxiliary task. The auxiliary
task helps in capturing the relevant scale-related information to improve the
performance of the main task. The main task model comprises two blocks: VGG-16
front-end for feature extraction and a dilated Convolutional Neural Network for
density map generation. The auxiliary task model shares the same front-end as
the main task, followed by a CNN classifier. Our proposed network achieves 5.8%
and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on
ShanghaiTech dataset without using any data augmentation. Our model also
outperforms with 10.5% lower MAE on UCF_CC_50 dataset.

</details>

### [33] [Collaborative Bayesian Optimization via Wasserstein Barycenters](https://arxiv.org/abs/2504.10770)
*Donglin Zhan,Haoting Zhang,Rhonda Righter,Zeyu Zheng,James Anderson*

Main category: cs.LG

TLDR: 本论文提出协作贝叶斯优化框架，解决黑箱优化和数据隐私问题，代理共享GP模型使用Wasserstein重心，证明有效性。


<details>
  <summary>Details</summary>
Motivation: 由于黑箱优化和数据隐私需求增长。

Method: 引入协作BO框架，代理共享GP模型不共享数据，构建Wasserstein重心中心模型和协作获取函数。

Result: 证明算法渐进一致和数值准确，实验中优于基线框架，与集中式方法竞争。

Conclusion: 该方法在隐私约束下实现高效协作优化。

Abstract: Motivated by the growing need for black-box optimization and data privacy, we
introduce a collaborative Bayesian optimization (BO) framework that addresses
both of these challenges. In this framework agents work collaboratively to
optimize a function they only have oracle access to. In order to mitigate
against communication and privacy constraints, agents are not allowed to share
their data but can share their Gaussian process (GP) surrogate models. To
enable collaboration under these constraints, we construct a central model to
approximate the objective function by leveraging the concept of Wasserstein
barycenters of GPs. This central model integrates the shared models without
accessing the underlying data. A key aspect of our approach is a collaborative
acquisition function that balances exploration and exploitation, allowing for
the optimization of decision variables collaboratively in each iteration. We
prove that our proposed algorithm is asymptotically consistent and that its
implementation via Monte Carlo methods is numerically accurate. Through
numerical experiments, we demonstrate that our approach outperforms other
baseline collaborative frameworks and is competitive with centralized
approaches that do not consider data privacy.

</details>

### [34] [AtlasD: Automatic Local Symmetry Discovery](https://arxiv.org/abs/2504.10777)
*Manu Bhat,Jonghyun Park,Jianke Yang,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TLDR: 本论文提出AtlasD方法，通过发现局部对称性来改进机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略局部邻域对称性，导致对称群 misrepresentation。

Method: 形式化局部对称性为图集等变性，训练局部预测网络并学习Lie群基。

Result: 在top-quark tagging和PDE实验中发现多连通分量局部对称群，提升气候分割和视觉任务性能。

Conclusion: 局部对称性作为归纳偏差，有助于改进下游任务效果。

Abstract: Existing symmetry discovery methods predominantly focus on global
transformations across the entire system or space, but they fail to consider
the symmetries in local neighborhoods. This may result in the reported symmetry
group being a misrepresentation of the true symmetry. In this paper, we
formalize the notion of local symmetry as atlas equivariance. Our proposed
pipeline, automatic local symmetry discovery (AtlasD), recovers the local
symmetries of a function by training local predictor networks and then learning
a Lie group basis to which the predictors are equivariant. We demonstrate
AtlasD is capable of discovering local symmetry groups with multiple connected
components in top-quark tagging and partial differential equation experiments.
The discovered local symmetry is shown to be a useful inductive bias that
improves the performance of downstream tasks in climate segmentation and vision
tasks.

</details>

### [35] [Power-scaled Bayesian Inference with Score-based Generative mModels](https://arxiv.org/abs/2504.10807)
*Huseyin Tuna Erdinc,Yunlin Zeng,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: cs.LG

TLDR: 本论文提出基于得分的生成算法，用于贝叶斯推理中幂缩放先验和似然的采样，应用于地震速度模型合成。


<details>
  <summary>Details</summary>
Motivation: 动机是实现对先验和似然影响的灵活控制，无需重新训练，并通过敏感性分析评估其对后验分布的影响。

Method: 方法使用基于得分的生成算法，从中间幂后验中采样，专注于条件地震图像的地震速度模型。

Result: 结果显示，增加似然幂提高样本数据保真度，减少先验幂增加结构多样性；适度似然缩放降低射击数据残差。

Conclusion: 结论是，该算法在后验细化和敏感性分析中具有实用价值。

Abstract: We propose a score-based generative algorithm for sampling from power-scaled
priors and likelihoods within the Bayesian inference framework. Our algorithm
enables flexible control over prior-likelihood influence without requiring
retraining for different power-scaling configurations. Specifically, we focus
on synthesizing seismic velocity models conditioned on imaged seismic. Our
method enables sensitivity analysis by sampling from intermediate power
posteriors, allowing us to assess the relative influence of the prior and
likelihood on samples of the posterior distribution. Through a comprehensive
set of experiments, we evaluate the effects of varying the power parameter in
different settings: applying it solely to the prior, to the likelihood of a
Bayesian formulation, and to both simultaneously. The results show that
increasing the power of the likelihood up to a certain threshold improves the
fidelity of posterior samples to the conditioning data (e.g., seismic images),
while decreasing the prior power promotes greater structural diversity among
samples. Moreover, we find that moderate scaling of the likelihood leads to a
reduced shot data residual, confirming its utility in posterior refinement.

</details>

### [36] [FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare](https://arxiv.org/abs/2504.10817)
*Penghao Wang,Qian Chen,Teng Zhang,Yingwei Zhang,Wang Lu,Yiqiang Chen*

Main category: cs.LG

TLDR: 本研究开发了Federated Healthcare Benchmark (FHBench)，一个针对真实医疗应用的基准测试，并引入了Efficient Personalized Federated Learning with Adaptive LoRA (EPFL)，以提升联邦学习在医疗领域的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据集多为多模态，计算资源有限，现有的联邦学习方法存在挑战，需要填补基准测试的空白。

Method: 开发了FHBench，涵盖神经、心血管、呼吸系统和一般病理领域的诊断任务，并引入EPFL框架，使用Adaptive LoRA实现高效的个性化联邦学习。

Result: 结果显示FHBench作为基准测试的稳健性，以及EPFL在各种医疗模式中的高效性和有效性。

Conclusion: EPFL框架解决了现有方法的局限性，推动了医疗领域联邦学习的创新发展。

Abstract: Federated Learning (FL) has emerged as an effective solution for
multi-institutional collaborations without sharing patient data, offering a
range of methods tailored for diverse applications. However, real-world medical
datasets are often multimodal, and computational resources are limited, posing
significant challenges for existing FL approaches. Recognizing these
limitations, we developed the Federated Healthcare Benchmark(FHBench), a
benchmark specifically designed from datasets derived from real-world
healthcare applications. FHBench encompasses critical diagnostic tasks across
domains such as the nervous, cardiovascular, and respiratory systems and
general pathology, providing comprehensive support for multimodal healthcare
evaluations and filling a significant gap in existing benchmarks. Building on
FHBench, we introduced Efficient Personalized Federated Learning with Adaptive
LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency
and effectiveness across various healthcare modalities. Our results highlight
the robustness of FHBench as a benchmarking tool and the potential of EPFL as
an innovative approach to advancing healthcare-focused FL, addressing key
limitations of existing methods.

</details>

### [37] [Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations](https://arxiv.org/abs/2504.10833)
*Shubham Kumar,Dwip Dalal,Narendra Ahuja*

Main category: cs.LG

TLDR: 这篇论文引入了Surrogate Faithfulness (SF)方法，以改进无监督概念解释方法的忠实度评估，并通过Optimally Faithful (OF)解释展示了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有忠实度指标忽略了概念的空间分布，导致解释不准确，因此需要更精确的评价方法。

Method: 提出SF方法，包括空间感知代理和两个新忠实度指标，并开发OF解释来最大化忠实度。

Result: 实验结果表明，添加空间感知提高了忠实度；OF解释的错误率降低了30%以上；OF概念在域外数据上泛化良好，并对对抗样本更鲁棒。

Conclusion: 新方法显著提升了解释的忠实度、泛化和鲁棒性。

Abstract: Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a
promising tool for generating semantic explanations of the decision-making
processes in deep neural networks, having applications in both model
improvement and understanding. It is vital that the explanation is accurate, or
faithful, to the model, yet we identify several limitations of prior
faithfulness metrics that inhibit an accurate evaluation; most notably, prior
metrics involve only the set of concepts present, ignoring how they may be
spatially distributed. We address these limitations with Surrogate Faithfulness
(SF), an evaluation method that introduces a spatially-aware surrogate and two
novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)
explanations, where concepts are found that maximize faithfulness. Our
experiments show that (1) adding spatial-awareness to prior U-CBEMs increases
faithfulness in all cases; (2) OF produces significantly more faithful
explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's
learned concepts generalize well to out-of-domain data and are more robust to
adversarial examples, where prior U-CBEMs struggle.

</details>

### [38] [How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?](https://arxiv.org/abs/2504.10850)
*Meiqi Liu,Zhuoqun Huang,Yue Xing*

Main category: cs.LG

TLDR: 本文提出一种无需访问基础模型权重的方法，通过训练鲁棒自编码器作为数据预处理，提高下游任务的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动机：对抗训练计算复杂度高，无法对基础模型进行微调，因此需要一种不更新模型权重的方法来提升下游任务的鲁棒性。

Method: 方法：基于理论分析，设计鲁棒自编码器作为数据预处理工具，训练时不访问基础模型。

Result: 结果：实验证明该方法有效提升下游任务鲁棒性，并验证特征鲁棒性与下游鲁棒性之间的联系。

Conclusion: 结论：该方法简单有效，证实了理论洞见。

Abstract: With the rise of powerful foundation models, a pre-training-fine-tuning
paradigm becomes increasingly popular these days: A foundation model is
pre-trained using a huge amount of data from various sources, and then the
downstream users only need to fine-tune and adapt it to specific downstream
tasks. However, due to the high computation complexity of adversarial training,
it is not feasible to fine-tune the foundation model to improve its robustness
on the downstream task. Observing the above challenge, we want to improve the
downstream robustness without updating/accessing the weights in the foundation
model. Inspired from existing literature in robustness inheritance (Kim et al.,
2020), through theoretical investigation, we identify a close relationship
between robust contrastive learning with the adversarial robustness of
supervised learning. To further validate and utilize this theoretical insight,
we design a simple-yet-effective robust auto-encoder as a data pre-processing
method before feeding the data into the foundation model. The proposed approach
has zero access to the foundation model when training the robust auto-encoder.
Extensive experiments demonstrate the effectiveness of the proposed method in
improving the robustness of downstream tasks, verifying the connection between
the feature robustness (implied by small adversarial contrastive loss) and the
robustness of the downstream task.

</details>

### [39] [ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning](https://arxiv.org/abs/2504.10851)
*Ruochen Jin,Boning Tong,Shu Yang,Bojian Hou,Li Shen*

Main category: cs.LG

TLDR: 本文引入ICAFS，一种新的垂直联邦学习特征选择方法，通过考虑客户端间交互，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法主要关注客户端内部，忽略了客户端间特征交互，导致模型性能不佳。

Method: 提出ICAFS多阶段集成方法，使用条件特征合成和可学习特征选择器，通过合成嵌入进行集成特征选择，绕过私有梯度共享限制。

Result: 在多个真实数据集实验中，ICAFS在预测准确性上超过了现有最先进方法。

Conclusion: ICAFS通过考虑客户端间交互，提供了一种有效的垂直联邦学习特征选择方案，提升了模型性能。

Abstract: Vertical federated learning (VFL) enables a paradigm for vertically
partitioned data across clients to collaboratively train machine learning
models. Feature selection (FS) plays a crucial role in Vertical Federated
Learning (VFL) due to the unique nature that data are distributed across
multiple clients. In VFL, different clients possess distinct subsets of
features for overlapping data samples, making the process of identifying and
selecting the most relevant features a complex yet essential task. Previous FS
efforts have primarily revolved around intra-client feature selection,
overlooking vital feature interaction across clients, leading to subpar model
outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for
effective FS in VFL by considering inter-client interactions. By employing
conditional feature synthesis alongside multiple learnable feature selectors,
ICAFS facilitates ensemble FS over these selectors using synthetic embeddings.
This method bypasses the limitations of private gradient sharing and allows for
model training using real data with refined embeddings. Experiments on multiple
real-world datasets demonstrate that ICAFS surpasses current state-of-the-art
methods in prediction accuracy.

</details>

### [40] [Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization](https://arxiv.org/abs/2504.10900)
*Peiliang Gong,Emadeldeen Eldele,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TLDR: 本论文提出ProtoNorm机制，改进Transformer在时间序列预训练中的规范化策略，解决数据分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 预训练大型数据集时，数据分布不匹配问题在时间序列数据中尤为严重，需要适应性强的规范化方法。

Method: 在Transformer中用原型引导的动态规范化(ProtoNorm)替换LayerNorm，通过样本与原型的亲和度选择规范化层。

Result: 实验显示，该方法在分类和预测任务上显著优于传统技术，并缓解分布偏移，在真实基准上证明鲁棒性和泛化性。

Conclusion: 该方法促进了更通用的时间序列基础模型发展。

Abstract: Foundation models have achieved remarkable success across diverse
machine-learning domains through large-scale pretraining on large, diverse
datasets. However, pretraining on such datasets introduces significant
challenges due to substantial mismatches in data distributions, a problem
particularly pronounced with time series data. In this paper, we tackle this
issue by proposing a domain-aware adaptive normalization strategy within the
Transformer architecture. Specifically, we replace the traditional LayerNorm
with a prototype-guided dynamic normalization mechanism (ProtoNorm), where
learned prototypes encapsulate distinct data distributions, and
sample-to-prototype affinity determines the appropriate normalization layer.
This mechanism effectively captures the heterogeneity of time series
characteristics, aligning pretrained representations with downstream tasks.
Through comprehensive empirical evaluation, we demonstrate that our method
significantly outperforms conventional pretraining techniques across both
classification and forecasting tasks, while effectively mitigating the adverse
effects of distribution shifts during pretraining. Incorporating ProtoNorm is
as simple as replacing a single line of code. Extensive experiments on diverse
real-world time series benchmarks validate the robustness and generalizability
of our approach, advancing the development of more versatile time series
foundation models.

</details>

### [41] [Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs](https://arxiv.org/abs/2504.10902)
*Rui Dai,Sile Hu,Xu Shen,Yonggang Zhang,Xinmei Tian,Jieping Ye*

Main category: cs.LG

TLDR: 本论文提出基于子模块线性性的模型合并策略，提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 作者认为子模块线性性高于整体模型，因此开发独立合并方法。

Method: 独立合并子模块（如层、self-attention和MLP），并推导最优合并权重的闭式解。

Result: 实验显示该方法优于标准任务算术和其他基准，在不同规模和任务中表现突出。

Conclusion: 强调利用子模块线性性的优势，为多任务模型合并提供新视角。

Abstract: Task arithmetic is a straightforward yet highly effective strategy for model
merging, enabling the resultant model to exhibit multi-task capabilities.
Recent research indicates that models demonstrating linearity enhance the
performance of task arithmetic. In contrast to existing methods that rely on
the global linearization of the model, we argue that this linearity already
exists within the model's submodules. In particular, we present a statistical
analysis and show that submodules (e.g., layers, self-attentions, and MLPs)
exhibit significantly higher linearity than the overall model. Based on these
findings, we propose an innovative model merging strategy that independently
merges these submodules. Especially, we derive a closed-form solution for
optimal merging weights grounded in the linear properties of these submodules.
Experimental results demonstrate that our method consistently outperforms the
standard task arithmetic approach and other established baselines across
different model scales and various tasks. This result highlights the benefits
of leveraging the linearity of submodules and provides a new perspective for
exploring solutions for effective and practical multi-task model merging.

</details>

### [42] [Towards A Universal Graph Structural Encoder](https://arxiv.org/abs/2504.10917)
*Jialin Chen,Haolan Zuo,Haoyu Peter Wang,Siqi Miao,Pan Li,Rex Ying*

Main category: cs.LG

TLDR: GFSE 是一种通用图结构编码器，通过自监督预训练提升跨域图表示学习性能，并在多数情况下达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕获和转移不同图域的结构信息，且无法有效处理复杂拓扑。

Method: 提出 GFSE，使用基于图 Transformer 的注意力机制和多自监督学习目标进行预训练，可与下游模型无缝整合。

Result: 实验显示 GFSE 显著提升性能，在 81.6% 的评估案例中达到最先进水平，并减少微调需求。

Conclusion: GFSE 作为强大通用的图结构编码器，具有广泛应用于图结构数据的潜力。

Abstract: Recent advancements in large-scale pre-training have shown the potential to
learn generalizable representations for downstream tasks. In the graph domain,
however, capturing and transferring structural information across different
graph domains remains challenging, primarily due to the inherent differences in
topological patterns across various contexts. Additionally, most existing
models struggle to capture the complexity of rich graph structures, leading to
inadequate exploration of the embedding space. To address these challenges, we
propose GFSE, a universal graph structural encoder designed to capture
transferable structural patterns across diverse domains such as molecular
graphs, social networks, and citation networks. GFSE is the first cross-domain
graph structural encoder pre-trained with multiple self-supervised learning
objectives. Built on a Graph Transformer, GFSE incorporates attention
mechanisms informed by graph inductive bias, enabling it to encode intricate
multi-level and fine-grained topological features. The pre-trained GFSE
produces generic and theoretically expressive positional and structural
encoding for graphs, which can be seamlessly integrated with various downstream
graph feature encoders, including graph neural networks for vectorized features
and Large Language Models for text-attributed graphs. Comprehensive experiments
on synthetic and real-world datasets demonstrate GFSE's capability to
significantly enhance the model's performance while requiring substantially
less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art
performance in 81.6% evaluated cases, spanning diverse graph models and
datasets, highlighting its potential as a powerful and versatile encoder for
graph-structured data.

</details>

### [43] [Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting](https://arxiv.org/abs/2504.10923)
*Mingyi Zhu,Zhaoxin Li,Qiao Lin,Li Ding*

Main category: cs.LG

TLDR: 本文提出Fast-Powerformer模型，用于中期风力发电预测，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 风力发电预测面临气象随机性和输出波动带来的准确性和计算效率挑战。

Method: 基于Reformer架构，添加轻量级LSTM嵌入、输入转置和FECAM机制，以增强特征提取和减少复杂度。

Result: 实验显示模型在真实数据集上精度和效率优于主流方法，并具有快速推理和低内存消耗。

Conclusion: 模型在实际部署中具有高实用价值。

Abstract: Wind power forecasting (WPF), as a significant research topic within
renewable energy, plays a crucial role in enhancing the security, stability,
and economic operation of power grids. However, due to the high stochasticity
of meteorological factors (e.g., wind speed) and significant fluctuations in
wind power output, mid-term wind power forecasting faces a dual challenge of
maintaining high accuracy and computational efficiency. To address these
issues, this paper proposes an efficient and lightweight mid-term wind power
forecasting model, termed Fast-Powerformer. The proposed model is built upon
the Reformer architecture, incorporating structural enhancements such as a
lightweight Long Short-Term Memory (LSTM) embedding module, an input
transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism
(FECAM). These improvements enable the model to strengthen temporal feature
extraction, optimize dependency modeling across variables, significantly reduce
computational complexity, and enhance sensitivity to periodic patterns and
dominant frequency components. Experimental results conducted on multiple
real-world wind farm datasets demonstrate that the proposed Fast-Powerformer
achieves superior prediction accuracy and operational efficiency compared to
mainstream forecasting approaches. Furthermore, the model exhibits fast
inference speed and low memory consumption, highlighting its considerable
practical value for real-world deployment scenarios.

</details>

### [44] [Transfer Learning for Temporal Link Prediction](https://arxiv.org/abs/2504.10925)
*Ayan Chatterjee,Barbara Ikica,Babak Ravandi,John Palowitch*

Main category: cs.LG

TLDR: 本论文研究了时间链接预测的转移学习任务，通过添加结构映射模块，使模型能够转移到新图上，并为无记忆基础模型铺平道路。


<details>
  <summary>Details</summary>
Motivation: 现有TLP模型无法直接应用于新图，因为记忆模块只存储训练时的节点信息，且结构信号对任务具有重要性。

Method: 在现有TLP模型架构中添加结构映射模块，学习从图结构特征到记忆嵌入的映射。

Result: 开发了转移有效的记忆负载模型，为时间链接预测的无记忆基础模型提供了可能性。

Conclusion: 这项工作推动了TLP任务中模型的泛化能力和基础模型的发展。

Abstract: Link prediction on graphs has applications spanning from recommender systems
to drug discovery. Temporal link prediction (TLP) refers to predicting future
links in a temporally evolving graph and adds additional complexity related to
the dynamic nature of graphs. State-of-the-art TLP models incorporate memory
modules alongside graph neural networks to learn both the temporal mechanisms
of incoming nodes and the evolving graph topology. However, memory modules only
store information about nodes seen at train time, and hence such models cannot
be directly transferred to entirely new graphs at test time and deployment. In
this work, we study a new transfer learning task for temporal link prediction,
and develop transfer-effective methods for memory-laden models. Specifically,
motivated by work showing the informativeness of structural signals for the TLP
task, we augment a structural mapping module to the existing TLP model
architectures, which learns a mapping from graph structural (topological)
features to memory embeddings. Our work paves the way for a memory-free
foundation model for TLP.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [45] [HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems](https://arxiv.org/abs/2504.10699)
*Nan Wang,Ricardo G. Sanfelice*

Main category: cs.RO

TLDR: 本文提出HyRRT-Connect算法，用于混合系统的运动规划，通过双向传播提高效率，并通过示例验证。


<details>
  <summary>Details</summary>
Motivation: 解决混合系统运动规划问题，特别是处理混合动力学中的不连续性和提高计算效率。

Method: 提出HyRRT-Connect算法，在混合时间内双向传播检测重叠，构建运动规划，并通过前向模拟重建后向部分消除不连续性。

Result: 应用于带驱动的弹跳球系统和步行机器人示例，展示了计算效率的显著提升。

Conclusion: HyRRT-Connect算法有效地解决了混合系统运动规划中的挑战，提高了整体性能。

Abstract: This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring that the motion plan satisfies the given hybrid dynamics. To
address the potential discontinuity along the flow caused by tolerating some
distance between the forward and backward partial motion plans, we reconstruct
the backward partial motion plan by a forward-in-hybrid-time simulation from
the final state of the forward partial motion plan. effectively eliminating the
discontinuity. The proposed algorithm is applied to an actuated bouncing ball
system and a walking robot example to highlight its computational improvement.

</details>

### [46] [$π$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles](https://arxiv.org/abs/2504.10962)
*Edvin Martin Andrejev,Amith Manoharan,Karl-Eerik Unt,Arun Kumar Singh*

Main category: cs.RO

TLDR: 本论文提出π-MPPI方法，通过添加投影过滤器改善MPPI算法的控制平滑性，适用于固定翼飞行器。


<details>
  <summary>Details</summary>
Motivation: 解决MPPI算法中控制序列不平滑导致系统振荡的问题，现有后处理方法无法有效约束控制导数。

Method: 引入投影过滤器π对控制样本进行最小修正，确保控制幅度和导数边界，并使用神经加速优化器减少计算开销。

Result: π-MPPI在固定翼飞行器上调优更容易，获得更平滑和鲁棒的性能。

Conclusion: π-MPPI提供简单实现任意平滑度控制序列的方法，可整合到任何MPPI管道中。

Abstract: Model Predictive Path Integral (MPPI) is a popular sampling-based Model
Predictive Control (MPC) algorithm for nonlinear systems. It optimizes
trajectories by sampling control sequences and averaging them. However, a key
issue with MPPI is the non-smoothness of the optimal control sequence, leading
to oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing
solutions use post-hoc smoothing, which fails to bound control derivatives.
This paper introduces a new approach: we add a projection filter $\pi$ to
minimally correct control samples, ensuring bounds on control magnitude and
higher-order derivatives. The filtered samples are then averaged using MPPI,
leading to our $\pi$-MPPI approach. We minimize computational overhead by using
a neural accelerated custom optimizer for the projection filter. $\pi$-MPPI
offers a simple way to achieve arbitrary smoothness in control sequences. While
we focus on FWVs, this projection filter can be integrated into any MPPI
pipeline. Applied to FWVs, $\pi$-MPPI is easier to tune than the baseline,
resulting in smoother, more robust performance.

</details>

### [47] [Neural Control Barrier Functions from Physics Informed Neural Networks](https://arxiv.org/abs/2504.11045)
*Shreenabh Agrawal,Manan Tayal,Aditya Singh,Shishir Kolathaya*

Main category: cs.RO

TLDR: 这篇论文提出了一种基于Zubov PDE的神经控制屏障函数方法，提高自主系统安全性，并在多个案例中验证。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统普及，确保安全性至关重要，传统CBFs设计困难，深度学习被用于合成神经CBFs。

Method: 引入物理启发神经网络框架，结合Zubov PDE和互惠CBFs，适用于高维系统，通过倒立摆、地面和空中导航案例验证。

Result: 方法可扩展，案例研究证明了其在不同系统中的有效性。

Conclusion: 提供了一种可扩展的神经CBFs合成方法，提升自主系统的安全性能。

Abstract: As autonomous systems become increasingly prevalent in daily life, ensuring
their safety is paramount. Control Barrier Functions (CBFs) have emerged as an
effective tool for guaranteeing safety; however, manually designing them for
specific applications remains a significant challenge. With the advent of deep
learning techniques, recent research has explored synthesizing CBFs using
neural networks-commonly referred to as neural CBFs. This paper introduces a
novel class of neural CBFs that leverages a physics-inspired neural network
framework by incorporating Zubov's Partial Differential Equation (PDE) within
the context of safety. This approach provides a scalable methodology for
synthesizing neural CBFs applicable to high-dimensional systems. Furthermore,
by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework
allows for the specification of flexible, user-defined safe regions. To
validate the effectiveness of the approach, we present case studies on three
different systems: an inverted pendulum, autonomous ground navigation, and
aerial navigation in obstacle-laden environments.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [48] [Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach](https://arxiv.org/abs/2504.10639)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于Koopman的安全终端电压估计方案，使用两阶段错误补偿来应对云BMS中的恶意传感器攻击。


<details>
  <summary>Details</summary>
Motivation: 为了在恶意攻击下确保电池终端电压数据的准确性，防止电池过充或欠充。

Method: 采用Koopman运算符的线性逼近，并通过两阶段错误补偿：第一阶段估计并补偿Koopman预测错误，第二阶段使用经验修正（如开路电压到荷态映射）或Gaussian过程回归数据驱动方法。

Result: 通过实验证明了使用经验和数据驱动修正的安全估计器的有效性。

Conclusion: 该方案能够有效恢复被攻击的电压数据，确保BMS的安全性。

Abstract: Cloud-based battery management system (BMS) requires accurate terminal
voltage measurement data to ensure optimal and safe charging of Lithium-ion
batteries. Unfortunately, an adversary can corrupt the battery terminal voltage
data as it passes from the local-BMS to the cloud-BMS through the communication
network, with the objective of under- or over-charging the battery. To ensure
accurate terminal voltage data under such malicious sensor attacks, this paper
investigates a Koopman-based secure terminal voltage estimation scheme using a
two-stage error-compensated self-learning feedback. During the first stage of
error correction, the potential Koopman prediction error is estimated to
compensate for the error accumulation due to the linear approximation of
Koopman operator. The second stage of error compensation aims to recover the
error amassing from the higher-order dynamics of the Lithium-ion batteries
missed by the self-learning strategy. Specifically, we have proposed two
different methods for this second stage error compensation. First, an
interpretable empirical correction strategy has been obtained using the open
circuit voltage to state-of-charge mapping for the battery. Second, a Gaussian
process regression-based data-driven method has been explored. Finally, we
demonstrate the efficacy of the proposed secure estimator using both empirical
and data-driven corrections.

</details>

### [49] [Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs](https://arxiv.org/abs/2504.10658)
*Sanchita Ghosh,Tanushree Roy*

Main category: eess.SY

TLDR: 这篇论文提出了一种使用可适应XgBoost模型检测电动汽车充电中传感器网络攻击的方法，并通过模拟实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于对手可能篡改电压传感器数据导致充电中断，因此需要实时检测攻击以确保充电安全，并适应不同电池配置。

Method: 提出对XgBoost-based的电池单元级模型进行可适应微调，使用有限的电池组级数据进行电压预测和残差生成，并利用PyBaMM和liionpack软件包的实验数据训练和测试。

Result: 算法在传感器交换和重放攻击下，对两个大容量电池组进行了评估，模拟结果突出了其可适应性和有效性。

Conclusion: 该检测算法在实时确保电动汽车充电安全方面是有效的，且具有良好的适应性。

Abstract: Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor
measurements from the battery pack to the cloud-controller of the smart
charging station. However, an adversary could corrupt the voltage sensor data
during transmission, potentially causing local to wide-scale disruptions.
Therefore, it is essential to detect sensor cyberattacks in real-time to ensure
secure EV charging, and the developed algorithms must be readily adaptable to
variations, including pack configurations. To tackle these challenges, we
propose adaptable fine-tuning of an XgBoost-based cell-level model using
limited pack-level data to use for voltage prediction and residual generation.
We used battery cell and pack data from high-fidelity charging experiments in
PyBaMM and `liionpack' package to train and test the detection algorithm. The
algorithm's performance has been evaluated for two large-format battery packs
under sensor swapping and replay attacks. The simulation results also highlight
the adaptability and efficacy of our proposed detection algorithm.

</details>

### [50] [Spectrum Sharing in STAR-RIS-assisted UAV with NOMA for Cognitive Radio Networks](https://arxiv.org/abs/2504.10691)
*Ali Nazari,Ali Olfat*

Main category: eess.SY

TLDR: 这篇论文提出了一种基于STAR-RIS辅助UAV的认知无线电网络优化方案，以提高频谱效率和缓解干扰。


<details>
  <summary>Details</summary>
Motivation: 为了解决认知无线电网络中动态信道挑战，并通过STAR-RIS和UAV技术改善主用户和次用户频谱效率。

Method: 提出通过非正交多址接入最大化总速率，联合优化UAV轨迹、传输-反射波束形成和功率分配，并开发了替代优化算法。

Result: 模拟结果研究了重要参数的影响、不同智能表面模式性能、联合轨迹和波束形成设计，以及STAR-RIS在缓解干扰方面的能力。

Conclusion: 该方法有效地提升了频谱效率，并展示了STAR-RIS在动态环境中的优势。

Abstract: As an emerging technology, the simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS) can improve the spectrum
efficiency (SE) of primary users (PUs) and secondary users (SUs) in cognitive
radio (CR) networks by mitigating the interference of the incident signals. The
STAR-RIS-assisted unmanned aerial vehicle (UAV) can fully cover the dynamic
environment through high mobility and fast deployment. According to the dynamic
air-to-ground channels, the STAR-RIS-assisted UAV may face a challenge
configuring their elements' coefficients (i.e., reflecting and transmitting the
amplitude and phases). Hence, to meet the requirements of dynamic channel
determination with the SE approach, this paper proposes the sum rate
maximization of both PUs and SUs through non-orthogonal multiple access in CR
network to jointly optimize the trajectory and transmission-reflection
beamforming design of the STAR-RIS-assisted UAV, and power allocation. Since
the non-convex joint optimization problem includes coupled optimization
variables, we develop an alternative optimization algorithm. Simulation results
study the impact of: 1) the significant parameters, 2) the performance of
different intelligence surface modes and STAR-RIS operating protocols, 3) the
joint trajectory and beamforming design with fixed and mobile users, and 4)
STAR-RIS capabilities such as mitigating the interference, and how variations
in the roles of elements dynamically.

</details>

### [51] [Vehicle Dynamics Control for Simultaneous Optimization of Tire Emissions and Performance in EVs](https://arxiv.org/abs/2504.10709)
*Chi-Bach Pham,Homayoun Hamedmoghadam Rafati,Robert Noel Shorten*

Main category: eess.SY

TLDR: 这篇论文提出一种控制方案，通过使用不同轮胎配置文件和优化扭矩分配，减少电动汽车轮胎排放，模拟结果显示有效。


<details>
  <summary>Details</summary>
Motivation: 电动汽车重量增加和牵引力导致轮胎排放上升，对健康和环境有害。

Method: 提出控制方案，利用低磨损低牵引力和高磨损高牵引力轮胎，并优化扭矩分布。

Result: 数值模拟显示显著减少轮胎排放，同时保持车辆性能。

Conclusion: 该方法有效减轻轮胎磨损并维持稳定性。

Abstract: In recent years, Electric Vehicles (EVs) have seen widespread public
adoption. While EVs produce zero tailpipe emissions, they contribute to an
increase in another type of vehicular emission: tire emissions.
Battery-operated EVs are generally heavier than their combustion-engine
counterparts and require greater acceleration forces, which their high-torque
electric motors provide. This combination of increased weight and traction
forces leads to higher tire emissions, which possess various adverse health and
environmental effects. Here, we propose a control solution with promising
results in mitigating tire wear in all-wheel-drive EVs. The idea is to utilize
different tire profiles on each drive axis: a low-wear, low-traction axis and a
high-wear, high-traction axis. Derived from detailed mathematical analyses, we
propose a simple control scheme to counteract the performance difference from
using the low-traction tires. The proposed control mechanism then distributes
torque optimally between the two axes, maximizing usage from the low-wear axis
and simultaneously maintaining stability and performance by leveraging
high-traction tires. Through detailed numerical simulations, we demonstrate
that the developed model significantly reduces tire emissions and maintains
vehicle drivability and performance.

</details>

### [52] [Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks](https://arxiv.org/abs/2504.10855)
*Yu Kawano,Zhiyong Sun*

Main category: eess.SY

TLDR: 这篇论文使用对角占优结构稳定未知非线性时延网络，通过分布式自适应控制，并在SIS网络疫情传播控制中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 推广虚拟收缩分析到时延系统，以稳定未知非线性时延网络，特别是处理传输延迟的问题。

Method: 利用输入矩阵的广义对角占优特性，通过对角高增益和分布式自适应调谐规则实现稳定。

Result: 证明了时延网络可以通过对角高增益稳定，并通过SIS网络疫情传播控制的案例研究验证了效果。

Conclusion: 提出的去中心化自适应控制确保闭环轨迹收敛到原点，在实际应用中表现出色。

Abstract: In this paper, we utilize a diagonally dominant structure for the
decentralized stabilization of unknown nonlinear time-delayed networks.
Generalizing the idea of virtual contraction analysis to time-delayed systems,
we demonstrate that nonlinear time-delayed networks can be stabilized by
diagonal high-gains if the input matrices possess certain generalized
(column/row) diagonally dominant properties. To achieve stabilization of
unknown networks, we further propose a distributed adaptive tuning rule for
each individual gain function, ensuring that all closed-loop trajectories
converge to the origin. The effectiveness of the proposed decentralized
adaptive control is verified in a case study on epidemic spreading control in
SIS networks with transmission delays.

</details>

### [53] [Offset-free Nonlinear MPC with Koopman-based Surrogate Models](https://arxiv.org/abs/2504.10954)
*Irene Schimperna,Lea Bold,Karl Worthmann*

Main category: eess.SY

TLDR: 本篇论文设计了基于扩展动态模式分解(EDMD)的偏移自由非线性模型预测控制(MPC)，用于代理模型，确保在建模误差下实现跟踪，通过数值模拟验证。


<details>
  <summary>Details</summary>
Motivation: 动机是处理建模误差并在系统平衡点信息不完整时实现偏移自由跟踪。

Method: 方法包括使用EDMD构建代理模型，增加干扰项通过观察器估计，并引入参考计算器计算MPC参考状态和输入。

Result: 结果通过van-der-Pol振荡器和四水箱过程的数值模拟展示了方法的有效性。

Conclusion: 结论是，在建模误差渐进常数假设下，算法保证了输出跟踪无偏移。

Abstract: In this paper, we design offset-free nonlinear Model Predictive Control (MPC)
for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The
model used for prediction in MPC is augmented with a disturbance term, that is
estimated by an observer. If the full information about the equilibrium of the
real system is not available, a reference calculator is introduced in the
algorithm to compute the MPC state and input references. The control algorithm
guarantees offset-free tracking of the controlled output under the assumption
that the modeling errors are asymptotically constant. The effectiveness of the
proposed approach is showcased with numerical simulations for two popular
benchmark systems: the van-der-Pol oscillator and the four-tanks process.

</details>

### [54] [A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks](https://arxiv.org/abs/2504.10960)
*Evagoras Makridis,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 这篇论文提出了一种名为RPPAC的线性分布式算法，用于处理多智能体系统中可能不平衡和有延迟的方向网络的平均共识问题。


<details>
  <summary>Details</summary>
Motivation: 动机是解决方向网络中不平衡和延迟问题下的平均共识挑战。

Method: 方法是提出RPPAC算法，利用剩余共识机制和链路信息，处理异步更新和时变延迟，通过时变矩阵的后向乘积分析收敛性。

Result: 结果是算法保证了状态平均，尽管存在不平衡和延迟。

Conclusion: 结论是RPPAC算法在方向网络中实现了可靠的平均共识。

Abstract: In this paper, we address the average consensus problem of multi-agent
systems for possibly unbalanced and delay-prone networks with directional
information flow. We propose a linear distributed algorithm (referred to as
RPPAC) that handles asynchronous updates and time-varying heterogeneous
information delays. Our proposed distributed algorithm utilizes a
surplus-consensus mechanism and information regarding the number of incoming
and outgoing links to guarantee state averaging, despite the imbalanced and
delayed information flow in directional networks. The convergence of the RPPAC
algorithm is examined using key properties of the backward product of
time-varying matrices that correspond to different snapshots of the directional
augmented network.

</details>

### [55] [Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks](https://arxiv.org/abs/2504.10964)
*Evagoras Makridis,Gabriele Oliva,Kasagatta Ramesh Narahari,Mohammadreza Doostmohammadian,Usman A. Khan,Themistoklis Charalambous*

Main category: eess.SY

TLDR: 本论文提出R-ADD-OPT算法，处理单向网络中异质有界延迟的分布式优化问题，并保证适当步长下收敛到最优解。


<details>
  <summary>Details</summary>
Motivation: 解决单向网络中可能存在异质但有界传输延迟的分布式优化问题。

Method: 提出Robustified ADD-OPT (R-ADD-OPT)算法的修改版本，以适应传输延迟。

Result: 梯度步长在特定范围内（取决于最大延迟）可保证节点收敛到最优解，且范围可预先计算。

Conclusion: R-ADD-OPT算法在有延迟网络中实现分布式优化的可靠收敛。

Abstract: In this paper, we address the distributed optimization problem over
unidirectional networks with possibly time-invariant heterogeneous bounded
transmission delays. In particular, we propose a modified version of the
Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein
called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed
optimization problem, even when the communication links suffer from
heterogeneous but bounded transmission delays. We show that if the gradient
step-size of the R-ADD-OPT algorithm is within a certain range, which also
depends on the maximum time delay in the network, then the nodes are guaranteed
to converge to the optimal solution of the distributed optimization problem.
The range of the gradient step-size that guarantees convergence can be computed
a priori based on the maximum time delay in the network.

</details>

### [56] [Steering Feedback in Dynamic Driving Simulators: Road-Induced and Non-Road-Induced Harshness](https://arxiv.org/abs/2504.11097)
*Maximilian Böhle,Bernhard Schick,Steffen Müller*

Main category: eess.SY

TLDR: 本论文研究了30-100 Hz频率范围内转向轮和车身激励对动态驾驶模拟器主观转向反馈的影响，通过42名参与者的对照研究，发现非道路诱发激励影响显著，而道路诱发激励影响较小。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨更高频率（30-100 Hz）转向反馈对主观评估的影响，因为传统方法主要关注低于30 Hz的频率。

Method: 方法包括使用动态驾驶模拟器进行单盲内部受试者设计的研究，比较参考车辆和四种模拟器变体，采用半经验和基于物理的轮胎模型，并添加发动机和车轮阶次激励。

Result: 结果显示，非道路诱发激励对主观转向反馈有强烈影响，而道路诱发激励的影响较不显著。

Conclusion: 结论强调非道路诱发激励在主观评估中的重要性，建议在驾驶模拟器开发中优先考虑这些因素。

Abstract: Steering feedback plays a substantial role in the validity of driving
simulators for the virtual development of modern vehicles. Established
objective steering characteristics typically assess the feedback behavior in
the frequency range of up to 30 Hz while factors such as steering wheel and
vehicle body vibrations at higher frequencies are mainly approached as comfort
issues. This work investigates the influence of steering wheel and vehicle body
excitations in the frequency range between 30 and 100 Hz on the subjective
evaluation of steering feedback in a dynamic driving simulator. A controlled
subject study with 42 participants was performed to compare a reference vehicle
with an electrical power steering system to four variants of its virtual
representation on a dynamic driving simulator. The effects of road-induced
excitations were investigated by comparing a semi-empirical and a physics-based
tire model, while the influence of non-road-induced excitations was
investigated by implementing engine and wheel orders. The simulator variants
were evaluated in comparison to the reference vehicle during closed-loop
driving on a country road in a single-blind within-subjects design. The
subjective evaluation focused on the perception of road feedback compared to
the reference vehicle. The statistical analysis of subjective results shows
that there is a strong effect of non-road-induced steering and vehicle body
excitations, while the effect of road-induced excitations is considerably less
pronounced.

</details>

### [57] [A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances](https://arxiv.org/abs/2504.11125)
*Dieter Teichrib,Moritz Schulze Darup*

Main category: eess.SY

TLDR: 本文提出了一种方法，通过混合整数线性约束表示分段仿射系统及其神经网络控制器的闭环动力学，并通过求解混合整数线性规划计算鲁棒正不变集，以验证稳定性和约束满足。


<details>
  <summary>Details</summary>
Motivation: 为了处理带有边界扰动和基于神经网络控制器的分段仿射系统，实现鲁棒稳定性和约束满足的认证。

Method: 将系统动力学表示为混合整数线性约束，并通过求解混合整数线性程序计算鲁棒正不变集。

Result: 能够计算鲁棒正不变集，认证稳定性和约束满足，并通过分段仿射逼近处理非线性系统及其误差边界。

Conclusion: 该方法为指定系统类提供了计算鲁棒正不变集的框架，从而确保稳定性和约束满足。

Abstract: We present a method for representing the closed-loop dynamics of piecewise
affine (PWA) systems with bounded additive disturbances and neural
network-based controllers as mixed-integer (MI) linear constraints. We show
that such representations enable the computation of robustly positively
invariant (RPI) sets for the specified system class by solving MI linear
programs. These RPI sets can subsequently be used to certify stability and
constraint satisfaction. Furthermore, the approach allows to handle non-linear
systems based on suitable PWA approximations and corresponding error bounds,
which can be interpreted as the bounded disturbances from above.

</details>

### [58] [Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning](https://arxiv.org/abs/2504.11261)
*Hannes Petrenz,Johannes Köhler,Francesco Borrelli*

Main category: eess.SY

TLDR: 本文提出了一种鲁棒自适应学习MPC框架，用于处理线性系统中的参数不确定性和扰动，通过迭代学习提升性能和安全性，仿真结果显示改进。


<details>
  <summary>Details</summary>
Motivation: 针对线性系统在迭代任务中参数不确定性和加性扰动的问题，需要改进控制性能和确保安全性。

Method: 使用集合成员估计迭代改进参数估计，从数据学习终端成本和终端集，确保递归可行性和约束满足。

Result: 保证递归可行性、约束满足和闭环成本鲁棒界限；仿真显示计算效率和控制性能优于现有方法。

Conclusion: 该方法有效提升了控制性能和效率，适用于类似系统。

Abstract: This paper presents a robust adaptive learning Model Predictive Control (MPC)
framework for linear systems with parametric uncertainties and additive
disturbances performing iterative tasks. The approach iteratively refines the
parameter estimates using set membership estimation. Performance enhancement
over iterations is achieved by learning the terminal cost from data. Safety is
enforced using a terminal set, which is also learned iteratively. The proposed
method guarantees recursive feasibility, constraint satisfaction, and a robust
bound on the closed-loop cost. Numerical simulations on a mass-spring-damper
system demonstrate improved computational efficiency and control performance
compared to an existing robust adaptive MPC approach.

</details>

### [59] [Balancing hydrogen delivery in national energy systems: impact of the temporal flexibility of hydrogen delivery on export prices](https://arxiv.org/abs/2504.11285)
*Hazem Abdel-Khalek,Eddy Jalbout,Caspar Schauß,Benjamin Pfluger*

Main category: eess.SY

TLDR: 本篇论文研究平衡成本如何影响氢气价格，针对不同交付时间表，在巴西、摩洛哥和土耳其发现价格上涨最高可达47%。


<details>
  <summary>Details</summary>
Motivation: 氢气在能源转型中扮演关键角色，现有的分析通常忽略交付时间，因此需要量化平衡成本对价格的影响。

Method: 通过模拟从完全灵活到完全稳定的氢气交付时间表，在巴西、摩洛哥和土耳其三个国家，以及10、50和200 TWh三种出口量下计算价格差异。

Result: 价格差异在巴西最高达36%，摩洛哥47%，土耳其18%，跨不同出口量。

Conclusion: 更严格的平衡约束显著增加氢气价格，强调在能源转型分析中必须考虑交付时间表。

Abstract: Hydrogen is expected to play a key role in the energy transition. Analyses
exploring the price of hydrogen usually calculate average or marginal
production costs regardless of the time of delivery. A key factor that affects
the price of hydrogen is the balancing costs, which we define as the expense of
ensuring a steady schedule of hydrogen delivery. We explore the effect of
delivering hydrogen to the export ports at different schedules, ranging from
fully flexible to moderately stable with a daily and weekly buffer, to fully
stable. We quantify the rise in hydrogen price with strict balancing constraint
in three countries: Brazil, Morocco and Turkey, and three export volumes: 10,
50 and 200 TWh. The price difference between the flexible and stable schedules
was found to reach a maximum of 36% in Brazil, 47% in Morocco and 18% in Turkey
across the different export volumes.

</details>

### [60] [Sensitivity Analysis of State Space Models for Scrap Composition Estimation in EAF and BOF](https://arxiv.org/abs/2504.11319)
*Yiqing Zhou,Karsten Naert,Dirk Nuyens*

Main category: eess.SY

TLDR: 本研究开发线性与非线性状态空间模型，估计炼钢中废钢元素组成，并评估对测量噪声的敏感性。


<details>
  <summary>Details</summary>
Motivation: 动机是提高电弧炉和碱性氧气炉中废钢成分估计的准确性，以支持工业应用。

Method: 方法包括使用质量平衡方程和卡尔曼滤波器（线性用修改版，非线性用UKF），以Cu和Cr元素测试噪声敏感性。

Result: 结果显示模型对大多数变量噪声鲁棒，但对炉渣质量噪声敏感。

Conclusion: 结论突出了该模型在实时估计中的实用性和限制。

Abstract: This study develops and analyzes linear and nonlinear state space models for
estimating the elemental composition of scrap steel used in steelmaking, with
applications to Electric Arc Furnace (EAF) and Basic Oxygen Furnace (BOF)
processes. The models incorporate mass balance equations and are fitted using a
modified Kalman filter for linear cases and the Unscented Kalman Filter (UKF)
for nonlinear cases. Using Cu and Cr as representative elements, we assess the
sensitivity of model predictions to measurement noise in key process variables,
including steel mass, steel composition, scrap input mass, slag mass, and iron
oxide fraction in slag. Results show that the models are robust to moderate
noise levels in most variables, particularly when errors are below $10\%$.
However, accuracy significantly deteriorates with noise in slag mass
estimation. These findings highlight the practical feasibility and limitations
of applying state space models for real-time scrap composition estimation in
industrial settings.

</details>

### [61] [Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes](https://arxiv.org/abs/2504.11355)
*Alberto Castillo,Elliot Pryor,Anas El Fathi,Boris Kovatchev,Marc Breton*

Main category: eess.SY

TLDR: 这篇论文提出优化采样数据集（OSDs）来训练神经网络模仿模型预测控制（MPC），在计算资源受限设备上实现高效控制，并通过胰岛素递送应用证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是虽然神经网络可以以更低计算成本复制MPC算法，但训练数据的组成对准确性至关重要，目前缺乏系统优化方法。

Method: 方法是引入OSDs，这是一个参数化的数据子集，能够保留MPC信息、避免重复状态并实现饱和，并提供高效生成算法。

Result: 结果是通过训练神经网络复制特定MPC算法，实现了准确性的四倍改进，并有基于OSDs的神经网络获得临床测试监管批准。

Conclusion: 结论是这种方法为在资源受限平台上部署复杂算法打开新途径，可能彻底改变优化实现方式。

Abstract: Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)
algorithms is an effective way to implement them in constrained embedded
devices. By collecting large amounts of input-output data, where inputs
represent system states and outputs are MPC-generated control actions, NNs can
be trained to replicate MPC behavior at a fraction of the computational cost.
However, although the composition of the training data critically influences
the final NN accuracy, methods for systematically optimizing it remain
underexplored. In this paper, we introduce the concept of Optimally-Sampled
Datasets (OSDs) as ideal training sets and present an efficient algorithm for
generating them. An OSD is a parametrized subset of all the available data that
(i) preserves existing MPC information up to a certain numerical resolution,
(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or
complete. We demonstrate the effectiveness of OSDs by training NNs to replicate
the University of Virginia's MPC algorithm for automated insulin delivery in
Type-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,
two OSD-trained NNs received regulatory clearance for clinical testing as the
first NN-based control algorithm for direct human insulin dosing. This
methodology opens new pathways for implementing advanced optimizations on
resource-constrained embedded platforms, potentially revolutionizing how
complex algorithms are deployed.

</details>

### [62] [A Winner-Takes-All Mechanism for Event Generation](https://arxiv.org/abs/2504.11374)
*Yongkang Huo,Fuvio Forni,Rodolphe Sepulchre*

Main category: eess.SY

TLDR: 本文提出一种新框架，用于中心模式发生器设计，结合神经元反弹兴奋性和赢者通吃计算，统一决策和节律模式生成。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一个简单、强大且鲁棒的网络架构，以统一决策和节律模式生成，解决相关领域的挑战。

Method: 方法使用全互抑连接并增强可设计兴奋交互的网络架构，结合神经元的内在反弹兴奋性和赢者通吃计算。

Result: 结果通过环形振荡器模型展示了自适应相位和频率调制，证明了框架在易实现、适应性和鲁棒性方面的优势。

Conclusion: 结论是该框架对神经形态系统和机器人应用具有重要前景。

Abstract: We present a novel framework for central pattern generator design that
leverages the intrinsic rebound excitability of neurons in combination with
winner-takes-all computation. Our approach unifies decision-making and rhythmic
pattern generation within a simple yet powerful network architecture that
employs all-to-all inhibitory connections enhanced by designable excitatory
interactions. This design offers significant advantages regarding ease of
implementation, adaptability, and robustness. We demonstrate its efficacy
through a ring oscillator model, which exhibits adaptive phase and frequency
modulation, making the framework particularly promising for applications in
neuromorphic systems and robotics.

</details>

### [63] [eXplainable AI for data driven control: an inverse optimal control approach](https://arxiv.org/abs/2504.11446)
*Federico Porcari,Donatello Materassi,Simone Formentin*

Main category: eess.SY

TLDR: 本论文提出XAI方法基于逆最优控制，解释黑箱控制器的行为，通过提取成本函数权重提供本地透明解释，与LIME相关联。


<details>
  <summary>Details</summary>
Motivation: 理解黑箱数据驱动控制器的行为是现代控制设计的关键挑战。

Method: 使用Inverse Optimal Control (IOC)方法，求解逆Linear Quadratic (LQ)问题，提取跟踪误差和控制努力的权重。

Result: 数值例子显示，推断的成本函数能更深入理解控制器的决策过程，揭示反直觉现象。

Conclusion: 该方法提供结构化、控制相关的解释，帮助阐明黑箱控制器的潜在目标。

Abstract: Understanding the behavior of black-box data-driven controllers is a key
challenge in modern control design. In this work, we propose an eXplainable AI
(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local
explanations for the behavior of a controller operating around a given region.
Specifically, we extract the weights assigned to tracking errors and control
effort in the implicit cost function that a black-box controller is optimizing,
offering a more transparent and interpretable representation of the
controller's underlying objectives. This approach presents connections with
well-established XAI techniques, such as Local Interpretable Model-agnostic
Explanations (LIME) since it is still based on a local approximation of the
control policy. However, rather being limited to a standard sensitivity
analysis, the explanation provided by our method relies on the solution of an
inverse Linear Quadratic (LQ) problem, offering a structured and more
control-relevant perspective. Numerical examples demonstrate that the inferred
cost function consistently provides a deeper understanding of the controller's
decision-making process, shedding light on otherwise counterintuitive or
unexpected phenomena.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [64] [A Review of Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving](https://arxiv.org/abs/2504.11372)
*Zhengbing He,Jorge Laval,Yu Han,Ryosuke Nishi,Cathy Wu*

Main category: physics.soc-ph

TLDR: 这篇论文回顾了可变限速（VSL）和拥堵吸收驾驶（JAD）在抑制高速公路停车-启动波方面的进展，桥接了这两个领域，并指出了研究机会。


<details>
  <summary>Details</summary>
Motivation: 停车-启动波导致交通效率降低、驾驶风险增加和车辆排放升高，因此需要人工干预。论文旨在综合VSL和JAD的碎片化进展。

Method: 论文首先全面回顾VSL和JAD的成就，然后从基础图、交通动态建模、交通状态估计和预测、随机性、策略验证场景以及现场测试和实际部署等角度桥接两个领域并识别研究机会。

Result: 通过回顾和桥接，论文促进了VSL和JAD之间的互补，推进了高速公路停车-启动波抑制的整体研究目标。

Conclusion: 期望通过这种桥接，一个领域可以利用另一个领域的优势来克服其局限性，从而推动停车-启动波抑制的进步。

Abstract: The main form of freeway traffic congestion is the familiar stop-and-go wave,
characterized by wide moving jams that propagate indefinitely upstream provided
enough traffic demand. They cause severe, long-lasting adverse effects, such as
reduced traffic efficiency, increased driving risks, and higher vehicle
emissions. This underscores the crucial importance of artificial intervention
in the propagation of stop-and-go waves. Over the past two decades, two
prominent strategies for stop-and-go wave suppression have emerged: variable
speed limit (VSL) and jam-absorption driving (JAD). Although they share similar
research motivations, objectives, and theoretical foundations, the development
of these strategies has remained relatively disconnected. To synthesize
fragmented advances and drive the field forward, this paper first provides a
comprehensive review of the achievements in the stop-and-go wave
suppression-oriented VSL and JAD, respectively. It then focuses on bridging the
two areas and identifying research opportunities from the following
perspectives: fundamental diagrams, traffic dynamics modeling, traffic state
estimation and prediction, stochasticity, scenarios for strategy validation,
and field tests and practical deployment. We expect that through this review,
one area can effectively address its limitations by identifying and leveraging
the strengths of the other, thus promoting the overall research goal of freeway
stop-and-go wave suppression.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [65] [EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)](https://arxiv.org/abs/2504.09861)
*Luyao Zhang*

Main category: cs.CY

TLDR: EthosGPT是一个开源框架，通过映射和评估大型语言模型（LLM）在全球人类价值观中的表现，应对LLM导致价值同质化的风险，促进价值多样性。


<details>
  <summary>Details</summary>
Motivation: 论文动机是解决LLM可能使人类价值观同质化的风险，类似于生物多样性丧失对生态韧性的威胁，强调价值多样性对社会信任、机构合法性和长期繁荣的重要性，并借鉴古希腊以太概念和联合国可持续发展目标。

Method: 使用国际调查数据、基于提示的评估和比较统计分析来映射和评估LLM在不同地区和文化中的表现。

Result: 揭示LLM的适应性和偏差，提供行动洞见，如多样化训练数据和保护濒危文化遗产，以确保AI系统的代表性。

Conclusion: 通过跨学科合作，促进技术稳健和伦理包容的AI系统，推进价值多元性，以实现可持续和公平的未来，并与联合国可持续发展目标（如减少不平等、文化遗产保护与和平正义）保持一致。

Abstract: Large language models (LLMs) are transforming global decision-making and
societal systems by processing diverse data at unprecedented scales. However,
their potential to homogenize human values poses critical risks, similar to
biodiversity loss undermining ecological resilience. Rooted in the ancient
Greek concept of ethos, meaning both individual character and the shared moral
fabric of communities, EthosGPT draws on a tradition that spans from
Aristotle's virtue ethics to Adam Smith's moral sentiments as the ethical
foundation of economic cooperation. These traditions underscore the vital role
of value diversity in fostering social trust, institutional legitimacy, and
long-term prosperity. EthosGPT addresses the challenge of value homogenization
by introducing an open-source framework for mapping and evaluating LLMs within
a global scale of human values. Using international survey data on cultural
indices, prompt-based assessments, and comparative statistical analyses,
EthosGPT reveals both the adaptability and biases of LLMs across regions and
cultures. It offers actionable insights for developing inclusive LLMs, such as
diversifying training data and preserving endangered cultural heritage to
ensure representation in AI systems. These contributions align with the United
Nations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced
Inequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace,
Justice and Strong Institutions). Through interdisciplinary collaboration,
EthosGPT promotes AI systems that are both technically robust and ethically
inclusive, advancing value plurality as a cornerstone for sustainable and
equitable futures.

</details>

<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [66] [Adaptive Synaptogenesis Implemented on a Nanomagnetic Platform](https://arxiv.org/abs/2504.10767)
*Faiyaz Elahi Mullick,Supriyo Bandyopadhyay,Rob Baxter,Tony J. Ragucci,Avik W. Ghosh*

Main category: cond-mat.dis-nn

TLDR: 本论文探讨人类大脑的adaptive synaptogenesis算法，用于AI避免遗忘和促进终身学习，通过模拟和硬件实现。


<details>
  <summary>Details</summary>
Motivation: 人类大脑与人工神经网络不同，具有adaptive synaptogenesis，能动态调整突触以避免灾难性遗忘和支持终身学习。

Method: 采用supervised Hebbian学习和海马体控制机制，通过模拟演示和设计纳米磁硬件加速器。

Result: 通过模拟验证了算法功能，并设计了针对边缘计算的硬件加速器。

Conclusion: adaptive synaptogenesis算法可提升AI性能，适用于边缘计算场景。

Abstract: The human brain functions very differently from artificial neural networks
(ANN) and possesses unique features that are absent in ANN. An important one
among them is "adaptive synaptogenesis" that modifies synaptic weights when
needed to avoid catastrophic forgetting and promote lifelong learning. The key
aspect of this algorithm is supervised Hebbian learning, where weight
modifications in the neocortex driven by temporal coincidence are further
accepted or vetoed by an added control mechanism from the hippocampus during
the training cycle, to make distant synaptic connections highly sparse and
strategic. In this work, we discuss various algorithmic aspects of adaptive
synaptogenesis tailored to edge computing, demonstrate its function using
simulations, and design nanomagnetic hardware accelerators for specific
functions of synaptogenesis.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [67] [Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning](https://arxiv.org/abs/2504.10489)
*Vikranth Udandarao,Noel Abraham Tiju,Muthuraj Vairamuthu,Harsh Mistry,Dhruv Kumar*

Main category: cs.HC

TLDR: 本文介绍了Roamify，一个AI驱动的旅行助手，使用大型语言模型生成个性化行程，用户调查显示AI方法更受欢迎，并强调设计考虑。


<details>
  <summary>Details</summary>
Motivation: 简化旅行规划过程，用户调查显示所有年龄组更偏好AI辅助，验证了此类助手的必要性。

Method: 使用Llama和T5等大型语言模型生成行程，结合网络爬虫获取实时目的地信息，并基于用户偏好调整推荐系统。

Result: 用户调查结果显示AI方法更受欢迎，验证了旅行助手的潜在需求。

Conclusion: Roamify可改善并简化不同年龄组的旅行规划体验。

Abstract: In this paper, we present Roamify, an Artificial Intelligence powered travel
assistant that aims to ease the process of travel planning. We have tested and
used multiple Large Language Models like Llama and T5 to generate personalised
itineraries per user preferences. Results from user surveys highlight the
preference for AI powered mediums over existing methods to help in travel
planning across all user age groups. These results firmly validate the
potential need of such a travel assistant. We highlight the two primary design
considerations for travel assistance: D1) incorporating a web-scraping method
to gather up-to-date news articles about destinations from various blog
sources, which significantly improves our itinerary suggestions, and D2)
utilising user preferences to create customised travel experiences along with a
recommendation system which changes the itinerary according to the user needs.
Our findings suggest that Roamify has the potential to improve and simplify how
users across multiple age groups plan their travel experiences.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [68] [HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs](https://arxiv.org/abs/2504.11421)
*Mahdi Hasanzadeh,Kasem Khalil,Cynthia Sturton,Ahmad Patooghy*

Main category: cs.AR

TLDR: 本文提出了一种轻量级多处理器片上系统（MPSoC）热攻击监测机制，实现了高效异常检测和低硬件开销。


<details>
  <summary>Details</summary>
Motivation: MPSoC易受热攻击影响，需要对抗动态热管理系统的操纵，以提升系统安全性。

Method: 采用自适应实时监测机制，通过加权移动平均和位移操作，在网络路由器中实现异常检测模块，并进行设计空间探索。

Result: 将温度波动从3.00°C降低到1.9°C，异常检测准确率达82%，硬件使用减少高达75%。

Conclusion: 提供实用、低成本解决方案，适用于资源受限环境，确保抵抗热攻击并维持性能。

Abstract: Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal
attacks that manipulate dynamic thermal management systems. To counter this, we
propose an adaptive real-time monitoring mechanism that detects abnormal
thermal patterns in chip tiles. Our design space exploration helped identify
key thermal features for an efficient anomaly detection module to be
implemented at routers of network-enabled MPSoCs. To minimize hardware
overhead, we employ weighted moving average (WMA) calculations and bit-shift
operations, ensuring a lightweight yet effective implementation. By defining a
spectrum of abnormal behaviors, our system successfully detects and mitigates
malicious temperature fluctuations, reducing severe cases from 3.00{\deg}C to
1.9{\deg}C. The anomaly detection module achieves up to 82% of accuracy in
detecting thermal attacks, which is only 10-15% less than top-performing
machine learning (ML) models like Random Forest. However, our approach reduces
hardware usage by up to 75% for logic resources and 100% for specialized
resources, making it significantly more efficient than ML-based solutions. This
method provides a practical, low-cost solution for resource-constrained
environments, ensuring resilience against thermal attacks while maintaining
system performance.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
*Yijun Liang,Ming Li,Chenrui Fan,Ziyue Li,Dang Nguyen,Kwesi Cobbina,Shweta Bhardwaj,Jiuhai Chen,Fuxiao Liu,Tianyi Zhou*

Main category: cs.CV

TLDR: 这篇论文引入ColorBench基准来评估视觉语言模型（VLMs）的颜色理解能力，并揭示了关键发现。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs是否和如何像人类一样感知和利用颜色，因为颜色在视觉推理中很重要。

Method: 引入ColorBench，这是一个精心设计的基准，包含各种测试场景，用于评估VLMs的颜色感知、推理和鲁棒性。

Result: 评估32个VLMs后发现：缩放定律成立，语言模型比视觉编码器更重要；性能差距小，表明颜色理解被忽略；CoT推理改善准确性和鲁棒性；颜色线索可能误导模型。

Conclusion: 当前VLMs在颜色理解方面有局限性，需要改进，ColorBench可作为推进多模态AI研究的基础工具。

Abstract: Color plays an important role in human perception and usually provides
critical clues in visual reasoning. However, it is unclear whether and how
vision-language models (VLMs) can perceive, understand, and leverage color as
humans. This paper introduces ColorBench, an innovative benchmark meticulously
crafted to assess the capabilities of VLMs in color understanding, including
color perception, reasoning, and robustness. By curating a suite of diverse
test scenarios, with grounding in real applications, ColorBench evaluates how
these models perceive colors, infer meanings from color-based cues, and
maintain consistent performance under varying color transformations. Through an
extensive evaluation of 32 VLMs with varying language models and vision
encoders, our paper reveals some undiscovered findings: (i) The scaling law
(larger models are better) still holds on ColorBench, while the language model
plays a more important role than the vision encoder. (ii) However, the
performance gaps across models are relatively small, indicating that color
understanding has been largely neglected by existing VLMs. (iii) CoT reasoning
improves color understanding accuracies and robustness, though they are
vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on
ColorBench but they can also mislead models in some tasks. These findings
highlight the critical limitations of current VLMs and underscore the need to
enhance color comprehension. Our ColorBenchcan serve as a foundational tool for
advancing the study of human-level color understanding of multimodal AI.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [70] [Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification](https://arxiv.org/abs/2504.10521)
*Pardis Moradbeiki,Mohammad Ali Zare Chahooki*

Main category: cs.SI

TLDR: 本研究通过整合文本分析、用户互动和个人资料，提高社交媒体情感分析准确率，尤其针对重大事件。


<details>
  <summary>Details</summary>
Motivation: 社交网络用户众多，传统方法仅依赖文本内容，无法有效捕捉复杂互动和事件相关情感。

Method: 采用混合方法，包括文本分析、通信树模型映射互动、用户个人资料和关注者分析。

Result: 与传统方法比较，情感分布模式提高准确率12%，用户资料提高15%。

Conclusion: 证明整合多源数据能更好地捕捉细微情感动态，传统方法不足以处理重大事件分析。

Abstract: As the popularity and reach of social networks continue to surge, a vast
reservoir of opinions and sentiments across various subjects inundates these
platforms. Among these, X social network (formerly Twitter) stands as a
juggernaut, boasting approximately 420 million active users. Extracting users'
emotional and mental states from their expressed opinions on social media has
become a common pursuit. While past methodologies predominantly focused on the
textual content of messages to analyze user sentiment, the interactive nature
of these platforms suggests a deeper complexity. This study employs hybrid
methodologies, integrating textual analysis, profile examination, follower
analysis, and emotion dissemination patterns. Initially, user interactions are
leveraged to refine emotion classification within messages, encompassing
exchanges where users respond to each other. Introducing the concept of a
communication tree, a model is extracted to map these interactions.
Subsequently, users' bios and interests from this tree are juxtaposed with
message text to enrich analysis. Finally, influential figures are identified
among users' followers in the communication tree, categorized into different
topics to gauge interests. The study highlights that traditional sentiment
analysis methodologies, focusing solely on textual content, are inadequate in
discerning sentiment towards significant events, notably the presidential
election. Comparative analysis with conventional methods reveals a substantial
improvement in accuracy with the incorporation of emotion distribution patterns
and user profiles. The proposed approach yields a 12% increase in accuracy with
emotion distribution patterns and a 15% increase when considering user
profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [71] [Focal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition](https://arxiv.org/abs/1906.05682)
*Suraj Tripathi,Abhay Kumar,Abhiram Ramesh,Chirag Singh,Promod Yenigalla*

Main category: eess.AS

TLDR: 本论文提出使用ResNet神经网络结合语音特征和Focal Loss来识别语音情感。


<details>
  <summary>Details</summary>
Motivation: 语音特征如Spectrogram和MFCCs比纯文本更能表征情感，Focal Loss帮助训练关注难例。

Method: 基于ResNet构建模型，使用语音特征和Focal Loss训练。

Result: 改进了情感识别性能，特别是对难例的处理。

Conclusion: 这种方法提升了语音情感识别的准确性和鲁棒性。

Abstract: This paper proposes a Residual Convolutional Neural Network (ResNet) based on
speech features and trained under Focal Loss to recognize emotion in speech.
Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients
(MFCCs) have shown the ability to characterize emotion better than just plain
text. Further Focal Loss, first used in One-Stage Object Detectors, has shown
the ability to focus the training process more towards hard-examples and
down-weight the loss assigned to well-classified examples, thus preventing the
model from being overwhelmed by easily classifiable examples.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [72] [A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field](https://arxiv.org/abs/2504.11064)
*Bo Ma,Yi Ji,Liyong Fang*

Main category: cs.MA

TLDR: 本文提出DSA-AAPF算法，改进传统APF方法，解决UAV障碍避免中的吸引力和局部极小值问题，模拟结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统APF方法存在吸引力分布不当和易陷局部极小值的局限性，导致UAV在复杂环境中可能碰撞障碍或无法达标。

Method: 提出DSA-AAPF算法，结合改进模拟退火和增强APF，融入领导者-跟随者策略、自适应重力增益和方向偏转机制，以平滑轨迹和逃离局部极小值。

Result: 模拟结果证明DSA-AAPF在编队重构、复杂障碍避免和逃离陷阱中的可行性、稳健性和优越性。

Conclusion: DSA-AAPF算法有效提升UAV障碍避免性能，解决了传统方法的缺陷。

Abstract: The traditional Artificial Potential Field (APF) method exhibits limitations
in its force distribution: excessive attraction when UAVs are far from the
target may cause collisions with obstacles, while insufficient attraction near
the goal often results in failure to reach the target. Furthermore, APF is
highly susceptible to local minima, compromising motion reliability in complex
environments. To address these challenges, this paper presents a novel hybrid
obstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial
Potential Field (DSA-AAPF)-which combines an improved simulated annealing
mechanism with an enhanced APF model. The proposed approach integrates a
Leader-Follower distributed formation strategy with the APF framework, where
the resultant force formulation is redefined to smooth UAV trajectories. An
adaptive gravitational gain function is introduced to dynamically adjust UAV
velocity based on environmental context, and a fast-converging controller
ensures accurate and efficient convergence to the target. Moreover, a
directional deflection mechanism is embedded within the simulated annealing
process, enabling UAVs to escape local minima caused by semi-enclosed obstacles
through continuous rotational motion. The simulation results, covering
formation reconfiguration, complex obstacle avoidance, and entrapment escape,
demonstrate the feasibility, robustness, and superiority of the proposed
DSA-AAPF algorithm.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [73] [Exploring Generative AI Techniques in Government: A Case Study](https://arxiv.org/abs/2504.10497)
*Sunyi Liu,Mengzhe Geng,Rebecca Hart*

Main category: cs.IR

TLDR: 这篇论文介绍了加拿大国家研究委员会 (NRC) 开发的智能代理 Pubbie，用于自动化性能测量、数据管理和报告，采用生成式 AI 技术减少手动工作。


<details>
  <summary>Details</summary>
Motivation: 认识到生成式人工智能的快速发展和变革潜力，NRC 启动试点项目，以探索将其整合到日常操作中提升性能。

Method: 使用 LLM 编排、RoBERTa 语义嵌入、战略微调和少样本学习等技术，设计用户友好的自然语言查询接口，支持文件上传/下载。

Result: Pubbie 减少了手动努力和可访问性障碍，提高了数据管理和报告的效率。

Conclusion: 本研究展示了生成式 AI 在组织性能优化中的应用潜力，为类似集成提供可行案例。

Abstract: The swift progress of Generative Artificial intelligence (GenAI), notably
Large Language Models (LLMs), is reshaping the digital landscape. Recognizing
this transformative potential, the National Research Council of Canada (NRC)
launched a pilot initiative to explore the integration of GenAI techniques into
its daily operation for performance excellence, where 22 projects were launched
in May 2024. Within these projects, this paper presents the development of the
intelligent agent Pubbie as a case study, targeting the automation of
performance measurement, data management and insight reporting at the NRC.
Cutting-edge techniques are explored, including LLM orchestration and semantic
embedding via RoBERTa, while strategic fine-tuning and few-shot learning
approaches are incorporated to infuse domain knowledge at an affordable cost.
The user-friendly interface of Pubbie allows general government users to input
queries in natural language and easily upload or download files with a simple
button click, greatly reducing manual efforts and accessibility barriers.

</details>

### [74] [ArxivBench: Can LLMs Assist Researchers in Conducting Research?](https://arxiv.org/abs/2504.10496)
*Ning Li,Jingran Zhang,Justin Cui*

Main category: cs.IR

TLDR: 本研究评估大型语言模型在生成arXiv相关论文和链接的准确性，引入arXivBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成事实错误内容的挑战，提升在学术环境中的可靠性。

Method: 使用arXivBench基准测试各种LLMs在八个arXiv主题和计算机科学五个子领域的性能。

Result: 准确性因主题而异，Claude-3.5-Sonnet表现最佳，人工智能子领域准确性较高。

Conclusion: 提供标准化工具，促进LLM在研究中的可靠使用，并开源代码和数据集。

Abstract: Large language models (LLMs) have demonstrated remarkable effectiveness in
completing various tasks such as reasoning, translation, and question
answering. However the issue of factual incorrect content in LLM-generated
responses remains a persistent challenge. In this study, we evaluate both
proprietary and open-source LLMs on their ability to respond with relevant
research papers and accurate links to articles hosted on the arXiv platform,
based on high level prompts. To facilitate this evaluation, we introduce
arXivBench, a benchmark specifically designed to assess LLM performance across
eight major subject categories on arXiv and five subfields within computer
science, one of the most popular categories among them. Our findings reveal a
concerning accuracy of LLM-generated responses depending on the subject, with
some subjects experiencing significantly lower accuracy than others. Notably,
Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant
and accurate responses. And interestingly, most LLMs achieve a much higher
accuracy in the Artificial Intelligence sub-field than other sub-fields. This
benchmark provides a standardized tool for evaluating the reliability of
LLM-generated scientific responses, promoting more dependable use of LLMs in
academic and research environments. Our code is open-sourced at
https://github.com/arxivBenchLLM/arXivBench and our dataset is available on
huggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.

</details>

### [75] [CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models](https://arxiv.org/abs/2504.10498)
*Jianling Lu,Mingqi Lv*

Main category: cs.IR

TLDR: 这篇论文提出CCSK方法，通过动态平衡大语言模型的自有知识和外部检索，改善Q&A任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前阈值-based方法静态单一，难以平衡LLM自有知识与外部IR，导致查询困难时IR决策可能无关。

Method: 提出CCSK，使用Siamese Network计算查询历史相似性，Response Quality Model基于LightGBM评估响应，并通过多头注意力机制融合特征进行动态联合决策。

Result: 在真实数据集上的广泛实验显示，CCSK显著提升了模型的信息检索有效性。

Conclusion: CCSK通过动态机制缓解了IR决策无关问题，证明了其在提升LLM Q&A性能方面的有效性。

Abstract: The performance of large language models (LLMs) in Q&A task increased
substantially through Retrieval-Augmented Generation (RAG) which brings in
external knowledge. However, the main difficulty lies in balancing the inherent
self-knowledge of LLMs with external information retrieval (IR). The current
threshold-based methods apply one-dimensional static mechanisms with single
criterion. As a result, their IR decisions might be irrelevant to the LLMs'
response under difficult queries. To alleviate this problem, we propose
Cognitive Convection of Self-Knowledge (CCSK). Different from traditional
methods that maintain single fixed IR activation criteria, CCSK implements a
dynamic joint decision process via a Siamese Network module and a Response
Quality Model. The Siamese Network calculates the cosine similarity between the
current query and the historical queries. The Response Quality Model evaluates
the responses of LLMs through LightGBM. The final decision of the CCSK is
derived from the outputs of the two modules, as well as text features fused
using a multi-head attention mechanism. Extensive experiments on real-world
datasets show that CCSK significantly enhances the model's effectiveness in
information retrieval.

</details>

### [76] [Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems](https://arxiv.org/abs/2504.10500)
*Eya Mhedhbi,Youssef Mourchid,Alice Othmani*

Main category: cs.IR

TLDR: 本论文提出了一种结合生成式自监督学习和残差图变换器的创新方法来提升推荐系统性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了通过更好的数据增强和用户物品交互学习来改进推荐系统。

Method: 整合生成式自监督学习与残差图变换器，使用理性aware SSL进行自动化预训练任务，结合拓扑aware 变换器和残差连接进行图表示学习，并通过自动蒸馏过程提炼自监督信号。

Result: 在多个数据集上的实验评估中，该方法 consistently 优于基线方法。

Conclusion: 该方法有效提升了推荐系统的性能，并展示了自监督学习在该领域的潜力。

Abstract: This paper introduces a cutting-edge method for enhancing recommender systems
through the integration of generative self-supervised learning (SSL) with a
Residual Graph Transformer. Our approach emphasizes the importance of superior
data enhancement through the use of pertinent pretext tasks, automated through
rationale-aware SSL to distill clear ways of how users and items interact. The
Residual Graph Transformer incorporates a topology-aware transformer for global
context and employs residual connections to improve graph representation
learning. Additionally, an auto-distillation process refines self-supervised
signals to uncover consistent collaborative rationales. Experimental
evaluations on multiple datasets demonstrate that our approach consistently
outperforms baseline methods.

</details>

### [77] [Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents](https://arxiv.org/abs/2504.10508)
*João Alberto de Oliveira Lima*

Main category: cs.IR

TLDR: 这篇论文提出Poly-Vector Retrieval方法，通过为法律条款分配多个嵌入向量来提升RAG在处理标签和交叉引用时的性能。


<details>
  <summary>Details</summary>
Motivation: 法律领域的RAG系统难以处理用户基于标签的查询和文本中的显式交叉引用。

Method: Poly-Vector Retrieval方法，为每个法律条款创建多个嵌入：一个捕捉内容，一个捕捉标签，并可选地捕捉其他名称。

Result: 在巴西联邦宪法实验中，该方法显著提高了标签导向查询的检索准确性，同时不影响纯语义查询的性能。

Conclusion: 讨论了分离引用和内容的哲学及实际含义，并提出未来应用于更广泛数据集的潜力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for
generating contextually accurate answers by integrating Large Language Models
(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently
reference norms by their labels or nicknames (e.g., Article 5 of the
Constitution or Consumer Defense Code (CDC)), rather than by their content,
posing challenges for traditional RAG approaches that rely solely on semantic
embeddings of text. Furthermore, legal texts themselves heavily rely on
explicit cross-references (e.g., "pursuant to Article 34") that function as
pointers. Both scenarios pose challenges for traditional RAG approaches that
rely solely on semantic embeddings of text, often failing to retrieve the
necessary referenced content. This paper introduces Poly-Vector Retrieval, a
method assigning multiple distinct embeddings to each legal provision: one
embedding captures the content (the full text), another captures the label (the
identifier or proper name), and optionally additional embeddings capture
alternative denominations. Inspired by Frege's distinction between Sense and
Reference, this poly-vector retrieval approach treats labels, identifiers and
reference markers as rigid designators and content embeddings as carriers of
semantic substance. Experiments on the Brazilian Federal Constitution
demonstrate that Poly-Vector Retrieval significantly improves retrieval
accuracy for label-centric queries and potential to resolve internal and
external cross-references, without compromising performance on purely semantic
queries. The study discusses philosophical and practical implications of
explicitly separating reference from content in vector embeddings and proposes
future research directions for applying this approach to broader legal datasets
and other domains characterized by explicit reference identifiers.

</details>

### [78] [Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion](https://arxiv.org/abs/2504.10509)
*Jakub Podolak,Leon Peric,Mina Janicijevic,Roxana Petcu*

Main category: cs.IR

TLDR: 本研究再现并扩展Setwise提示方法，用于LLM的零样本排名，引入Setwise Insertion以提高效率，实验显示查询时间减少31%、模型推理减少23%，并略微提升效果。


<details>
  <summary>Details</summary>
Motivation: 动机是验证Setwise方法的有效性和效率，并通过新方法减少计算开销以平衡性能。

Method: 方法包括再现原Setwise方法、提出Setwise Insertion（利用初始排名作为先验知识）、并在Flan-T5、Vicuna和Llama模型上进行实验。

Result: 结果显示Setwise Insertion比原方法查询时间减少31%、模型推理减少23%，并略微提高了重新排名的有效性。

Conclusion: 结论是，融入先验排名知识能提升Setwise方法的实用性，使零样本文档排名更高效准确。

Abstract: This study presents a comprehensive reproducibility and extension analysis of
the Setwise prompting methodology for zero-shot ranking with Large Language
Models (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and
efficiency compared to traditional Pointwise, Pairwise, and Listwise approaches
in document ranking tasks. Our reproduction confirms the findings of Zhuang et
al., highlighting the trade-offs between computational efficiency and ranking
effectiveness in Setwise methods. Building on these insights, we introduce
Setwise Insertion, a novel approach that leverages the initial document ranking
as prior knowledge, reducing unnecessary comparisons and uncertainty by
focusing on candidates more likely to improve the ranking results. Experimental
results across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show
that Setwise Insertion yields a 31% reduction in query time, a 23% reduction in
model inferences, and a slight improvement in reranking effectiveness compared
to the original Setwise method. These findings highlight the practical
advantage of incorporating prior ranking knowledge into Setwise prompting for
efficient and accurate zero-shot document reranking.

</details>

### [79] [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
*Minh-Anh Nguyen,Dung D. Le*

Main category: cs.IR

TLDR: 提出JEPA4Rec框架，通过结合联合嵌入预测架构和语言建模，提高序列推荐性能，解决数据稀疏和常识偏好问题。


<details>
  <summary>Details</summary>
Motivation: 语言表示学习在序列推荐中虽有优势，但面临数据稀疏和对用户常识偏好的有限理解。

Method: JEPA4Rec将项目表示为文本句子，使用双向Transformer编码器、掩码预测策略，并采用两阶段自监督学习。

Result: 在六个真实数据集上实验，JEPA4Rec在跨域、跨平台和低资源场景中优于最先进方法。

Conclusion: 该框架提升推荐性能，减少对大规模预训练数据的依赖。

Abstract: Language representation learning has emerged as a promising approach for
sequential recommendation, thanks to its ability to learn generalizable
representations. However, despite its advantages, this approach still struggles
with data sparsity and a limited understanding of common-sense user
preferences. To address these limitations, we propose $\textbf{JEPA4Rec}$, a
framework that combines $\textbf{J}$oint $\textbf{E}$mbedding
$\textbf{P}$redictive $\textbf{A}$rchitecture with language modeling of item
textual descriptions. JEPA4Rec captures semantically rich and transferable
representations, improving recommendation performance and reducing reliance on
large-scale pre-training data. Specifically, JEPA4Rec represents items as text
sentences by flattening descriptive information such as $\textit{title,
category}$, and other attributes. To encode these sentences, we employ a
bidirectional Transformer encoder with modified embedding layers tailored for
capturing item information in recommendation datasets. We apply masking to text
sentences and use them to predict the representations of the unmasked
sentences, helping the model learn generalizable item embeddings. To further
improve recommendation performance and language understanding, we employ a
two-stage training strategy incorporating self-supervised learning losses.
Experiments on six real-world datasets demonstrate that JEPA4Rec consistently
outperforms state-of-the-art methods, particularly in cross-domain,
cross-platform, and low-resource scenarios.

</details>