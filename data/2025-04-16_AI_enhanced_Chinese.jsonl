{"id": "2504.10639", "pdf": "https://arxiv.org/pdf/2504.10639", "abs": "https://arxiv.org/abs/2504.10639", "authors": ["Sanchita Ghosh", "Tanushree Roy"], "title": "Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach", "categories": ["eess.SY", "cs.SY"], "comment": "10 pages, 5 figures", "summary": "Cloud-based battery management system (BMS) requires accurate terminal\nvoltage measurement data to ensure optimal and safe charging of Lithium-ion\nbatteries. Unfortunately, an adversary can corrupt the battery terminal voltage\ndata as it passes from the local-BMS to the cloud-BMS through the communication\nnetwork, with the objective of under- or over-charging the battery. To ensure\naccurate terminal voltage data under such malicious sensor attacks, this paper\ninvestigates a Koopman-based secure terminal voltage estimation scheme using a\ntwo-stage error-compensated self-learning feedback. During the first stage of\nerror correction, the potential Koopman prediction error is estimated to\ncompensate for the error accumulation due to the linear approximation of\nKoopman operator. The second stage of error compensation aims to recover the\nerror amassing from the higher-order dynamics of the Lithium-ion batteries\nmissed by the self-learning strategy. Specifically, we have proposed two\ndifferent methods for this second stage error compensation. First, an\ninterpretable empirical correction strategy has been obtained using the open\ncircuit voltage to state-of-charge mapping for the battery. Second, a Gaussian\nprocess regression-based data-driven method has been explored. Finally, we\ndemonstrate the efficacy of the proposed secure estimator using both empirical\nand data-driven corrections.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKoopman\u7684\u5b89\u5168\u7ec8\u7aef\u7535\u538b\u4f30\u8ba1\u65b9\u6848\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u9519\u8bef\u8865\u507f\u6765\u5e94\u5bf9\u4e91BMS\u4e2d\u7684\u6076\u610f\u4f20\u611f\u5668\u653b\u51fb\u3002", "motivation": "\u4e3a\u4e86\u5728\u6076\u610f\u653b\u51fb\u4e0b\u786e\u4fdd\u7535\u6c60\u7ec8\u7aef\u7535\u538b\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u9632\u6b62\u7535\u6c60\u8fc7\u5145\u6216\u6b20\u5145\u3002", "method": "\u91c7\u7528Koopman\u8fd0\u7b97\u7b26\u7684\u7ebf\u6027\u903c\u8fd1\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u9519\u8bef\u8865\u507f\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f30\u8ba1\u5e76\u8865\u507fKoopman\u9884\u6d4b\u9519\u8bef\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u7ecf\u9a8c\u4fee\u6b63\uff08\u5982\u5f00\u8def\u7535\u538b\u5230\u8377\u6001\u6620\u5c04\uff09\u6216Gaussian\u8fc7\u7a0b\u56de\u5f52\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4f7f\u7528\u7ecf\u9a8c\u548c\u6570\u636e\u9a71\u52a8\u4fee\u6b63\u7684\u5b89\u5168\u4f30\u8ba1\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u80fd\u591f\u6709\u6548\u6062\u590d\u88ab\u653b\u51fb\u7684\u7535\u538b\u6570\u636e\uff0c\u786e\u4fddBMS\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2504.10658", "pdf": "https://arxiv.org/pdf/2504.10658", "abs": "https://arxiv.org/abs/2504.10658", "authors": ["Sanchita Ghosh", "Tanushree Roy"], "title": "Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "9 pages, 5 figures", "summary": "Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor\nmeasurements from the battery pack to the cloud-controller of the smart\ncharging station. However, an adversary could corrupt the voltage sensor data\nduring transmission, potentially causing local to wide-scale disruptions.\nTherefore, it is essential to detect sensor cyberattacks in real-time to ensure\nsecure EV charging, and the developed algorithms must be readily adaptable to\nvariations, including pack configurations. To tackle these challenges, we\npropose adaptable fine-tuning of an XgBoost-based cell-level model using\nlimited pack-level data to use for voltage prediction and residual generation.\nWe used battery cell and pack data from high-fidelity charging experiments in\nPyBaMM and `liionpack' package to train and test the detection algorithm. The\nalgorithm's performance has been evaluated for two large-format battery packs\nunder sensor swapping and replay attacks. The simulation results also highlight\nthe adaptability and efficacy of our proposed detection algorithm.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53ef\u9002\u5e94XgBoost\u6a21\u578b\u68c0\u6d4b\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u4e2d\u4f20\u611f\u5668\u7f51\u7edc\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u5bf9\u624b\u53ef\u80fd\u7be1\u6539\u7535\u538b\u4f20\u611f\u5668\u6570\u636e\u5bfc\u81f4\u5145\u7535\u4e2d\u65ad\uff0c\u56e0\u6b64\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u653b\u51fb\u4ee5\u786e\u4fdd\u5145\u7535\u5b89\u5168\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7535\u6c60\u914d\u7f6e\u3002", "method": "\u63d0\u51fa\u5bf9XgBoost-based\u7684\u7535\u6c60\u5355\u5143\u7ea7\u6a21\u578b\u8fdb\u884c\u53ef\u9002\u5e94\u5fae\u8c03\uff0c\u4f7f\u7528\u6709\u9650\u7684\u7535\u6c60\u7ec4\u7ea7\u6570\u636e\u8fdb\u884c\u7535\u538b\u9884\u6d4b\u548c\u6b8b\u5dee\u751f\u6210\uff0c\u5e76\u5229\u7528PyBaMM\u548cliionpack\u8f6f\u4ef6\u5305\u7684\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u7b97\u6cd5\u5728\u4f20\u611f\u5668\u4ea4\u6362\u548c\u91cd\u653e\u653b\u51fb\u4e0b\uff0c\u5bf9\u4e24\u4e2a\u5927\u5bb9\u91cf\u7535\u6c60\u7ec4\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6a21\u62df\u7ed3\u679c\u7a81\u51fa\u4e86\u5176\u53ef\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u68c0\u6d4b\u7b97\u6cd5\u5728\u5b9e\u65f6\u786e\u4fdd\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u5b89\u5168\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2504.10691", "pdf": "https://arxiv.org/pdf/2504.10691", "abs": "https://arxiv.org/abs/2504.10691", "authors": ["Ali Nazari", "Ali Olfat"], "title": "Spectrum Sharing in STAR-RIS-assisted UAV with NOMA for Cognitive Radio Networks", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "As an emerging technology, the simultaneous transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS) can improve the spectrum\nefficiency (SE) of primary users (PUs) and secondary users (SUs) in cognitive\nradio (CR) networks by mitigating the interference of the incident signals. The\nSTAR-RIS-assisted unmanned aerial vehicle (UAV) can fully cover the dynamic\nenvironment through high mobility and fast deployment. According to the dynamic\nair-to-ground channels, the STAR-RIS-assisted UAV may face a challenge\nconfiguring their elements' coefficients (i.e., reflecting and transmitting the\namplitude and phases). Hence, to meet the requirements of dynamic channel\ndetermination with the SE approach, this paper proposes the sum rate\nmaximization of both PUs and SUs through non-orthogonal multiple access in CR\nnetwork to jointly optimize the trajectory and transmission-reflection\nbeamforming design of the STAR-RIS-assisted UAV, and power allocation. Since\nthe non-convex joint optimization problem includes coupled optimization\nvariables, we develop an alternative optimization algorithm. Simulation results\nstudy the impact of: 1) the significant parameters, 2) the performance of\ndifferent intelligence surface modes and STAR-RIS operating protocols, 3) the\njoint trajectory and beamforming design with fixed and mobile users, and 4)\nSTAR-RIS capabilities such as mitigating the interference, and how variations\nin the roles of elements dynamically.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSTAR-RIS\u8f85\u52a9UAV\u7684\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u4f18\u5316\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u548c\u7f13\u89e3\u5e72\u6270\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u4e2d\u52a8\u6001\u4fe1\u9053\u6311\u6218\uff0c\u5e76\u901a\u8fc7STAR-RIS\u548cUAV\u6280\u672f\u6539\u5584\u4e3b\u7528\u6237\u548c\u6b21\u7528\u6237\u9891\u8c31\u6548\u7387\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\u6700\u5927\u5316\u603b\u901f\u7387\uff0c\u8054\u5408\u4f18\u5316UAV\u8f68\u8ff9\u3001\u4f20\u8f93-\u53cd\u5c04\u6ce2\u675f\u5f62\u6210\u548c\u529f\u7387\u5206\u914d\uff0c\u5e76\u5f00\u53d1\u4e86\u66ff\u4ee3\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u7814\u7a76\u4e86\u91cd\u8981\u53c2\u6570\u7684\u5f71\u54cd\u3001\u4e0d\u540c\u667a\u80fd\u8868\u9762\u6a21\u5f0f\u6027\u80fd\u3001\u8054\u5408\u8f68\u8ff9\u548c\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\uff0c\u4ee5\u53caSTAR-RIS\u5728\u7f13\u89e3\u5e72\u6270\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u63d0\u5347\u4e86\u9891\u8c31\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86STAR-RIS\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.10709", "pdf": "https://arxiv.org/pdf/2504.10709", "abs": "https://arxiv.org/abs/2504.10709", "authors": ["Chi-Bach Pham", "Homayoun Hamedmoghadam Rafati", "Robert Noel Shorten"], "title": "Vehicle Dynamics Control for Simultaneous Optimization of Tire Emissions and Performance in EVs", "categories": ["eess.SY", "cs.SY"], "comment": "25 pages, 12 figures", "summary": "In recent years, Electric Vehicles (EVs) have seen widespread public\nadoption. While EVs produce zero tailpipe emissions, they contribute to an\nincrease in another type of vehicular emission: tire emissions.\nBattery-operated EVs are generally heavier than their combustion-engine\ncounterparts and require greater acceleration forces, which their high-torque\nelectric motors provide. This combination of increased weight and traction\nforces leads to higher tire emissions, which possess various adverse health and\nenvironmental effects. Here, we propose a control solution with promising\nresults in mitigating tire wear in all-wheel-drive EVs. The idea is to utilize\ndifferent tire profiles on each drive axis: a low-wear, low-traction axis and a\nhigh-wear, high-traction axis. Derived from detailed mathematical analyses, we\npropose a simple control scheme to counteract the performance difference from\nusing the low-traction tires. The proposed control mechanism then distributes\ntorque optimally between the two axes, maximizing usage from the low-wear axis\nand simultaneously maintaining stability and performance by leveraging\nhigh-traction tires. Through detailed numerical simulations, we demonstrate\nthat the developed model significantly reduces tire emissions and maintains\nvehicle drivability and performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u8f6e\u80ce\u914d\u7f6e\u6587\u4ef6\u548c\u4f18\u5316\u626d\u77e9\u5206\u914d\uff0c\u51cf\u5c11\u7535\u52a8\u6c7d\u8f66\u8f6e\u80ce\u6392\u653e\uff0c\u6a21\u62df\u7ed3\u679c\u663e\u793a\u6709\u6548\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u91cd\u91cf\u589e\u52a0\u548c\u7275\u5f15\u529b\u5bfc\u81f4\u8f6e\u80ce\u6392\u653e\u4e0a\u5347\uff0c\u5bf9\u5065\u5eb7\u548c\u73af\u5883\u6709\u5bb3\u3002", "method": "\u63d0\u51fa\u63a7\u5236\u65b9\u6848\uff0c\u5229\u7528\u4f4e\u78e8\u635f\u4f4e\u7275\u5f15\u529b\u548c\u9ad8\u78e8\u635f\u9ad8\u7275\u5f15\u529b\u8f6e\u80ce\uff0c\u5e76\u4f18\u5316\u626d\u77e9\u5206\u5e03\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u663e\u8457\u51cf\u5c11\u8f6e\u80ce\u6392\u653e\uff0c\u540c\u65f6\u4fdd\u6301\u8f66\u8f86\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u8f7b\u8f6e\u80ce\u78e8\u635f\u5e76\u7ef4\u6301\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.10519", "pdf": "https://arxiv.org/pdf/2504.10519", "abs": "https://arxiv.org/abs/2504.10519", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "title": "Toward Super Agent System with Hybrid AI Routers", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8d85\u7ea7\u4ee3\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u6df7\u5408\u672c\u5730\u548c\u4e91\u7aef\u6a21\u578b\u6765\u9ad8\u6548\u5904\u7406\u7528\u6237\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u8d85\u7ea7\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53ef\u90e8\u7f72\uff0c\u9700\u8981\u4f18\u5316\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u68c0\u6d4b\u7528\u6237\u610f\u56fe\u3001\u8def\u7531\u5230\u4e13\u7528\u4ee3\u7406\u6216\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u672c\u5730\u6216\u4e91\u7aef\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u589e\u5f3a\u578b\u672c\u5730\u8bbe\u5907\u7684\u8d85\u7ea7\u4ee3\u7406\u84dd\u56fe\uff0c\u51cf\u5c11\u4e86\u5bf9\u4e91\u7aef\u7684\u4f9d\u8d56\u3002", "conclusion": "\u5c55\u671b\u672a\u6765\uff0c\u8d85\u7ea7\u4ee3\u7406\u5c06\u901a\u8fc7\u672c\u5730\u8ba1\u7b97\u548c\u4e91\u7aef\u534f\u4f5c\u65e0\u7f1d\u878d\u5165\u65e5\u5e38\u751f\u6d3b\u3002"}}
{"id": "2504.10490", "pdf": "https://arxiv.org/pdf/2504.10490", "abs": "https://arxiv.org/abs/2504.10490", "authors": ["Gabriel Bo", "Marc Bernardino", "Justin Gu"], "title": "GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 11 figures. This submission cites arXiv:2404.19756.\n  Supplementary materials and additional information are available at\n  arXiv:2404.19756", "summary": "We explore the potential of integrating learnable and interpretable\nmodules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based\nrepresentations--within a pre-trained GPT-2 model to enhance multi-task\nlearning accuracy. Motivated by the recent surge in using KAN and graph\nattention (GAT) architectures in chain-of-thought (CoT) models and debates over\ntheir benefits compared to simpler architectures like MLPs, we begin by\nenhancing a standard self-attention transformer using Low-Rank Adaptation\n(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This\napproach yields significant improvements. To further boost interpretability and\nricher representations, we develop two variants that attempt to improve the\nstandard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,\nsystematic evaluations reveal that neither variant outperforms the optimized\nLoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,\n99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On\nsonnet generation, we get a CHRF score of 42.097. These findings highlight that\nefficient parameter adaptation via LoRA remains the most effective strategy for\nour tasks: sentiment analysis, paraphrase detection, and sonnet generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c1d\u8bd5\u5c06KAN\u548c\u56fe\u8868\u793a\u6574\u5408\u5230GPT-2\u4e2d\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u51c6\u786e\u6027\uff0c\u4f46\u53d1\u73b0LoRA\u4f18\u5316\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u53d7KAN\u548cGAT\u5728CoT\u6a21\u578b\u5e94\u7528\u589e\u52a0\u4ee5\u53ca\u4e0eMLP\u6bd4\u8f83\u7684\u4e89\u8bae\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u4efb\u52a1\u5b66\u4e60\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528LoRA\u589e\u5f3atransformer\uff0c\u8fdb\u884c\u8d85\u53c2\u6570\u5fae\u8c03\u548cL2\u6b63\u5219\u5316\uff1b\u5f00\u53d1Graph LoRA\u548cHybrid-KAN LoRA\u53d8\u4f53\uff0c\u5e76\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "LoRA\u589e\u5f3atransformer\u5728SST\u6d4b\u8bd5\u96c6\u51c6\u786e\u738755.249%\uff0cCFIMDB dev\u96c699.18%\uff0c\u91ca\u4e49\u68c0\u6d4b89.9%\uff0c\u5341\u56db\u884c\u8bd7\u751f\u6210CHRF\u5206\u657042.097\uff1b\u53d8\u4f53\u672a\u4f18\u4e8e\u57fa\u51c6\u3002", "conclusion": "\u9ad8\u6548\u53c2\u6570\u9002\u914d\u901a\u8fc7LoRA\u662f\u9488\u5bf9\u60c5\u611f\u5206\u6790\u3001\u91ca\u4e49\u68c0\u6d4b\u548c\u5341\u56db\u884c\u8bd7\u751f\u6210\u7684\u6700\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2504.10855", "pdf": "https://arxiv.org/pdf/2504.10855", "abs": "https://arxiv.org/abs/2504.10855", "authors": ["Yu Kawano", "Zhiyong Sun"], "title": "Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks", "categories": ["eess.SY", "cs.SY", "math.OC"], "comment": null, "summary": "In this paper, we utilize a diagonally dominant structure for the\ndecentralized stabilization of unknown nonlinear time-delayed networks.\nGeneralizing the idea of virtual contraction analysis to time-delayed systems,\nwe demonstrate that nonlinear time-delayed networks can be stabilized by\ndiagonal high-gains if the input matrices possess certain generalized\n(column/row) diagonally dominant properties. To achieve stabilization of\nunknown networks, we further propose a distributed adaptive tuning rule for\neach individual gain function, ensuring that all closed-loop trajectories\nconverge to the origin. The effectiveness of the proposed decentralized\nadaptive control is verified in a case study on epidemic spreading control in\nSIS networks with transmission delays.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u5bf9\u89d2\u5360\u4f18\u7ed3\u6784\u7a33\u5b9a\u672a\u77e5\u975e\u7ebf\u6027\u65f6\u5ef6\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u5e76\u5728SIS\u7f51\u7edc\u75ab\u60c5\u4f20\u64ad\u63a7\u5236\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u63a8\u5e7f\u865a\u62df\u6536\u7f29\u5206\u6790\u5230\u65f6\u5ef6\u7cfb\u7edf\uff0c\u4ee5\u7a33\u5b9a\u672a\u77e5\u975e\u7ebf\u6027\u65f6\u5ef6\u7f51\u7edc\uff0c\u7279\u522b\u662f\u5904\u7406\u4f20\u8f93\u5ef6\u8fdf\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u8f93\u5165\u77e9\u9635\u7684\u5e7f\u4e49\u5bf9\u89d2\u5360\u4f18\u7279\u6027\uff0c\u901a\u8fc7\u5bf9\u89d2\u9ad8\u589e\u76ca\u548c\u5206\u5e03\u5f0f\u81ea\u9002\u5e94\u8c03\u8c10\u89c4\u5219\u5b9e\u73b0\u7a33\u5b9a\u3002", "result": "\u8bc1\u660e\u4e86\u65f6\u5ef6\u7f51\u7edc\u53ef\u4ee5\u901a\u8fc7\u5bf9\u89d2\u9ad8\u589e\u76ca\u7a33\u5b9a\uff0c\u5e76\u901a\u8fc7SIS\u7f51\u7edc\u75ab\u60c5\u4f20\u64ad\u63a7\u5236\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u63a7\u5236\u786e\u4fdd\u95ed\u73af\u8f68\u8ff9\u6536\u655b\u5230\u539f\u70b9\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2504.10527", "pdf": "https://arxiv.org/pdf/2504.10527", "abs": "https://arxiv.org/abs/2504.10527", "authors": ["Leonardo Arrighi", "Ingrid Alves de Moraes", "Marco Zullich", "Michele Simonato", "Douglas Fernandes Barbin", "Sylvio Barbon Junior"], "title": "Explainable Artificial Intelligence techniques for interpretation of food datasets: a review", "categories": ["cs.AI", "cs.CY", "A.1"], "comment": "33 pages, 8 figures, 5 tables", "summary": "Artificial Intelligence (AI) has become essential for analyzing complex data\nand solving highly-challenging tasks. It is being applied across numerous\ndisciplines beyond computer science, including Food Engineering, where there is\na growing demand for accurate and trustworthy predictions to meet stringent\nfood quality standards. However, this requires increasingly complex AI models,\nraising reliability concerns. In response, eXplainable AI (XAI) has emerged to\nprovide insights into AI decision-making, aiding model interpretation by\ndevelopers and users. Nevertheless, XAI remains underutilized in Food\nEngineering, limiting model reliability. For instance, in food quality control,\nAI models using spectral imaging can detect contaminants or assess freshness\nlevels, but their opaque decision-making process hinders adoption. XAI\ntechniques such as SHAP (Shapley Additive Explanations) and Grad-CAM\n(Gradient-weighted Class Activation Mapping) can pinpoint which spectral\nwavelengths or image regions contribute most to a prediction, enhancing\ntransparency and aiding quality control inspectors in verifying AI-generated\nassessments. This survey presents a taxonomy for classifying food quality\nresearch using XAI techniques, organized by data types and explanation methods,\nto guide researchers in choosing suitable approaches. We also highlight trends,\nchallenges, and opportunities to encourage the adoption of XAI in Food\nEngineering.", "AI": {"tldr": "\u8fd9\u7bc7\u8c03\u67e5\u4ecb\u7ecd\u4e86\u5728\u98df\u54c1\u5de5\u7a0b\u4e2d\u4f7f\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u4ee5\u63d0\u9ad8AI\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\uff0c\u5e76\u6307\u5bfc\u7814\u7a76\u4eba\u5458\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002", "motivation": "AI\u5728\u98df\u54c1\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u590d\u6742\uff0c\u5bfc\u81f4\u53ef\u9760\u6027\u95ee\u9898\uff0c\u800cXAI\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u63d0\u5347AI\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u4ee5\u6ee1\u8db3\u98df\u54c1\u8d28\u91cf\u6807\u51c6\u3002", "method": "\u5448\u73b0\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u7c7b\u578b\u548c\u89e3\u91ca\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u4f7f\u7528XAI\u6280\u672f\u7684\u98df\u54c1\u8d28\u91cf\u7814\u7a76\u3002", "result": "\u7a81\u51fa\u4e86XAI\u5728\u98df\u54c1\u5de5\u7a0b\u4e2d\u7684\u8d8b\u52bf\u3001\u6311\u6218\u548c\u673a\u4f1a\uff0c\u4ee5\u9f13\u52b1\u5176\u91c7\u7528\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5206\u7c7b\u6cd5\u548c\u89c1\u89e3\uff0c\u6307\u5bfc\u7814\u7a76\u4eba\u5458\u5e76\u63a8\u52a8XAI\u5728\u98df\u54c1\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.10536", "pdf": "https://arxiv.org/pdf/2504.10536", "abs": "https://arxiv.org/abs/2504.10536", "authors": ["Lihong Zhang", "Yue Li"], "title": "Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\norganizations without sharing raw data, addressing crucial privacy concerns in\nhealthcare natural language processing (NLP). However, training large language\nmodels (LLMs) in federated settings faces significant challenges, including\ncommunication overhead and data heterogeneity. We propose Layer-Skipping\nFederated Learning, where only selected layers of a pre-trained LLM are\nfine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B,\nour approach reduces communication costs by approximately 70% while maintaining\nperformance within 2% of centralized training. We evaluate our method on\nclinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our\nexperiments demonstrate that Layer-Skipping FL outperforms competitive\nbaselines, handles non-IID clinical data distributions effectively, and shows\nrobustness when combined with differential privacy. This approach represents a\npractical solution for privacy-preserving collaborative learning in healthcare\nNLP.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5c42\u8df3\u8dc3\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u533b\u7597NLP\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u950070%\uff0c\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597NLP\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u901a\u4fe1\u5f00\u9500\u548c\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5c42\u8df3\u8dc3\u8054\u90a6\u5b66\u4e60\uff0c\u4ec5\u5fae\u8c03\u9884\u8bad\u7ec3LLM\uff08\u5982LLaMA 3.2-1B\uff09\u7684\u9009\u5b9a\u5c42\uff0c\u5176\u4ed6\u5c42\u51bb\u7ed3\u3002", "result": "\u901a\u4fe1\u6210\u672c\u51cf\u5c11\u7ea670%\uff0c\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u76f8\u5dee\u4e0d\u52302%\uff0c\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5904\u7406\u975eIID\u6570\u636e\u5e76\u4e0e\u5dee\u5206\u9690\u79c1\u517c\u5bb9\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u533b\u7597NLP\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u5b66\u4e60\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.10954", "pdf": "https://arxiv.org/pdf/2504.10954", "abs": "https://arxiv.org/abs/2504.10954", "authors": ["Irene Schimperna", "Lea Bold", "Karl Worthmann"], "title": "Offset-free Nonlinear MPC with Koopman-based Surrogate Models", "categories": ["eess.SY", "cs.SY", "math.OC"], "comment": "10 pages, 3 figures", "summary": "In this paper, we design offset-free nonlinear Model Predictive Control (MPC)\nfor surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The\nmodel used for prediction in MPC is augmented with a disturbance term, that is\nestimated by an observer. If the full information about the equilibrium of the\nreal system is not available, a reference calculator is introduced in the\nalgorithm to compute the MPC state and input references. The control algorithm\nguarantees offset-free tracking of the controlled output under the assumption\nthat the modeling errors are asymptotically constant. The effectiveness of the\nproposed approach is showcased with numerical simulations for two popular\nbenchmark systems: the van-der-Pol oscillator and the four-tanks process.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6269\u5c55\u52a8\u6001\u6a21\u5f0f\u5206\u89e3(EDMD)\u7684\u504f\u79fb\u81ea\u7531\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\uff0c\u7528\u4e8e\u4ee3\u7406\u6a21\u578b\uff0c\u786e\u4fdd\u5728\u5efa\u6a21\u8bef\u5dee\u4e0b\u5b9e\u73b0\u8ddf\u8e2a\uff0c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002", "motivation": "\u52a8\u673a\u662f\u5904\u7406\u5efa\u6a21\u8bef\u5dee\u5e76\u5728\u7cfb\u7edf\u5e73\u8861\u70b9\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u5b9e\u73b0\u504f\u79fb\u81ea\u7531\u8ddf\u8e2a\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528EDMD\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u589e\u52a0\u5e72\u6270\u9879\u901a\u8fc7\u89c2\u5bdf\u5668\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u53c2\u8003\u8ba1\u7b97\u5668\u8ba1\u7b97MPC\u53c2\u8003\u72b6\u6001\u548c\u8f93\u5165\u3002", "result": "\u7ed3\u679c\u901a\u8fc7van-der-Pol\u632f\u8361\u5668\u548c\u56db\u6c34\u7bb1\u8fc7\u7a0b\u7684\u6570\u503c\u6a21\u62df\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u5728\u5efa\u6a21\u8bef\u5dee\u6e10\u8fdb\u5e38\u6570\u5047\u8bbe\u4e0b\uff0c\u7b97\u6cd5\u4fdd\u8bc1\u4e86\u8f93\u51fa\u8ddf\u8e2a\u65e0\u504f\u79fb\u3002"}}
{"id": "2504.10649", "pdf": "https://arxiv.org/pdf/2504.10649", "abs": "https://arxiv.org/abs/2504.10649", "authors": ["Matthew Zalesak", "Hins Hu", "Samitha Samaranayake"], "title": "Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics", "categories": ["cs.AI", "cs.ET"], "comment": null, "summary": "On-demand ride-pooling has emerged as a popular urban transportation\nsolution, addressing the efficiency limitations of traditional ride-hailing\nservices by grouping multiple riding requests with spatiotemporal proximity\ninto a single vehicle. Although numerous algorithms have been developed for the\nRide-pool Assignment Problem (RAP) -- a core component of ride-pooling systems,\nthere is a lack of open-source implementations, making it difficult to\nbenchmark these algorithms on a common dataset and objective. In this paper, we\npresent the implementation details of a ride-pool simulator that encompasses\nseveral key ride-pool assignment algorithms, along with associated components\nsuch as vehicle routing and rebalancing. We also open-source a highly optimized\nand modular C++ codebase, designed to facilitate the extension of new\nalgorithms and features. Additionally, we introduce a family of swapping-based\nlocal-search heuristics to enhance existing ride-pool assignment algorithms,\nachieving a better balance between performance and computational efficiency.\nExtensive experiments on a large-scale, real-world dataset from Manhattan, NYC\nreveal that while all selected algorithms perform comparably, the newly\nproposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE)\nalgorithm achieves a state-of-the-art service rate with significantly reduced\ncomputational time. Furthermore, an in-depth analysis suggests that a\nperformance barrier exists for all myopic ride-pool assignment algorithms due\nto the system's capacity bottleneck, and incorporating future information could\nbe key to overcoming this limitation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f00\u6e90\u4e86\u4e58\u8f66\u6c60\u6a21\u62df\u5668\uff0c\u5f15\u5165\u65b0\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u7f3a\u4e4f\u5f00\u6e90\u5b9e\u73b0\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u56f0\u96be\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86\u5f00\u6e90\u5de5\u5177\u4ee5\u4fbf\u7b97\u6cd5\u6bd4\u8f83\u3002", "method": "\u5b9e\u73b0\u4e86\u4e58\u8f66\u6c60\u6a21\u62df\u5668\uff0c\u5305\u62ec\u8f66\u8f86\u8def\u7531\u548c\u518d\u5e73\u8861\u7ec4\u4ef6\uff0c\u5f15\u5165\u4ea4\u6362-based\u5c40\u90e8\u641c\u7d22\u542f\u53d1\u5f0f\uff0c\u5e76\u63d0\u51faLA-MR-CE\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLA-MR-CE\u7b97\u6cd5\u5728\u670d\u52a1\u7387\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u4f18\u8d8a\uff0c\u6240\u6709\u77ed\u89c6\u7b97\u6cd5\u53d7\u5bb9\u91cf\u74f6\u9888\u9650\u5236\u3002", "conclusion": "\u5efa\u8bae\u6574\u5408\u672a\u6765\u4fe1\u606f\u4ee5\u514b\u670d\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u5347\u7b97\u6cd5\u6548\u679c\u3002"}}
{"id": "2504.10551", "pdf": "https://arxiv.org/pdf/2504.10551", "abs": "https://arxiv.org/abs/2504.10551", "authors": ["Lili Zhao", "Qi Liu", "Wei Chen", "Liyi Chen", "Ruijun Sun", "Min Hou", "Yang Wang", "Shijin Wang"], "title": "MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Empirical Risk Minimization (ERM) models often rely on spurious correlations\nbetween features and labels during the learning process, leading to shortcut\nlearning behavior that undermines robustness generalization performance.\nCurrent research mainly targets identifying or mitigating a single shortcut;\nhowever, in real-world scenarios, cues within the data are diverse and unknown.\nIn empirical studies, we reveal that the models rely to varying extents on\ndifferent shortcuts. Compared to weak shortcuts, models depend more heavily on\nstrong shortcuts, resulting in their poor generalization ability. To address\nthese challenges, we propose MiMu, a novel method integrated with\nTransformer-based ERMs designed to Mitigate Multiple shortcut learning\nbehavior, which incorporates self-calibration strategy and self-improvement\nstrategy. In the source model, we preliminarily propose the self-calibration\nstrategy to prevent the model from relying on shortcuts and make overconfident\npredictions. Then, we further design self-improvement strategy in target model\nto reduce the reliance on multiple shortcuts. The random mask strategy involves\nrandomly masking partial attention positions to diversify the focus of target\nmodel other than concentrating on a fixed region. Meanwhile, the adaptive\nattention alignment module facilitates the alignment of attention weights to\nthe calibrated source model, without the need for post-hoc attention maps or\nsupervision. Finally, extensive experiments conducted on Natural Language\nProcessing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu\nin improving robustness generalization abilities.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faMiMu\u65b9\u6cd5\uff0c\u7f13\u89e3Empirical Risk Minimization (ERM)\u6a21\u578b\u7684\u591a\u91cd\u6377\u5f84\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "ERM\u6a21\u578b\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u5bfc\u81f4\u6377\u5f84\u5b66\u4e60\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u9488\u5bf9\u5355\u4e00\u6377\u5f84\uff0c\u800c\u73b0\u5b9e\u4e2d\u6377\u5f84\u591a\u6837\u4e14\u672a\u77e5\uff0c\u6a21\u578b\u5bf9\u5f3a\u6377\u5f84\u8fc7\u5ea6\u4f9d\u8d56\u5f71\u54cd\u6cdb\u5316\u3002", "method": "\u63d0\u51faMiMu\u65b9\u6cd5\uff0c\u5305\u62ec\u81ea\u6821\u51c6\u7b56\u7565\uff08\u9632\u6b62\u4f9d\u8d56\u6377\u5f84\uff09\u548c\u81ea\u63d0\u5347\u7b56\u7565\uff08\u968f\u673a\u63a9\u7801\u53ca\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u5bf9\u9f50\uff09\u3002", "result": "\u5b9e\u9a8c\u5728NLP\u548cCV\u9886\u57df\u663e\u793aMiMu\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MiMu\u901a\u8fc7\u51cf\u8f7b\u591a\u91cd\u6377\u5f84\u5b66\u4e60\u884c\u4e3a\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.10960", "pdf": "https://arxiv.org/pdf/2504.10960", "abs": "https://arxiv.org/abs/2504.10960", "authors": ["Evagoras Makridis", "Themistoklis Charalambous"], "title": "A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "In this paper, we address the average consensus problem of multi-agent\nsystems for possibly unbalanced and delay-prone networks with directional\ninformation flow. We propose a linear distributed algorithm (referred to as\nRPPAC) that handles asynchronous updates and time-varying heterogeneous\ninformation delays. Our proposed distributed algorithm utilizes a\nsurplus-consensus mechanism and information regarding the number of incoming\nand outgoing links to guarantee state averaging, despite the imbalanced and\ndelayed information flow in directional networks. The convergence of the RPPAC\nalgorithm is examined using key properties of the backward product of\ntime-varying matrices that correspond to different snapshots of the directional\naugmented network.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRPPAC\u7684\u7ebf\u6027\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53ef\u80fd\u4e0d\u5e73\u8861\u548c\u6709\u5ef6\u8fdf\u7684\u65b9\u5411\u7f51\u7edc\u7684\u5e73\u5747\u5171\u8bc6\u95ee\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u65b9\u5411\u7f51\u7edc\u4e2d\u4e0d\u5e73\u8861\u548c\u5ef6\u8fdf\u95ee\u9898\u4e0b\u7684\u5e73\u5747\u5171\u8bc6\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u662f\u63d0\u51faRPPAC\u7b97\u6cd5\uff0c\u5229\u7528\u5269\u4f59\u5171\u8bc6\u673a\u5236\u548c\u94fe\u8def\u4fe1\u606f\uff0c\u5904\u7406\u5f02\u6b65\u66f4\u65b0\u548c\u65f6\u53d8\u5ef6\u8fdf\uff0c\u901a\u8fc7\u65f6\u53d8\u77e9\u9635\u7684\u540e\u5411\u4e58\u79ef\u5206\u6790\u6536\u655b\u6027\u3002", "result": "\u7ed3\u679c\u662f\u7b97\u6cd5\u4fdd\u8bc1\u4e86\u72b6\u6001\u5e73\u5747\uff0c\u5c3d\u7ba1\u5b58\u5728\u4e0d\u5e73\u8861\u548c\u5ef6\u8fdf\u3002", "conclusion": "\u7ed3\u8bba\u662fRPPAC\u7b97\u6cd5\u5728\u65b9\u5411\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5e73\u5747\u5171\u8bc6\u3002"}}
{"id": "2504.10831", "pdf": "https://arxiv.org/pdf/2504.10831", "abs": "https://arxiv.org/abs/2504.10831", "authors": ["Hyojun Ahn", "Seungcheol Oh", "Gyu Seon Kim", "Soyi Jung", "Soohyun Park", "Joongheon Kim"], "title": "Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control", "categories": ["cs.AI", "cs.RO", "68T05"], "comment": null, "summary": "This paper proposes SafeGPT, a two-tiered framework that integrates\ngenerative pretrained transformers (GPTs) with reinforcement learning (RL) for\nefficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In\nthe proposed design, a Global GPT module assigns high-level tasks such as\nsector allocation, while an On-Device GPT manages real-time local route\nplanning. An RL-based safety filter monitors each GPT decision and overrides\nunsafe actions that could lead to battery depletion or duplicate visits,\neffectively mitigating hallucinations. Furthermore, a dual replay buffer\nmechanism helps both the GPT modules and the RL agent refine their strategies\nover time. Simulation results demonstrate that SafeGPT achieves higher delivery\nsuccess rates compared to a GPT-only baseline, while substantially reducing\nbattery consumption and travel distance. These findings validate the efficacy\nof combining GPT-based semantic reasoning with formal safety guarantees,\ncontributing a viable solution for robust and energy-efficient UAV logistics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSafeGPT\u6846\u67b6\uff0c\u7ed3\u5408GPT\u548cRL\u63d0\u5347UAV\u6700\u540e\u4e00\u6bb5\u4ea4\u4ed8\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u65e0\u4eba\u673a(UAV)\u6700\u540e\u4e00\u6bb5\u4ea4\u4ed8\uff0c\u89e3\u51b3GPT\u53ef\u80fd\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "SafeGPT\u91c7\u7528\u4e24\u5c42\u7ed3\u6784\uff1aGlobal GPT\u5206\u914d\u9ad8\u6c34\u5e73\u4efb\u52a1\uff0cOn-Device GPT\u5904\u7406\u5b9e\u65f6\u8def\u7ebf\u89c4\u5212\uff1bRL\u5b89\u5168\u8fc7\u6ee4\u5668\u76d1\u63a7\u5e76\u8986\u76d6\u4e0d\u5b89\u5168\u51b3\u7b56\uff1b\u53cc\u91cd\u56de\u653e\u7f13\u51b2\u533a\u4f18\u5316\u7b56\u7565\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cSafeGPT\u6bd4GPT-only\u57fa\u51c6\u6709\u66f4\u9ad8\u4ea4\u4ed8\u6210\u529f\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u7535\u6c60\u6d88\u8017\u548c\u65c5\u884c\u8ddd\u79bb\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u7ed3\u5408GPT\u8bed\u4e49\u63a8\u7406\u4e0e\u6b63\u5f0f\u5b89\u5168\u4fdd\u8bc1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9c81\u68d2\u8282\u80fd\u7684UAV\u7269\u6d41\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.10552", "pdf": "https://arxiv.org/pdf/2504.10552", "abs": "https://arxiv.org/abs/2504.10552", "authors": ["Arash Torabi Goodarzi", "Roman Kochnev", "Waleed Khalid", "Furui Qin", "Tolgay Atinc Uzun", "Yashkumar Sanjaybhai Dhameliya", "Yash Kanubhai Kathiriya", "Zofia Antonina Bentyn", "Dmitry Ignatov", "Radu Timofte"], "title": "LEMUR Neural Network Dataset: Towards Seamless AutoML", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DL"], "comment": null, "summary": "Neural networks are fundamental in artificial intelligence, driving progress\nin computer vision and natural language processing. High-quality datasets are\ncrucial for their development, and there is growing interest in datasets\ncomposed of neural networks themselves to support benchmarking, automated\nmachine learning (AutoML), and model analysis. We introduce LEMUR, an open\nsource dataset of neural network models with well-structured code for diverse\narchitectures across tasks such as object detection, image classification,\nsegmentation, and natural language processing. LEMUR is primarily designed to\nenable fine-tuning of large language models (LLMs) for AutoML tasks, providing\na rich source of structured model representations and associated performance\ndata. Leveraging Python and PyTorch, LEMUR enables seamless extension to new\ndatasets and models while maintaining consistency. It integrates an\nOptuna-powered framework for evaluation, hyperparameter optimization,\nstatistical analysis, and graphical insights. LEMUR provides an extension that\nenables models to run efficiently on edge devices, facilitating deployment in\nresource-constrained environments. Providing tools for model evaluation,\npreprocessing, and database management, LEMUR supports researchers and\npractitioners in developing, testing, and analyzing neural networks.\nAdditionally, it offers an API that delivers comprehensive information about\nneural network models and their complete performance statistics with a single\nrequest, which can be used in experiments with code-generating large language\nmodels. The LEMUR will be released as an open source project under the MIT\nlicense upon acceptance of the paper.", "AI": {"tldr": "LEMUR \u662f\u4e00\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4ee3\u7801\uff0c\u7528\u4e8e AutoML \u548c\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u8868\u793a\u548c\u6027\u80fd\u6570\u636e\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9\u795e\u7ecf\u7f51\u7edc\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u9700\u652f\u6301\u57fa\u51c6\u6d4b\u8bd5\u3001AutoML \u548c\u6a21\u578b\u5206\u6790\u3002", "method": "\u5f15\u5165 LEMUR\uff0c\u4f7f\u7528 Python \u548c PyTorch \u6784\u5efa\uff0c\u96c6\u6210 Optuna \u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3001\u4f18\u5316\u548c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u542f\u7528 LLM \u5fae\u8c03\u3001\u63d0\u4f9b\u8bc4\u4f30\u5de5\u5177\u3001API \u548c\u8fb9\u8bbe\u5907\u90e8\u7f72\uff0c\u652f\u6301\u6a21\u578b\u5f00\u53d1\u548c\u6027\u80fd\u7edf\u8ba1\u3002", "conclusion": "LEMUR \u5c06\u5f00\u6e90\u53d1\u5e03\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u9ad8\u6548\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002"}}
{"id": "2504.10964", "pdf": "https://arxiv.org/pdf/2504.10964", "abs": "https://arxiv.org/abs/2504.10964", "authors": ["Evagoras Makridis", "Gabriele Oliva", "Kasagatta Ramesh Narahari", "Mohammadreza Doostmohammadian", "Usman A. Khan", "Themistoklis Charalambous"], "title": "Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "In this paper, we address the distributed optimization problem over\nunidirectional networks with possibly time-invariant heterogeneous bounded\ntransmission delays. In particular, we propose a modified version of the\nAccelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein\ncalled Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed\noptimization problem, even when the communication links suffer from\nheterogeneous but bounded transmission delays. We show that if the gradient\nstep-size of the R-ADD-OPT algorithm is within a certain range, which also\ndepends on the maximum time delay in the network, then the nodes are guaranteed\nto converge to the optimal solution of the distributed optimization problem.\nThe range of the gradient step-size that guarantees convergence can be computed\na priori based on the maximum time delay in the network.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faR-ADD-OPT\u7b97\u6cd5\uff0c\u5904\u7406\u5355\u5411\u7f51\u7edc\u4e2d\u5f02\u8d28\u6709\u754c\u5ef6\u8fdf\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4fdd\u8bc1\u9002\u5f53\u6b65\u957f\u4e0b\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002", "motivation": "\u89e3\u51b3\u5355\u5411\u7f51\u7edc\u4e2d\u53ef\u80fd\u5b58\u5728\u5f02\u8d28\u4f46\u6709\u754c\u4f20\u8f93\u5ef6\u8fdf\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faRobustified ADD-OPT (R-ADD-OPT)\u7b97\u6cd5\u7684\u4fee\u6539\u7248\u672c\uff0c\u4ee5\u9002\u5e94\u4f20\u8f93\u5ef6\u8fdf\u3002", "result": "\u68af\u5ea6\u6b65\u957f\u5728\u7279\u5b9a\u8303\u56f4\u5185\uff08\u53d6\u51b3\u4e8e\u6700\u5927\u5ef6\u8fdf\uff09\u53ef\u4fdd\u8bc1\u8282\u70b9\u6536\u655b\u5230\u6700\u4f18\u89e3\uff0c\u4e14\u8303\u56f4\u53ef\u9884\u5148\u8ba1\u7b97\u3002", "conclusion": "R-ADD-OPT\u7b97\u6cd5\u5728\u6709\u5ef6\u8fdf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f18\u5316\u7684\u53ef\u9760\u6536\u655b\u3002"}}
{"id": "2504.10865", "pdf": "https://arxiv.org/pdf/2504.10865", "abs": "https://arxiv.org/abs/2504.10865", "authors": ["Han-Dong Lim", "Donghwan Lee"], "title": "Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration", "categories": ["cs.AI", "cs.LG"], "comment": "Initial submission", "summary": "In this paper, we study the theoretical properties of the projected Bellman\nequation (PBE) and two algorithms to solve this equation: linear Q-learning and\napproximate value iteration (AVI). We consider two sufficient conditions for\nthe existence of a solution to PBE : strictly negatively row dominating\ndiagonal (SNRDD) assumption and a condition motivated by the convergence of\nAVI. The SNRDD assumption also ensures the convergence of linear Q-learning,\nand its relationship with the convergence of AVI is examined. Lastly, several\ninteresting observations on the solution of PBE are provided when using\n$\\epsilon$-greedy policy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6295\u5f71Bellman\u65b9\u7a0b\u7684\u7406\u8bba\u5c5e\u6027\u3001\u7ebf\u6027Q\u5b66\u4e60\u548c\u8fd1\u4f3c\u503c\u8fed\u4ee3\u7b97\u6cd5\uff0c\u63a2\u8ba8\u89e3\u7684\u5b58\u5728\u6027\u548c\u6536\u655b\u6761\u4ef6\u3002", "motivation": "\u52a8\u673a\u662f\u5206\u6790\u6295\u5f71Bellman\u65b9\u7a0b\u89e3\u7684\u5b58\u5728\u4ee5\u53ca\u7b97\u6cd5\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u4e25\u683c\u8d1f\u884c\u4e3b\u5bfc\u5bf9\u89d2\u5047\u8bbe\u548cAVI\u6536\u655b\u6761\u4ef6\uff0c\u68c0\u67e5\u7ebf\u6027Q\u5b66\u4e60\u7684\u6536\u655b\uff0c\u5e76\u89c2\u5bdf\u03b5-\u8d2a\u5a6a\u7b56\u7565\u4e0b\u7684\u89e3\u3002", "result": "\u7ed3\u679c\u663e\u793aSNRDD\u5047\u8bbe\u786e\u4fdd\u7ebf\u6027Q\u5b66\u4e60\u7684\u6536\u655b\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0eAVI\u6536\u655b\u7684\u5173\u7cfb\uff0c\u63d0\u4f9b\u76f8\u5173\u89c2\u5bdf\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5bf9\u6295\u5f71Bellman\u65b9\u7a0b\u7406\u8bba\u6027\u8d28\u7684\u6df1\u5165\u7406\u89e3\u548c\u7b97\u6cd5\u6539\u8fdb\u7684\u542f\u793a\u3002"}}
{"id": "2504.10555", "pdf": "https://arxiv.org/pdf/2504.10555", "abs": "https://arxiv.org/abs/2504.10555", "authors": ["Marco Salm\u00e8", "Lorenzo Tronchin", "Rosa Sicilia", "Paolo Soda", "Valerio Guarrasi"], "title": "Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Data scarcity remains a critical bottleneck impeding technological\nadvancements across various domains, including but not limited to medicine and\nprecision agriculture. To address this challenge, we explore the potential of\nDeep Generative Models (DGMs) in producing synthetic data that satisfies the\nGenerative Learning Trilemma: fidelity, diversity, and sampling efficiency.\nHowever, recognizing that these criteria alone are insufficient for practical\napplications, we extend the trilemma to include utility, robustness, and\nprivacy, factors crucial for ensuring the applicability of DGMs in real-world\nscenarios. Evaluating these metrics becomes particularly challenging in\ndata-scarce environments, as DGMs traditionally rely on large datasets to\nperform optimally. This limitation is especially pronounced in domains like\nmedicine and precision agriculture, where ensuring acceptable model performance\nunder data constraints is vital. To address these challenges, we assess the\nGenerative Learning Trilemma in data-scarcity settings using state-of-the-art\nevaluation metrics, comparing three prominent DGMs: Variational Autoencoders\n(VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs).\nFurthermore, we propose a comprehensive framework to assess utility,\nrobustness, and privacy in synthetic data generated by DGMs. Our findings\ndemonstrate varying strengths among DGMs, with each model exhibiting unique\nadvantages based on the application context. This study broadens the scope of\nthe Generative Learning Trilemma, aligning it with real-world demands and\nproviding actionable guidance for selecting DGMs tailored to specific\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u5e94\u7528\uff0c\u6269\u5c55\u751f\u6210\u5b66\u4e60\u4e09\u96be\u56f0\u5883\u5e76\u8bc4\u4f30VAE\u3001GAN\u548cDM\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u7a00\u7f3a\u963b\u788d\u4e86\u533b\u5b66\u548c\u7cbe\u51c6\u519c\u4e1a\u7b49\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\uff0c\u9700\u8981\u5408\u6210\u6570\u636e\u89e3\u51b3\u751f\u6210\u5b66\u4e60\u4e09\u96be\u56f0\u5883\u7684\u9650\u5236\u3002", "method": "\u8bc4\u4f30VAE\u3001GAN\u548cDM\u5728\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u91c7\u6837\u6548\u7387\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u6846\u67b6\u8bc4\u4f30\u5408\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u9690\u79c1\u6027\u3002", "result": "\u4e0d\u540cDGM\u6a21\u578b\u663e\u793a\u51fa\u5e94\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u72ec\u7279\u4f18\u52bf\u3002", "conclusion": "\u6269\u5c55\u751f\u6210\u5b66\u4e60\u4e09\u96be\u56f0\u5883\uff0c\u63d0\u4f9b\u9009\u62e9DGM\u6a21\u578b\u7684\u6307\u5bfc\uff0c\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u3002"}}
{"id": "2504.11097", "pdf": "https://arxiv.org/pdf/2504.11097", "abs": "https://arxiv.org/abs/2504.11097", "authors": ["Maximilian B\u00f6hle", "Bernhard Schick", "Steffen M\u00fcller"], "title": "Steering Feedback in Dynamic Driving Simulators: Road-Induced and Non-Road-Induced Harshness", "categories": ["eess.SY", "cs.SY"], "comment": "11 pages, 5 figures, 5 tables, submitted to the IEEE Transactions on\n  Intelligent Vehicles. arXiv admin note: substantial text overlap with\n  arXiv:2403.17800", "summary": "Steering feedback plays a substantial role in the validity of driving\nsimulators for the virtual development of modern vehicles. Established\nobjective steering characteristics typically assess the feedback behavior in\nthe frequency range of up to 30 Hz while factors such as steering wheel and\nvehicle body vibrations at higher frequencies are mainly approached as comfort\nissues. This work investigates the influence of steering wheel and vehicle body\nexcitations in the frequency range between 30 and 100 Hz on the subjective\nevaluation of steering feedback in a dynamic driving simulator. A controlled\nsubject study with 42 participants was performed to compare a reference vehicle\nwith an electrical power steering system to four variants of its virtual\nrepresentation on a dynamic driving simulator. The effects of road-induced\nexcitations were investigated by comparing a semi-empirical and a physics-based\ntire model, while the influence of non-road-induced excitations was\ninvestigated by implementing engine and wheel orders. The simulator variants\nwere evaluated in comparison to the reference vehicle during closed-loop\ndriving on a country road in a single-blind within-subjects design. The\nsubjective evaluation focused on the perception of road feedback compared to\nthe reference vehicle. The statistical analysis of subjective results shows\nthat there is a strong effect of non-road-induced steering and vehicle body\nexcitations, while the effect of road-induced excitations is considerably less\npronounced.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e8630-100 Hz\u9891\u7387\u8303\u56f4\u5185\u8f6c\u5411\u8f6e\u548c\u8f66\u8eab\u6fc0\u52b1\u5bf9\u52a8\u6001\u9a7e\u9a76\u6a21\u62df\u5668\u4e3b\u89c2\u8f6c\u5411\u53cd\u9988\u7684\u5f71\u54cd\uff0c\u901a\u8fc742\u540d\u53c2\u4e0e\u8005\u7684\u5bf9\u7167\u7814\u7a76\uff0c\u53d1\u73b0\u975e\u9053\u8def\u8bf1\u53d1\u6fc0\u52b1\u5f71\u54cd\u663e\u8457\uff0c\u800c\u9053\u8def\u8bf1\u53d1\u6fc0\u52b1\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u52a8\u673a\u662f\u63a2\u8ba8\u66f4\u9ad8\u9891\u7387\uff0830-100 Hz\uff09\u8f6c\u5411\u53cd\u9988\u5bf9\u4e3b\u89c2\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f4e\u4e8e30 Hz\u7684\u9891\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u52a8\u6001\u9a7e\u9a76\u6a21\u62df\u5668\u8fdb\u884c\u5355\u76f2\u5185\u90e8\u53d7\u8bd5\u8005\u8bbe\u8ba1\u7684\u7814\u7a76\uff0c\u6bd4\u8f83\u53c2\u8003\u8f66\u8f86\u548c\u56db\u79cd\u6a21\u62df\u5668\u53d8\u4f53\uff0c\u91c7\u7528\u534a\u7ecf\u9a8c\u548c\u57fa\u4e8e\u7269\u7406\u7684\u8f6e\u80ce\u6a21\u578b\uff0c\u5e76\u6dfb\u52a0\u53d1\u52a8\u673a\u548c\u8f66\u8f6e\u9636\u6b21\u6fc0\u52b1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u975e\u9053\u8def\u8bf1\u53d1\u6fc0\u52b1\u5bf9\u4e3b\u89c2\u8f6c\u5411\u53cd\u9988\u6709\u5f3a\u70c8\u5f71\u54cd\uff0c\u800c\u9053\u8def\u8bf1\u53d1\u6fc0\u52b1\u7684\u5f71\u54cd\u8f83\u4e0d\u663e\u8457\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u975e\u9053\u8def\u8bf1\u53d1\u6fc0\u52b1\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u5f00\u53d1\u4e2d\u4f18\u5148\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u3002"}}
{"id": "2504.10893", "pdf": "https://arxiv.org/pdf/2504.10893", "abs": "https://arxiv.org/abs/2504.10893", "authors": ["Yize Zhang", "Tianshu Wang", "Sirui Chen", "Kun Wang", "Xingyu Zeng", "Hongyu Lin", "Xianpei Han", "Le Sun", "Chaochao Lu"], "title": "ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search", "categories": ["cs.AI", "cs.CL"], "comment": "Project homepage: https://opencausalab.github.io/ARise", "summary": "Large language models (LLMs) have demonstrated impressive capabilities and\nare receiving increasing attention to enhance their reasoning through scaling\ntest--time compute. However, their application in open--ended,\nknowledge--intensive, complex reasoning scenarios is still limited.\nReasoning--oriented methods struggle to generalize to open--ended scenarios due\nto implicit assumptions of complete world knowledge. Meanwhile,\nknowledge--augmented reasoning (KAR) methods fail to address two core\nchallenges: 1) error propagation, where errors in early steps cascade through\nthe chain, and 2) verification bottleneck, where the explore--exploit tradeoff\narises in multi--branch decision processes. To overcome these limitations, we\nintroduce ARise, a novel framework that integrates risk assessment of\nintermediate reasoning states with dynamic retrieval--augmented generation\n(RAG) within a Monte Carlo tree search paradigm. This approach enables\neffective construction and optimization of reasoning plans across multiple\nmaintained hypothesis branches. Experimental results show that ARise\nsignificantly outperforms the state--of--the--art KAR methods by up to 23.10%,\nand the latest RAG-equipped large reasoning models by up to 25.37%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARise\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u52a8\u6001RAG\u548cMonte Carlo\u6811\u641c\u7d22\u4f18\u5316LLMs\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u5f00\u653e\u5f0f\u77e5\u8bc6\u5bc6\u96c6\u63a8\u7406\u4e2d\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u95ee\u9898\u3001\u9519\u8bef\u4f20\u64ad\u548c\u9a8c\u8bc1\u74f6\u9888\u3002", "method": "ARise\u6846\u67b6\u6574\u5408\u4e2d\u95f4\u72b6\u6001\u98ce\u9669\u8bc4\u4f30\u3001\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548cMonte Carlo\u6811\u641c\u7d22\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aARise\u6bd4\u6700\u5148\u8fdbKAR\u65b9\u6cd5\u63d0\u534723.10%\uff0c\u6bd4RAG\u589e\u5f3a\u6a21\u578b\u63d0\u534725.37%\u3002", "conclusion": "ARise\u6846\u67b6\u6709\u6548\u89e3\u51b3\u63a8\u7406\u5c40\u9650\u6027\uff0c\u4f18\u5316\u591a\u5206\u652f\u5047\u8bbe\u63a8\u7406\u8ba1\u5212\u3002"}}
{"id": "2504.10556", "pdf": "https://arxiv.org/pdf/2504.10556", "abs": "https://arxiv.org/abs/2504.10556", "authors": ["Lucas Heublein", "Simon Kocher", "Tobias Feigl", "Alexander R\u00fcgamer", "Christopher Mutschler", "Felix Ott"], "title": "VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "94-05, 82-11", "E.0; I.2.0; I.5.4; I.5.1"], "comment": "7 pages, 9 figures", "summary": "Distributed learning and Edge AI necessitate efficient data processing,\nlow-latency communication, decentralized model training, and stringent data\nprivacy to facilitate real-time intelligence on edge devices while reducing\ndependency on centralized infrastructure and ensuring high model performance.\nIn the context of global navigation satellite system (GNSS) applications, the\nprimary objective is to accurately monitor and classify interferences that\ndegrade system performance in distributed environments, thereby enhancing\nsituational awareness. To achieve this, machine learning (ML) models can be\ndeployed on low-resource devices, ensuring minimal communication latency and\npreserving data privacy. The key challenge is to compress ML models while\nmaintaining high classification accuracy. In this paper, we propose variational\nautoencoders (VAEs) for disentanglement to extract essential latent features\nthat enable accurate classification of interferences. We demonstrate that the\ndisentanglement approach can be leveraged for both data compression and data\naugmentation by interpolating the lower-dimensional latent representations of\nsignal power. To validate our approach, we evaluate three VAE variants -\nvanilla, factorized, and conditional generative - on four distinct datasets,\nincluding two collected in controlled indoor environments and two real-world\nhighway datasets. Additionally, we conduct extensive hyperparameter searches to\noptimize performance. Our proposed VAE achieves a data compression rate ranging\nfrom 512 to 8,192 and achieves an accuracy up to 99.92%.", "AI": {"tldr": "\u672c\u8bba\u6587\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5b9e\u73b0GNSS\u5e72\u6270\u5206\u7c7b\u7684\u6a21\u578b\u538b\u7f29\u548c\u6570\u636e\u589e\u5f3a\uff0c\u8fbe\u5230\u9ad8\u8fbe99.92%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u5b66\u4e60\u548cEdge AI\u9700\u8981\u9ad8\u6548\u6570\u636e\u5904\u7406\u3001\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u3001\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u548c\u6570\u636e\u9690\u79c1\uff1b\u5728GNSS\u5e94\u7528\u4e2d\uff0c\u9700\u51c6\u786e\u76d1\u63a7\u548c\u5206\u7c7b\u5e72\u6270\u4ee5\u63d0\u5347 situational awareness\uff0c\u6311\u6218\u662f\u538b\u7f29ML\u6a21\u578b\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faVAE\u7684disentanglement\u65b9\u6cd5\u63d0\u53d6latent features\uff0c\u7528\u4e8e\u5e72\u6270\u5206\u7c7b\uff1b\u5305\u62ecvanilla\u3001factorized\u548cconditional generative\u4e09\u79cd\u53d8\u4f53\uff1b\u7528\u4e8e\u6570\u636e\u538b\u7f29\u548c\u589e\u5f3a\uff0c\u901a\u8fc7latent\u8868\u793a\u63d2\u503c\uff1b\u8bc4\u4f30\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u6570\u636e\u538b\u7f29\u7387\u4ece512\u52308192\uff1b\u51c6\u786e\u7387\u9ad8\u8fbe99.92%\u3002", "conclusion": "VAE\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u538b\u7f29\u7387\u548c\u9ad8\u51c6\u786e\u7387\u7684\u5e72\u6270\u5206\u7c7b\uff0c\u8bc1\u660e\u4e86\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2504.11125", "pdf": "https://arxiv.org/pdf/2504.11125", "abs": "https://arxiv.org/abs/2504.11125", "authors": ["Dieter Teichrib", "Moritz Schulze Darup"], "title": "A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances", "categories": ["eess.SY", "cs.SY", "math.OC"], "comment": "8 pages, 3 figures, to be published in the proceedings of the 23rd\n  European Control Conference (2025)", "summary": "We present a method for representing the closed-loop dynamics of piecewise\naffine (PWA) systems with bounded additive disturbances and neural\nnetwork-based controllers as mixed-integer (MI) linear constraints. We show\nthat such representations enable the computation of robustly positively\ninvariant (RPI) sets for the specified system class by solving MI linear\nprograms. These RPI sets can subsequently be used to certify stability and\nconstraint satisfaction. Furthermore, the approach allows to handle non-linear\nsystems based on suitable PWA approximations and corresponding error bounds,\nwhich can be interpreted as the bounded disturbances from above.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u7ea6\u675f\u8868\u793a\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\u53ca\u5176\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u95ed\u73af\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u8ba1\u7b97\u9c81\u68d2\u6b63\u4e0d\u53d8\u96c6\uff0c\u4ee5\u9a8c\u8bc1\u7a33\u5b9a\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\u3002", "motivation": "\u4e3a\u4e86\u5904\u7406\u5e26\u6709\u8fb9\u754c\u6270\u52a8\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9c81\u68d2\u7a33\u5b9a\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\u7684\u8ba4\u8bc1\u3002", "method": "\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u8868\u793a\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u6df7\u5408\u6574\u6570\u7ebf\u6027\u7a0b\u5e8f\u8ba1\u7b97\u9c81\u68d2\u6b63\u4e0d\u53d8\u96c6\u3002", "result": "\u80fd\u591f\u8ba1\u7b97\u9c81\u68d2\u6b63\u4e0d\u53d8\u96c6\uff0c\u8ba4\u8bc1\u7a33\u5b9a\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u901a\u8fc7\u5206\u6bb5\u4eff\u5c04\u903c\u8fd1\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u53ca\u5176\u8bef\u5dee\u8fb9\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6307\u5b9a\u7cfb\u7edf\u7c7b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9c81\u68d2\u6b63\u4e0d\u53d8\u96c6\u7684\u6846\u67b6\uff0c\u4ece\u800c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\u3002"}}
{"id": "2504.11075", "pdf": "https://arxiv.org/pdf/2504.11075", "abs": "https://arxiv.org/abs/2504.11075", "authors": ["Dongmin Kim", "Hoshinori Kanazawa", "Naoto Yoshida", "Yasuo Kuniyoshi"], "title": "Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior", "categories": ["cs.AI", "68T05, 68T40, 68T42", "I.2.0; I.2.6; I.2.9"], "comment": "20 pages, Code is available at\n  https://github.com/kim135797531/self-prior", "summary": "Infants often exhibit goal-directed behaviors, such as reaching for a sensory\nstimulus, even when no external reward criterion is provided. These\nintrinsically motivated behaviors facilitate spontaneous exploration and\nlearning of the body and environment during early developmental stages.\nAlthough computational modeling can offer insight into the mechanisms\nunderlying such behaviors, many existing studies on intrinsic motivation focus\nprimarily on how exploration contributes to acquiring external rewards. In this\npaper, we propose a novel density model for an agent's own multimodal sensory\nexperiences, called the \"self-prior,\" and investigate whether it can\nautonomously induce goal-directed behavior. Integrated within an active\ninference framework based on the free energy principle, the self-prior\ngenerates behavioral references purely from an intrinsic process that minimizes\nmismatches between average past sensory experiences and current observations.\nThis mechanism is also analogous to the acquisition and utilization of a body\nschema through continuous interaction with the environment. We examine this\napproach in a simulated environment and confirm that the agent spontaneously\nreaches toward a tactile stimulus. Our study implements intrinsically motivated\nbehavior shaped by the agent's own sensory experiences, demonstrating the\nspontaneous emergence of intentional behavior during early development.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa'self-prior'\u5bc6\u5ea6\u6a21\u578b\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8fc7\u53bb\u548c\u5f53\u524d\u611f\u5b98\u4f53\u9a8c\u5dee\u5f02\uff0c\u8bf1\u5bfc\u4ee3\u7406\u5185\u5728\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff1b\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u81ea\u53d1\u4f38\u624b\u89e6\u78b0\u89e6\u89c9\u523a\u6fc0\uff0c\u5c55\u793a\u4e86\u5185\u5728\u52a8\u673a\u884c\u4e3a\u7684\u6d8c\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u63a2\u7d22\u5982\u4f55\u83b7\u5f97\u5916\u90e8\u5956\u52b1\uff0c\u672c\u8bba\u6587\u63a2\u8ba8\u7eaf\u5185\u5728\u8fc7\u7a0b\u5982\u4f55\u9a71\u52a8\u65e0\u5916\u90e8\u5956\u52b1\u7684 spontaneous \u63a2\u7d22\u548c\u5b66\u4e60\u3002", "method": "\u63d0\u51fa'self-prior'\u591a\u6a21\u6001\u611f\u5b98\u4f53\u9a8c\u5bc6\u5ea6\u6a21\u578b\uff0c\u5e76\u6574\u5408\u5230\u57fa\u4e8e\u81ea\u7531\u80fd\u91cf\u539f\u7406\u7684\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e2d\uff1b\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u4ee3\u7406\u81ea\u53d1\u5730\u4f38\u624b\u89e6\u78b0\u89e6\u89c9\u523a\u6fc0\uff0c\u8bc1\u5b9e\u4e86\u5185\u5728\u52a8\u673a\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c55\u793a\u4e86\u4ee3\u7406\u901a\u8fc7\u81ea\u8eab\u611f\u5b98\u4f53\u9a8c\u5851\u9020\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff0c\u7c7b\u4f3c\u4e8e\u5a74\u513f\u65e9\u671f\u53d1\u5c55\u7684 intentional \u884c\u4e3a\u6d8c\u73b0\u3002"}}
{"id": "2504.10559", "pdf": "https://arxiv.org/pdf/2504.10559", "abs": "https://arxiv.org/abs/2504.10559", "authors": ["Keyu Duan", "Zichen Liu", "Xin Mao", "Tianyu Pang", "Changyu Chen", "Qiguang Chen", "Michael Qizhe Shieh", "Longxu Dou"], "title": "Efficient Process Reward Model Training via Active Learning", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.", "AI": {"tldr": "ActPRM \u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11 50% \u7684\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u7279\u5b9a\u57fa\u51c6\u4e0a\u8fbe\u5230 SOTA\u3002", "motivation": "\u8bad\u7ec3\u6570\u636e\u6807\u6ce8\u7684\u89c4\u6a21\u5316\u5bf9\u4eba\u7c7b\u548c LLM \u90fd\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u63d0\u51fa ActPRM \u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5 ActPRM\uff0c\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u6700\u9ad8\u7684\u6837\u672c\uff0c\u4f7f\u7528 PRM \u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u7531\u9ad8\u6210\u672c\u7684\u63a8\u7406\u6a21\u578b\u6807\u6ce8\u6570\u636e\uff0c\u7136\u540e\u8ba1\u7b97\u635f\u5931\u5e76\u66f4\u65b0 PRM \u7684\u6743\u91cd\u3002", "result": "\u51cf\u5c11 50% \u7684\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff1b\u901a\u8fc7\u8fc7\u6ee4\u8d85\u8fc7 100 \u4e07\u7684\u6570\u5b66\u63a8\u7406\u8f68\u8ff9\uff0c\u4fdd\u7559 60% \u7684\u6570\u636e\uff0c\u5728 ProcessBench (75.0%) \u548c PRMBench (65.5%) \u4e0a\u8fbe\u5230\u65b0\u7684 SOTA\u3002", "conclusion": "ActPRM \u5728\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u548c\u63d0\u9ad8\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5b9e\u73b0\u4e86\u65b0\u7684\u72b6\u6001-of-the-art \u7ed3\u679c\u3002"}}
{"id": "2504.11261", "pdf": "https://arxiv.org/pdf/2504.11261", "abs": "https://arxiv.org/abs/2504.11261", "authors": ["Hannes Petrenz", "Johannes K\u00f6hler", "Francesco Borrelli"], "title": "Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning", "categories": ["eess.SY", "cs.SY", "math.OC"], "comment": "Github link to the example:\n  https://github.com/HannesPetrenz/RALMPC_Linear_Uncertain_Systems", "summary": "This paper presents a robust adaptive learning Model Predictive Control (MPC)\nframework for linear systems with parametric uncertainties and additive\ndisturbances performing iterative tasks. The approach iteratively refines the\nparameter estimates using set membership estimation. Performance enhancement\nover iterations is achieved by learning the terminal cost from data. Safety is\nenforced using a terminal set, which is also learned iteratively. The proposed\nmethod guarantees recursive feasibility, constraint satisfaction, and a robust\nbound on the closed-loop cost. Numerical simulations on a mass-spring-damper\nsystem demonstrate improved computational efficiency and control performance\ncompared to an existing robust adaptive MPC approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u81ea\u9002\u5e94\u5b66\u4e60MPC\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u548c\u6270\u52a8\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u6539\u8fdb\u3002", "motivation": "\u9488\u5bf9\u7ebf\u6027\u7cfb\u7edf\u5728\u8fed\u4ee3\u4efb\u52a1\u4e2d\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u548c\u52a0\u6027\u6270\u52a8\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u63a7\u5236\u6027\u80fd\u548c\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u96c6\u5408\u6210\u5458\u4f30\u8ba1\u8fed\u4ee3\u6539\u8fdb\u53c2\u6570\u4f30\u8ba1\uff0c\u4ece\u6570\u636e\u5b66\u4e60\u7ec8\u7aef\u6210\u672c\u548c\u7ec8\u7aef\u96c6\uff0c\u786e\u4fdd\u9012\u5f52\u53ef\u884c\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\u3002", "result": "\u4fdd\u8bc1\u9012\u5f52\u53ef\u884c\u6027\u3001\u7ea6\u675f\u6ee1\u8db3\u548c\u95ed\u73af\u6210\u672c\u9c81\u68d2\u754c\u9650\uff1b\u4eff\u771f\u663e\u793a\u8ba1\u7b97\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u7c7b\u4f3c\u7cfb\u7edf\u3002"}}
{"id": "2504.11159", "pdf": "https://arxiv.org/pdf/2504.11159", "abs": "https://arxiv.org/abs/2504.11159", "authors": ["Annemarie Jutte", "Faizan Ahmed", "Jeroen Linssen", "Maurice van Keulen"], "title": "C-SHAP for time series: An approach to high-level temporal explanations", "categories": ["cs.AI"], "comment": "10 pages, 6 figures", "summary": "Time series are ubiquitous in domains such as energy forecasting, healthcare,\nand industry. Using AI systems, some tasks within these domains can be\nefficiently handled. Explainable AI (XAI) aims to increase the reliability of\nAI solutions by explaining model reasoning. For time series, many XAI methods\nprovide point- or sequence-based attribution maps. These methods explain model\nreasoning in terms of low-level patterns. However, they do not capture\nhigh-level patterns that may also influence model reasoning. We propose a\nconcept-based method to provide explanations in terms of these high-level\npatterns. In this paper, we present C-SHAP for time series, an approach which\ndetermines the contribution of concepts to a model outcome. We provide a\ngeneral definition of C-SHAP and present an example implementation using time\nseries decomposition. Additionally, we demonstrate the effectiveness of the\nmethodology through a use case from the energy domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faC-SHAP\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e\u6982\u5ff5\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217AI\u6a21\u578b\uff0c\u63d0\u4f9b\u9ad8\u7ea7\u6a21\u5f0f\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u4ec5\u89e3\u91ca\u4f4e\u7ea7\u6a21\u5f0f\uff0c\u5ffd\u7565\u9ad8\u6c34\u5e73\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6982\u5ff5-based\u65b9\u6cd5\u63d0\u5347AI\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faC-SHAP\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4f5c\u4e3a\u793a\u4f8b\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u80fd\u6e90\u9886\u57df\u7528\u4f8b\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "C-SHAP\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217AI\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6355\u6349\u9ad8\u6c34\u5e73\u6a21\u5f0f\u3002"}}
{"id": "2504.10561", "pdf": "https://arxiv.org/pdf/2504.10561", "abs": "https://arxiv.org/abs/2504.10561", "authors": ["Runqing Wu", "Fei Ye", "Rongyao Hu", "Guoxi Huang"], "title": "Self-Controlled Dynamic Expansion Model for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 3 figures, 6 tables, Continual Learning, Cross-Domain\n  Continual Learning, Mixture Model", "summary": "Continual Learning (CL) epitomizes an advanced training paradigm wherein\nprior data samples remain inaccessible during the acquisition of new tasks.\nNumerous investigations have delved into leveraging a pre-trained Vision\nTransformer (ViT) to enhance model efficacy in continual learning. Nonetheless,\nthese approaches typically utilize a singular, static backbone, which\ninadequately adapts to novel tasks, particularly when engaging with diverse\ndata domains, due to a substantial number of inactive parameters. This paper\naddresses this limitation by introducing an innovative Self-Controlled Dynamic\nExpansion Model (SCDEM), which orchestrates multiple distinct trainable\npre-trained ViT backbones to furnish diverse and semantically enriched\nrepresentations. Specifically, by employing the multi-backbone architecture as\na shared module, the proposed SCDEM dynamically generates a new expert with\nminimal parameters to accommodate a new task. A novel Collaborative\nOptimization Mechanism (COM) is introduced to synergistically optimize multiple\nbackbones by harnessing prediction signals from historical experts, thereby\nfacilitating new task learning without erasing previously acquired knowledge.\nAdditionally, a novel Feature Distribution Consistency (FDC) approach is\nproposed to align semantic similarity between previously and currently learned\nrepresentations through an optimal transport distance-based mechanism,\neffectively mitigating negative knowledge transfer effects. Furthermore, to\nalleviate over-regularization challenges, this paper presents a novel Dynamic\nLayer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the\npenalization intensity on each trainable representation layer. An extensive\nseries of experiments have been conducted to evaluate the proposed\nmethodology's efficacy, with empirical results corroborating that the approach\nattains state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u63a7\u52a8\u6001\u6269\u5c55\u6a21\u578b\uff08SCDEM\uff09\u6765\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u901a\u8fc7\u591a\u9aa8\u5e72\u7f51\u52a8\u6001\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4f18\u5316\u673a\u5236\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u9759\u6001Vision Transformer\u9aa8\u5e72\u7f51\uff0c\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u6570\u636e\u57df\uff0c\u5bfc\u81f4\u53c2\u6570\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u5f15\u5165SCDEM\u6a21\u578b\uff0c\u4f7f\u7528\u591aViT\u9aa8\u5e72\u7f51\u52a8\u6001\u751f\u6210\u65b0\u4e13\u5bb6\uff0c\u7ed3\u5408\u534f\u4f5c\u4f18\u5316\u673a\u5236\uff08COM\uff09\u3001\u7279\u5f81\u5206\u5e03\u4e00\u81f4\u6027\uff08FDC\uff09\u53ca\u52a8\u6001\u5c42\u7ea7\u7279\u5f81\u6ce8\u610f\u529b\u673a\u5236\uff08DLWFAM\uff09\u6765\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9002\u5e94\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.11285", "pdf": "https://arxiv.org/pdf/2504.11285", "abs": "https://arxiv.org/abs/2504.11285", "authors": ["Hazem Abdel-Khalek", "Eddy Jalbout", "Caspar Schau\u00df", "Benjamin Pfluger"], "title": "Balancing hydrogen delivery in national energy systems: impact of the temporal flexibility of hydrogen delivery on export prices", "categories": ["eess.SY", "cs.SY"], "comment": "6 pages, 3 figures, 2 tables", "summary": "Hydrogen is expected to play a key role in the energy transition. Analyses\nexploring the price of hydrogen usually calculate average or marginal\nproduction costs regardless of the time of delivery. A key factor that affects\nthe price of hydrogen is the balancing costs, which we define as the expense of\nensuring a steady schedule of hydrogen delivery. We explore the effect of\ndelivering hydrogen to the export ports at different schedules, ranging from\nfully flexible to moderately stable with a daily and weekly buffer, to fully\nstable. We quantify the rise in hydrogen price with strict balancing constraint\nin three countries: Brazil, Morocco and Turkey, and three export volumes: 10,\n50 and 200 TWh. The price difference between the flexible and stable schedules\nwas found to reach a maximum of 36% in Brazil, 47% in Morocco and 18% in Turkey\nacross the different export volumes.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7814\u7a76\u5e73\u8861\u6210\u672c\u5982\u4f55\u5f71\u54cd\u6c22\u6c14\u4ef7\u683c\uff0c\u9488\u5bf9\u4e0d\u540c\u4ea4\u4ed8\u65f6\u95f4\u8868\uff0c\u5728\u5df4\u897f\u3001\u6469\u6d1b\u54e5\u548c\u571f\u8033\u5176\u53d1\u73b0\u4ef7\u683c\u4e0a\u6da8\u6700\u9ad8\u53ef\u8fbe47%\u3002", "motivation": "\u6c22\u6c14\u5728\u80fd\u6e90\u8f6c\u578b\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u73b0\u6709\u7684\u5206\u6790\u901a\u5e38\u5ffd\u7565\u4ea4\u4ed8\u65f6\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u5e73\u8861\u6210\u672c\u5bf9\u4ef7\u683c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u4ece\u5b8c\u5168\u7075\u6d3b\u5230\u5b8c\u5168\u7a33\u5b9a\u7684\u6c22\u6c14\u4ea4\u4ed8\u65f6\u95f4\u8868\uff0c\u5728\u5df4\u897f\u3001\u6469\u6d1b\u54e5\u548c\u571f\u8033\u5176\u4e09\u4e2a\u56fd\u5bb6\uff0c\u4ee5\u53ca10\u300150\u548c200 TWh\u4e09\u79cd\u51fa\u53e3\u91cf\u4e0b\u8ba1\u7b97\u4ef7\u683c\u5dee\u5f02\u3002", "result": "\u4ef7\u683c\u5dee\u5f02\u5728\u5df4\u897f\u6700\u9ad8\u8fbe36%\uff0c\u6469\u6d1b\u54e547%\uff0c\u571f\u8033\u517618%\uff0c\u8de8\u4e0d\u540c\u51fa\u53e3\u91cf\u3002", "conclusion": "\u66f4\u4e25\u683c\u7684\u5e73\u8861\u7ea6\u675f\u663e\u8457\u589e\u52a0\u6c22\u6c14\u4ef7\u683c\uff0c\u5f3a\u8c03\u5728\u80fd\u6e90\u8f6c\u578b\u5206\u6790\u4e2d\u5fc5\u987b\u8003\u8651\u4ea4\u4ed8\u65f6\u95f4\u8868\u3002"}}
{"id": "2504.11190", "pdf": "https://arxiv.org/pdf/2504.11190", "abs": "https://arxiv.org/abs/2504.11190", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u903b\u8f91\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u8bed\u4e49\u77e5\u8bc6\u56fe\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9690\u55bb\u68c0\u6d4b\u548c\u7406\u89e3\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u4ece\u81ea\u7136\u8bed\u8a00\u4e2d\u63d0\u53d6\u9690\u542b\u77e5\u8bc6\uff0c\u56e0\u6b64\u4f7f\u7528\u8bed\u4e49\u77e5\u8bc6\u56fe\u4f5c\u4e3a\u6982\u5ff5\u7a7a\u95f4\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5e94\u7528\u903b\u8f91\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u77e5\u8bc6\u56fe\u548c\u63d0\u793a\u542f\u53d1\u5f0f\uff0c\u751f\u6210\u9690\u542b\u7684\u7c7b\u6bd4\u8fde\u63a5\uff0c\u5e76\u7528\u4e8e\u9690\u55bb\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u65b9\u6cd5\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5728\u89c6\u89c9\u9690\u55bb\u7406\u89e3\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5bf9\u9886\u57df\u7279\u5b9a\u9690\u55bb\u6709\u5c40\u9650\uff0c\u5e76\u8fdb\u884c\u4e86\u9519\u8bef\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5efa\u8bae\u6539\u8fdb\u9690\u55bb\u6ce8\u91ca\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2504.10612", "pdf": "https://arxiv.org/pdf/2504.10612", "abs": "https://arxiv.org/abs/2504.10612", "authors": ["Michal Balcerak", "Tamaz Amiranashvili", "Suprosanna Shit", "Antonio Terpin", "Sebastian Kaltenbach", "Petros Koumoutsakos", "Bjoern Menze"], "title": "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Generative models often map noise to data by matching flows or scores, but\nthese approaches become cumbersome for incorporating partial observations or\nadditional priors. Inspired by recent advances in Wasserstein gradient flows,\nwe propose Energy Matching, a framework that unifies flow-based approaches with\nthe flexibility of energy-based models (EBMs). Far from the data manifold,\nsamples move along curl-free, optimal transport paths from noise to data. As\nthey approach the data manifold, an entropic energy term guides the system into\na Boltzmann equilibrium distribution, explicitly capturing the underlying\nlikelihood structure of the data. We parameterize this dynamic with a single\ntime-independent scalar field, which serves as both a powerful generator and a\nflexible prior for effective regularization of inverse problems. Our method\nsubstantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97\ncompared to 8.61), while retaining the simulation-free training of\ntransport-based approaches away from the data manifold. Additionally, we\nexploit the flexibility of our method and introduce an interaction energy for\ndiverse mode exploration. Our approach focuses on learning a static scalar\npotential energy -- without time conditioning, auxiliary generators, or\nadditional networks -- marking a significant departure from recent EBM methods.\nWe believe this simplified framework significantly advances EBM capabilities\nand paves the way for their broader adoption in generative modeling across\ndiverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEnergy Matching\u6846\u67b6\uff0c\u7edf\u4e00\u6d41\u65b9\u6cd5\u548c\u80fd\u91cf\u6a21\u578b\uff0c\u63d0\u9ad8\u751f\u6210\u6027\u80fd\uff0c\u5728CIFAR-10\u4e0aFID\u8fbe3.97\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u90e8\u5206\u89c2\u5bdf\u6216\u989d\u5916\u5148\u9a8c\u65f6\u590d\u6742\uff0c\u53d7\u5230Wasserstein\u68af\u5ea6\u6d41\u8fdb\u5c55\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faEnergy Matching\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u65f6\u95f4\u65e0\u5173\u6807\u91cf\u573a\uff0c\u7ed3\u5408\u6d41\u65b9\u6cd5\u548c\u80fd\u91cf\u6a21\u578b\u7075\u6d3b\u6027\uff0c\u65e0\u9700\u65f6\u95f4\u6761\u4ef6\u6216\u989d\u5916\u7f51\u7edc\u3002", "result": "\u5728CIFAR-10\u751f\u6210\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709EBM\uff08FID 3.97 vs 8.61\uff09\uff0c\u4fdd\u7559\u65e0\u6a21\u62df\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u4ea4\u4e92\u80fd\u91cf\u652f\u6301\u591a\u6837\u6a21\u5f0f\u63a2\u7d22\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6807\u5fd7\u7740EBM\u7684\u91cd\u5927\u521b\u65b0\uff0c\u7b80\u5316\u6846\u67b6\uff0c\u63d0\u5347\u80fd\u529b\uff0c\u4fc3\u8fdb\u5728\u5404\u79cd\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2504.11319", "pdf": "https://arxiv.org/pdf/2504.11319", "abs": "https://arxiv.org/abs/2504.11319", "authors": ["Yiqing Zhou", "Karsten Naert", "Dirk Nuyens"], "title": "Sensitivity Analysis of State Space Models for Scrap Composition Estimation in EAF and BOF", "categories": ["eess.SY", "cs.SY", "93C41, 90B30, 80A19, 93E11, 93C10"], "comment": null, "summary": "This study develops and analyzes linear and nonlinear state space models for\nestimating the elemental composition of scrap steel used in steelmaking, with\napplications to Electric Arc Furnace (EAF) and Basic Oxygen Furnace (BOF)\nprocesses. The models incorporate mass balance equations and are fitted using a\nmodified Kalman filter for linear cases and the Unscented Kalman Filter (UKF)\nfor nonlinear cases. Using Cu and Cr as representative elements, we assess the\nsensitivity of model predictions to measurement noise in key process variables,\nincluding steel mass, steel composition, scrap input mass, slag mass, and iron\noxide fraction in slag. Results show that the models are robust to moderate\nnoise levels in most variables, particularly when errors are below $10\\%$.\nHowever, accuracy significantly deteriorates with noise in slag mass\nestimation. These findings highlight the practical feasibility and limitations\nof applying state space models for real-time scrap composition estimation in\nindustrial settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u4f30\u8ba1\u70bc\u94a2\u4e2d\u5e9f\u94a2\u5143\u7d20\u7ec4\u6210\uff0c\u5e76\u8bc4\u4f30\u5bf9\u6d4b\u91cf\u566a\u58f0\u7684\u654f\u611f\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u63d0\u9ad8\u7535\u5f27\u7089\u548c\u78b1\u6027\u6c27\u6c14\u7089\u4e2d\u5e9f\u94a2\u6210\u5206\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u652f\u6301\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u8d28\u91cf\u5e73\u8861\u65b9\u7a0b\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08\u7ebf\u6027\u7528\u4fee\u6539\u7248\uff0c\u975e\u7ebf\u6027\u7528UKF\uff09\uff0c\u4ee5Cu\u548cCr\u5143\u7d20\u6d4b\u8bd5\u566a\u58f0\u654f\u611f\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5bf9\u5927\u591a\u6570\u53d8\u91cf\u566a\u58f0\u9c81\u68d2\uff0c\u4f46\u5bf9\u7089\u6e23\u8d28\u91cf\u566a\u58f0\u654f\u611f\u3002", "conclusion": "\u7ed3\u8bba\u7a81\u51fa\u4e86\u8be5\u6a21\u578b\u5728\u5b9e\u65f6\u4f30\u8ba1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u9650\u5236\u3002"}}
{"id": "2504.11200", "pdf": "https://arxiv.org/pdf/2504.11200", "abs": "https://arxiv.org/abs/2504.11200", "authors": ["Irene Celino", "Mario Scrocca", "Agnese Chiatti"], "title": "Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs", "categories": ["cs.AI"], "comment": "26 pages, 13 figures, 1 table; pre-print version of book chapter", "summary": "This chapter investigates the concept of mutual understanding between humans\nand systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI)\nmethods can significantly enhance this mutual understanding by leveraging\nexplicit symbolic knowledge representations with data-driven learning models.\nWe start by introducing three critical dimensions to characterize mutual\nunderstanding: sharing knowledge, exchanging knowledge, and governing\nknowledge. Sharing knowledge involves aligning the conceptual models of\ndifferent agents to enable a shared understanding of the domain of interest.\nExchanging knowledge relates to ensuring the effective and accurate\ncommunication between agents. Governing knowledge concerns establishing rules\nand processes to regulate the interaction between agents. Then, we present\nseveral different use case scenarios that demonstrate the application of NeSy\nAI and Knowledge Graphs to aid meaningful exchanges between human, artificial,\nand robotic agents. These scenarios highlight both the potential and the\nchallenges of combining top-down symbolic reasoning with bottom-up neural\nlearning, guiding the discussion of the coverage provided by current solutions\nalong the dimensions of sharing, exchanging, and governing knowledge.\nConcurrently, this analysis facilitates the identification of gaps and less\ndeveloped aspects in mutual understanding to address in future research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7b26\u53f7\u4eba\u5de5\u667a\u80fd\uff08NeSy AI\uff09\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u6765\u63d0\u5347\u4eba\u7c7b\u4e0e\u7cfb\u7edf\u4e4b\u95f4\u7684\u76f8\u4e92\u7406\u89e3\uff0c\u6db5\u76d6\u77e5\u8bc6\u5206\u4eab\u3001\u4ea4\u6362\u548c\u6cbb\u7406\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5206\u6790\u8bc6\u522b\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u5f3a\u8c03NeSy AI\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u7cfb\u7edf\u76f8\u4e92\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u6765\u8868\u5f81\u548c\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u77e5\u8bc6\u5206\u4eab\u3001\u4ea4\u6362\u548c\u6cbb\u7406\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5448\u73b0NeSy AI\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u540c\u7ef4\u5ea6\u7684\u8986\u76d6\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u7a81\u51fa\u4e86NeSy AI\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5e76\u8bc6\u522b\u4e86\u672a\u6765\u7814\u7a76\u4e2d\u76f8\u4e92\u7406\u89e3\u7684\u7a7a\u767d\u548c\u4e0d\u8db3\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8fd9\u79cd\u5206\u6790\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7814\u7a76\uff0c\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u5728\u76f8\u4e92\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.10677", "pdf": "https://arxiv.org/pdf/2504.10677", "abs": "https://arxiv.org/abs/2504.10677", "authors": ["Muhammad Al-Zafar Khan", "Jamal Al-Karaki"], "title": "Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "14 pages, 4 figures, submitted to the 10th International Conference\n  on Information and Communication Technology for Intelligent Systems (ICTIS)", "summary": "In this paper, we present a multi-agent reinforcement learning (MARL)\nframework for optimizing tissue repair processes using engineered biological\nagents. Our approach integrates: (1) stochastic reaction-diffusion systems\nmodeling molecular signaling, (2) neural-like electrochemical communication\nwith Hebbian plasticity, and (3) a biologically informed reward function\ncombining chemical gradient tracking, neural synchronization, and robust\npenalties. A curriculum learning scheme guides the agent through progressively\ncomplex repair scenarios. In silico experiments demonstrate emergent repair\nstrategies, including dynamic secretion control and spatial coordination.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u4f7f\u7528\u5de5\u7a0b\u751f\u7269\u4ee3\u7406\u7684\u7ec4\u7ec7\u4fee\u590d\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u7ec4\u7ec7\u4fee\u590d\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u751f\u7269\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u6574\u5408\u4e86\u968f\u673a\u53cd\u5e94-\u6269\u6563\u7cfb\u7edf\u3001\u795e\u7ecf\u6837\u7535\u5316\u5b66\u901a\u4fe1\uff08\u5305\u62ecHebbian\u53ef\u5851\u6027\uff09\u3001\u751f\u7269\u5b66\u4fe1\u606f\u5956\u52b1\u51fd\u6570\uff08\u7ed3\u5408\u5316\u5b66\u68af\u5ea6\u8ddf\u8e2a\u3001\u795e\u7ecf\u540c\u6b65\u548c\u9c81\u68d2\u60e9\u7f5a\uff09\u3001\u4ee5\u53ca\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6848\u3002", "result": "\u901a\u8fc7in silico\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u7d27\u6025\u4fee\u590d\u7b56\u7565\uff0c\u5305\u62ec\u52a8\u6001\u5206\u6ccc\u63a7\u5236\u548c\u7a7a\u95f4\u534f\u8c03\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u7ec4\u7ec7\u4fee\u590d\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u4e86\u7d27\u6025\u884c\u4e3a\u548c\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2504.11355", "pdf": "https://arxiv.org/pdf/2504.11355", "abs": "https://arxiv.org/abs/2504.11355", "authors": ["Alberto Castillo", "Elliot Pryor", "Anas El Fathi", "Boris Kovatchev", "Marc Breton"], "title": "Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "Training Neural Networks (NNs) to behave as Model Predictive Control (MPC)\nalgorithms is an effective way to implement them in constrained embedded\ndevices. By collecting large amounts of input-output data, where inputs\nrepresent system states and outputs are MPC-generated control actions, NNs can\nbe trained to replicate MPC behavior at a fraction of the computational cost.\nHowever, although the composition of the training data critically influences\nthe final NN accuracy, methods for systematically optimizing it remain\nunderexplored. In this paper, we introduce the concept of Optimally-Sampled\nDatasets (OSDs) as ideal training sets and present an efficient algorithm for\ngenerating them. An OSD is a parametrized subset of all the available data that\n(i) preserves existing MPC information up to a certain numerical resolution,\n(ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or\ncomplete. We demonstrate the effectiveness of OSDs by training NNs to replicate\nthe University of Virginia's MPC algorithm for automated insulin delivery in\nType-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably,\ntwo OSD-trained NNs received regulatory clearance for clinical testing as the\nfirst NN-based control algorithm for direct human insulin dosing. This\nmethodology opens new pathways for implementing advanced optimizations on\nresource-constrained embedded platforms, potentially revolutionizing how\ncomplex algorithms are deployed.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4f18\u5316\u91c7\u6837\u6570\u636e\u96c6\uff08OSDs\uff09\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u4eff\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u80f0\u5c9b\u7d20\u9012\u9001\u5e94\u7528\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u867d\u7136\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4ee5\u66f4\u4f4e\u8ba1\u7b97\u6210\u672c\u590d\u5236MPC\u7b97\u6cd5\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u7684\u7ec4\u6210\u5bf9\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u662f\u5f15\u5165OSDs\uff0c\u8fd9\u662f\u4e00\u4e2a\u53c2\u6570\u5316\u7684\u6570\u636e\u5b50\u96c6\uff0c\u80fd\u591f\u4fdd\u7559MPC\u4fe1\u606f\u3001\u907f\u514d\u91cd\u590d\u72b6\u6001\u5e76\u5b9e\u73b0\u9971\u548c\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u751f\u6210\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u662f\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u590d\u5236\u7279\u5b9aMPC\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u7684\u56db\u500d\u6539\u8fdb\uff0c\u5e76\u6709\u57fa\u4e8eOSDs\u7684\u795e\u7ecf\u7f51\u7edc\u83b7\u5f97\u4e34\u5e8a\u6d4b\u8bd5\u76d1\u7ba1\u6279\u51c6\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u90e8\u7f72\u590d\u6742\u7b97\u6cd5\u6253\u5f00\u65b0\u9014\u5f84\uff0c\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u4f18\u5316\u5b9e\u73b0\u65b9\u5f0f\u3002"}}
{"id": "2504.11239", "pdf": "https://arxiv.org/pdf/2504.11239", "abs": "https://arxiv.org/abs/2504.11239", "authors": ["Chang Yang", "Ruiyu Wang", "Junzhe Jiang", "Qi Jiang", "Qinggang Zhang", "Yanchen Deng", "Shuxin Li", "Shuyue Hu", "Bo Li", "Florian T. Pokorny", "Xiao Huang", "Xinrun Wang"], "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preliminary work, 10 pages for main text", "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNPPC\uff0c\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6c38\u7eed\u6269\u5c55\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8eNP\u5b8c\u5168\u95ee\u9898\uff0c\u65e8\u5728\u4e0d\u53ef\u51fb\u6e83\u3001\u4e0d\u53ef\u9ed1\u5ba2\u653b\u51fb\u3001\u81ea\u52a8\u53ef\u9a8c\u8bc1\u548c\u901a\u7528\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bb9\u6613\u88ab\u5feb\u901f\u51fb\u6e83\u548c\u9ed1\u5ba2\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u6c38\u7eed\u6269\u5c55\u3001\u4e0d\u53ef\u51fb\u6e83\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faNPPC\u6846\u67b6\uff0c\u5305\u62ecnpgym\uff08\u751f\u6210NP\u5b8c\u5168\u95ee\u9898\u5b9e\u4f8b\uff09\u3001npsolver\uff08\u8bc4\u4f30LLMs\u6027\u80fd\uff09\u548cnpeval\uff08\u5206\u6790\u6027\u80fd\u6307\u6807\uff09\u4e09\u4e2a\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNPPC\u5c06\u9ad8\u7ea7LLMs\u6027\u80fd\u964d\u81f310%\u4ee5\u4e0b\uff0cDeepSeek-R1\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u89c2\u5bdf\u5230\u4ee4\u724c\u6570\u548c\u987f\u609f\u65f6\u523b\u968f\u96be\u5ea6\u53d8\u5316\u7684\u6a21\u5f0f\u3002", "conclusion": "NPPC\u662f\u9996\u4e2a\u6c38\u7eed\u6269\u5c55\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u4f5c\u4e3aAGI\u53d1\u5c55\u7684\u4e0d\u53ef\u51fb\u6e83\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2504.10694", "pdf": "https://arxiv.org/pdf/2504.10694", "abs": "https://arxiv.org/abs/2504.10694", "authors": ["Kristina Nikoli\u0107", "Luze Sun", "Jie Zhang", "Florian Tram\u00e8r"], "title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Jailbreak attacks bypass the guardrails of large language models to produce\nharmful outputs. In this paper, we ask whether the model outputs produced by\nexisting jailbreaks are actually useful. For example, when jailbreaking a model\nto give instructions for building a bomb, does the jailbreak yield good\ninstructions? Since the utility of most unsafe answers (e.g., bomb\ninstructions) is hard to evaluate rigorously, we build new jailbreak evaluation\nsets with known ground truth answers, by aligning models to refuse questions\nrelated to benign and easy-to-evaluate topics (e.g., biology or math). Our\nevaluation of eight representative jailbreaks across five utility benchmarks\nreveals a consistent drop in model utility in jailbroken responses, which we\nterm the jailbreak tax. For example, while all jailbreaks we tested bypass\nguardrails in models aligned to refuse to answer math, this comes at the\nexpense of a drop of up to 92% in accuracy. Overall, our work proposes the\njailbreak tax as a new important metric in AI safety, and introduces benchmarks\nto evaluate existing and future jailbreaks. We make the benchmark available at\nhttps://github.com/ethz-spylab/jailbreak-tax", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc4\u4f30\u4e86\u8d8a\u72f1\u653b\u51fb\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7528\uff0c\u5f15\u5165\u4e86'\u8d8a\u72f1\u7a0e'\u6982\u5ff5\uff0c\u5f3a\u8c03\u653b\u51fb\u6210\u529f\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\u3002", "motivation": "\u8d28\u7591\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u7684\u8f93\u51fa\u662f\u5426\u771f\u6b63\u6709\u7528\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u4ea7\u751f\u6709\u5bb3\u4f46\u4f4e\u6548\u7528\u7684\u54cd\u5e94\u3002", "method": "\u6784\u5efa\u65b0\u8bc4\u4f30\u96c6\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u62d2\u7edd\u826f\u6027\u4e3b\u9898\uff08\u5982\u751f\u7269\u6216\u6570\u5b66\uff09\u7684\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u516b\u79cd\u4ee3\u8868\u6027\u8d8a\u72f1\u653b\u51fb\u5728\u4e94\u4e2a\u6548\u7528\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u8d8a\u72f1\u54cd\u5e94\u4e2d\u6a21\u578b\u6548\u7528\u4e00\u81f4\u4e0b\u964d\uff0c\u79f0\u4e3a\u8d8a\u72f1\u7a0e\uff0c\u4f8b\u5982\u6570\u5b66\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe92%\u3002", "conclusion": "\u63d0\u51fa\u8d8a\u72f1\u7a0e\u4f5c\u4e3aAI\u5b89\u5168\u65b0\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u57fa\u51c6\uff0c\u53d1\u5e03\u5728https://github.com/ethz-spylab/jailbreak-tax\u3002"}}
{"id": "2504.11374", "pdf": "https://arxiv.org/pdf/2504.11374", "abs": "https://arxiv.org/abs/2504.11374", "authors": ["Yongkang Huo", "Fuvio Forni", "Rodolphe Sepulchre"], "title": "A Winner-Takes-All Mechanism for Event Generation", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "We present a novel framework for central pattern generator design that\nleverages the intrinsic rebound excitability of neurons in combination with\nwinner-takes-all computation. Our approach unifies decision-making and rhythmic\npattern generation within a simple yet powerful network architecture that\nemploys all-to-all inhibitory connections enhanced by designable excitatory\ninteractions. This design offers significant advantages regarding ease of\nimplementation, adaptability, and robustness. We demonstrate its efficacy\nthrough a ring oscillator model, which exhibits adaptive phase and frequency\nmodulation, making the framework particularly promising for applications in\nneuromorphic systems and robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4e2d\u5fc3\u6a21\u5f0f\u53d1\u751f\u5668\u8bbe\u8ba1\uff0c\u7ed3\u5408\u795e\u7ecf\u5143\u53cd\u5f39\u5174\u594b\u6027\u548c\u8d62\u8005\u901a\u5403\u8ba1\u7b97\uff0c\u7edf\u4e00\u51b3\u7b56\u548c\u8282\u5f8b\u6a21\u5f0f\u751f\u6210\u3002", "motivation": "\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u4e2a\u7b80\u5355\u3001\u5f3a\u5927\u4e14\u9c81\u68d2\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u4ee5\u7edf\u4e00\u51b3\u7b56\u548c\u8282\u5f8b\u6a21\u5f0f\u751f\u6210\uff0c\u89e3\u51b3\u76f8\u5173\u9886\u57df\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528\u5168\u4e92\u6291\u8fde\u63a5\u5e76\u589e\u5f3a\u53ef\u8bbe\u8ba1\u5174\u594b\u4ea4\u4e92\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u795e\u7ecf\u5143\u7684\u5185\u5728\u53cd\u5f39\u5174\u594b\u6027\u548c\u8d62\u8005\u901a\u5403\u8ba1\u7b97\u3002", "result": "\u7ed3\u679c\u901a\u8fc7\u73af\u5f62\u632f\u8361\u5668\u6a21\u578b\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u76f8\u4f4d\u548c\u9891\u7387\u8c03\u5236\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u5728\u6613\u5b9e\u73b0\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u5bf9\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u524d\u666f\u3002"}}
{"id": "2504.11243", "pdf": "https://arxiv.org/pdf/2504.11243", "abs": "https://arxiv.org/abs/2504.11243", "authors": ["Balahari Vignesh Balu", "Florian Geissler", "Francesco Carella", "Joao-Vitor Zacchi", "Josef Jiru", "Nuria Mata", "Reinhard Stolle"], "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u4ee3\u7406-based RAG\u7ed3\u5408LLM\u81ea\u52a8\u63a8\u5bfc\u81ea\u9a7e\u8f66\u5b89\u5168\u8981\u6c42\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u7684\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u5728Apollo\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfLLM\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\uff0c\u73b0\u6709RAG\u5728\u5904\u7406\u5b89\u5168\u76f8\u5173\u590d\u6742\u67e5\u8be2\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4ee3\u7406-based RAG\u63d0\u5347\u4fe1\u606f\u68c0\u7d22\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4ee3\u7406-based RAG\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u6c7d\u8f66\u6807\u51c6\u548cApollo\u6848\u4f8b\uff0c\u4f7f\u7528\u4eceApollo\u6570\u636e\u63d0\u53d6\u7684\u5b89\u5168\u8981\u6c42\u95ee\u9898\u96c6\u8fdb\u884c\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7RAG\u6307\u6807\u8bc4\u4f30\uff0c\u4ee3\u7406-based \u65b9\u6cd5\u6bd4\u9ed8\u8ba4RAG\u68c0\u7d22\u4fe1\u606f\u66f4\u76f8\u5173\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u4ee3\u7406-based RAG\u5728\u81ea\u9a7e\u8f66\u5b89\u5168\u8981\u6c42\u63a8\u5bfc\u4e2d\u66f4\u6709\u6548\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2504.10720", "pdf": "https://arxiv.org/pdf/2504.10720", "abs": "https://arxiv.org/abs/2504.10720", "authors": ["Kamaljyoti Nath", "Khemraj Shukla", "Victor C. Tsai", "Umair bin Waheed", "Christian Huber", "Omer Alpak", "Chuen-Song Chen", "Ligang Lu", "Amik St-Cyr"], "title": "Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)", "categories": ["cs.LG"], "comment": null, "summary": "Full Waveform Inversion (FWI) is an important geophysical technique\nconsidered in subsurface property prediction. It solves the inverse problem of\npredicting high-resolution Earth interior models from seismic data. Traditional\nFWI methods are computationally demanding. Inverse problems in geophysics often\nface challenges of non-uniqueness due to limited data, as data are often\ncollected only on the surface. In this study, we introduce a novel methodology\nthat leverages Deep Operator Networks (DeepONet) to attempt to improve both the\nefficiency and accuracy of FWI. The proposed DeepONet methodology inverts\nseismic waveforms for the subsurface velocity field. This approach is able to\ncapture some key features of the subsurface velocity field. We have shown that\nthe architecture can be applied to noisy seismic data with an accuracy that is\nbetter than some other machine learning methods. We also test our proposed\nmethod with out-of-distribution prediction for different velocity models. The\nproposed DeepONet shows comparable and better accuracy in some velocity models\nthan some other machine learning methods. To improve the FWI workflow, we\npropose using the DeepONet output as a starting model for conventional FWI and\nthat it may improve FWI performance. While we have only shown that DeepONet\nfacilitates faster convergence than starting with a homogeneous velocity field,\nit may have some benefits compared to other approaches to constructing starting\nmodels. This integration of DeepONet into FWI may accelerate the inversion\nprocess and may also enhance its robustness and reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Deep Operator Networks (DeepONet)\u6539\u8fdbFull Waveform Inversion (FWI)\uff0c\u4ee5\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfFWI\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u4e14\u9006\u95ee\u9898\u56e0\u6570\u636e\u6709\u9650\u53ef\u80fd\u975e\u552f\u4e00\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDeepONet\u53cd\u6f14\u5730\u9707\u6ce2\u5f62\u83b7\u53d6\u5730\u4e0b\u901f\u5ea6\u573a\uff0c\u5305\u62ec\u5904\u7406\u566a\u58f0\u6570\u636e\u3001\u51fa\u5206\u5e03\u9884\u6d4b\uff0c\u5e76\u4f5c\u4e3a\u4f20\u7edfFWI\u8d77\u59cb\u6a21\u578b\u3002", "result": "DeepONet\u6355\u83b7\u5173\u952e\u7279\u5f81\uff0c\u5728\u566a\u58f0\u6570\u636e\u548c\u67d0\u4e9b\u901f\u5ea6\u6a21\u578b\u4e2d\u51c6\u786e\u6027\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u52a0\u901fFWI\u6536\u655b\u3002", "conclusion": "\u6574\u5408DeepONet\u53ef\u52a0\u901fFWI\u8fc7\u7a0b\uff0c\u5e76\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.11446", "pdf": "https://arxiv.org/pdf/2504.11446", "abs": "https://arxiv.org/abs/2504.11446", "authors": ["Federico Porcari", "Donatello Materassi", "Simone Formentin"], "title": "eXplainable AI for data driven control: an inverse optimal control approach", "categories": ["eess.SY", "cs.SY"], "comment": "Submitted to CDC 2025", "summary": "Understanding the behavior of black-box data-driven controllers is a key\nchallenge in modern control design. In this work, we propose an eXplainable AI\n(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local\nexplanations for the behavior of a controller operating around a given region.\nSpecifically, we extract the weights assigned to tracking errors and control\neffort in the implicit cost function that a black-box controller is optimizing,\noffering a more transparent and interpretable representation of the\ncontroller's underlying objectives. This approach presents connections with\nwell-established XAI techniques, such as Local Interpretable Model-agnostic\nExplanations (LIME) since it is still based on a local approximation of the\ncontrol policy. However, rather being limited to a standard sensitivity\nanalysis, the explanation provided by our method relies on the solution of an\ninverse Linear Quadratic (LQ) problem, offering a structured and more\ncontrol-relevant perspective. Numerical examples demonstrate that the inferred\ncost function consistently provides a deeper understanding of the controller's\ndecision-making process, shedding light on otherwise counterintuitive or\nunexpected phenomena.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faXAI\u65b9\u6cd5\u57fa\u4e8e\u9006\u6700\u4f18\u63a7\u5236\uff0c\u89e3\u91ca\u9ed1\u7bb1\u63a7\u5236\u5668\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u63d0\u53d6\u6210\u672c\u51fd\u6570\u6743\u91cd\u63d0\u4f9b\u672c\u5730\u900f\u660e\u89e3\u91ca\uff0c\u4e0eLIME\u76f8\u5173\u8054\u3002", "motivation": "\u7406\u89e3\u9ed1\u7bb1\u6570\u636e\u9a71\u52a8\u63a7\u5236\u5668\u7684\u884c\u4e3a\u662f\u73b0\u4ee3\u63a7\u5236\u8bbe\u8ba1\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528Inverse Optimal Control (IOC)\u65b9\u6cd5\uff0c\u6c42\u89e3\u9006Linear Quadratic (LQ)\u95ee\u9898\uff0c\u63d0\u53d6\u8ddf\u8e2a\u8bef\u5dee\u548c\u63a7\u5236\u52aa\u529b\u7684\u6743\u91cd\u3002", "result": "\u6570\u503c\u4f8b\u5b50\u663e\u793a\uff0c\u63a8\u65ad\u7684\u6210\u672c\u51fd\u6570\u80fd\u66f4\u6df1\u5165\u7406\u89e3\u63a7\u5236\u5668\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63ed\u793a\u53cd\u76f4\u89c9\u73b0\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u63a7\u5236\u76f8\u5173\u7684\u89e3\u91ca\uff0c\u5e2e\u52a9\u9610\u660e\u9ed1\u7bb1\u63a7\u5236\u5668\u7684\u6f5c\u5728\u76ee\u6807\u3002"}}
{"id": "2504.11301", "pdf": "https://arxiv.org/pdf/2504.11301", "abs": "https://arxiv.org/abs/2504.11301", "authors": ["Yangyang Zhuang", "Wenjia Jiang", "Jiayu Zhang", "Ze Yang", "Joey Tianyi Zhou", "Chi Zhang"], "title": "Learning to Be A Doctor: Searching for Effective Medical Agent Architectures", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents have demonstrated strong capabilities\nacross a wide range of tasks, and their application in the medical domain holds\nparticular promise due to the demand for high generalizability and reliance on\ninterdisciplinary knowledge. However, existing medical agent systems often rely\non static, manually crafted workflows that lack the flexibility to accommodate\ndiverse diagnostic requirements and adapt to emerging clinical scenarios.\nMotivated by the success of automated machine learning (AutoML), this paper\nintroduces a novel framework for the automated design of medical agent\narchitectures. Specifically, we define a hierarchical and expressive agent\nsearch space that enables dynamic workflow adaptation through structured\nmodifications at the node, structural, and framework levels. Our framework\nconceptualizes medical agents as graph-based architectures composed of diverse,\nfunctional node types and supports iterative self-improvement guided by\ndiagnostic feedback. Experimental results on skin disease diagnosis tasks\ndemonstrate that the proposed method effectively evolves workflow structures\nand significantly enhances diagnostic accuracy over time. This work represents\nthe first fully automated framework for medical agent architecture design and\noffers a scalable, adaptable foundation for deploying intelligent agents in\nreal-world clinical environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bbe\u8ba1\u533b\u7597\u4ee3\u7406\u67b6\u6784\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u641c\u7d22\u7a7a\u95f4\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u548c\u8bca\u65ad\u51c6\u786e\u6027\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u5de5\u4f5c\u6d41\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1b\u53d7AutoML\u6210\u529f\u542f\u53d1\uff0c\u9700\u8981\u81ea\u52a8\u8bbe\u8ba1\u4ee5\u9002\u5e94\u591a\u6837\u5316\u8bca\u65ad\u9700\u6c42\u3002", "method": "\u5b9a\u4e49\u5206\u5c42\u4ee3\u7406\u641c\u7d22\u7a7a\u95f4\uff0c\u5c06\u533b\u7597\u4ee3\u7406\u89c6\u4e3a\u57fa\u4e8e\u56fe\u7684\u67b6\u6784\uff0c\u652f\u6301\u8282\u70b9\u3001\u7ed3\u6784\u548c\u6846\u67b6\u7ea7\u4fee\u6539\uff0c\u4ee5\u53ca\u8bca\u65ad\u53cd\u9988\u5f15\u5bfc\u7684\u8fed\u4ee3\u81ea\u6539\u8fdb\u3002", "result": "\u5728\u76ae\u80a4\u75c5\u8bca\u65ad\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u6709\u6548\u6f14\u5316\u5de5\u4f5c\u6d41\u7ed3\u6784\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u533b\u7597\u4ee3\u7406\u67b6\u6784\u8bbe\u8ba1\u6846\u67b6\uff0c\u4e3a\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u667a\u80fd\u4ee3\u7406\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2504.10735", "pdf": "https://arxiv.org/pdf/2504.10735", "abs": "https://arxiv.org/abs/2504.10735", "authors": ["Timur Carstensen", "Neeratyoy Mallik", "Frank Hutter", "Martin Rapp"], "title": "Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As model sizes grow, finding efficient and cost-effective hyperparameter\noptimization (HPO) methods becomes increasingly crucial for deep learning\npipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources\nrequired for DL training with lower fidelity estimations, existing fidelity\nsources often fail under lower compute and memory constraints. We propose a\nnovel fidelity source: the number of layers that are trained or frozen during\ntraining. For deep networks, this approach offers significant compute and\nmemory savings while preserving rank correlations between hyperparameters at\nlow fidelities compared to full model training. We demonstrate this in our\nempirical evaluation across ResNets and Transformers and additionally analyze\nthe utility of frozen layers as a fidelity in using GPU resources as a fidelity\nin HPO, and for a combined MF-HPO with other fidelity sources. This\ncontribution opens new applications for MF-HPO with hardware resources as a\nfidelity and creates opportunities for improved algorithms navigating joint\nfidelity spaces.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4fdd\u771f\u5ea6\u8d85\u53c2\u6570\u4f18\u5316\uff08MF-HPO\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bad\u7ec3\u6216\u51bb\u7ed3\u5c42\u6570\u4f5c\u4e3a\u4fdd\u771f\u5ea6\u6765\u6e90\uff0c\u4ee5\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u957f\uff0c\u9ad8\u6548\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u7684\u4fdd\u771f\u5ea6\u6765\u6e90\u5728\u4f4e\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u5c06\u8bad\u7ec3\u6216\u51bb\u7ed3\u5c42\u6570\u4f5c\u4e3a\u4fdd\u771f\u5ea6\u6765\u6e90\uff0c\u901a\u8fc7\u51bb\u7ed3\u90e8\u5206\u5c42\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u53c2\u6570\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728ResNets\u548cTransformers\u4e0a\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u4e86\u826f\u597d\u76f8\u5173\u6027\uff0c\u5e76\u5206\u6790\u4e86\u4e0e\u5176\u4ed6\u4fdd\u771f\u5ea6\u6765\u6e90\u7ed3\u5408\u7684\u6548\u7528\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f7f\u7528\u786c\u4ef6\u8d44\u6e90\u4f5c\u4e3a\u4fdd\u771f\u5ea6\u7684MF-HPO\u6253\u5f00\u65b0\u5e94\u7528\uff0c\u5e76\u4e3a\u6539\u8fdb\u8054\u5408\u4fdd\u771f\u5ea6\u7a7a\u95f4\u7684\u7b97\u6cd5\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2504.10497", "pdf": "https://arxiv.org/pdf/2504.10497", "abs": "https://arxiv.org/abs/2504.10497", "authors": ["Sunyi Liu", "Mengzhe Geng", "Rebecca Hart"], "title": "Exploring Generative AI Techniques in Government: A Case Study", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.MA", "cs.SY", "eess.SY"], "comment": "In submission to IEEE Intelligent Systems", "summary": "The swift progress of Generative Artificial intelligence (GenAI), notably\nLarge Language Models (LLMs), is reshaping the digital landscape. Recognizing\nthis transformative potential, the National Research Council of Canada (NRC)\nlaunched a pilot initiative to explore the integration of GenAI techniques into\nits daily operation for performance excellence, where 22 projects were launched\nin May 2024. Within these projects, this paper presents the development of the\nintelligent agent Pubbie as a case study, targeting the automation of\nperformance measurement, data management and insight reporting at the NRC.\nCutting-edge techniques are explored, including LLM orchestration and semantic\nembedding via RoBERTa, while strategic fine-tuning and few-shot learning\napproaches are incorporated to infuse domain knowledge at an affordable cost.\nThe user-friendly interface of Pubbie allows general government users to input\nqueries in natural language and easily upload or download files with a simple\nbutton click, greatly reducing manual efforts and accessibility barriers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u52a0\u62ff\u5927\u56fd\u5bb6\u7814\u7a76\u59d4\u5458\u4f1a (NRC) \u5f00\u53d1\u7684\u667a\u80fd\u4ee3\u7406 Pubbie\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6027\u80fd\u6d4b\u91cf\u3001\u6570\u636e\u7ba1\u7406\u548c\u62a5\u544a\uff0c\u91c7\u7528\u751f\u6210\u5f0f AI \u6280\u672f\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u3002", "motivation": "\u8ba4\u8bc6\u5230\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u548c\u53d8\u9769\u6f5c\u529b\uff0cNRC \u542f\u52a8\u8bd5\u70b9\u9879\u76ee\uff0c\u4ee5\u63a2\u7d22\u5c06\u5176\u6574\u5408\u5230\u65e5\u5e38\u64cd\u4f5c\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528 LLM \u7f16\u6392\u3001RoBERTa \u8bed\u4e49\u5d4c\u5165\u3001\u6218\u7565\u5fae\u8c03\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u8bbe\u8ba1\u7528\u6237\u53cb\u597d\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u63a5\u53e3\uff0c\u652f\u6301\u6587\u4ef6\u4e0a\u4f20/\u4e0b\u8f7d\u3002", "result": "Pubbie \u51cf\u5c11\u4e86\u624b\u52a8\u52aa\u529b\u548c\u53ef\u8bbf\u95ee\u6027\u969c\u788d\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u7ba1\u7406\u548c\u62a5\u544a\u7684\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0f AI \u5728\u7ec4\u7ec7\u6027\u80fd\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u7c7b\u4f3c\u96c6\u6210\u63d0\u4f9b\u53ef\u884c\u6848\u4f8b\u3002"}}
{"id": "2504.11354", "pdf": "https://arxiv.org/pdf/2504.11354", "abs": "https://arxiv.org/abs/2504.11354", "authors": ["Haiming Wang", "Mert Unsal", "Xiaohan Lin", "Mantas Baksys", "Junqi Liu", "Marco Dos Santos", "Flood Sung", "Marina Vinyes", "Zhenzhe Ying", "Zekai Zhu", "Jianqiao Lu", "Hugues de Saxc\u00e9", "Bolton Bailey", "Chendong Song", "Chenjun Xiao", "Dehao Zhang", "Ebony Zhang", "Frederick Pu", "Han Zhu", "Jiawei Liu", "Jonas Bayer", "Julien Michel", "Longhui Yu", "L\u00e9o Dreyfus-Schmidt", "Lewis Tunstall", "Luigi Pagani", "Moreira Machado", "Pauline Bourigault", "Ran Wang", "Stanislas Polu", "Thibaut Barroyer", "Wen-Ding Li", "Yazhe Niu", "Yann Fleureau", "Yangyang Hu", "Zhouliang Yu", "Zihan Wang", "Zhilin Yang", "Zhengying Liu", "Jia Li"], "title": "Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning", "categories": ["cs.AI"], "comment": "22 pages", "summary": "We introduce Kimina-Prover Preview, a large language model that pioneers a\nnovel reasoning-driven exploration paradigm for formal theorem proving, as\nshowcased in this preview release. Trained with a large-scale reinforcement\nlearning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong\nperformance in Lean 4 proof generation by employing a structured reasoning\npattern we term \\textit{formal reasoning pattern}. This approach allows the\nmodel to emulate human problem-solving strategies in Lean, iteratively\ngenerating and refining proof steps. Kimina-Prover sets a new state-of-the-art\non the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved\nbenchmark performance, our work yields several key insights: (1) Kimina-Prover\nexhibits high sample efficiency, delivering strong results even with minimal\nsampling (pass@1) and scaling effectively with computational budget, stemming\nfrom its unique reasoning pattern and RL training; (2) we demonstrate clear\nperformance scaling with model size, a trend previously unobserved for neural\ntheorem provers in formal mathematics; (3) the learned reasoning style,\ndistinct from traditional search algorithms, shows potential to bridge the gap\nbetween formal verification and informal mathematical intuition. We open source\ndistilled versions with 1.5B and 7B parameters of Kimina-Prover", "AI": {"tldr": "Kimina-Prover \u662f\u4e00\u4e2a\u65b0\u578b\u63a8\u7406\u9a71\u52a8\u7684\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728 Lean 4 \u8bc1\u660e\u751f\u6210\u4e2d\u8fbe\u5230 miniF2F \u57fa\u51c6 80.7% pass@8192 \u7684\u65b0\u7eaa\u5f55\uff0c\u5e76\u5f00\u6e90\u7cbe\u7b80\u7248\u672c\u3002", "motivation": "\u521b\u65b0\u63a8\u7406\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u7b56\u7565\uff0c\u6865\u63a5\u6b63\u5f0f\u9a8c\u8bc1\u4e0e\u975e\u6b63\u5f0f\u6570\u5b66\u76f4\u89c9\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u4ece Qwen2.5-72B \u8bad\u7ec3\uff0c\u4f7f\u7528 '\u6b63\u5f0f\u63a8\u7406\u6a21\u5f0f' \u8fed\u4ee3\u751f\u6210\u548c\u5b8c\u5584\u8bc1\u660e\u6b65\u9aa4\u3002", "result": "\u5728 miniF2F \u57fa\u51c6\u4e0a\u8fbe\u5230 80.7% pass@8192 \u7684\u65b0\u72b6\u6001-of-the-art\uff0c\u9ad8\u6837\u672c\u6548\u7387\u3001\u6a21\u578b\u5927\u5c0f\u6027\u80fd scaling\uff0c\u4ee5\u53ca\u72ec\u7279\u63a8\u7406\u98ce\u683c\u3002", "conclusion": "\u5c55\u793a\u4e86\u6837\u672c\u6548\u7387\u3001\u6027\u80fd scaling \u7684\u4f18\u52bf\uff0c\u5e76\u6709\u6f5c\u529b\u5f25\u5408\u6b63\u5f0f\u9a8c\u8bc1\u4e0e\u6570\u5b66\u76f4\u89c9\u7684\u5dee\u8ddd\uff1b\u5f00\u6e90 1.5B \u548c 7B \u53c2\u6570\u7248\u672c\u3002"}}
{"id": "2504.10752", "pdf": "https://arxiv.org/pdf/2504.10752", "abs": "https://arxiv.org/abs/2504.10752", "authors": ["Neil Mehta", "Ines Goncalves", "Alberto Montagna", "Mathis Fleury", "Gustavo Caetano", "Ines Esteves", "Athanasios Vourvopoulos", "Pulkit Grover", "Patricia Figueiredo"], "title": "Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Simultaneous EEG-fMRI recordings are increasingly used to investigate brain\nactivity by leveraging the complementary high spatial and high temporal\nresolution of fMRI and EEG signals respectively. It remains unclear, however,\nto what degree these two imaging modalities capture shared information about\nneural activity. Here, we investigate whether it is possible to predict both\ntask-evoked and spontaneous fMRI signals of motor brain networks from EEG\ntime-varying spectral power using interpretable models trained for individual\nsubjects with Sparse Group Lasso regularization. Critically, we test the\ntrained models on data acquired from each subject on a different day and obtain\nstatistical validation by comparison with appropriate null models as well as\nthe conventional EEG sensorimotor rhythm. We find significant prediction\nresults in most subjects, although less frequently for resting-state compared\nto task-based conditions. Furthermore, we interpret the model learned\nparameters to understand representations of EEG-fMRI coupling in terms of\npredictive EEG channels, frequencies, and haemodynamic delays. In conclusion,\nour work provides evidence of the ability to predict fMRI motor brain activity\nfrom EEG recordings alone across different days, in both task-evoked and\nspontaneous conditions, with statistical significance in individual subjects.\nThese results present great potential for translation to EEG neurofeedback\napplications.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660eEEG\u53ef\u4ee5\u9884\u6d4bfMRI\u8fd0\u52a8\u8111\u7f51\u7edc\u4fe1\u53f7\uff0c\u5728\u4efb\u52a1\u548c\u81ea\u53d1\u6761\u4ef6\u4e0b\u8de8\u4e0d\u540c\u5929\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff0c\u5e76\u6709\u795e\u7ecf\u53cd\u9988\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8EEG\u548cfMRI\u662f\u5426\u6355\u6349\u5171\u4eab\u795e\u7ecf\u6d3b\u52a8\u4fe1\u606f\uff0c\u7279\u522b\u662f\u9884\u6d4bfMRI\u4fe1\u53f7\u7684\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528Sparse Group Lasso\u6b63\u5219\u5316\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u9488\u5bf9\u4e2a\u4f53\u53d7\u8bd5\u8005\u8bad\u7ec3\uff0c\u5e76\u5728\u4e0d\u540c\u5929\u6d4b\u8bd5\uff0c\u4e0e\u7a7a\u6a21\u578b\u548c\u4f20\u7edfEEG\u4f20\u611f\u5668\u8fd0\u52a8\u8282\u5f8b\u6bd4\u8f83\u3002", "result": "\u5728\u5927\u591a\u6570\u53d7\u8bd5\u8005\u4e2d\u83b7\u5f97\u663e\u8457\u9884\u6d4b\uff0c\u4efb\u52a1\u6761\u4ef6\u4e0b\u66f4\u9891\u7e41\uff0c\u89e3\u91ca\u4e86EEG\u901a\u9053\u3001\u9891\u7387\u548c\u8840\u6d41\u52a8\u529b\u5b66\u5ef6\u8fdf\u3002", "conclusion": "\u8bc1\u660eEEG\u53ef\u8de8\u4e0d\u540c\u5929\u9884\u6d4bfMRI\u6d3b\u52a8\uff0c\u5177\u6709\u4e2a\u4f53\u7edf\u8ba1\u5b66\u610f\u4e49\uff0c\u5e76\u4e3aEEG\u795e\u7ecf\u53cd\u9988\u5e94\u7528\u63d0\u4f9b\u6f5c\u529b\u3002"}}
{"id": "2504.10699", "pdf": "https://arxiv.org/pdf/2504.10699", "abs": "https://arxiv.org/abs/2504.10699", "authors": ["Nan Wang", "Ricardo G. Sanfelice"], "title": "HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "59 pages, 9 figures, submitted to IJRR. arXiv admin note: substantial\n  text overlap with arXiv:2403.18413; text overlap with arXiv:2406.01802", "summary": "This paper proposes a bidirectional rapidly-exploring random trees (RRT)\nalgorithm to solve the motion planning problem for hybrid systems. The proposed\nalgorithm, called HyRRT-Connect, propagates in both forward and backward\ndirections in hybrid time until an overlap between the forward and backward\npropagation results is detected. Then, HyRRT-Connect constructs a motion plan\nthrough the reversal and concatenation of functions defined on hybrid time\ndomains, ensuring that the motion plan satisfies the given hybrid dynamics. To\naddress the potential discontinuity along the flow caused by tolerating some\ndistance between the forward and backward partial motion plans, we reconstruct\nthe backward partial motion plan by a forward-in-hybrid-time simulation from\nthe final state of the forward partial motion plan. effectively eliminating the\ndiscontinuity. The proposed algorithm is applied to an actuated bouncing ball\nsystem and a walking robot example to highlight its computational improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyRRT-Connect\u7b97\u6cd5\uff0c\u7528\u4e8e\u6df7\u5408\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u53cc\u5411\u4f20\u64ad\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u7cfb\u7edf\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u6df7\u5408\u52a8\u529b\u5b66\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u548c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faHyRRT-Connect\u7b97\u6cd5\uff0c\u5728\u6df7\u5408\u65f6\u95f4\u5185\u53cc\u5411\u4f20\u64ad\u68c0\u6d4b\u91cd\u53e0\uff0c\u6784\u5efa\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u524d\u5411\u6a21\u62df\u91cd\u5efa\u540e\u5411\u90e8\u5206\u6d88\u9664\u4e0d\u8fde\u7eed\u6027\u3002", "result": "\u5e94\u7528\u4e8e\u5e26\u9a71\u52a8\u7684\u5f39\u8df3\u7403\u7cfb\u7edf\u548c\u6b65\u884c\u673a\u5668\u4eba\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HyRRT-Connect\u7b97\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6df7\u5408\u7cfb\u7edf\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2504.11419", "pdf": "https://arxiv.org/pdf/2504.11419", "abs": "https://arxiv.org/abs/2504.11419", "authors": ["Li Jin", "Liu Jia"], "title": "Embodied World Models Emerge from Navigational Task in Open-Ended Environments", "categories": ["cs.AI", "cs.NE"], "comment": "Research on explainable meta-reinforcement learning AI", "summary": "Understanding how artificial systems can develop spatial awareness and\nreasoning has long been a challenge in AI research. Traditional models often\nrely on passive observation, but embodied cognition theory suggests that deeper\nunderstanding emerges from active interaction with the environment. This study\ninvestigates whether neural networks can autonomously internalize spatial\nconcepts through interaction, focusing on planar navigation tasks. Using Gated\nRecurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we\nshow that agents can learn to encode spatial properties like direction,\ndistance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)\nto model the agent-environment interaction as a closed dynamical system,\nrevealing stable limit cycles that correspond to optimal navigation strategies.\nRidge Representation allows us to map navigation paths into a fixed-dimensional\nbehavioral space, enabling comparison with neural states. Canonical Correlation\nAnalysis (CCA) confirms strong alignment between these representations,\nsuggesting that the agent's neural states actively encode spatial knowledge.\nIntervention experiments further show that specific neural dimensions are\ncausally linked to navigation performance. This work provides an approach to\nbridging the gap between action and perception in AI, offering new insights\ninto building adaptive, interpretable models that can generalize across complex\nenvironments. The causal validation of neural representations also opens new\navenues for understanding and controlling the internal mechanisms of AI\nsystems, pushing the boundaries of how machines learn and reason in dynamic,\nreal-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5143\u5f3a\u5316\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5c55\u793a\u4e86AI\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u4e3b\u52a8\u4e92\u52a8\u5b66\u4e60\u7a7a\u95f4\u610f\u8bc6\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u9a8c\u8bc1\u786e\u8ba4\u795e\u7ecf\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "motivation": "AI\u7814\u7a76\u4e2d\uff0c\u7a7a\u95f4\u610f\u8bc6\u548c\u63a8\u7406\u53d1\u5c55\u957f\u671f\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\uff0c\u800c\u5177\u8eab\u8ba4\u77e5\u7406\u8bba\u5f3a\u8c03\u4e3b\u52a8\u4e92\u52a8\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u5355\u5143(GRU)\u7ed3\u5408\u5143\u5f3a\u5316\u5b66\u4e60(Meta-RL)\uff0c\u5f15\u5165\u6df7\u5408\u52a8\u529b\u7cfb\u7edf(HDS)\u5efa\u6a21\u4ea4\u4e92\uff0c\u810a\u8868\u793a\u6620\u5c04\u8def\u5f84\uff0c\u89c4\u8303\u76f8\u5173\u5206\u6790(CCA)\u786e\u8ba4\u5bf9\u9f50\uff0c\u5e76\u8fdb\u884c\u5e72\u9884\u5b9e\u9a8c\u3002", "result": "\u4ee3\u7406\u5b66\u4f1a\u7f16\u7801\u65b9\u5411\u3001\u8ddd\u79bb\u548c\u969c\u788d\u907f\u514d\uff0c\u663e\u793a\u7a33\u5b9a\u6781\u9650\u73af\uff0cCCA\u786e\u8ba4\u795e\u7ecf\u72b6\u6001\u4e0e\u884c\u4e3a\u7a7a\u95f4\u5f3a\u5bf9\u9f50\uff0c\u5e72\u9884\u5b9e\u9a8c\u8bc1\u5b9e\u7279\u5b9a\u795e\u7ecf\u7ef4\u5ea6\u4e0e\u5bfc\u822a\u6027\u80fd\u7684\u56e0\u679c\u8054\u7cfb\u3002", "conclusion": "\u6865\u63a5\u884c\u52a8\u4e0e\u611f\u77e5\u7684\u9e3f\u6c9f\uff0c\u63d0\u4f9b\u6784\u5efa\u9002\u5e94\u6027\u3001\u53ef\u89e3\u91caAI\u6a21\u578b\u7684\u6d1e\u89c1\uff0c\u5e76\u4e3a\u7406\u89e3\u548c\u63a7\u5236AI\u5185\u90e8\u673a\u5236\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.10754", "pdf": "https://arxiv.org/pdf/2504.10754", "abs": "https://arxiv.org/abs/2504.10754", "authors": ["Arjun Subramonian", "Elvis Dohmatob"], "title": "auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory", "categories": ["cs.LG", "cs.MS"], "comment": "Work in progress", "summary": "A large part of modern machine learning theory often involves computing the\nhigh-dimensional expected trace of a rational expression of large rectangular\nrandom matrices. To symbolically compute such quantities using free probability\ntheory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that\ncan automatically produce a reduced system of fixed-point equations which can\nbe solved for the quantities of interest, and effectively constitutes a theory.\nWe overview the algorithmic ideas underlying auto-fpt and its applications to\nvarious interesting problems, such as the high-dimensional error of linearized\nfeed-forward neural networks, recovering well-known results. We hope that\nauto-fpt streamlines the majority of calculations involved in high-dimensional\nanalysis, while helping the machine learning community reproduce known and\nuncover new phenomena.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165auto-fpt\u5de5\u5177\uff0c\u4f7f\u7528\u81ea\u7531\u6982\u7387\u7406\u8bba\u81ea\u52a8\u8ba1\u7b97\u9ad8\u7ef4\u968f\u673a\u77e9\u9635\u671f\u671b\u8ff9\uff0c\u7b80\u5316\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7406\u8bba\u4e2d\u9ad8\u7ef4\u8ba1\u7b97\u590d\u6742\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u7b80\u5316\u7b26\u53f7\u8ba1\u7b97\u3002", "method": "\u5f00\u53d1\u57fa\u4e8ePython\u548cSymPy\u7684auto-fpt\u5de5\u5177\uff0c\u81ea\u52a8\u751f\u6210\u5e76\u6c42\u89e3\u56fa\u5b9a\u70b9\u65b9\u7a0b\u7cfb\u7edf\u3002", "result": "\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u9ad8\u7ef4\u8bef\u5dee\u5206\u6790\uff0c\u590d\u73b0\u5df2\u77e5\u7ed3\u679c\u3002", "conclusion": "\u5e0c\u671bauto-fpt\u7b80\u5316\u9ad8\u7ef4\u5206\u6790\uff0c\u5e2e\u52a9\u673a\u5668\u5b66\u4e60\u793e\u533a\u590d\u73b0\u548c\u53d1\u73b0\u65b0\u73b0\u8c61\u3002"}}
{"id": "2504.10767", "pdf": "https://arxiv.org/pdf/2504.10767", "abs": "https://arxiv.org/abs/2504.10767", "authors": ["Faiyaz Elahi Mullick", "Supriyo Bandyopadhyay", "Rob Baxter", "Tony J. Ragucci", "Avik W. Ghosh"], "title": "Adaptive Synaptogenesis Implemented on a Nanomagnetic Platform", "categories": ["cond-mat.dis-nn", "cs.NE", "cs.SY", "eess.SY"], "comment": null, "summary": "The human brain functions very differently from artificial neural networks\n(ANN) and possesses unique features that are absent in ANN. An important one\namong them is \"adaptive synaptogenesis\" that modifies synaptic weights when\nneeded to avoid catastrophic forgetting and promote lifelong learning. The key\naspect of this algorithm is supervised Hebbian learning, where weight\nmodifications in the neocortex driven by temporal coincidence are further\naccepted or vetoed by an added control mechanism from the hippocampus during\nthe training cycle, to make distant synaptic connections highly sparse and\nstrategic. In this work, we discuss various algorithmic aspects of adaptive\nsynaptogenesis tailored to edge computing, demonstrate its function using\nsimulations, and design nanomagnetic hardware accelerators for specific\nfunctions of synaptogenesis.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4eba\u7c7b\u5927\u8111\u7684adaptive synaptogenesis\u7b97\u6cd5\uff0c\u7528\u4e8eAI\u907f\u514d\u9057\u5fd8\u548c\u4fc3\u8fdb\u7ec8\u8eab\u5b66\u4e60\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u73b0\u3002", "motivation": "\u4eba\u7c7b\u5927\u8111\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\uff0c\u5177\u6709adaptive synaptogenesis\uff0c\u80fd\u52a8\u6001\u8c03\u6574\u7a81\u89e6\u4ee5\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u548c\u652f\u6301\u7ec8\u8eab\u5b66\u4e60\u3002", "method": "\u91c7\u7528supervised Hebbian\u5b66\u4e60\u548c\u6d77\u9a6c\u4f53\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u62df\u6f14\u793a\u548c\u8bbe\u8ba1\u7eb3\u7c73\u78c1\u786c\u4ef6\u52a0\u901f\u5668\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u529f\u80fd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002", "conclusion": "adaptive synaptogenesis\u7b97\u6cd5\u53ef\u63d0\u5347AI\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "1906.05682", "pdf": "https://arxiv.org/pdf/1906.05682", "abs": "https://arxiv.org/abs/1906.05682", "authors": ["Suraj Tripathi", "Abhay Kumar", "Abhiram Ramesh", "Chirag Singh", "Promod Yenigalla"], "title": "Focal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "stat.ML"], "comment": "Accepted in CICLing 2019", "summary": "This paper proposes a Residual Convolutional Neural Network (ResNet) based on\nspeech features and trained under Focal Loss to recognize emotion in speech.\nSpeech features such as Spectrogram and Mel-frequency Cepstral Coefficients\n(MFCCs) have shown the ability to characterize emotion better than just plain\ntext. Further Focal Loss, first used in One-Stage Object Detectors, has shown\nthe ability to focus the training process more towards hard-examples and\ndown-weight the loss assigned to well-classified examples, thus preventing the\nmodel from being overwhelmed by easily classifiable examples.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528ResNet\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u8bed\u97f3\u7279\u5f81\u548cFocal Loss\u6765\u8bc6\u522b\u8bed\u97f3\u60c5\u611f\u3002", "motivation": "\u8bed\u97f3\u7279\u5f81\u5982Spectrogram\u548cMFCCs\u6bd4\u7eaf\u6587\u672c\u66f4\u80fd\u8868\u5f81\u60c5\u611f\uff0cFocal Loss\u5e2e\u52a9\u8bad\u7ec3\u5173\u6ce8\u96be\u4f8b\u3002", "method": "\u57fa\u4e8eResNet\u6784\u5efa\u6a21\u578b\uff0c\u4f7f\u7528\u8bed\u97f3\u7279\u5f81\u548cFocal Loss\u8bad\u7ec3\u3002", "result": "\u6539\u8fdb\u4e86\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u96be\u4f8b\u7684\u5904\u7406\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u63d0\u5347\u4e86\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.10766", "pdf": "https://arxiv.org/pdf/2504.10766", "abs": "https://arxiv.org/abs/2504.10766", "authors": ["Ming Li", "Yanhong Li", "Ziyue Li", "Tianyi Zhou"], "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u68af\u5ea6\u8c31\u5206\u6790\u7edf\u4e00\u6570\u636e\u8d28\u91cf\u6307\u6807\uff0c\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u5bf9LLM\u5fae\u8c03\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u6b64\u9886\u57df\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5c42\u7ea7\u68af\u5ea6\u7684\u5947\u5f02\u503c\u5206\u89e3(SVD)\u8fdb\u884c\u8c31\u5206\u6790\uff0c\u8bc4\u4f30\u6307\u4ee4\u548c\u63a8\u7406\u6570\u636e\u7684\u8d28\u91cf\u3002", "result": "\u9ad8\u8d28\u91cf\u6570\u636e\u5177\u6709\u8f83\u4f4e\u6838\u8303\u6570\u548c\u8f83\u9ad8\u6709\u6548\u79e9\uff1b\u6709\u6548\u79e9\u66f4\u80fd\u6355\u6349\u5fae\u5999\u5dee\u5f02\uff1b\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u68af\u5ea6\u6a21\u5f0f\u76f8\u4f3c\uff0c\u4e0d\u540c\u5bb6\u65cf\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u63d0\u4f9b\u6570\u636e\u8d28\u91cf\u5f71\u54cd\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u5e2e\u52a9\u5f00\u53d1\u66f4\u597d\u7684\u6570\u636e\u63a2\u7d22\u7b56\u7565\u3002"}}
{"id": "2504.10962", "pdf": "https://arxiv.org/pdf/2504.10962", "abs": "https://arxiv.org/abs/2504.10962", "authors": ["Edvin Martin Andrejev", "Amith Manoharan", "Karl-Eerik Unt", "Arun Kumar Singh"], "title": "$\u03c0$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 4 figures, submitted to IEEE RA-L", "summary": "Model Predictive Path Integral (MPPI) is a popular sampling-based Model\nPredictive Control (MPC) algorithm for nonlinear systems. It optimizes\ntrajectories by sampling control sequences and averaging them. However, a key\nissue with MPPI is the non-smoothness of the optimal control sequence, leading\nto oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing\nsolutions use post-hoc smoothing, which fails to bound control derivatives.\nThis paper introduces a new approach: we add a projection filter $\\pi$ to\nminimally correct control samples, ensuring bounds on control magnitude and\nhigher-order derivatives. The filtered samples are then averaged using MPPI,\nleading to our $\\pi$-MPPI approach. We minimize computational overhead by using\na neural accelerated custom optimizer for the projection filter. $\\pi$-MPPI\noffers a simple way to achieve arbitrary smoothness in control sequences. While\nwe focus on FWVs, this projection filter can be integrated into any MPPI\npipeline. Applied to FWVs, $\\pi$-MPPI is easier to tune than the baseline,\nresulting in smoother, more robust performance.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u03c0-MPPI\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u6295\u5f71\u8fc7\u6ee4\u5668\u6539\u5584MPPI\u7b97\u6cd5\u7684\u63a7\u5236\u5e73\u6ed1\u6027\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7ffc\u98de\u884c\u5668\u3002", "motivation": "\u89e3\u51b3MPPI\u7b97\u6cd5\u4e2d\u63a7\u5236\u5e8f\u5217\u4e0d\u5e73\u6ed1\u5bfc\u81f4\u7cfb\u7edf\u632f\u8361\u7684\u95ee\u9898\uff0c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u7ea6\u675f\u63a7\u5236\u5bfc\u6570\u3002", "method": "\u5f15\u5165\u6295\u5f71\u8fc7\u6ee4\u5668\u03c0\u5bf9\u63a7\u5236\u6837\u672c\u8fdb\u884c\u6700\u5c0f\u4fee\u6b63\uff0c\u786e\u4fdd\u63a7\u5236\u5e45\u5ea6\u548c\u5bfc\u6570\u8fb9\u754c\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u52a0\u901f\u4f18\u5316\u5668\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u03c0-MPPI\u5728\u56fa\u5b9a\u7ffc\u98de\u884c\u5668\u4e0a\u8c03\u4f18\u66f4\u5bb9\u6613\uff0c\u83b7\u5f97\u66f4\u5e73\u6ed1\u548c\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "\u03c0-MPPI\u63d0\u4f9b\u7b80\u5355\u5b9e\u73b0\u4efb\u610f\u5e73\u6ed1\u5ea6\u63a7\u5236\u5e8f\u5217\u7684\u65b9\u6cd5\uff0c\u53ef\u6574\u5408\u5230\u4efb\u4f55MPPI\u7ba1\u9053\u4e2d\u3002"}}
{"id": "1908.08652", "pdf": "https://arxiv.org/pdf/1908.08652", "abs": "https://arxiv.org/abs/1908.08652", "authors": ["Abhay Kumar", "Nishant Jain", "Suraj Tripathi", "Chirag Singh", "Kamal Krishna"], "title": "MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "5 pages, 3 figures, Accepted in IEEE AVSS 2019", "summary": "We propose a Multi-Task Learning (MTL) paradigm based deep neural network\narchitecture, called MTCNet (Multi-Task Crowd Network) for crowd density and\ncount estimation. Crowd count estimation is challenging due to the non-uniform\nscale variations and the arbitrary perspective of an individual image. The\nproposed model has two related tasks, with Crowd Density Estimation as the main\ntask and Crowd-Count Group Classification as the auxiliary task. The auxiliary\ntask helps in capturing the relevant scale-related information to improve the\nperformance of the main task. The main task model comprises two blocks: VGG-16\nfront-end for feature extraction and a dilated Convolutional Neural Network for\ndensity map generation. The auxiliary task model shares the same front-end as\nthe main task, followed by a CNN classifier. Our proposed network achieves 5.8%\nand 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on\nShanghaiTech dataset without using any data augmentation. Our model also\noutperforms with 10.5% lower MAE on UCF_CC_50 dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u7f51\u7edcMTCNet\uff0c\u7528\u4e8e\u4eba\u7fa4\u5bc6\u5ea6\u548c\u8ba1\u6570\u4f30\u8ba1\uff0c\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u63d0\u5347\u4e86\u4e3b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7fa4\u8ba1\u6570\u4f30\u8ba1\u9762\u4e34\u975e\u5747\u5300\u5c3a\u5ea6\u53d8\u5316\u548c\u4efb\u610f\u89c6\u89d2\u7684\u6311\u6218\uff0c\u56e0\u6b64\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u6355\u6349\u76f8\u5173\u5c3a\u5ea6\u4fe1\u606f\u3002", "method": "\u6a21\u578b\u5305\u62ec\u4e3b\u4efb\u52a1\uff08\u4f7f\u7528VGG-16\u63d0\u53d6\u7279\u5f81\u548c\u6269\u5f20CNN\u751f\u6210\u5bc6\u5ea6\u56fe\uff09\u548c\u8f85\u52a9\u4efb\u52a1\uff08\u5171\u4eab\u524d\u7aef\u7684CNN\u5206\u7c7b\u5668\uff09\u3002", "result": "\u5728ShanghaiTech\u6570\u636e\u96c6\u4e0a\uff0cMAE\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u4f4e5.8%\u548c14.9%\uff08\u65e0\u6570\u636e\u589e\u5f3a\uff09\uff0c\u5728UCF_CC_50\u6570\u636e\u96c6\u4e0a\u4f4e10.5%\u3002", "conclusion": "\u8bc1\u660e\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5728\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.10770", "pdf": "https://arxiv.org/pdf/2504.10770", "abs": "https://arxiv.org/abs/2504.10770", "authors": ["Donglin Zhan", "Haoting Zhang", "Rhonda Righter", "Zeyu Zheng", "James Anderson"], "title": "Collaborative Bayesian Optimization via Wasserstein Barycenters", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Motivated by the growing need for black-box optimization and data privacy, we\nintroduce a collaborative Bayesian optimization (BO) framework that addresses\nboth of these challenges. In this framework agents work collaboratively to\noptimize a function they only have oracle access to. In order to mitigate\nagainst communication and privacy constraints, agents are not allowed to share\ntheir data but can share their Gaussian process (GP) surrogate models. To\nenable collaboration under these constraints, we construct a central model to\napproximate the objective function by leveraging the concept of Wasserstein\nbarycenters of GPs. This central model integrates the shared models without\naccessing the underlying data. A key aspect of our approach is a collaborative\nacquisition function that balances exploration and exploitation, allowing for\nthe optimization of decision variables collaboratively in each iteration. We\nprove that our proposed algorithm is asymptotically consistent and that its\nimplementation via Monte Carlo methods is numerically accurate. Through\nnumerical experiments, we demonstrate that our approach outperforms other\nbaseline collaborative frameworks and is competitive with centralized\napproaches that do not consider data privacy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u534f\u4f5c\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u9ed1\u7bb1\u4f18\u5316\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u4ee3\u7406\u5171\u4eabGP\u6a21\u578b\u4f7f\u7528Wasserstein\u91cd\u5fc3\uff0c\u8bc1\u660e\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u9ed1\u7bb1\u4f18\u5316\u548c\u6570\u636e\u9690\u79c1\u9700\u6c42\u589e\u957f\u3002", "method": "\u5f15\u5165\u534f\u4f5cBO\u6846\u67b6\uff0c\u4ee3\u7406\u5171\u4eabGP\u6a21\u578b\u4e0d\u5171\u4eab\u6570\u636e\uff0c\u6784\u5efaWasserstein\u91cd\u5fc3\u4e2d\u5fc3\u6a21\u578b\u548c\u534f\u4f5c\u83b7\u53d6\u51fd\u6570\u3002", "result": "\u8bc1\u660e\u7b97\u6cd5\u6e10\u8fdb\u4e00\u81f4\u548c\u6570\u503c\u51c6\u786e\uff0c\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6846\u67b6\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u4f18\u5316\u3002"}}
{"id": "2504.11045", "pdf": "https://arxiv.org/pdf/2504.11045", "abs": "https://arxiv.org/abs/2504.11045", "authors": ["Shreenabh Agrawal", "Manan Tayal", "Aditya Singh", "Shishir Kolathaya"], "title": "Neural Control Barrier Functions from Physics Informed Neural Networks", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 5 figures", "summary": "As autonomous systems become increasingly prevalent in daily life, ensuring\ntheir safety is paramount. Control Barrier Functions (CBFs) have emerged as an\neffective tool for guaranteeing safety; however, manually designing them for\nspecific applications remains a significant challenge. With the advent of deep\nlearning techniques, recent research has explored synthesizing CBFs using\nneural networks-commonly referred to as neural CBFs. This paper introduces a\nnovel class of neural CBFs that leverages a physics-inspired neural network\nframework by incorporating Zubov's Partial Differential Equation (PDE) within\nthe context of safety. This approach provides a scalable methodology for\nsynthesizing neural CBFs applicable to high-dimensional systems. Furthermore,\nby utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework\nallows for the specification of flexible, user-defined safe regions. To\nvalidate the effectiveness of the approach, we present case studies on three\ndifferent systems: an inverted pendulum, autonomous ground navigation, and\naerial navigation in obstacle-laden environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZubov PDE\u7684\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u65b9\u6cd5\uff0c\u63d0\u9ad8\u81ea\u4e3b\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u666e\u53ca\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edfCBFs\u8bbe\u8ba1\u56f0\u96be\uff0c\u6df1\u5ea6\u5b66\u4e60\u88ab\u7528\u4e8e\u5408\u6210\u795e\u7ecfCBFs\u3002", "method": "\u5f15\u5165\u7269\u7406\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408Zubov PDE\u548c\u4e92\u60e0CBFs\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\uff0c\u901a\u8fc7\u5012\u7acb\u6446\u3001\u5730\u9762\u548c\u7a7a\u4e2d\u5bfc\u822a\u6848\u4f8b\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u53ef\u6269\u5c55\uff0c\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u795e\u7ecfCBFs\u5408\u6210\u65b9\u6cd5\uff0c\u63d0\u5347\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u80fd\u3002"}}
{"id": "2504.09861", "pdf": "https://arxiv.org/pdf/2504.09861", "abs": "https://arxiv.org/abs/2504.09861", "authors": ["Luyao Zhang"], "title": "EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)", "categories": ["cs.CY", "cs.AI", "cs.HC", "econ.GN", "q-fin.EC", "stat.AP"], "comment": null, "summary": "Large language models (LLMs) are transforming global decision-making and\nsocietal systems by processing diverse data at unprecedented scales. However,\ntheir potential to homogenize human values poses critical risks, similar to\nbiodiversity loss undermining ecological resilience. Rooted in the ancient\nGreek concept of ethos, meaning both individual character and the shared moral\nfabric of communities, EthosGPT draws on a tradition that spans from\nAristotle's virtue ethics to Adam Smith's moral sentiments as the ethical\nfoundation of economic cooperation. These traditions underscore the vital role\nof value diversity in fostering social trust, institutional legitimacy, and\nlong-term prosperity. EthosGPT addresses the challenge of value homogenization\nby introducing an open-source framework for mapping and evaluating LLMs within\na global scale of human values. Using international survey data on cultural\nindices, prompt-based assessments, and comparative statistical analyses,\nEthosGPT reveals both the adaptability and biases of LLMs across regions and\ncultures. It offers actionable insights for developing inclusive LLMs, such as\ndiversifying training data and preserving endangered cultural heritage to\nensure representation in AI systems. These contributions align with the United\nNations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced\nInequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace,\nJustice and Strong Institutions). Through interdisciplinary collaboration,\nEthosGPT promotes AI systems that are both technically robust and ethically\ninclusive, advancing value plurality as a cornerstone for sustainable and\nequitable futures.", "AI": {"tldr": "EthosGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u6620\u5c04\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e2d\u7684\u8868\u73b0\uff0c\u5e94\u5bf9LLM\u5bfc\u81f4\u4ef7\u503c\u540c\u8d28\u5316\u7684\u98ce\u9669\uff0c\u4fc3\u8fdb\u4ef7\u503c\u591a\u6837\u6027\u3002", "motivation": "\u8bba\u6587\u52a8\u673a\u662f\u89e3\u51b3LLM\u53ef\u80fd\u4f7f\u4eba\u7c7b\u4ef7\u503c\u89c2\u540c\u8d28\u5316\u7684\u98ce\u9669\uff0c\u7c7b\u4f3c\u4e8e\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u5bf9\u751f\u6001\u97e7\u6027\u7684\u5a01\u80c1\uff0c\u5f3a\u8c03\u4ef7\u503c\u591a\u6837\u6027\u5bf9\u793e\u4f1a\u4fe1\u4efb\u3001\u673a\u6784\u5408\u6cd5\u6027\u548c\u957f\u671f\u7e41\u8363\u7684\u91cd\u8981\u6027\uff0c\u5e76\u501f\u9274\u53e4\u5e0c\u814a\u4ee5\u592a\u6982\u5ff5\u548c\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002", "method": "\u4f7f\u7528\u56fd\u9645\u8c03\u67e5\u6570\u636e\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u8bc4\u4f30\u548c\u6bd4\u8f83\u7edf\u8ba1\u5206\u6790\u6765\u6620\u5c04\u548c\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u5730\u533a\u548c\u6587\u5316\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63ed\u793aLLM\u7684\u9002\u5e94\u6027\u548c\u504f\u5dee\uff0c\u63d0\u4f9b\u884c\u52a8\u6d1e\u89c1\uff0c\u5982\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u548c\u4fdd\u62a4\u6fd2\u5371\u6587\u5316\u9057\u4ea7\uff0c\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u7684\u4ee3\u8868\u6027\u3002", "conclusion": "\u901a\u8fc7\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u4fc3\u8fdb\u6280\u672f\u7a33\u5065\u548c\u4f26\u7406\u5305\u5bb9\u7684AI\u7cfb\u7edf\uff0c\u63a8\u8fdb\u4ef7\u503c\u591a\u5143\u6027\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u548c\u516c\u5e73\u7684\u672a\u6765\uff0c\u5e76\u4e0e\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08\u5982\u51cf\u5c11\u4e0d\u5e73\u7b49\u3001\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u4e0e\u548c\u5e73\u6b63\u4e49\uff09\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2504.10777", "pdf": "https://arxiv.org/pdf/2504.10777", "abs": "https://arxiv.org/abs/2504.10777", "authors": ["Manu Bhat", "Jonghyun Park", "Jianke Yang", "Nima Dehmamy", "Robin Walters", "Rose Yu"], "title": "AtlasD: Automatic Local Symmetry Discovery", "categories": ["cs.LG"], "comment": null, "summary": "Existing symmetry discovery methods predominantly focus on global\ntransformations across the entire system or space, but they fail to consider\nthe symmetries in local neighborhoods. This may result in the reported symmetry\ngroup being a misrepresentation of the true symmetry. In this paper, we\nformalize the notion of local symmetry as atlas equivariance. Our proposed\npipeline, automatic local symmetry discovery (AtlasD), recovers the local\nsymmetries of a function by training local predictor networks and then learning\na Lie group basis to which the predictors are equivariant. We demonstrate\nAtlasD is capable of discovering local symmetry groups with multiple connected\ncomponents in top-quark tagging and partial differential equation experiments.\nThe discovered local symmetry is shown to be a useful inductive bias that\nimproves the performance of downstream tasks in climate segmentation and vision\ntasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faAtlasD\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d1\u73b0\u5c40\u90e8\u5bf9\u79f0\u6027\u6765\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5c40\u90e8\u90bb\u57df\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u5bf9\u79f0\u7fa4 misrepresentation\u3002", "method": "\u5f62\u5f0f\u5316\u5c40\u90e8\u5bf9\u79f0\u6027\u4e3a\u56fe\u96c6\u7b49\u53d8\u6027\uff0c\u8bad\u7ec3\u5c40\u90e8\u9884\u6d4b\u7f51\u7edc\u5e76\u5b66\u4e60Lie\u7fa4\u57fa\u3002", "result": "\u5728top-quark tagging\u548cPDE\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u591a\u8fde\u901a\u5206\u91cf\u5c40\u90e8\u5bf9\u79f0\u7fa4\uff0c\u63d0\u5347\u6c14\u5019\u5206\u5272\u548c\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u5c40\u90e8\u5bf9\u79f0\u6027\u4f5c\u4e3a\u5f52\u7eb3\u504f\u5dee\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u3002"}}
{"id": "2504.11064", "pdf": "https://arxiv.org/pdf/2504.11064", "abs": "https://arxiv.org/abs/2504.11064", "authors": ["Bo Ma", "Yi Ji", "Liyong Fang"], "title": "A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field", "categories": ["cs.MA", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "The traditional Artificial Potential Field (APF) method exhibits limitations\nin its force distribution: excessive attraction when UAVs are far from the\ntarget may cause collisions with obstacles, while insufficient attraction near\nthe goal often results in failure to reach the target. Furthermore, APF is\nhighly susceptible to local minima, compromising motion reliability in complex\nenvironments. To address these challenges, this paper presents a novel hybrid\nobstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial\nPotential Field (DSA-AAPF)-which combines an improved simulated annealing\nmechanism with an enhanced APF model. The proposed approach integrates a\nLeader-Follower distributed formation strategy with the APF framework, where\nthe resultant force formulation is redefined to smooth UAV trajectories. An\nadaptive gravitational gain function is introduced to dynamically adjust UAV\nvelocity based on environmental context, and a fast-converging controller\nensures accurate and efficient convergence to the target. Moreover, a\ndirectional deflection mechanism is embedded within the simulated annealing\nprocess, enabling UAVs to escape local minima caused by semi-enclosed obstacles\nthrough continuous rotational motion. The simulation results, covering\nformation reconfiguration, complex obstacle avoidance, and entrapment escape,\ndemonstrate the feasibility, robustness, and superiority of the proposed\nDSA-AAPF algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSA-AAPF\u7b97\u6cd5\uff0c\u6539\u8fdb\u4f20\u7edfAPF\u65b9\u6cd5\uff0c\u89e3\u51b3UAV\u969c\u788d\u907f\u514d\u4e2d\u7684\u5438\u5f15\u529b\u548c\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u6a21\u62df\u7ed3\u679c\u663e\u793a\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfAPF\u65b9\u6cd5\u5b58\u5728\u5438\u5f15\u529b\u5206\u5e03\u4e0d\u5f53\u548c\u6613\u9677\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4UAV\u5728\u590d\u6742\u73af\u5883\u4e2d\u53ef\u80fd\u78b0\u649e\u969c\u788d\u6216\u65e0\u6cd5\u8fbe\u6807\u3002", "method": "\u63d0\u51faDSA-AAPF\u7b97\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u6a21\u62df\u9000\u706b\u548c\u589e\u5f3aAPF\uff0c\u878d\u5165\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7b56\u7565\u3001\u81ea\u9002\u5e94\u91cd\u529b\u589e\u76ca\u548c\u65b9\u5411\u504f\u8f6c\u673a\u5236\uff0c\u4ee5\u5e73\u6ed1\u8f68\u8ff9\u548c\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8bc1\u660eDSA-AAPF\u5728\u7f16\u961f\u91cd\u6784\u3001\u590d\u6742\u969c\u788d\u907f\u514d\u548c\u9003\u79bb\u9677\u9631\u4e2d\u7684\u53ef\u884c\u6027\u3001\u7a33\u5065\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "DSA-AAPF\u7b97\u6cd5\u6709\u6548\u63d0\u5347UAV\u969c\u788d\u907f\u514d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\u3002"}}
{"id": "2504.10489", "pdf": "https://arxiv.org/pdf/2504.10489", "abs": "https://arxiv.org/abs/2504.10489", "authors": ["Vikranth Udandarao", "Noel Abraham Tiju", "Muthuraj Vairamuthu", "Harsh Mistry", "Dhruv Kumar"], "title": "Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "for code implementation, check\n  https://github.com/Roamify-Research/Roamify", "summary": "In this paper, we present Roamify, an Artificial Intelligence powered travel\nassistant that aims to ease the process of travel planning. We have tested and\nused multiple Large Language Models like Llama and T5 to generate personalised\nitineraries per user preferences. Results from user surveys highlight the\npreference for AI powered mediums over existing methods to help in travel\nplanning across all user age groups. These results firmly validate the\npotential need of such a travel assistant. We highlight the two primary design\nconsiderations for travel assistance: D1) incorporating a web-scraping method\nto gather up-to-date news articles about destinations from various blog\nsources, which significantly improves our itinerary suggestions, and D2)\nutilising user preferences to create customised travel experiences along with a\nrecommendation system which changes the itinerary according to the user needs.\nOur findings suggest that Roamify has the potential to improve and simplify how\nusers across multiple age groups plan their travel experiences.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Roamify\uff0c\u4e00\u4e2aAI\u9a71\u52a8\u7684\u65c5\u884c\u52a9\u624b\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u884c\u7a0b\uff0c\u7528\u6237\u8c03\u67e5\u663e\u793aAI\u65b9\u6cd5\u66f4\u53d7\u6b22\u8fce\uff0c\u5e76\u5f3a\u8c03\u8bbe\u8ba1\u8003\u8651\u3002", "motivation": "\u7b80\u5316\u65c5\u884c\u89c4\u5212\u8fc7\u7a0b\uff0c\u7528\u6237\u8c03\u67e5\u663e\u793a\u6240\u6709\u5e74\u9f84\u7ec4\u66f4\u504f\u597dAI\u8f85\u52a9\uff0c\u9a8c\u8bc1\u4e86\u6b64\u7c7b\u52a9\u624b\u7684\u5fc5\u8981\u6027\u3002", "method": "\u4f7f\u7528Llama\u548cT5\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u884c\u7a0b\uff0c\u7ed3\u5408\u7f51\u7edc\u722c\u866b\u83b7\u53d6\u5b9e\u65f6\u76ee\u7684\u5730\u4fe1\u606f\uff0c\u5e76\u57fa\u4e8e\u7528\u6237\u504f\u597d\u8c03\u6574\u63a8\u8350\u7cfb\u7edf\u3002", "result": "\u7528\u6237\u8c03\u67e5\u7ed3\u679c\u663e\u793aAI\u65b9\u6cd5\u66f4\u53d7\u6b22\u8fce\uff0c\u9a8c\u8bc1\u4e86\u65c5\u884c\u52a9\u624b\u7684\u6f5c\u5728\u9700\u6c42\u3002", "conclusion": "Roamify\u53ef\u6539\u5584\u5e76\u7b80\u5316\u4e0d\u540c\u5e74\u9f84\u7ec4\u7684\u65c5\u884c\u89c4\u5212\u4f53\u9a8c\u3002"}}
{"id": "2504.10807", "pdf": "https://arxiv.org/pdf/2504.10807", "abs": "https://arxiv.org/abs/2504.10807", "authors": ["Huseyin Tuna Erdinc", "Yunlin Zeng", "Abhinav Prakash Gahlot", "Felix J. Herrmann"], "title": "Power-scaled Bayesian Inference with Score-based Generative mModels", "categories": ["cs.LG", "cs.CV", "physics.geo-ph"], "comment": "8 pages, 4 figures", "summary": "We propose a score-based generative algorithm for sampling from power-scaled\npriors and likelihoods within the Bayesian inference framework. Our algorithm\nenables flexible control over prior-likelihood influence without requiring\nretraining for different power-scaling configurations. Specifically, we focus\non synthesizing seismic velocity models conditioned on imaged seismic. Our\nmethod enables sensitivity analysis by sampling from intermediate power\nposteriors, allowing us to assess the relative influence of the prior and\nlikelihood on samples of the posterior distribution. Through a comprehensive\nset of experiments, we evaluate the effects of varying the power parameter in\ndifferent settings: applying it solely to the prior, to the likelihood of a\nBayesian formulation, and to both simultaneously. The results show that\nincreasing the power of the likelihood up to a certain threshold improves the\nfidelity of posterior samples to the conditioning data (e.g., seismic images),\nwhile decreasing the prior power promotes greater structural diversity among\nsamples. Moreover, we find that moderate scaling of the likelihood leads to a\nreduced shot data residual, confirming its utility in posterior refinement.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5f97\u5206\u7684\u751f\u6210\u7b97\u6cd5\uff0c\u7528\u4e8e\u8d1d\u53f6\u65af\u63a8\u7406\u4e2d\u5e42\u7f29\u653e\u5148\u9a8c\u548c\u4f3c\u7136\u7684\u91c7\u6837\uff0c\u5e94\u7528\u4e8e\u5730\u9707\u901f\u5ea6\u6a21\u578b\u5408\u6210\u3002", "motivation": "\u52a8\u673a\u662f\u5b9e\u73b0\u5bf9\u5148\u9a8c\u548c\u4f3c\u7136\u5f71\u54cd\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u8bc4\u4f30\u5176\u5bf9\u540e\u9a8c\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u5f97\u5206\u7684\u751f\u6210\u7b97\u6cd5\uff0c\u4ece\u4e2d\u95f4\u5e42\u540e\u9a8c\u4e2d\u91c7\u6837\uff0c\u4e13\u6ce8\u4e8e\u6761\u4ef6\u5730\u9707\u56fe\u50cf\u7684\u5730\u9707\u901f\u5ea6\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u589e\u52a0\u4f3c\u7136\u5e42\u63d0\u9ad8\u6837\u672c\u6570\u636e\u4fdd\u771f\u5ea6\uff0c\u51cf\u5c11\u5148\u9a8c\u5e42\u589e\u52a0\u7ed3\u6784\u591a\u6837\u6027\uff1b\u9002\u5ea6\u4f3c\u7136\u7f29\u653e\u964d\u4f4e\u5c04\u51fb\u6570\u636e\u6b8b\u5dee\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8be5\u7b97\u6cd5\u5728\u540e\u9a8c\u7ec6\u5316\u548c\u654f\u611f\u6027\u5206\u6790\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.11372", "pdf": "https://arxiv.org/pdf/2504.11372", "abs": "https://arxiv.org/abs/2504.11372", "authors": ["Zhengbing He", "Jorge Laval", "Yu Han", "Ryosuke Nishi", "Cathy Wu"], "title": "A Review of Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving", "categories": ["physics.soc-ph", "cs.SY", "eess.SY", "stat.AP"], "comment": null, "summary": "The main form of freeway traffic congestion is the familiar stop-and-go wave,\ncharacterized by wide moving jams that propagate indefinitely upstream provided\nenough traffic demand. They cause severe, long-lasting adverse effects, such as\nreduced traffic efficiency, increased driving risks, and higher vehicle\nemissions. This underscores the crucial importance of artificial intervention\nin the propagation of stop-and-go waves. Over the past two decades, two\nprominent strategies for stop-and-go wave suppression have emerged: variable\nspeed limit (VSL) and jam-absorption driving (JAD). Although they share similar\nresearch motivations, objectives, and theoretical foundations, the development\nof these strategies has remained relatively disconnected. To synthesize\nfragmented advances and drive the field forward, this paper first provides a\ncomprehensive review of the achievements in the stop-and-go wave\nsuppression-oriented VSL and JAD, respectively. It then focuses on bridging the\ntwo areas and identifying research opportunities from the following\nperspectives: fundamental diagrams, traffic dynamics modeling, traffic state\nestimation and prediction, stochasticity, scenarios for strategy validation,\nand field tests and practical deployment. We expect that through this review,\none area can effectively address its limitations by identifying and leveraging\nthe strengths of the other, thus promoting the overall research goal of freeway\nstop-and-go wave suppression.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u56de\u987e\u4e86\u53ef\u53d8\u9650\u901f\uff08VSL\uff09\u548c\u62e5\u5835\u5438\u6536\u9a7e\u9a76\uff08JAD\uff09\u5728\u6291\u5236\u9ad8\u901f\u516c\u8def\u505c\u8f66-\u542f\u52a8\u6ce2\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u6865\u63a5\u4e86\u8fd9\u4e24\u4e2a\u9886\u57df\uff0c\u5e76\u6307\u51fa\u4e86\u7814\u7a76\u673a\u4f1a\u3002", "motivation": "\u505c\u8f66-\u542f\u52a8\u6ce2\u5bfc\u81f4\u4ea4\u901a\u6548\u7387\u964d\u4f4e\u3001\u9a7e\u9a76\u98ce\u9669\u589e\u52a0\u548c\u8f66\u8f86\u6392\u653e\u5347\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002\u8bba\u6587\u65e8\u5728\u7efc\u5408VSL\u548cJAD\u7684\u788e\u7247\u5316\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u9996\u5148\u5168\u9762\u56de\u987eVSL\u548cJAD\u7684\u6210\u5c31\uff0c\u7136\u540e\u4ece\u57fa\u7840\u56fe\u3001\u4ea4\u901a\u52a8\u6001\u5efa\u6a21\u3001\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u548c\u9884\u6d4b\u3001\u968f\u673a\u6027\u3001\u7b56\u7565\u9a8c\u8bc1\u573a\u666f\u4ee5\u53ca\u73b0\u573a\u6d4b\u8bd5\u548c\u5b9e\u9645\u90e8\u7f72\u7b49\u89d2\u5ea6\u6865\u63a5\u4e24\u4e2a\u9886\u57df\u5e76\u8bc6\u522b\u7814\u7a76\u673a\u4f1a\u3002", "result": "\u901a\u8fc7\u56de\u987e\u548c\u6865\u63a5\uff0c\u8bba\u6587\u4fc3\u8fdb\u4e86VSL\u548cJAD\u4e4b\u95f4\u7684\u4e92\u8865\uff0c\u63a8\u8fdb\u4e86\u9ad8\u901f\u516c\u8def\u505c\u8f66-\u542f\u52a8\u6ce2\u6291\u5236\u7684\u6574\u4f53\u7814\u7a76\u76ee\u6807\u3002", "conclusion": "\u671f\u671b\u901a\u8fc7\u8fd9\u79cd\u6865\u63a5\uff0c\u4e00\u4e2a\u9886\u57df\u53ef\u4ee5\u5229\u7528\u53e6\u4e00\u4e2a\u9886\u57df\u7684\u4f18\u52bf\u6765\u514b\u670d\u5176\u5c40\u9650\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u505c\u8f66-\u542f\u52a8\u6ce2\u6291\u5236\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.10496", "pdf": "https://arxiv.org/pdf/2504.10496", "abs": "https://arxiv.org/abs/2504.10496", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "title": "ArxivBench: Can LLMs Assist Researchers in Conducting Research?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable effectiveness in\ncompleting various tasks such as reasoning, translation, and question\nanswering. However the issue of factual incorrect content in LLM-generated\nresponses remains a persistent challenge. In this study, we evaluate both\nproprietary and open-source LLMs on their ability to respond with relevant\nresearch papers and accurate links to articles hosted on the arXiv platform,\nbased on high level prompts. To facilitate this evaluation, we introduce\narXivBench, a benchmark specifically designed to assess LLM performance across\neight major subject categories on arXiv and five subfields within computer\nscience, one of the most popular categories among them. Our findings reveal a\nconcerning accuracy of LLM-generated responses depending on the subject, with\nsome subjects experiencing significantly lower accuracy than others. Notably,\nClaude-3.5-Sonnet exhibits a substantial advantage in generating both relevant\nand accurate responses. And interestingly, most LLMs achieve a much higher\naccuracy in the Artificial Intelligence sub-field than other sub-fields. This\nbenchmark provides a standardized tool for evaluating the reliability of\nLLM-generated scientific responses, promoting more dependable use of LLMs in\nacademic and research environments. Our code is open-sourced at\nhttps://github.com/arxivBenchLLM/arXivBench and our dataset is available on\nhuggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210arXiv\u76f8\u5173\u8bba\u6587\u548c\u94fe\u63a5\u7684\u51c6\u786e\u6027\uff0c\u5f15\u5165arXivBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3LLMs\u751f\u6210\u4e8b\u5b9e\u9519\u8bef\u5185\u5bb9\u7684\u6311\u6218\uff0c\u63d0\u5347\u5728\u5b66\u672f\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528arXivBench\u57fa\u51c6\u6d4b\u8bd5\u5404\u79cdLLMs\u5728\u516b\u4e2aarXiv\u4e3b\u9898\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e94\u4e2a\u5b50\u9886\u57df\u7684\u6027\u80fd\u3002", "result": "\u51c6\u786e\u6027\u56e0\u4e3b\u9898\u800c\u5f02\uff0cClaude-3.5-Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u4eba\u5de5\u667a\u80fd\u5b50\u9886\u57df\u51c6\u786e\u6027\u8f83\u9ad8\u3002", "conclusion": "\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u4fc3\u8fdbLLM\u5728\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u4f7f\u7528\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2504.10817", "pdf": "https://arxiv.org/pdf/2504.10817", "abs": "https://arxiv.org/abs/2504.10817", "authors": ["Penghao Wang", "Qian Chen", "Teng Zhang", "Yingwei Zhang", "Wang Lu", "Yiqiang Chen"], "title": "FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as an effective solution for\nmulti-institutional collaborations without sharing patient data, offering a\nrange of methods tailored for diverse applications. However, real-world medical\ndatasets are often multimodal, and computational resources are limited, posing\nsignificant challenges for existing FL approaches. Recognizing these\nlimitations, we developed the Federated Healthcare Benchmark(FHBench), a\nbenchmark specifically designed from datasets derived from real-world\nhealthcare applications. FHBench encompasses critical diagnostic tasks across\ndomains such as the nervous, cardiovascular, and respiratory systems and\ngeneral pathology, providing comprehensive support for multimodal healthcare\nevaluations and filling a significant gap in existing benchmarks. Building on\nFHBench, we introduced Efficient Personalized Federated Learning with Adaptive\nLoRA(EPFL), a personalized FL framework that demonstrates superior efficiency\nand effectiveness across various healthcare modalities. Our results highlight\nthe robustness of FHBench as a benchmarking tool and the potential of EPFL as\nan innovative approach to advancing healthcare-focused FL, addressing key\nlimitations of existing methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86Federated Healthcare Benchmark (FHBench)\uff0c\u4e00\u4e2a\u9488\u5bf9\u771f\u5b9e\u533b\u7597\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f15\u5165\u4e86Efficient Personalized Federated Learning with Adaptive LoRA (EPFL)\uff0c\u4ee5\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u591a\u4e3a\u591a\u6a21\u6001\uff0c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u586b\u8865\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86FHBench\uff0c\u6db5\u76d6\u795e\u7ecf\u3001\u5fc3\u8840\u7ba1\u3001\u547c\u5438\u7cfb\u7edf\u548c\u4e00\u822c\u75c5\u7406\u9886\u57df\u7684\u8bca\u65ad\u4efb\u52a1\uff0c\u5e76\u5f15\u5165EPFL\u6846\u67b6\uff0c\u4f7f\u7528Adaptive LoRA\u5b9e\u73b0\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u3002", "result": "\u7ed3\u679c\u663e\u793aFHBench\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a33\u5065\u6027\uff0c\u4ee5\u53caEPFL\u5728\u5404\u79cd\u533b\u7597\u6a21\u5f0f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "EPFL\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u533b\u7597\u9886\u57df\u8054\u90a6\u5b66\u4e60\u7684\u521b\u65b0\u53d1\u5c55\u3002"}}
{"id": "2504.11421", "pdf": "https://arxiv.org/pdf/2504.11421", "abs": "https://arxiv.org/abs/2504.11421", "authors": ["Mahdi Hasanzadeh", "Kasem Khalil", "Cynthia Sturton", "Ahmad Patooghy"], "title": "HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs", "categories": ["cs.AR", "cs.SY", "eess.SY"], "comment": "14 pages,", "summary": "Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal\nattacks that manipulate dynamic thermal management systems. To counter this, we\npropose an adaptive real-time monitoring mechanism that detects abnormal\nthermal patterns in chip tiles. Our design space exploration helped identify\nkey thermal features for an efficient anomaly detection module to be\nimplemented at routers of network-enabled MPSoCs. To minimize hardware\noverhead, we employ weighted moving average (WMA) calculations and bit-shift\noperations, ensuring a lightweight yet effective implementation. By defining a\nspectrum of abnormal behaviors, our system successfully detects and mitigates\nmalicious temperature fluctuations, reducing severe cases from 3.00{\\deg}C to\n1.9{\\deg}C. The anomaly detection module achieves up to 82% of accuracy in\ndetecting thermal attacks, which is only 10-15% less than top-performing\nmachine learning (ML) models like Random Forest. However, our approach reduces\nhardware usage by up to 75% for logic resources and 100% for specialized\nresources, making it significantly more efficient than ML-based solutions. This\nmethod provides a practical, low-cost solution for resource-constrained\nenvironments, ensuring resilience against thermal attacks while maintaining\nsystem performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u5904\u7406\u5668\u7247\u4e0a\u7cfb\u7edf\uff08MPSoC\uff09\u70ed\u653b\u51fb\u76d1\u6d4b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u548c\u4f4e\u786c\u4ef6\u5f00\u9500\u3002", "motivation": "MPSoC\u6613\u53d7\u70ed\u653b\u51fb\u5f71\u54cd\uff0c\u9700\u8981\u5bf9\u6297\u52a8\u6001\u70ed\u7ba1\u7406\u7cfb\u7edf\u7684\u64cd\u7eb5\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u5b9e\u65f6\u76d1\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u52a0\u6743\u79fb\u52a8\u5e73\u5747\u548c\u4f4d\u79fb\u64cd\u4f5c\uff0c\u5728\u7f51\u7edc\u8def\u7531\u5668\u4e2d\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\uff0c\u5e76\u8fdb\u884c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u5c06\u6e29\u5ea6\u6ce2\u52a8\u4ece3.00\u00b0C\u964d\u4f4e\u52301.9\u00b0C\uff0c\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe82%\uff0c\u786c\u4ef6\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe75%\u3002", "conclusion": "\u63d0\u4f9b\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u786e\u4fdd\u62b5\u6297\u70ed\u653b\u51fb\u5e76\u7ef4\u6301\u6027\u80fd\u3002"}}
{"id": "2504.10833", "pdf": "https://arxiv.org/pdf/2504.10833", "abs": "https://arxiv.org/abs/2504.10833", "authors": ["Shubham Kumar", "Dwip Dalal", "Narendra Ahuja"], "title": "Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a\npromising tool for generating semantic explanations of the decision-making\nprocesses in deep neural networks, having applications in both model\nimprovement and understanding. It is vital that the explanation is accurate, or\nfaithful, to the model, yet we identify several limitations of prior\nfaithfulness metrics that inhibit an accurate evaluation; most notably, prior\nmetrics involve only the set of concepts present, ignoring how they may be\nspatially distributed. We address these limitations with Surrogate Faithfulness\n(SF), an evaluation method that introduces a spatially-aware surrogate and two\nnovel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)\nexplanations, where concepts are found that maximize faithfulness. Our\nexperiments show that (1) adding spatial-awareness to prior U-CBEMs increases\nfaithfulness in all cases; (2) OF produces significantly more faithful\nexplanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's\nlearned concepts generalize well to out-of-domain data and are more robust to\nadversarial examples, where prior U-CBEMs struggle.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u4e86Surrogate Faithfulness (SF)\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u65e0\u76d1\u7763\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u7684\u5fe0\u5b9e\u5ea6\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7Optimally Faithful (OF)\u89e3\u91ca\u5c55\u793a\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u5fe0\u5b9e\u5ea6\u6307\u6807\u5ffd\u7565\u4e86\u6982\u5ff5\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u51c6\u786e\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSF\u65b9\u6cd5\uff0c\u5305\u62ec\u7a7a\u95f4\u611f\u77e5\u4ee3\u7406\u548c\u4e24\u4e2a\u65b0\u5fe0\u5b9e\u5ea6\u6307\u6807\uff0c\u5e76\u5f00\u53d1OF\u89e3\u91ca\u6765\u6700\u5927\u5316\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6dfb\u52a0\u7a7a\u95f4\u611f\u77e5\u63d0\u9ad8\u4e86\u5fe0\u5b9e\u5ea6\uff1bOF\u89e3\u91ca\u7684\u9519\u8bef\u7387\u964d\u4f4e\u4e8630%\u4ee5\u4e0a\uff1bOF\u6982\u5ff5\u5728\u57df\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u5e76\u5bf9\u5bf9\u6297\u6837\u672c\u66f4\u9c81\u68d2\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u3001\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.10498", "pdf": "https://arxiv.org/pdf/2504.10498", "abs": "https://arxiv.org/abs/2504.10498", "authors": ["Jianling Lu", "Mingqi Lv"], "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faCCSK\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6709\u77e5\u8bc6\u548c\u5916\u90e8\u68c0\u7d22\uff0c\u6539\u5584Q&A\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u9608\u503c-based\u65b9\u6cd5\u9759\u6001\u5355\u4e00\uff0c\u96be\u4ee5\u5e73\u8861LLM\u81ea\u6709\u77e5\u8bc6\u4e0e\u5916\u90e8IR\uff0c\u5bfc\u81f4\u67e5\u8be2\u56f0\u96be\u65f6IR\u51b3\u7b56\u53ef\u80fd\u65e0\u5173\u3002", "method": "\u63d0\u51faCCSK\uff0c\u4f7f\u7528Siamese Network\u8ba1\u7b97\u67e5\u8be2\u5386\u53f2\u76f8\u4f3c\u6027\uff0cResponse Quality Model\u57fa\u4e8eLightGBM\u8bc4\u4f30\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\u8fdb\u884c\u52a8\u6001\u8054\u5408\u51b3\u7b56\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0cCCSK\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u4fe1\u606f\u68c0\u7d22\u6709\u6548\u6027\u3002", "conclusion": "CCSK\u901a\u8fc7\u52a8\u6001\u673a\u5236\u7f13\u89e3\u4e86IR\u51b3\u7b56\u65e0\u5173\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347LLM Q&A\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.10850", "pdf": "https://arxiv.org/pdf/2504.10850", "abs": "https://arxiv.org/abs/2504.10850", "authors": ["Meiqi Liu", "Zhuoqun Huang", "Yue Xing"], "title": "How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?", "categories": ["cs.LG", "cs.CR"], "comment": "22 pages, 2 figures, 12 tables. Include 10 pages of appendices", "summary": "With the rise of powerful foundation models, a pre-training-fine-tuning\nparadigm becomes increasingly popular these days: A foundation model is\npre-trained using a huge amount of data from various sources, and then the\ndownstream users only need to fine-tune and adapt it to specific downstream\ntasks. However, due to the high computation complexity of adversarial training,\nit is not feasible to fine-tune the foundation model to improve its robustness\non the downstream task. Observing the above challenge, we want to improve the\ndownstream robustness without updating/accessing the weights in the foundation\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\n2020), through theoretical investigation, we identify a close relationship\nbetween robust contrastive learning with the adversarial robustness of\nsupervised learning. To further validate and utilize this theoretical insight,\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\nmethod before feeding the data into the foundation model. The proposed approach\nhas zero access to the foundation model when training the robust auto-encoder.\nExtensive experiments demonstrate the effectiveness of the proposed method in\nimproving the robustness of downstream tasks, verifying the connection between\nthe feature robustness (implied by small adversarial contrastive loss) and the\nrobustness of the downstream task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u57fa\u7840\u6a21\u578b\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u9c81\u68d2\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u6570\u636e\u9884\u5904\u7406\uff0c\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u52a8\u673a\uff1a\u5bf9\u6297\u8bad\u7ec3\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u65e0\u6cd5\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e0d\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7406\u8bba\u5206\u6790\uff0c\u8bbe\u8ba1\u9c81\u68d2\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u6570\u636e\u9884\u5904\u7406\u5de5\u5177\uff0c\u8bad\u7ec3\u65f6\u4e0d\u8bbf\u95ee\u57fa\u7840\u6a21\u578b\u3002", "result": "\u7ed3\u679c\uff1a\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u9c81\u68d2\u6027\uff0c\u5e76\u9a8c\u8bc1\u7279\u5f81\u9c81\u68d2\u6027\u4e0e\u4e0b\u6e38\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u8bc1\u5b9e\u4e86\u7406\u8bba\u6d1e\u89c1\u3002"}}
{"id": "2504.10500", "pdf": "https://arxiv.org/pdf/2504.10500", "abs": "https://arxiv.org/abs/2504.10500", "authors": ["Eya Mhedhbi", "Youssef Mourchid", "Alice Othmani"], "title": "Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces a cutting-edge method for enhancing recommender systems\nthrough the integration of generative self-supervised learning (SSL) with a\nResidual Graph Transformer. Our approach emphasizes the importance of superior\ndata enhancement through the use of pertinent pretext tasks, automated through\nrationale-aware SSL to distill clear ways of how users and items interact. The\nResidual Graph Transformer incorporates a topology-aware transformer for global\ncontext and employs residual connections to improve graph representation\nlearning. Additionally, an auto-distillation process refines self-supervised\nsignals to uncover consistent collaborative rationales. Experimental\nevaluations on multiple datasets demonstrate that our approach consistently\noutperforms baseline methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6b8b\u5dee\u56fe\u53d8\u6362\u5668\u7684\u521b\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u66f4\u597d\u7684\u6570\u636e\u589e\u5f3a\u548c\u7528\u6237\u7269\u54c1\u4ea4\u4e92\u5b66\u4e60\u6765\u6539\u8fdb\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u6574\u5408\u751f\u6210\u5f0f\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u6b8b\u5dee\u56fe\u53d8\u6362\u5668\uff0c\u4f7f\u7528\u7406\u6027aware SSL\u8fdb\u884c\u81ea\u52a8\u5316\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u7ed3\u5408\u62d3\u6251aware \u53d8\u6362\u5668\u548c\u6b8b\u5dee\u8fde\u63a5\u8fdb\u884c\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u84b8\u998f\u8fc7\u7a0b\u63d0\u70bc\u81ea\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5 consistently \u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.10851", "pdf": "https://arxiv.org/pdf/2504.10851", "abs": "https://arxiv.org/abs/2504.10851", "authors": ["Ruochen Jin", "Boning Tong", "Shu Yang", "Bojian Hou", "Li Shen"], "title": "ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Vertical federated learning (VFL) enables a paradigm for vertically\npartitioned data across clients to collaboratively train machine learning\nmodels. Feature selection (FS) plays a crucial role in Vertical Federated\nLearning (VFL) due to the unique nature that data are distributed across\nmultiple clients. In VFL, different clients possess distinct subsets of\nfeatures for overlapping data samples, making the process of identifying and\nselecting the most relevant features a complex yet essential task. Previous FS\nefforts have primarily revolved around intra-client feature selection,\noverlooking vital feature interaction across clients, leading to subpar model\noutcomes. We introduce ICAFS, a novel multi-stage ensemble approach for\neffective FS in VFL by considering inter-client interactions. By employing\nconditional feature synthesis alongside multiple learnable feature selectors,\nICAFS facilitates ensemble FS over these selectors using synthetic embeddings.\nThis method bypasses the limitations of private gradient sharing and allows for\nmodel training using real data with refined embeddings. Experiments on multiple\nreal-world datasets demonstrate that ICAFS surpasses current state-of-the-art\nmethods in prediction accuracy.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165ICAFS\uff0c\u4e00\u79cd\u65b0\u7684\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u5ba2\u6237\u7aef\u95f4\u4ea4\u4e92\uff0c\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5ba2\u6237\u7aef\u5185\u90e8\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u95f4\u7279\u5f81\u4ea4\u4e92\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faICAFS\u591a\u9636\u6bb5\u96c6\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u6761\u4ef6\u7279\u5f81\u5408\u6210\u548c\u53ef\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u5668\uff0c\u901a\u8fc7\u5408\u6210\u5d4c\u5165\u8fdb\u884c\u96c6\u6210\u7279\u5f81\u9009\u62e9\uff0c\u7ed5\u8fc7\u79c1\u6709\u68af\u5ea6\u5171\u4eab\u9650\u5236\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cICAFS\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "ICAFS\u901a\u8fc7\u8003\u8651\u5ba2\u6237\u7aef\u95f4\u4ea4\u4e92\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.10508", "pdf": "https://arxiv.org/pdf/2504.10508", "abs": "https://arxiv.org/abs/2504.10508", "authors": ["Jo\u00e3o Alberto de Oliveira Lima"], "title": "Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents", "categories": ["cs.IR", "cs.AI", "I.2.8"], "comment": "39 pages, 5 figures", "summary": "Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for\ngenerating contextually accurate answers by integrating Large Language Models\n(LLMs) with retrieval mechanisms. However, in legal contexts, users frequently\nreference norms by their labels or nicknames (e.g., Article 5 of the\nConstitution or Consumer Defense Code (CDC)), rather than by their content,\nposing challenges for traditional RAG approaches that rely solely on semantic\nembeddings of text. Furthermore, legal texts themselves heavily rely on\nexplicit cross-references (e.g., \"pursuant to Article 34\") that function as\npointers. Both scenarios pose challenges for traditional RAG approaches that\nrely solely on semantic embeddings of text, often failing to retrieve the\nnecessary referenced content. This paper introduces Poly-Vector Retrieval, a\nmethod assigning multiple distinct embeddings to each legal provision: one\nembedding captures the content (the full text), another captures the label (the\nidentifier or proper name), and optionally additional embeddings capture\nalternative denominations. Inspired by Frege's distinction between Sense and\nReference, this poly-vector retrieval approach treats labels, identifiers and\nreference markers as rigid designators and content embeddings as carriers of\nsemantic substance. Experiments on the Brazilian Federal Constitution\ndemonstrate that Poly-Vector Retrieval significantly improves retrieval\naccuracy for label-centric queries and potential to resolve internal and\nexternal cross-references, without compromising performance on purely semantic\nqueries. The study discusses philosophical and practical implications of\nexplicitly separating reference from content in vector embeddings and proposes\nfuture research directions for applying this approach to broader legal datasets\nand other domains characterized by explicit reference identifiers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faPoly-Vector Retrieval\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6cd5\u5f8b\u6761\u6b3e\u5206\u914d\u591a\u4e2a\u5d4c\u5165\u5411\u91cf\u6765\u63d0\u5347RAG\u5728\u5904\u7406\u6807\u7b7e\u548c\u4ea4\u53c9\u5f15\u7528\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u6cd5\u5f8b\u9886\u57df\u7684RAG\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u7528\u6237\u57fa\u4e8e\u6807\u7b7e\u7684\u67e5\u8be2\u548c\u6587\u672c\u4e2d\u7684\u663e\u5f0f\u4ea4\u53c9\u5f15\u7528\u3002", "method": "Poly-Vector Retrieval\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u6cd5\u5f8b\u6761\u6b3e\u521b\u5efa\u591a\u4e2a\u5d4c\u5165\uff1a\u4e00\u4e2a\u6355\u6349\u5185\u5bb9\uff0c\u4e00\u4e2a\u6355\u6349\u6807\u7b7e\uff0c\u5e76\u53ef\u9009\u5730\u6355\u6349\u5176\u4ed6\u540d\u79f0\u3002", "result": "\u5728\u5df4\u897f\u8054\u90a6\u5baa\u6cd5\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u7b7e\u5bfc\u5411\u67e5\u8be2\u7684\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u7eaf\u8bed\u4e49\u67e5\u8be2\u7684\u6027\u80fd\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5206\u79bb\u5f15\u7528\u548c\u5185\u5bb9\u7684\u54f2\u5b66\u53ca\u5b9e\u9645\u542b\u4e49\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.10900", "pdf": "https://arxiv.org/pdf/2504.10900", "abs": "https://arxiv.org/abs/2504.10900", "authors": ["Peiliang Gong", "Emadeldeen Eldele", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Foundation models have achieved remarkable success across diverse\nmachine-learning domains through large-scale pretraining on large, diverse\ndatasets. However, pretraining on such datasets introduces significant\nchallenges due to substantial mismatches in data distributions, a problem\nparticularly pronounced with time series data. In this paper, we tackle this\nissue by proposing a domain-aware adaptive normalization strategy within the\nTransformer architecture. Specifically, we replace the traditional LayerNorm\nwith a prototype-guided dynamic normalization mechanism (ProtoNorm), where\nlearned prototypes encapsulate distinct data distributions, and\nsample-to-prototype affinity determines the appropriate normalization layer.\nThis mechanism effectively captures the heterogeneity of time series\ncharacteristics, aligning pretrained representations with downstream tasks.\nThrough comprehensive empirical evaluation, we demonstrate that our method\nsignificantly outperforms conventional pretraining techniques across both\nclassification and forecasting tasks, while effectively mitigating the adverse\neffects of distribution shifts during pretraining. Incorporating ProtoNorm is\nas simple as replacing a single line of code. Extensive experiments on diverse\nreal-world time series benchmarks validate the robustness and generalizability\nof our approach, advancing the development of more versatile time series\nfoundation models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faProtoNorm\u673a\u5236\uff0c\u6539\u8fdbTransformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u4e2d\u7684\u89c4\u8303\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u578b\u6570\u636e\u96c6\u65f6\uff0c\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u5c24\u4e3a\u4e25\u91cd\uff0c\u9700\u8981\u9002\u5e94\u6027\u5f3a\u7684\u89c4\u8303\u5316\u65b9\u6cd5\u3002", "method": "\u5728Transformer\u4e2d\u7528\u539f\u578b\u5f15\u5bfc\u7684\u52a8\u6001\u89c4\u8303\u5316(ProtoNorm)\u66ff\u6362LayerNorm\uff0c\u901a\u8fc7\u6837\u672c\u4e0e\u539f\u578b\u7684\u4eb2\u548c\u5ea6\u9009\u62e9\u89c4\u8303\u5316\u5c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u5e76\u7f13\u89e3\u5206\u5e03\u504f\u79fb\uff0c\u5728\u771f\u5b9e\u57fa\u51c6\u4e0a\u8bc1\u660e\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u66f4\u901a\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2504.10509", "pdf": "https://arxiv.org/pdf/2504.10509", "abs": "https://arxiv.org/abs/2504.10509", "authors": ["Jakub Podolak", "Leon Peric", "Mina Janicijevic", "Roxana Petcu"], "title": "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "This study presents a comprehensive reproducibility and extension analysis of\nthe Setwise prompting methodology for zero-shot ranking with Large Language\nModels (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and\nefficiency compared to traditional Pointwise, Pairwise, and Listwise approaches\nin document ranking tasks. Our reproduction confirms the findings of Zhuang et\nal., highlighting the trade-offs between computational efficiency and ranking\neffectiveness in Setwise methods. Building on these insights, we introduce\nSetwise Insertion, a novel approach that leverages the initial document ranking\nas prior knowledge, reducing unnecessary comparisons and uncertainty by\nfocusing on candidates more likely to improve the ranking results. Experimental\nresults across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show\nthat Setwise Insertion yields a 31% reduction in query time, a 23% reduction in\nmodel inferences, and a slight improvement in reranking effectiveness compared\nto the original Setwise method. These findings highlight the practical\nadvantage of incorporating prior ranking knowledge into Setwise prompting for\nefficient and accurate zero-shot document reranking.", "AI": {"tldr": "\u672c\u7814\u7a76\u518d\u73b0\u5e76\u6269\u5c55Setwise\u63d0\u793a\u65b9\u6cd5\uff0c\u7528\u4e8eLLM\u7684\u96f6\u6837\u672c\u6392\u540d\uff0c\u5f15\u5165Setwise Insertion\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u67e5\u8be2\u65f6\u95f4\u51cf\u5c1131%\u3001\u6a21\u578b\u63a8\u7406\u51cf\u5c1123%\uff0c\u5e76\u7565\u5fae\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u52a8\u673a\u662f\u9a8c\u8bc1Setwise\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u65b0\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u4ee5\u5e73\u8861\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u518d\u73b0\u539fSetwise\u65b9\u6cd5\u3001\u63d0\u51faSetwise Insertion\uff08\u5229\u7528\u521d\u59cb\u6392\u540d\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff09\u3001\u5e76\u5728Flan-T5\u3001Vicuna\u548cLlama\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7ed3\u679c\u663e\u793aSetwise Insertion\u6bd4\u539f\u65b9\u6cd5\u67e5\u8be2\u65f6\u95f4\u51cf\u5c1131%\u3001\u6a21\u578b\u63a8\u7406\u51cf\u5c1123%\uff0c\u5e76\u7565\u5fae\u63d0\u9ad8\u4e86\u91cd\u65b0\u6392\u540d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u878d\u5165\u5148\u9a8c\u6392\u540d\u77e5\u8bc6\u80fd\u63d0\u5347Setwise\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u4f7f\u96f6\u6837\u672c\u6587\u6863\u6392\u540d\u66f4\u9ad8\u6548\u51c6\u786e\u3002"}}
{"id": "2504.10902", "pdf": "https://arxiv.org/pdf/2504.10902", "abs": "https://arxiv.org/abs/2504.10902", "authors": ["Rui Dai", "Sile Hu", "Xu Shen", "Yonggang Zhang", "Xinmei Tian", "Jieping Ye"], "title": "Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs", "categories": ["cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Task arithmetic is a straightforward yet highly effective strategy for model\nmerging, enabling the resultant model to exhibit multi-task capabilities.\nRecent research indicates that models demonstrating linearity enhance the\nperformance of task arithmetic. In contrast to existing methods that rely on\nthe global linearization of the model, we argue that this linearity already\nexists within the model's submodules. In particular, we present a statistical\nanalysis and show that submodules (e.g., layers, self-attentions, and MLPs)\nexhibit significantly higher linearity than the overall model. Based on these\nfindings, we propose an innovative model merging strategy that independently\nmerges these submodules. Especially, we derive a closed-form solution for\noptimal merging weights grounded in the linear properties of these submodules.\nExperimental results demonstrate that our method consistently outperforms the\nstandard task arithmetic approach and other established baselines across\ndifferent model scales and various tasks. This result highlights the benefits\nof leveraging the linearity of submodules and provides a new perspective for\nexploring solutions for effective and practical multi-task model merging.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5b50\u6a21\u5757\u7ebf\u6027\u6027\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f5c\u8005\u8ba4\u4e3a\u5b50\u6a21\u5757\u7ebf\u6027\u6027\u9ad8\u4e8e\u6574\u4f53\u6a21\u578b\uff0c\u56e0\u6b64\u5f00\u53d1\u72ec\u7acb\u5408\u5e76\u65b9\u6cd5\u3002", "method": "\u72ec\u7acb\u5408\u5e76\u5b50\u6a21\u5757\uff08\u5982\u5c42\u3001self-attention\u548cMLP\uff09\uff0c\u5e76\u63a8\u5bfc\u6700\u4f18\u5408\u5e76\u6743\u91cd\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u4efb\u52a1\u7b97\u672f\u548c\u5176\u4ed6\u57fa\u51c6\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5f3a\u8c03\u5229\u7528\u5b50\u6a21\u5757\u7ebf\u6027\u6027\u7684\u4f18\u52bf\uff0c\u4e3a\u591a\u4efb\u52a1\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.10512", "pdf": "https://arxiv.org/pdf/2504.10512", "abs": "https://arxiv.org/abs/2504.10512", "authors": ["Minh-Anh Nguyen", "Dung D. Le"], "title": "JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.", "AI": {"tldr": "\u63d0\u51faJEPA4Rec\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\u548c\u8bed\u8a00\u5efa\u6a21\uff0c\u63d0\u9ad8\u5e8f\u5217\u63a8\u8350\u6027\u80fd\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u758f\u548c\u5e38\u8bc6\u504f\u597d\u95ee\u9898\u3002", "motivation": "\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u5728\u5e8f\u5217\u63a8\u8350\u4e2d\u867d\u6709\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u758f\u548c\u5bf9\u7528\u6237\u5e38\u8bc6\u504f\u597d\u7684\u6709\u9650\u7406\u89e3\u3002", "method": "JEPA4Rec\u5c06\u9879\u76ee\u8868\u793a\u4e3a\u6587\u672c\u53e5\u5b50\uff0c\u4f7f\u7528\u53cc\u5411Transformer\u7f16\u7801\u5668\u3001\u63a9\u7801\u9884\u6d4b\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cJEPA4Rec\u5728\u8de8\u57df\u3001\u8de8\u5e73\u53f0\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u63a8\u8350\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.10917", "pdf": "https://arxiv.org/pdf/2504.10917", "abs": "https://arxiv.org/abs/2504.10917", "authors": ["Jialin Chen", "Haolan Zuo", "Haoyu Peter Wang", "Siqi Miao", "Pan Li", "Rex Ying"], "title": "Towards A Universal Graph Structural Encoder", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in large-scale pre-training have shown the potential to\nlearn generalizable representations for downstream tasks. In the graph domain,\nhowever, capturing and transferring structural information across different\ngraph domains remains challenging, primarily due to the inherent differences in\ntopological patterns across various contexts. Additionally, most existing\nmodels struggle to capture the complexity of rich graph structures, leading to\ninadequate exploration of the embedding space. To address these challenges, we\npropose GFSE, a universal graph structural encoder designed to capture\ntransferable structural patterns across diverse domains such as molecular\ngraphs, social networks, and citation networks. GFSE is the first cross-domain\ngraph structural encoder pre-trained with multiple self-supervised learning\nobjectives. Built on a Graph Transformer, GFSE incorporates attention\nmechanisms informed by graph inductive bias, enabling it to encode intricate\nmulti-level and fine-grained topological features. The pre-trained GFSE\nproduces generic and theoretically expressive positional and structural\nencoding for graphs, which can be seamlessly integrated with various downstream\ngraph feature encoders, including graph neural networks for vectorized features\nand Large Language Models for text-attributed graphs. Comprehensive experiments\non synthetic and real-world datasets demonstrate GFSE's capability to\nsignificantly enhance the model's performance while requiring substantially\nless task-specific fine-tuning. Notably, GFSE achieves state-of-the-art\nperformance in 81.6% evaluated cases, spanning diverse graph models and\ndatasets, highlighting its potential as a powerful and versatile encoder for\ngraph-structured data.", "AI": {"tldr": "GFSE \u662f\u4e00\u79cd\u901a\u7528\u56fe\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u5347\u8de8\u57df\u56fe\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u83b7\u548c\u8f6c\u79fb\u4e0d\u540c\u56fe\u57df\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u62d3\u6251\u3002", "method": "\u63d0\u51fa GFSE\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe Transformer \u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u53ef\u4e0e\u4e0b\u6e38\u6a21\u578b\u65e0\u7f1d\u6574\u5408\u3002", "result": "\u5b9e\u9a8c\u663e\u793a GFSE \u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728 81.6% \u7684\u8bc4\u4f30\u6848\u4f8b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u51cf\u5c11\u5fae\u8c03\u9700\u6c42\u3002", "conclusion": "GFSE \u4f5c\u4e3a\u5f3a\u5927\u901a\u7528\u7684\u56fe\u7ed3\u6784\u7f16\u7801\u5668\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165ColorBench\u57fa\u51c6\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u989c\u8272\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5173\u952e\u53d1\u73b0\u3002", "motivation": "\u63a2\u8ba8VLMs\u662f\u5426\u548c\u5982\u4f55\u50cf\u4eba\u7c7b\u4e00\u6837\u611f\u77e5\u548c\u5229\u7528\u989c\u8272\uff0c\u56e0\u4e3a\u989c\u8272\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u5f88\u91cd\u8981\u3002", "method": "\u5f15\u5165ColorBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u51c6\uff0c\u5305\u542b\u5404\u79cd\u6d4b\u8bd5\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30VLMs\u7684\u989c\u8272\u611f\u77e5\u3001\u63a8\u7406\u548c\u9c81\u68d2\u6027\u3002", "result": "\u8bc4\u4f3032\u4e2aVLMs\u540e\u53d1\u73b0\uff1a\u7f29\u653e\u5b9a\u5f8b\u6210\u7acb\uff0c\u8bed\u8a00\u6a21\u578b\u6bd4\u89c6\u89c9\u7f16\u7801\u5668\u66f4\u91cd\u8981\uff1b\u6027\u80fd\u5dee\u8ddd\u5c0f\uff0c\u8868\u660e\u989c\u8272\u7406\u89e3\u88ab\u5ffd\u7565\uff1bCoT\u63a8\u7406\u6539\u5584\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff1b\u989c\u8272\u7ebf\u7d22\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u989c\u8272\u7406\u89e3\u65b9\u9762\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u6539\u8fdb\uff0cColorBench\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001AI\u7814\u7a76\u7684\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2504.10923", "pdf": "https://arxiv.org/pdf/2504.10923", "abs": "https://arxiv.org/abs/2504.10923", "authors": ["Mingyi Zhu", "Zhaoxin Li", "Qiao Lin", "Li Ding"], "title": "Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting", "categories": ["cs.LG", "eess.SP"], "comment": "Mingyi Zhu is the first author. Li Ding is the corresponding author", "summary": "Wind power forecasting (WPF), as a significant research topic within\nrenewable energy, plays a crucial role in enhancing the security, stability,\nand economic operation of power grids. However, due to the high stochasticity\nof meteorological factors (e.g., wind speed) and significant fluctuations in\nwind power output, mid-term wind power forecasting faces a dual challenge of\nmaintaining high accuracy and computational efficiency. To address these\nissues, this paper proposes an efficient and lightweight mid-term wind power\nforecasting model, termed Fast-Powerformer. The proposed model is built upon\nthe Reformer architecture, incorporating structural enhancements such as a\nlightweight Long Short-Term Memory (LSTM) embedding module, an input\ntransposition mechanism, and a Frequency Enhanced Channel Attention Mechanism\n(FECAM). These improvements enable the model to strengthen temporal feature\nextraction, optimize dependency modeling across variables, significantly reduce\ncomputational complexity, and enhance sensitivity to periodic patterns and\ndominant frequency components. Experimental results conducted on multiple\nreal-world wind farm datasets demonstrate that the proposed Fast-Powerformer\nachieves superior prediction accuracy and operational efficiency compared to\nmainstream forecasting approaches. Furthermore, the model exhibits fast\ninference speed and low memory consumption, highlighting its considerable\npractical value for real-world deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFast-Powerformer\u6a21\u578b\uff0c\u7528\u4e8e\u4e2d\u671f\u98ce\u529b\u53d1\u7535\u9884\u6d4b\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u98ce\u529b\u53d1\u7535\u9884\u6d4b\u9762\u4e34\u6c14\u8c61\u968f\u673a\u6027\u548c\u8f93\u51fa\u6ce2\u52a8\u5e26\u6765\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u6311\u6218\u3002", "method": "\u57fa\u4e8eReformer\u67b6\u6784\uff0c\u6dfb\u52a0\u8f7b\u91cf\u7ea7LSTM\u5d4c\u5165\u3001\u8f93\u5165\u8f6c\u7f6e\u548cFECAM\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u548c\u51cf\u5c11\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7cbe\u5ea6\u548c\u6548\u7387\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u5feb\u901f\u63a8\u7406\u548c\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5177\u6709\u9ad8\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.10521", "pdf": "https://arxiv.org/pdf/2504.10521", "abs": "https://arxiv.org/abs/2504.10521", "authors": ["Pardis Moradbeiki", "Mohammad Ali Zare Chahooki"], "title": "Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification", "categories": ["cs.SI", "cs.AI", "cs.LG"], "comment": null, "summary": "As the popularity and reach of social networks continue to surge, a vast\nreservoir of opinions and sentiments across various subjects inundates these\nplatforms. Among these, X social network (formerly Twitter) stands as a\njuggernaut, boasting approximately 420 million active users. Extracting users'\nemotional and mental states from their expressed opinions on social media has\nbecome a common pursuit. While past methodologies predominantly focused on the\ntextual content of messages to analyze user sentiment, the interactive nature\nof these platforms suggests a deeper complexity. This study employs hybrid\nmethodologies, integrating textual analysis, profile examination, follower\nanalysis, and emotion dissemination patterns. Initially, user interactions are\nleveraged to refine emotion classification within messages, encompassing\nexchanges where users respond to each other. Introducing the concept of a\ncommunication tree, a model is extracted to map these interactions.\nSubsequently, users' bios and interests from this tree are juxtaposed with\nmessage text to enrich analysis. Finally, influential figures are identified\namong users' followers in the communication tree, categorized into different\ntopics to gauge interests. The study highlights that traditional sentiment\nanalysis methodologies, focusing solely on textual content, are inadequate in\ndiscerning sentiment towards significant events, notably the presidential\nelection. Comparative analysis with conventional methods reveals a substantial\nimprovement in accuracy with the incorporation of emotion distribution patterns\nand user profiles. The proposed approach yields a 12% increase in accuracy with\nemotion distribution patterns and a 15% increase when considering user\nprofiles, underscoring its efficacy in capturing nuanced sentiment dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u6587\u672c\u5206\u6790\u3001\u7528\u6237\u4e92\u52a8\u548c\u4e2a\u4eba\u8d44\u6599\uff0c\u63d0\u9ad8\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u51c6\u786e\u7387\uff0c\u5c24\u5176\u9488\u5bf9\u91cd\u5927\u4e8b\u4ef6\u3002", "motivation": "\u793e\u4ea4\u7f51\u7edc\u7528\u6237\u4f17\u591a\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6587\u672c\u5185\u5bb9\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u4e92\u52a8\u548c\u4e8b\u4ef6\u76f8\u5173\u60c5\u611f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u6587\u672c\u5206\u6790\u3001\u901a\u4fe1\u6811\u6a21\u578b\u6620\u5c04\u4e92\u52a8\u3001\u7528\u6237\u4e2a\u4eba\u8d44\u6599\u548c\u5173\u6ce8\u8005\u5206\u6790\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u60c5\u611f\u5206\u5e03\u6a21\u5f0f\u63d0\u9ad8\u51c6\u786e\u738712%\uff0c\u7528\u6237\u8d44\u6599\u63d0\u9ad815%\u3002", "conclusion": "\u8bc1\u660e\u6574\u5408\u591a\u6e90\u6570\u636e\u80fd\u66f4\u597d\u5730\u6355\u6349\u7ec6\u5fae\u60c5\u611f\u52a8\u6001\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5904\u7406\u91cd\u5927\u4e8b\u4ef6\u5206\u6790\u3002"}}
{"id": "2504.10925", "pdf": "https://arxiv.org/pdf/2504.10925", "abs": "https://arxiv.org/abs/2504.10925", "authors": ["Ayan Chatterjee", "Barbara Ikica", "Babak Ravandi", "John Palowitch"], "title": "Transfer Learning for Temporal Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "Link prediction on graphs has applications spanning from recommender systems\nto drug discovery. Temporal link prediction (TLP) refers to predicting future\nlinks in a temporally evolving graph and adds additional complexity related to\nthe dynamic nature of graphs. State-of-the-art TLP models incorporate memory\nmodules alongside graph neural networks to learn both the temporal mechanisms\nof incoming nodes and the evolving graph topology. However, memory modules only\nstore information about nodes seen at train time, and hence such models cannot\nbe directly transferred to entirely new graphs at test time and deployment. In\nthis work, we study a new transfer learning task for temporal link prediction,\nand develop transfer-effective methods for memory-laden models. Specifically,\nmotivated by work showing the informativeness of structural signals for the TLP\ntask, we augment a structural mapping module to the existing TLP model\narchitectures, which learns a mapping from graph structural (topological)\nfeatures to memory embeddings. Our work paves the way for a memory-free\nfoundation model for TLP.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u7684\u8f6c\u79fb\u5b66\u4e60\u4efb\u52a1\uff0c\u901a\u8fc7\u6dfb\u52a0\u7ed3\u6784\u6620\u5c04\u6a21\u5757\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8f6c\u79fb\u5230\u65b0\u56fe\u4e0a\uff0c\u5e76\u4e3a\u65e0\u8bb0\u5fc6\u57fa\u7840\u6a21\u578b\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u73b0\u6709TLP\u6a21\u578b\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u65b0\u56fe\uff0c\u56e0\u4e3a\u8bb0\u5fc6\u6a21\u5757\u53ea\u5b58\u50a8\u8bad\u7ec3\u65f6\u7684\u8282\u70b9\u4fe1\u606f\uff0c\u4e14\u7ed3\u6784\u4fe1\u53f7\u5bf9\u4efb\u52a1\u5177\u6709\u91cd\u8981\u6027\u3002", "method": "\u5728\u73b0\u6709TLP\u6a21\u578b\u67b6\u6784\u4e2d\u6dfb\u52a0\u7ed3\u6784\u6620\u5c04\u6a21\u5757\uff0c\u5b66\u4e60\u4ece\u56fe\u7ed3\u6784\u7279\u5f81\u5230\u8bb0\u5fc6\u5d4c\u5165\u7684\u6620\u5c04\u3002", "result": "\u5f00\u53d1\u4e86\u8f6c\u79fb\u6709\u6548\u7684\u8bb0\u5fc6\u8d1f\u8f7d\u6a21\u578b\uff0c\u4e3a\u65f6\u95f4\u94fe\u63a5\u9884\u6d4b\u7684\u65e0\u8bb0\u5fc6\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86TLP\u4efb\u52a1\u4e2d\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
