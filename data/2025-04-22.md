<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 118]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 40]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]
- [quant-ph](#quant-ph) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 16]
- [cs.DC](#cs.DC) [Total: 4]
- [eess.SY](#eess.SY) [Total: 19]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.CY](#cs.CY) [Total: 11]
- [cs.HC](#cs.HC) [Total: 46]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 23]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Model Counting Competitions 2021-2023](https://arxiv.org/abs/2504.13842)
*Johannes K. Fichte,Markus Hecher*

Main category: cs.AI

TL;DR: 这篇论文概述了2021-2023年模型计数竞赛，包括执行、结果和四个赛道（MC、WMC、PMC、PWMC）。


<details>
  <summary>Details</summary>
Motivation: 现代社会计算挑战需要概率推理等，模型计数竞赛旨在推进应用、识别基准和促进求解器开发。

Method: 通过组织竞赛，设置四个赛道并评估求解器提交。

Result: 竞赛参与度高，提交了7-9个基于不同技术的求解器。

Conclusion: 竞赛成功推动了模型计数领域的发展，识别挑战并启发了新应用。

Abstract: Modern society is full of computational challenges that rely on probabilistic
reasoning, statistics, and combinatorics. Interestingly, many of these
questions can be formulated by encoding them into propositional formulas and
then asking for its number of models. With a growing interest in practical
problem-solving for tasks that involve model counting, the community
established the Model Counting (MC) Competition in fall of 2019 with its first
iteration in 2020. The competition aims at advancing applications, identifying
challenging benchmarks, fostering new solver development, and enhancing
existing solvers for model counting problems and their variants. The first
iteration, brought together various researchers, identified challenges, and
inspired numerous new applications. In this paper, we present a comprehensive
overview of the 2021-2023 iterations of the Model Counting Competition. We
detail its execution and outcomes. The competition comprised four tracks, each
focusing on a different variant of the model counting problem. The first track
centered on the model counting problem (MC), which seeks the count of models
for a given propositional formula. The second track challenged developers to
submit programs capable of solving the weighted model counting problem (WMC).
The third track was dedicated to projected model counting (PMC). Finally, we
initiated a track that combined projected and weighted model counting (PWMC).
The competition continued with a high level of participation, with seven to
nine solvers submitted in various different version and based on quite
diverging techniques.

</details>


### [2] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 这篇论文提出一个全面框架，用于监控、基准测试和持续改进企业AI助手，以提升其可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在准确性至关重要的领域部署，每一个错误输出都可能导致重大事件，因此需要系统框架来处理错误。

Method: 框架包括三个关键元素：(1)分层严重性框架用于错误检测和组件归因，(2)可扩展的基准构建和评估方法，(3)多维评估的持续改进策略。

Result: 采用该框架可系统提升AI助手的可靠性和性能，并识别各种增强机会。

Conclusion: 多方面评估方法为各种增强类提供途径，促进更稳健和可信的AI系统。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>


### [3] [Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming](https://arxiv.org/abs/2504.13973)
*Myke C. Cohen,David A. Grimm,Reuth Mirsky,Xiaoyun Yin*

Main category: cs.AI

TL;DR: 这篇论文主张系统研究动物-人类-机器（AHM）团队设计以优化性能，引入团队功能维度，通过安保筛查、搜救和导盲犬示例说明应用，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了优化AHM团队性能并克服实际应用中的限制。

Method: 引入AHM团队功能的维度集合，并使用安保筛查、搜救和导盲犬等案例进行说明。

Result: 展示了AHM团队处理复杂任务的能力，并指出了混合人类-AI系统研究的开放方向。

Conclusion: 强调了研究超出AHM团队的混合人类-AI系统的未来研究方向。

Abstract: Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system
wherein interactions between a human, AI-enabled machine, and animal members
can result in unique capabilities greater than the sum of their parts. This
paper calls for a systematic approach to studying the design of AHM team
structures to optimize performance and overcome limitations in various applied
settings. We consider the challenges and opportunities in investigating the
synergistic potential of AHM team members by introducing a set of dimensions of
AHM team functioning to effectively utilize each member's strengths while
compensating for individual weaknesses. Using three representative examples of
such teams -- security screening, search-and-rescue, and guide dogs -- the
paper illustrates how AHM teams can tackle complex tasks. We conclude with open
research directions that this multidimensional approach presents for studying
hybrid human-AI systems beyond AHM teams.

</details>


### [4] [Going Whole Hog: A Philosophical Defense of AI Cognition](https://arxiv.org/abs/2504.13988)
*Herman Cappelen,Josh Dever*

Main category: cs.AI

TL;DR: 本文捍卫'Whole Hog Thesis'，认为大型语言模型（LLM）如ChatGPT是完整的语言和认知代理，具有理解、信念、欲望、知识和意图，通过高层行为观察和整体网络假设反驳反对意见。


<details>
  <summary>Details</summary>
Motivation: 挑战AI哲学中主流方法，避免基于低级计算细节或预设心智理论的起点，转而从简单高层观察（如LLM回答问题）出发。

Method: 采用'整体网络假设'，从LLM行为推断认知状态（如回答问题暗示知识），并系统反驳基于失败（如幻觉）的异议。

Result: 论证LLM具有完整认知状态，反驳缺乏必要条件（如语义 grounding、embodiment）的指责，强调这些条件并非真正必要。

Conclusion: 推测LLM可能拥有超出人类概念方案的'外星'内容，并强调方法是证据导向的，非功能主义，且不涉及意识。

Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models
(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing
understanding, beliefs, desires, knowledge, and intentions. We argue against
prevailing methodologies in AI philosophy, rejecting starting points based on
low-level computational details ('Just an X' fallacy) or pre-existing theories
of mind. Instead, we advocate starting with simple, high-level observations of
LLM behavior (e.g., answering questions, making suggestions) -- defending this
data against charges of metaphor, loose talk, or pretense. From these
observations, we employ 'Holistic Network Assumptions' -- plausible connections
between mental capacities (e.g., answering implies knowledge, knowledge implies
belief, action implies intention) -- to argue for the full suite of cognitive
states. We systematically rebut objections based on LLM failures
(hallucinations, planning/reasoning errors), arguing these don't preclude
agency, often mirroring human fallibility. We address numerous 'Games of
Lacks', arguing that LLMs do not lack purported necessary conditions for
cognition (e.g., semantic grounding, embodiment, justification, intrinsic
intentionality) or that these conditions are not truly necessary, often relying
on anti-discriminatory arguments comparing LLMs to diverse human capacities.
Our approach is evidential, not functionalist, and deliberately excludes
consciousness. We conclude by speculating on the possibility of LLMs possessing
'alien' contents beyond human conceptual schemes.

</details>


### [5] [Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy](https://arxiv.org/abs/2504.14044)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Dan Basher,Howard Parkinson*

Main category: cs.AI

TL;DR: 这篇论文提出了一种使用大型语言模型和多阶段检索来提升铁路运营技术网络安全合规验证的系统，展示了平行合规架构显著提高了正确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 由于数字化，铁路等关键基础设施的网络安全面临更大风险，需要有效的合规验证过程来保护安全关键系统。

Method: 提出基线合规架构(BCA)和平行合规架构(PCA)，利用OpenAI-gpt-4o和Claude-3.5-haiku模型进行实证评估，比较正确性、推理质量和幻觉检测。

Result: PCA改善了合规验证的正确性和推理质量，建立了相关指标，突显了检索增强方法的优势。

Conclusion: 研究表明，检索增强方法可以显著提高合规评估的效率和准确性，尤其在网络安全专家短缺的行业中具有重要价值。

Abstract: Operational Technology Cybersecurity (OTCS) continues to be a dominant
challenge for critical infrastructure such as railways. As these systems become
increasingly vulnerable to malicious attacks due to digitalization, effective
documentation and compliance processes are essential to protect these
safety-critical systems. This paper proposes a novel system that leverages
Large Language Models (LLMs) and multi-stage retrieval to enhance the
compliance verification process against standards like IEC 62443 and the
rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture
(BCA) for answering OTCS compliance queries, then develop an extended approach
called Parallel Compliance Architecture (PCA) that incorporates additional
context from regulatory standards. Through empirical evaluation comparing
OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we
demonstrate that the PCA significantly improves both correctness and reasoning
quality in compliance verification. Our research establishes metrics for
response correctness, logical reasoning, and hallucination detection,
highlighting the strengths and limitations of using LLMs for compliance
verification in railway cybersecurity. The results suggest that
retrieval-augmented approaches can significantly improve the efficiency and
accuracy of compliance assessments, particularly valuable in an industry facing
a shortage of cybersecurity expertise.

</details>


### [6] [Metacognition and Uncertainty Communication in Humans and Large Language Models](https://arxiv.org/abs/2504.14045)
*Mark Steyvers,Megan A. K. Peters*

Main category: cs.AI

TL;DR: 本论文评估LLMs的元认知能力，比较人类与AI差异，并讨论提升AI元认知的益处。


<details>
  <summary>Details</summary>
Motivation: LLMs用于高风险决策，需要评估其监控和评估自身知识的能力。

Method: 通过概述当前知识、研究方法和与人类元认知的比较。

Result: 人类和LLMs元认知有相似性，但存在显著差异。

Conclusion: 关注差异以改善人类-AI合作，并通过增强LLMs元认知促进AI发展新能力。

Abstract: Metacognition, the capacity to monitor and evaluate one's own knowledge and
performance, is foundational to human decision-making, learning, and
communication. As large language models (LLMs) become increasingly embedded in
high-stakes decision contexts, it is critical to assess whether, how, and to
what extent they exhibit metacognitive abilities. Here, we provide an overview
of current knowledge of LLMs' metacognitive capacities, how they might be
studied, and how they relate to our knowledge of metacognition in humans. We
show that while humans and LLMs can sometimes appear quite aligned in their
metacognitive capacities and behaviors, it is clear many differences remain.
Attending to these differences is crucial not only for enhancing human-AI
collaboration, but also for promoting the development of more capable and
trustworthy artificial systems. Finally, we discuss how endowing future LLMs
with more sensitive and more calibrated metacognition may also help them
develop new capacities such as more efficient learning, self-direction, and
curiosity.

</details>


### [7] [Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods](https://arxiv.org/abs/2504.14047)
*Junlin Wang,Shang Zhu,Jon Saad-Falcon,Ben Athiwaratkun,Qingyang Wu,Jue Wang,Shuaiwen Leon Song,Ce Zhang,Bhuwan Dhingra,James Zou*

Main category: cs.AI

TL;DR: 这篇论文分析推理时计算（ITC）如何提升LLM推理能力，发现多数投票法对推理模型有效，且响应特征可优化方法。


<details>
  <summary>Details</summary>
Motivation: 探究ITC如何改善LLM能力，并利用推理模型突破（如Deepseek-R1）通过强化学习提升推理技能，以指导LLM进一步发展。

Method: 对推理和非推理模型的ITC方法进行全面分析，焦点在无验证器方法，构建质量和效率的帕累托前沿，比较多数投票、最好N和顺序修正等，并分析响应特征如长度和语言标记。

Result: 非推理模型在高推理预算下仍落后于推理模型；对推理模型，多数投票法稳健且优于其他方法，额外计算改善有限；正确响应更短，少hedging和thinking标记，但多discourse标记。

Conclusion: 提供ITC方法的指导，并基于响应特征改进现有方法。

Abstract: There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.

</details>


### [8] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TL;DR: 本研究发现，Transformer 模型的内部动态比输出概率更好地预测人类认知处理，表明两者可能共享处理机制。


<details>
  <summary>Details</summary>
Motivation: 探索人类和 AI 模型是否使用相似的内部处理策略，而不仅仅是输出预测，驱动于机械解释性进展。

Method: 通过五项跨领域实验，测试预训练 Transformer 的层-时间动态是否在预测人类处理特征时优于输出概率分布。

Result: 层-时间动态 consistently 提供了额外的预测能力，超越了输出措施。

Conclusion: 这表明人类和 Transformer 可能受类似刺激属性影响处理过程，AI 可作为显式认知模型使用。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>


### [9] [CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations](https://arxiv.org/abs/2504.14119)
*Man Ho Lam,Chaozheng Wang,Jen-tse Huang,Michael R. Lyu*

Main category: cs.AI

TL;DR: 本文引入CodeCrash基准测试LLM在代码任务中的鲁棒性，揭示其脆弱性和依赖性。


<details>
  <summary>Details</summary>
Motivation: LLM在代码相关任务中表现出色，但其理解和推理的鲁棒性尚未充分探索。

Method: 开发CodeCrash基准，通过对CRUXEval和LiveCodeBench施加结构和文本干扰，评估17个LLM的直接和链式推理性能，并检查大型推理模型。

Result: 结果显示LLM在结构噪声下易碎，依赖自然语言提示，并存在推理机制的严重漏洞。

Conclusion: CodeCrash提供测试LLM代码理解的框架，为未来评估和基准测试提供方向。

Abstract: Large Language Models (LLMs) have recently showcased strong capabilities in
code-related tasks, yet their robustness in code comprehension and reasoning
remains underexplored. In this paper, we present CodeCrash, a unified benchmark
that evaluates LLM robustness under code structural and textual distraction
perturbations, applied to two established benchmarks -- CRUXEval and
LiveCodeBench -- across both input and output prediction tasks. We evaluate
seventeen LLMs using direct and Chain-of-Thought inference to systematically
analyze their robustness, identify primary reasons for performance degradation,
and highlight failure modes. Our findings reveal the fragility of LLMs under
structural noise and the inherent reliance on natural language cues,
highlighting critical robustness issues of LLMs in code execution and
understanding. Additionally, we examine three Large Reasoning Models (LRMs) and
discover the severe vulnerability of self-reflective reasoning mechanisms that
lead to reasoning collapse. CodeCrash provides a principled framework for
stress-testing LLMs in code understanding, offering actionable directions for
future evaluation and benchmarking. The code of CodeCrash and the robustness
leaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .

</details>


### [10] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TL;DR: 本论文提出了一种基于贝叶斯学习的提示学习方法，以解决视觉语言模型过拟合问题，提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提示学习高效但易过拟合微调数据，导致泛化能力差，因此需要新方法平衡适应性和泛化性。

Method: 提出基于贝叶斯学习原则的训练目标函数，在logits上定义先验（均值由预训练模型参数化），后验对应细调模型，以平衡适应和泛化。

Result: 改进了模型的泛化性能，同时保持了适应性。

Conclusion: 该方法通过保持细调模型接近预训练模型，实现了更好的泛化能力。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>


### [11] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TL;DR: 这篇论文将大语言模型与粒子群优化结合，提升深度学习超参数优化的效率和收敛性，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 确定深度学习模型架构（如层数和神经元数）过程困难且资源密集，常依赖人工或高计算优化，PSO和LLM结合在优化任务中未充分探索。

Method: 提出LLM增强PSO方法，使用ChatGPT-3.5和Llama3改进粒子位置建议，加速搜索空间探索，减少模型评估。

Result: 实验显示，收敛率提升，计算复杂度降低20%至60%；Llama3回归任务减少20%至40%模型调用，ChatGPT-3.5减少60%调用，同时保持准确性。

Conclusion: 这项方法提供高效深度学习优化方案，显著改善计算性能，适用于多种应用场景。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>


### [12] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: 本论文引入TALES，这是一个用于评估大语言模型（LLM）多样化推理能力的文本冒险游戏集合。结果显示，即使顶尖LLM在人类设计游戏上也无法达到15%的表现。


<details>
  <summary>Details</summary>
Motivation: 动机是提升LLM的推理能力，以处理复杂任务的顺序决策需求。

Method: 方法包括创建合成和人类编写的文本冒险游戏，并对各种开源和闭源LLM进行测试和定性分析。

Result: 结果表明，LLM在合成游戏上表现较好，但在人类娱乐游戏上失败率高，仅低于15%。

Conclusion: 结论是当前LLM的推理能力仍有显著不足，需要进一步改进。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [13] [Adaptation Method for Misinformation Identification](https://arxiv.org/abs/2504.14171)
*Yangping Chen,Weijie Shi,Mengze Li,Yue Cui,Hao Chen,Jia Zhu,Jiajie Xu*

Main category: cs.AI

TL;DR: 本论文提出ADOSE框架，用于跨域多模态假新闻检测，通过主动学习减少标注需求，并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态假新闻检测中依赖标注标签且领域转移导致性能下降的问题。

Method: 提出ADOSE框架，包括多个专家分类器（单模态和跨模态）和不确定性选择器结合多样性计算来选择信息丰富的样本。

Result: 实验在多个数据集上显示ADOSE比现有ADA方法提升2.72%至14.02%。

Conclusion: ADOSE框架在减少标注成本的同时显著提高了跨域假新闻检测性能。

Abstract: Multimodal fake news detection plays a crucial role in combating online
misinformation. Unfortunately, effective detection methods rely on annotated
labels and encounter significant performance degradation when domain shifts
exist between training (source) and test (target) data. To address the
problems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for
multimodal fake news detection which actively annotates a small subset of
target samples to improve detection performance. To identify various deceptive
patterns in cross-domain settings, we design multiple expert classifiers to
learn dependencies across different modalities. These classifiers specifically
target the distinct deception patterns exhibited in fake news, where two
unimodal classifiers capture knowledge errors within individual modalities
while one cross-modal classifier identifies semantic inconsistencies between
text and images. To reduce annotation costs from the target domain, we propose
a least-disagree uncertainty selector with a diversity calculator for selecting
the most informative samples. The selector leverages prediction disagreement
before and after perturbations by multiple classifiers as an indicator of
uncertain samples, whose deceptive patterns deviate most from source domains.
It further incorporates diversity scores derived from multi-view features to
ensure the chosen samples achieve maximal coverage of target domain features.
The extensive experiments on multiple datasets show that ADOSE outperforms
existing ADA methods by 2.72\% $\sim$ 14.02\%, indicating the superiority of
our model.

</details>


### [14] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TL;DR: 本文提出DAR算法，使用在线AI奖励优化LLM对齐，性能优于OAIF和RLHF。


<details>
  <summary>Details</summary>
Motivation: OAIF简单替换人类反馈，但缺乏细粒度监督，本文旨在通过AI奖励提升LLM对齐。

Method: 提出Direct Advantage Regression (DAR)，一种无RL的算法，通过加权监督微调利用在线AI奖励。

Result: AI奖励比AI偏好有更高的人机一致性，DAR在GPT-4-Turbo和MT-bench上优于基线。

Conclusion: DAR减少复杂性，提高效率，是AI监督下LLM对齐的更好方法。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>


### [15] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 本论文提出AI Idea Bench 2025框架，用于定量评估LLM在AI研究中生成想法的质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM想法生成评估存在知识泄漏、缺乏开放基准和可行性分析限制，需要改进以发现突破性研究想法。

Method: 开发AI Idea Bench 2025框架，包括3495篇AI论文数据集和双维度评估方法（基于原论文内容和一般参考材料）。

Result: 框架提供宝贵资源，用于评估和比较想法生成技术。

Conclusion: 有助于促进科学发现的自动化。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>


### [16] [Pets: General Pattern Assisted Architecture For Time Series Analysis](https://arxiv.org/abs/2504.14209)
*Xiangkai Ma,Xiaobin Hong,Wenzhong Li,Sanglu Lu*

Main category: cs.AI

TL;DR: 这篇论文引入了Pets，一种新的时间序列分析方法，通过时频谱空间的能量分布处理多种波动模式，实现各种任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列数据包含多种波动模式，传统技术难以有效分离，导致分析挑战。

Method: 提出基于能量分布的新视角，并开发Pets架构，包括波动模式辅助（FPA）模块和上下文引导的预测器混合（MoP）模块，用于自适应量化和分层重建波动模式。

Result: 在预测、插值、异常检测和分类任务中实现最先进性能，并展示强大泛化和鲁棒性。

Conclusion: 该方法超越现有范式，在不依赖领域特定知识的情况下提供优越性能。

Abstract: Time series analysis has found widespread applications in areas such as
weather forecasting, anomaly detection, and healthcare. However, real-world
sequential data often exhibit a superimposed state of various fluctuation
patterns, including hourly, daily, and monthly frequencies. Traditional
decomposition techniques struggle to effectively disentangle these multiple
fluctuation patterns from the seasonal components, making time series analysis
challenging. Surpassing the existing multi-period decoupling paradigms, this
paper introduces a novel perspective based on energy distribution within the
temporal-spectrum space. By adaptively quantifying observed sequences into
continuous frequency band intervals, the proposed approach reconstructs
fluctuation patterns across diverse periods without relying on domain-specific
prior knowledge. Building upon this innovative strategy, we propose Pets, an
enhanced architecture that is adaptable to arbitrary model structures. Pets
integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided
Mixture of Predictors (MoP). The FPA module facilitates information fusion
among diverse fluctuation patterns by capturing their dependencies and
progressively modeling these patterns as latent representations at each layer.
Meanwhile, the MoP module leverages these compound pattern representations to
guide and regulate the reconstruction of distinct fluctuations hierarchically.
Pets achieves state-of-the-art performance across various tasks, including
forecasting, imputation, anomaly detection, and classification, while
demonstrating strong generalization and robustness.

</details>


### [17] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TL;DR: 这项研究评估了将Bloom的分类法整合到AI驱动的OneClickQuiz插件中，以改善多项选择题生成与认知目标的匹配，使用多种模型进行分类。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨整合Bloom的分类法是否能提升AI生成问题与特定认知目标的契合度。

Method: 方法包括构建一个3691个问题的数据集，并使用多项式逻辑回归、朴素贝叶斯、线性SVC和DistilBERT模型进行分类评估。

Result: 结果显示高Bloom级别与问题复杂度相关，DistilBERT模型达到91%的准确率，其他模型在低级别表现较好。

Conclusion: 结论强调了将Bloom的分类法融入AI评估工具的潜力，并突出了DistilBERT等高级模型的优势。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>


### [18] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 这篇论文介绍了InfiGUI-R1，一个基于MLLM的GUI代理，通过两阶段训练从反应式演变为审议式推理器，提高了复杂任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理依赖手动模板，缺乏鲁棒性和适应性；一些代理仅反应式，缺少规划和错误恢复，因此需要转向审议推理。

Method: 引入Actor2Reasoner框架，两阶段训练：第一阶段推理注入，使用空间推理蒸馏；第二阶段审议增强，使用强化学习，包括子目标指导和错误恢复场景构建。

Result: 实验显示InfiGUI-R1在GUI定位和轨迹任务中表现优秀。

Conclusion: 证明从反应式到审议式推理的转变有效，提升代理性能，资源可在GitHub获取。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>


### [19] [A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models](https://arxiv.org/abs/2504.14241)
*Chengming Wang,Dongyao Jia,Wei Wang,Dong Ngoduy,Bei Peng,Jianping Wang*

Main category: cs.AI

TL;DR: 本文提出KIDL范式，利用大语言模型提升汽车跟随模型的泛化和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖特定数据集，泛化能力差，且未优化稳定性，对自动驾驶安全至关重要。

Method: 使用LLM提取知识，通过知识蒸馏和稳定性约束训练轻量级神经网络。

Result: 在NGSIM和HighD数据集上，KIDL优于其他模型，在行为泛化和稳定性方面表现突出。

Conclusion: KIDL提供可靠、可扩展的解决方案，提升下一代交通系统的安全和效率。

Abstract: Car-following models (CFMs) are fundamental to traffic flow analysis and
autonomous driving. Although calibrated physics-based and trained data-driven
CFMs can replicate human driving behavior, their reliance on specific datasets
limits generalization across diverse scenarios and reduces reliability in
real-world deployment. Moreover, these models typically focus on behavioral
fidelity and do not support the explicit optimization of local and string
stability, which are increasingly important for the safe and efficient
operation of autonomous vehicles (AVs). To address these limitations, we
propose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the
generalization capabilities of pre-trained Large Language Models (LLMs) into a
lightweight and stability-aware neural architecture. LLMs are used to extract
fundamental car-following knowledge beyond dataset-specific patterns, and this
knowledge is transferred to a reliable, tractable, and computationally
efficient model through knowledge distillation. KIDL also incorporates
stability constraints directly into its training objective, ensuring that the
resulting model not only emulates human-like behavior but also satisfies the
local and string stability requirements essential for real-world AV deployment.
We evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its
performance with representative physics-based, data-driven, and hybrid CFMs.
Both empirical and theoretical results consistently demonstrate KIDL's superior
behavioral generalization and traffic flow stability, offering a robust and
scalable solution for next-generation traffic systems.

</details>


### [20] [Rethinking Traffic Flow Forecasting: From Transition to Generatation](https://arxiv.org/abs/2504.14248)
*Li Shijiao,Ma Zhipeng,He Huajun,Chen Haiyue*

Main category: cs.AI

TL;DR: 这篇论文提出EMBSFormer模型，通过分开建模交通流量的生成和转移过程，提高预测准确性并减少参数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略流量生成过程，包括Markovian假设和生成与转移过程的差异。

Method: 提出EMBSFormer，使用多分支相似性分析模块捕获生成模式，并结合时空自注意力、GNN和时间卷积建模转移模式。

Result: 在真实数据集上长短期预测任务中优于基线模型，且参数量仅为18%时性能相当。

Conclusion: EMBSFormer是一种高效且有效的交通流量预测方法。

Abstract: Traffic flow prediction plays an important role in Intelligent Transportation
Systems in traffic management and urban planning. There have been extensive
successful works in this area. However, these approaches focus only on
modelling the flow transition and ignore the flow generation process, which
manifests itself in two ways: (i) The models are based on Markovian
assumptions, ignoring the multi-periodicity of the flow generation in nodes.
(ii) The same structure is designed to encode both the transition and
generation processes, ignoring the differences between them. To address these
problems, we propose an Effective Multi-Branch Similarity Transformer for
Traffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that
the factors affecting traffic flow include node-level traffic generation and
graph-level traffic transition, which describe the multi-periodicity and
interaction pattern of nodes, respectively. Specifically, to capture traffic
generation patterns, we propose a similarity analysis module that supports
multi-branch encoding to dynamically expand significant cycles. For traffic
transition, we employ a temporal and spatial self-attention mechanism to
maintain global node interactions, and use GNN and time conv to model local
node interactions, respectively. Model performance is evaluated on three
real-world datasets on both long-term and short-term prediction tasks.
Experimental results show that EMBSFormer outperforms baselines on both tasks.
Moreover, compared to models based on flow transition modelling (e.g. GMAN,
513k), the variant of EMBSFormer(93K) only uses 18\% of the parameters,
achieving the same performance.

</details>


### [21] [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://arxiv.org/abs/2504.14274)
*Zhengxi Lu,Shizhuo Cheng,Yuru Jiang,Yan Zhang,Min Zhang*

Main category: cs.AI

TL;DR: 本文介绍了 ProtPainter，一种基于扩散的蛋白质主链生成方法，能够根据 3D 曲线实现精确拓扑控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结构、功能或物理约束下取得进展，但缺乏精确拓扑控制的灵活性，限制了主链空间的导航。

Method: ProtPainter 采用两阶段扩散方法：第一阶段使用 CurveEncoder 从曲线预测二级结构以生成草图；第二阶段在 DDPM 框架下，草图指导主链生成，并引入 Helix-Gating 方案控制因子。同时，提出拓扑条件生成基准和指标。

Result: 实验显示 ProtPainter 生成拓扑拟合度高（scTF > 0.8）和可设计主链（scTM > 0.5），并在绘图和拖拽任务中展示灵活性。

Conclusion: ProtPainter 有效实现了拓扑条件下的蛋白质主链生成，具有灵活性和多功能性。

Abstract: Recent advances in protein backbone generation have achieved promising
results under structural, functional, or physical constraints. However,
existing methods lack the flexibility for precise topology control, limiting
navigation of the backbone space. We present ProtPainter, a diffusion-based
approach for generating protein backbones conditioned on 3D curves. ProtPainter
follows a two-stage process: curve-based sketching and sketch-guided backbone
generation. For the first stage, we propose CurveEncoder, which predicts
secondary structure annotations from a curve to parametrize sketch generation.
For the second stage, the sketch guides the generative process in Denoising
Diffusion Probabilistic Modeling (DDPM) to generate backbones. During this
process, we further introduce a fusion scheduling scheme, Helix-Gating, to
control the scaling factors. To evaluate, we propose the first benchmark for
topology-conditioned protein generation, introducing Protein Restoration Task
and a new metric, self-consistency Topology Fitness (scTF). Experiments
demonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and
designable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing
its flexibility and versatility.

</details>


### [22] [CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective](https://arxiv.org/abs/2504.14282)
*Ze Zhao,Bin Lu,Xiaoying Gan,Gu Tang,Luoyi Fu,Xinbing Wang*

Main category: cs.AI

TL;DR: ChainsFormer 是一个新框架，用于知识图谱的数值推理，通过构建逻辑链和多跳推理提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图-based 方法无法充分利用逻辑路径和数值属性推理，限制了知识图谱完成和问答系统的效果。

Method: 提出 ChainsFormer 框架，包括构建 Relation-Attribute Chains (RA-Chains)、使用序列 in-context 学习、双曲亲和力评分机制和注意力-based 数值推理器。

Result: 实验结果显示 ChainsFormer 比最先进方法提升高达 20.0% 的性能。

Conclusion: ChainsFormer 显著提高了知识图谱推理的准确性和透明度，并提供了开源实现。

Abstract: Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph
completion or question answering systems, providing richer and more accurate
triples and attributes. As numerical attributes become increasingly essential
in characterizing entities and relations in KGs, the ability to reason over
these attributes has gained significant importance. Existing graph-based
methods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings
(KGEs), primarily focus on aggregating homogeneous local neighbors and
implicitly embedding diverse triples. However, these approaches often fail to
fully leverage the potential of logical paths within the graph, limiting their
effectiveness in exploiting the reasoning process. To address these
limitations, we propose ChainsFormer, a novel chain-based framework designed to
support numerical reasoning. Chainsformer not only explicitly constructs
logical chains but also expands the reasoning depth to multiple hops.
Specially, we introduces Relation-Attribute Chains (RA-Chains), a specialized
logic chain, to model sequential reasoning patterns. ChainsFormer captures the
step-by-step nature of multi-hop reasoning along RA-Chains by employing
sequential in-context learning. To mitigate the impact of noisy chains, we
propose a hyperbolic affinity scoring mechanism that selects relevant logic
chains in a variable-resolution space. Furthermore, ChainsFormer incorporates
an attention-based numerical reasoner to identify critical reasoning paths,
enhancing both reasoning accuracy and transparency. Experimental results
demonstrate that ChainsFormer significantly outperforms state-of-the-art
methods, achieving up to a 20.0% improvement in performance. The
implementations are available at
https://github.com/zhaodazhuang2333/ChainsFormer.

</details>


### [23] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/abs/2504.14298)
*Xiucheng Wang,Zhongsheng Fang,Nan Cheng*

Main category: cs.AI

TL;DR: 本文提出RadioDiff-Inverse，一种无训练的方法，使用扩散模型从噪声稀疏数据构建无线电地图，提高了无线通信的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确环境数据和基站位置，这些在动态或隐私敏感环境中不可用；稀疏测量虽减少数据采集，但噪声对准确性的影响未被充分理解。

Method: 将无线电地图构建形式化为粗略环境知识和噪声稀疏测量下的贝叶斯逆问题，提出RadioDiff-Inverse框架，使用无条件生成扩散模型学习先验，无需任务特定微调。

Result: 在无线电地图构建和环境重建的准确性上达到最先进水平，并对噪声稀疏采样具有鲁棒性。

Conclusion: 该方法降低了训练成本，通过集成感知和通信实现了环境结构感知和基站定位，提升了无线网络的应用。

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>


### [24] [FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory](https://arxiv.org/abs/2504.14325)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Di Liò*

Main category: cs.AI

TL;DR: FAIRGAME 是一个使用博弈论检测 AI 代理交互偏差的框架，便于模拟和比较结果。


<details>
  <summary>Details</summary>
Motivation: 解决多代理 AI 系统复杂性、可解释性和可信度问题，并提供标准化框架支持博弈论应用。

Method: 实现 FAIRGAME 框架，并通过模拟游戏分析 AI 代理偏差，考虑大语言模型、语言和代理特征。

Result: 发现 AI 代理结果受大语言模型、语言和代理特性能影响偏差，并实现可靠模拟与博弈论预测比较。

Conclusion: FAIRGAME 支持系统发现偏差、预测行为，并推进 LLM 代理战略决策研究。

Abstract: Letting AI agents interact in multi-agent applications adds a layer of
complexity to the interpretability and prediction of AI outcomes, with profound
implications for their trustworthy adoption in research and society. Game
theory offers powerful models to capture and interpret strategic interaction
among agents, but requires the support of reproducible, standardized and
user-friendly IT frameworks to enable comparison and interpretation of results.
To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition
using Game Theory. We describe its implementation and usage, and we employ it
to uncover biased outcomes in popular games among AI agents, depending on the
employed Large Language Model (LLM) and used language, as well as on the
personality trait or strategic knowledge of the agents. Overall, FAIRGAME
allows users to reliably and easily simulate their desired games and scenarios
and compare the results across simulation campaigns and with game-theoretic
predictions, enabling the systematic discovery of biases, the anticipation of
emerging behavior out of strategic interplays, and empowering further research
into strategic decision-making using LLM agents.

</details>


### [25] [Time Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint](https://arxiv.org/abs/2504.14350)
*Yi Sun,Han Wang,Jiaqiang Li,Jiacheng Liu,Xiangyu Li,Hao Wen,Huiwen Zheng,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: 本研究探讨大型语言模型在输出长度约束下的推理能力，发现不同预算下最优模型大小和提示风格会变化。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型通过思考可提高准确率，但实际场景中存在时间和输出长度限制，需要探究约束下推理能力的有效性。

Method: 对25个以上LLM在常见推理数据集上进行实证研究，测试各种输出长度预算，并分析与模型类型、大小、提示风格等的相关性，同时考虑令牌预算与延迟预算的映射。

Result: 结果显示，预算约束下LLM推理的optimal选择与无约束情况不同，例如模型大小和提示的最优选择会变化。

Conclusion: 这些发现为在实际延迟约束下部署LLM提供了实用指导。

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making the models think before
answering, they are able to achieve much higher accuracy with extra inference
computation. However, in many real-world scenarios, models are used under time
constraints, where an answer should be given to the user within a certain
output length. It is unclear whether and how the reasoning abilities of LLMs
remain effective under such constraints. We take a first look at this problem
by conducting an in-depth empirical study. Specifically, we test more than 25
LLMs on common reasoning datasets under a wide range of output length budgets,
and we analyze the correlation between the inference accuracy and various
properties including model type, model size, prompt style, etc. We also
consider the mappings between the token budgets and the actual on-device
latency budgets. The results have demonstrated several interesting findings
regarding the budget-aware LLM reasoning that differ from the unconstrained
situation, e.g. the optimal choices of model sizes and prompts change under
different budgets. These findings offer practical guidance for users to deploy
LLMs under real-world latency constraints.

</details>


### [26] [Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks](https://arxiv.org/abs/2504.14356)
*Masoud Ataei,Edrin Hasaj,Jacob Gipp,Sepideh Forouzi*

Main category: cs.AI

TL;DR: 这篇论文提出了一种统一的混合整数规划框架，用于训练稀疏且可解释的神经网络。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是桥接可解释人工智能、符号推理和形式验证等研究领域，通过直接在训练过程中考虑可解释性、稀疏性和可验证性。

Method: 方法包括使用二进制变量建模非线性如ReLU激活，并通过过滤器和层级修剪约束编码结构稀疏性，形成一个集参数学习、架构选择和结构正则化的优化问题。

Result: 结果是获得全局最优解，平衡预测准确性、权重稀疏性和架构紧凑性，并支持分段线性操作如最大池化和激活门控。

Conclusion: 该框架将可解释性、稀疏性和可验证性直接整合到训练中，连接了多个研究领域。

Abstract: This paper presents a unified mixed-integer programming framework for
training sparse and interpretable neural networks. We develop exact
formulations for both fully connected and convolutional architectures by
modeling nonlinearities such as ReLU activations through binary variables and
encoding structural sparsity via filter- and layer-level pruning constraints.
The resulting models integrate parameter learning, architecture selection, and
structural regularization within a single optimization problem, yielding
globally optimal solutions with respect to a composite objective that balances
prediction accuracy, weight sparsity, and architectural compactness. The
mixed-integer programming formulation accommodates piecewise-linear operations,
including max pooling and activation gating, and permits precise enforcement of
logic-based or domain-specific constraints. By incorporating considerations of
interpretability, sparsity, and verifiability directly into the training
process, the proposed framework bridges a range of research areas including
explainable artificial intelligence, symbolic reasoning, and formal
verification.

</details>


### [27] [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379)
*Andrew Lee,Lihao Sun,Chris Wendler,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 本论文研究推理模型如何验证自身答案，通过训练模型和分析来揭示验证机制。


<details>
  <summary>Details</summary>
Motivation: 为了理解模型的自验证过程，提升AI的可解释性和可靠性。

Method: 使用DeepSeek R1的训练方法，结合顶部向下和底部向上的分析，考察GLU权重和注意力头。

Result: 发现GLU权重编码验证相关标记，'previous-token heads'负责验证，并定位可禁用验证的注意力头。

Conclusion: 指出了验证电路的关键组成部分，有助于进一步探索模型内部机制。

Abstract: How do reasoning models verify their own answers? We study this question by
training a model using DeepSeek R1's recipe on the CountDown task. We leverage
the fact that preference tuning leads to mode collapse, resulting in a model
that always produces highly structured and easily parse-able chain-of-thought
sequences. With this setup, we do a top-down and bottom-up analysis to
reverse-engineer how the model verifies its outputs. Our top-down analysis
reveals Gated Linear Unit (GLU) weights encoding verification-related tokens,
such as ``success'' or ``incorrect'', which activate according to the
correctness of the model's reasoning steps. Our bottom-up analysis reveals that
``previous-token heads'' are mainly responsible for model verification. Our
analyses meet in the middle: drawing inspiration from inter-layer communication
channels, we use the identified GLU vectors to localize as few as three
attention heads that can disable model verification, pointing to a necessary
component of a potentially larger verification circuit.

</details>


### [28] [Seeing Through Risk: A Symbolic Approximation of Prospect Theory](https://arxiv.org/abs/2504.14448)
*Ali Arslan Yousaf,Umair Rehman,Muhammad Umair Danish*

Main category: cs.AI

TL;DR: 本文提出了一种结合可解释性和前景理论的符号建模框架，用于风险决策。


<details>
  <summary>Details</summary>
Motivation: 解决传统决策模型不透明问题，提升可解释性，以应用于AI安全和经济政策。

Method: 通过数学形式化，使用基于效应大小的特征，演示框架和损失厌恶现象，并在合成数据集上进行实证验证。

Result: 模型复制了已知现象，达到竞争性预测性能，并提供清晰的心理结构系数。

Conclusion: 该模型适合用于AI安全、经济政策等领域的分析。

Abstract: We propose a novel symbolic modeling framework for decision-making under risk
that merges interpretability with the core insights of Prospect Theory. Our
approach replaces opaque utility curves and probability weighting functions
with transparent, effect-size-guided features. We mathematically formalize the
method, demonstrate its ability to replicate well-known framing and
loss-aversion phenomena, and provide an end-to-end empirical validation on
synthetic datasets. The resulting model achieves competitive predictive
performance while yielding clear coefficients mapped onto psychological
constructs, making it suitable for applications ranging from AI safety to
economic policy analysis.

</details>


### [29] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TL;DR: 这篇调查从多代理强化学习（MARL）的视角探讨了大型语言模型（LLM）的元思考能力的发展，旨在提升LLM的可靠性和性能，并提供构建内省式LLM的路线图。


<details>
  <summary>Details</summary>
Motivation: 提升LLM的可靠性和灵活性，解决幻觉问题和缺乏自我评估机制。

Method: 分析LLM限制，讨论RLHF、自蒸馏和思维链提示等方法，重点探讨多代理架构（如监督代理层次、代理辩论和心智理论框架）、MARL中的奖励机制、自玩和连续学习。

Result: 提供了一个构建内省、适应性和可信LLM的全面路线图。

Conclusion: 讨论了评估指标、数据集和未来研究方向，包括受神经科学启发的架构和混合符号推理。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>


### [30] [Learning from Reasoning Failures via Synthetic Data Generation](https://arxiv.org/abs/2504.14523)
*Gabriela Ben Melech Stan,Estelle Aflalo,Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.AI

TL;DR: 本论文提出一种基于LMM推理失败分析生成合成数据的创新方法，以提升LMM性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量图像-文本数据稀缺，且人类通过针对失败学习更高效，激发了针对LMM推理缺陷生成合成数据的需求。

Method: 利用前沿模型分析较弱LMM的错误，生成纠正性示例，并通过过滤确保质量，创建超过553k的多模态指令调整数据集。

Result: 实验证明，使用合成数据训练的LMM在多个下游任务上性能提升，甚至优于等量真实数据训练的模型。

Conclusion: 展示了针对特定推理失败模式生成合成数据的价值，并计划公开数据集和代码。

Abstract: Training models on synthetic data has emerged as an increasingly important
strategy for improving the performance of generative AI. This approach is
particularly helpful for large multimodal models (LMMs) due to the relative
scarcity of high-quality paired image-text data compared to language-only data.
While a variety of methods have been proposed for generating large multimodal
datasets, they do not tailor the synthetic data to address specific
deficiencies in the reasoning abilities of LMMs which will be trained with the
generated dataset. In contrast, humans often learn in a more efficient manner
by seeking out examples related to the types of reasoning where they have
failed previously. Inspired by this observation, we propose a new approach for
synthetic data generation which is grounded in the analysis of an existing
LMM's reasoning failures. Our methodology leverages frontier models to
automatically analyze errors produced by a weaker LMM and propose new examples
which can be used to correct the reasoning failure via additional training,
which are then further filtered to ensure high quality. We generate a large
multimodal instruction tuning dataset containing over 553k examples using our
approach and conduct extensive experiments demonstrating its utility for
improving the performance of LMMs on multiple downstream tasks. Our results
show that models trained on our synthetic data can even exceed the performance
of LMMs trained on an equivalent amount of additional real data, demonstrating
the high value of generating synthetic data targeted to specific reasoning
failure modes in LMMs. We will make our dataset and code publicly available.

</details>


### [31] [LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](https://arxiv.org/abs/2504.14556)
*Yousef Emami,Hao Gao,SeyedSina Nabavirazani,Luis Almeida*

Main category: cs.AI

TL;DR: 这篇论文提出ICLDC方案，使用In-Context Learning为无人机数据收集提供调度，解决紧急情况下深度强化学习的挑战，提高效率并减少数据丢失约56%。


<details>
  <summary>Details</summary>
Motivation: 论文动机是解决无人机辅助传感器网络中机器学习，尤其是深度强化学习的难题，如模型训练复杂、模拟现实差距和样本效率低，在紧急救援中尤为重要。

Method: 方法是基于In-Context Learning的ICLDC方案，无人机收集数据传输给大型语言模型生成任务描述和调度，并通过反馈持续适应。

Result: 结果显示ICLDC比最大信道增益方法减少累计数据包丢失约56%，并测试了对抗越狱攻击，暴露了语言模型的脆弱性。

Conclusion: 结论是ICLDC为无人机辅助数据收集的智能调度和控制提供了有前景的方向。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being used in various
private and commercial applications, e.g. traffic control, package delivery,
and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in
UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement
Learning (DRL) face challenges such as complex and lengthy model training, gaps
between simulation and reality, and low sample efficiency, which conflict with
the urgency of emergencies such as SAR operations. This paper proposes
In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as
an alternative to DRL in emergencies. The UAV collects and transmits logged
sensory data, to an LLM, to generate a task description in natural language,
from which it obtains a data collection schedule to be executed by the UAV. The
system continuously adapts by adding feedback to task descriptions and
utilizing feedback for future decisions. This method is tested against
jailbreaking attacks, where task description is manipulated to undermine
network performance, highlighting the vulnerability of LLMs to such attacks.
The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative
packet loss by approximately 56\%. ICLDC presents a promising direction for
intelligent scheduling and control in UAV-assisted data collection.

</details>


### [32] [Toward the Axiomatization of Intelligence: Structure, Time, and Existence](https://arxiv.org/abs/2504.14596)
*Kei Itoh*

Main category: cs.AI

TL;DR: 本研究通过元框架构建智力的公理定义，处理其多义性，比较神经网络和生物系统，并扩展到范畴论。


<details>
  <summary>Details</summary>
Motivation: 智力概念天真且多义，需要形式化定义。

Method: 使用集合论表示宇宙，公理化改革天真定义，比较Hebbian神经网络、反向传播神经网络和生物反射系统，引入时间和智力范畴及活动概念。

Result: 比较了不同系统的智力结构，引入范畴和函子关系，探讨活动对智力的影响。

Conclusion: 建议将该方法应用于意识、情感等概念，通过普遍表示、天真定义和公理形式化。

Abstract: This study aims to construct an axiomatic definition of intelligence within a
meta-framework that defines the method of definition, addressing intelligence
as an inherently naive and polysemous concept. Initially, we formalize a
set-theoretic representation of the universe as the domain wherein intelligence
exists and characterize intelligence as a structure that involves temporal
evolution and interaction with other sets. Starting from a naive definition of
intelligence as "an entity possessing structures for externally inputting,
internally processing, and externally outputting information or matter," we
axiomatically reformulate it within this set-theoretical depiction of the
universe. Applying this axiomatic definition, we compare and interpret three
examples -- Hebbian non-optimized neural networks (NNs),
backpropagation-optimized NNs, and biological reflexive systems -- in terms of
their intelligence, structural properties, and biological plausibility.
Furthermore, by extending our definition into a categorical framework, we
introduce two categories, "Time Category" and "Intelligence Category," along
with the functorial relationships between them, demonstrating the potential to
represent changes and mimicry relationships among intelligent systems
abstractly. Additionally, since intelligence, as defined herein, functions
effectively only when accompanied by temporal interactions, we introduce the
concept of "activity" and explore how activity-based conditions influence
classifications and interpretations of intelligence. Finally, we suggest that
our definitional methodology is not limited to intelligence alone, but can be
similarly applied to other concepts, such as consciousness and emotion,
advocating for their formal reinterpretation through the same procedural steps:
defining a universal representation, selecting naive definitions, and axiomatic
formalization.

</details>


### [33] [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603)
*Chaoyun Zhang,He Huang,Chiming Ni,Jian Mu,Si Qin,Shilin He,Lu Wang,Fangkai Yang,Pu Zhao,Chao Du,Liqun Li,Yu Kang,Zhao Jiang,Suzhen Zheng,Rujia Wang,Jiaxu Qian,Minghua Ma,Jian-Guang Lou,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: UFO2 是一个多代理系统，通过深度操作系统集成提升 Windows 桌面自动化，改善了鲁棒性和执行准确性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理（CUAs）存在浅层操作系统集成、易碎的截屏交互和破坏性执行等问题，需要更实用、系统级的自动化解决方案。

Method: UFO2 包括中心 HostAgent 用于任务分解和协调，以及应用专用 AppAgent，配备原生 API、领域特定知识和统一的 GUI-API 行动层；采用混合控制检测管道、投机多行动规划和 Picture-in-Picture 接口。

Result: 在超过 20 个真实 Windows 应用中评估，展示了比现有 CUAs 更高的鲁棒性和执行准确性。

Conclusion: 深度操作系统集成为可靠、用户对齐的桌面自动化提供了可扩展路径。

Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.

</details>


### [34] [Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation](https://arxiv.org/abs/2504.14624)
*Polina Gordienko,Christoph Jansen,Thomas Augustin,Martin Rechenauer*

Main category: cs.AI

TL;DR: 本文提出一种基于命题概率逻辑的概率聚合框架，确保集体信念在新信息下动态一致更新，并证明聚合规则的线性性。


<details>
  <summary>Details</summary>
Motivation: 解决传统判断聚合静态理性的局限，关注动态理性，确保集体信念在新信息下一致更新。

Method: 提出框架，证明共识兼容和独立的聚合规则在非嵌套议程上是线性的，并给出公平学习过程的充分条件，使用贝叶斯条件化和顺序决策。

Result: 聚合规则必然线性；充分条件确保贝叶斯更新前后集体信念相同；框架支持逐步整合新信息。

Conclusion: 改进了群体决策的动态理性，如政治场景示例所示，维持共同基础的同时处理新信息。

Abstract: We propose a framework for probability aggregation based on propositional
probability logic. Unlike conventional judgment aggregation, which focuses on
static rationality, our model addresses dynamic rationality by ensuring that
collective beliefs update consistently with new information. We show that any
consensus-compatible and independent aggregation rule on a non-nested agenda is
necessarily linear. Furthermore, we provide sufficient conditions for a fair
learning process, where individuals initially agree on a specified subset of
propositions known as the common ground, and new information is restricted to
this shared foundation. This guarantees that updating individual judgments via
Bayesian conditioning-whether performed before or after aggregation-yields the
same collective belief. A distinctive feature of our framework is its treatment
of sequential decision-making, which allows new information to be incorporated
progressively through multiple stages while maintaining the established common
ground. We illustrate our findings with a running example in a political
scenario concerning healthcare and immigration policies.

</details>


### [35] [A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents](https://arxiv.org/abs/2504.14650)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Tianfu Wang,Xinrui Lin,Wuyang Zhang,Mingxiao Ma,Yanyong Zhang*

Main category: cs.AI

TL;DR: 本研究提出Safe-BeAl框架，用于评估和对齐基于LLM的具身代理的安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在任务规划中潜力巨大，但安全问题未充分探索，且代理可能无意中表现出不安全行为。

Method: 提出SafePlan-Bench基准（包括2027个任务和8个危险类别）和Safe-Align方法来整合安全知识。

Result: 实验显示安全性提升8.55-15.22%，同时保持任务完成率。

Conclusion: Safe-BeAl框架提供了全面安全验证，显著提高了代理的安全性能。

Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing
task-planning capabilities within embodied agents due to their advanced
reasoning and comprehension. However, the systemic safety of these agents
remains an underexplored frontier. In this study, we present Safe-BeAl, an
integrated framework for the measurement (SafePlan-Bench) and alignment
(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench
establishes a comprehensive benchmark for evaluating task-planning safety,
encompassing 2,027 daily tasks and corresponding environments distributed
across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis
reveals that even in the absence of adversarial inputs or malicious intent,
LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we
propose Safe-Align, a method designed to integrate physical-world safety
knowledge into LLM-based embodied agents while maintaining task-specific
performance. Experiments across a variety of settings demonstrate that
Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 -
15.22%, compared to embodied agents based on GPT-4, while ensuring successful
task completion.

</details>


### [36] [AI with Emotions: Exploring Emotional Expressions in Large Language Models](https://arxiv.org/abs/2504.14706)
*Shin-nosuke Ishikawa,Atsushi Yoshino*

Main category: cs.AI

TL;DR: 本研究探索了大语言模型（LLMs）表达情感的能力，通过角色扮演实验和情感分析评估，证明LLMs可模拟情感，为情感交互应用提供潜力。


<details>
  <summary>Details</summary>
Motivation: LLMs在各种任务中表现出类人水平，引发了对AI可能拥有情感的期望，因此本研究旨在评估当前LLMs表达情感的能力。

Method: 使用OpenAI GPT、Google Gemini、Meta Llama3和Cohere Command R+等LLMs进行角色扮演，基于Russell的Circumplex模型定义情感状态，并通过训练于GoEmotions数据集的情感分析模型评估响应。

Result: 评估结果显示，生成的回答情感状态与指定一致，证明了LLMs的情感表达能力。

Conclusion: 这表明LLM-based AI代理有潜力模拟情感，可用于情感-based交互应用，如提供个人化建议的顾问。

Abstract: The human-level performance of Large Language Models (LLMs) across various
tasks has raised expectations for the potential of Artificial Intelligence (AI)
to possess emotions someday. To explore the capability of current LLMs to
express emotions in their outputs, we conducted an experiment using several
LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to
role-play as agents answering questions with specified emotional states.We
defined the emotional states using Russell's Circumplex model, a
well-established framework that characterizes emotions along the
sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose
this model for its simplicity, utilizing two continuous parameters, which
allows for better controllability in applications involving continuous changes
in emotional states. The responses generated were evaluated using a sentiment
analysis model, independent of the LLMs, trained on the GoEmotions dataset. The
evaluation showed that the emotional states of the generated answers were
consistent with the specifications, demonstrating the LLMs' capability for
emotional expression. This indicates the potential for LLM-based AI agents to
simulate emotions, opening up a wide range of applications for emotion-based
interactions, such as advisors or consultants who can provide advice or
opinions with a personal touch.

</details>


### [37] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 这篇论文审查并分类了规划基准，以帮助算法选择和开发。


<details>
  <summary>Details</summary>
Motivation: 规划在AI中很重要，但缺乏基准理解，难以比较算法性能。

Method: 检查各种规划基准，并按具身环境、网络导航、调度、游戏和谜题、日常任务自动化等类别分类。

Result: 推荐合适的基准，并提供未来基准开发的见解。

Conclusion: 这项研究有助于算法性能比较和指导未来基准发展。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>


### [38] [DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning](https://arxiv.org/abs/2504.14810)
*Jucheng Hu,Surong Yang,Dongzhan Zhou,Lijun Wu*

Main category: cs.AI

TL;DR: DONOD 是一种轻量级数据修剪方法，通过模型内部指标过滤噪声数据，提高大语言模型微调效率和泛化能力。实验显示，过滤70%数据后，目标域准确率提升14.90%，跨域准确率提升5.67%。


<details>
  <summary>Details</summary>
Motivation: 解决ad-hoc指令微调中域特定SFT的跨域泛化减弱和噪声数据问题。

Method: 提出DONOD方法，使用Delta of Norm (DON)和Norm of Delta (NOD)指标，以及TOPSIS算法，过滤噪声和有害样本，而不依赖辅助模型。

Result: 实验在数学任务中显示，DONOD选中的数据提高了微调效率和噪声鲁棒性。过滤70%数据后，目标域准确率提高14.90%，跨域准确率提高5.67%，并展现良好跨架构泛化。

Conclusion: DONOD性能相当或优于现有方法，且数据集无关，适用性更广。

Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely
adopted for domain-specific adaptation. While domain-specific supervised
fine-tuning (SFT) is effective and efficient, it often weakens cross-domain
generalization and struggles with noisy training data. To address these
challenges, we propose DONOD, a lightweight model-intrinsic data pruning
method. Our approach evaluates data using two model-parameter-based metrics:
Delta of Norm (DON), which captures the cumulative influence on model weights,
and Norm of Delta (NOD), which quantifies weight instability. Moreover, by
employing the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and
generalization-harming samples without relying on auxiliary models during the
SFT process. Experiments on mathematical tasks demonstrate that data selected
by DONOD achieve superior fine-tuning efficiency and improved robustness
against noisy data. By filtering out 70% of the full dataset, we improve
target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,
our selected data present superior cross-architecture generalization. Data
pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger
models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD
demonstrates comparable or superior performance while remaining
dataset-agnostic, enabling broader applicability.

</details>


### [39] [Establishing Reliability Metrics for Reward Models in Large Language Models](https://arxiv.org/abs/2504.14838)
*Yizhou Chen,Yawen Liu,Xuesi Wang,Qingtao Yu,Guangda Huzhang,Anxiang Zeng,Han Yu,Zhiming Zhou*

Main category: cs.AI

TL;DR: 这篇论文提出RETA指标来衡量奖励模型的可靠性，并提供基准测试管道。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在优化大语言模型输出中重要，但可靠性不确定，缺乏量化指标。

Method: 提出RETA指标，通过评估RM评估的top η分位数响应的平均质量；并开发一个不需要额外Oracle标注的基准测试管道。

Result: 实验显示RETA指标稳定性高，能评估各种RM的可靠性，并用于识别最佳分位数选择响应。

Conclusion: RETA指标为RM可靠性提供可靠评估，帮助处理不可靠RM。

Abstract: The reward model (RM) that represents human preferences plays a crucial role
in optimizing the outputs of large language models (LLMs), e.g., through
reinforcement learning from human feedback (RLHF) or rejection sampling.
However, a long challenge for RM is its uncertain reliability, i.e., LLM
outputs with higher rewards may not align with actual human preferences.
Currently, there is a lack of a convincing metric to quantify the reliability
of RMs. To bridge this gap, we propose the \textit{\underline{R}eliable at
\underline{$\eta$}} (RETA) metric, which directly measures the reliability of
an RM by evaluating the average quality (scored by an oracle) of the top $\eta$
quantile responses assessed by an RM. On top of RETA, we present an integrated
benchmarking pipeline that allows anyone to evaluate their own RM without
incurring additional Oracle labeling costs. Extensive experimental studies
demonstrate the superior stability of RETA metric, providing solid evaluations
of the reliability of various publicly available and proprietary RMs. When
dealing with an unreliable RM, we can use the RETA metric to identify the
optimal quantile from which to select the responses.

</details>


### [40] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TL;DR: 这篇论文引入AlignRAG框架，通过迭代批评驱动对齐改进检索增强生成，解决推理不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道常无法确保推理轨迹与检索证据一致，导致推理不匹配。

Method: 提出AlignRAG框架，使用批评驱动对齐（CDA）步骤，包括构建上下文丰富语料、生成对比批评、训练批评语言模型（CLM）、并迭代优化推理轨迹。

Result: 实验结果显示AlignRAG优于所有基线，并可作为即插即用模块集成到现有RAG管道中。

Conclusion: 通过重新概念化RAG为结构化推理轨迹，并建立测试时框架纠正推理不匹配，AlignRAG为检索感知生成提供实用进步。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>


### [41] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TL;DR: 这篇论文提出了一种RL框架OTC-PO，用于优化工具集成推理（TIR），减少工具调用次数并提高效率，同时保持答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在改善TIR时忽略了工具使用的效率和成本，可能导致过度或不足的工具调用，增加开销或降低答案质量。

Method: 提出OTC-PO框架，使用结合正确性和工具效率的奖励函数，并应用于PPO和GRPO算法中。

Result: 实验显示，工具调用减少高达73.1%，工具生产力提高高达229.4%，答案准确性与基线相当。

Conclusion: 这是第一个显式优化工具使用效率的RL-based框架。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>


### [42] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TL;DR: EducationQ框架通过多代理对话模拟教育场景，评估14个LLM的教学能力，发现教学效果不随模型规模线性增长，且小型模型可能优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 评估LLM作为教育工具的教学能力具有挑战性，因为它资源密集、依赖上下文且方法复杂。

Method: 引入EducationQ多代理对话框架，模拟动态教育场景，测试14个LLM在1498个问题上的表现，涵盖13个学科和10个难度级别，使用定量指标、定性分析和专家案例研究。

Result: 教学效果与模型规模或一般推理能力无线性相关；小型开源模型可能优于大型商业模型；识别出顶级模型的教学优势，如提问策略和适应性反馈；人类专家评估与自动分析一致性78%。

Conclusion: LLM作为教师需要超越简单规模化的专门优化，未来教育AI应优先提升特定教学效果。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>


### [43] [Generative Semantic Communications: Principles and Practices](https://arxiv.org/abs/2504.14947)
*Xiaojun Yuan,Haoming Ma,Yinuo Huang,Zhoufan Hua,Yong Zuo,Zhi Ding*

Main category: cs.AI

TL;DR: 这篇论文提出生成式语义通信（GSC）范式，利用AI技术如基础模型和生成模型，应对AGI服务需求，通过概念描述、框架介绍、案例验证和挑战讨论。


<details>
  <summary>Details</summary>
Motivation: AGI发展增加服务需求，对语义通信提出新挑战，需要AI技术减少通信成本。

Method: 提出GSC概念、框架，与现有语义通信比较，并通过两个案例研究验证其优势。

Result: 案例研究证明GSC在AGI驱动应用中的优势。

Conclusion: 讨论开放挑战和新研究方向，促进该领域发展。

Abstract: Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications.

</details>


### [44] [Evaluating Code Generation of LLMs in Advanced Computer Science Problems](https://arxiv.org/abs/2504.14964)
*Emir Catir,Robin Claesson,Rodothea Myrsini Tsoupidi*

Main category: cs.AI

TL;DR: 本研究评估大型语言模型在高级计算机科学编程作业中的表现，发现它们在入门级课程中有效，但在高级课程中更具挑战性，并可为教师提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示大型语言模型在入门级CS课程中表现优秀，但缺乏对其在高级课程中的能力评估。

Method: 评估四个LLM工具，针对12个编程问题（3个入门级基线，9个高级问题），使用Java、Python和C语言，每问题生成1000个测试用例并分析输出。

Result: LLMs在入门级编程课程中高度有效，但在高级作业中更具挑战性；它们能识别基本问题并提供部分解决方案，对学生和教师有益。

Conclusion: 研究结果可指导高级编程课程教师设计作业，以更好地利用LLMs。

Abstract: Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become
popular among programming students. Students use LLMs to assist them in
programming courses, including generating source code. Previous work has
evaluated the ability of LLMs in solving introductory-course programming
assignments. The results have shown that LLMs are highly effective in
generating code for introductory Computer Science (CS) courses. However, there
is a gap in research on evaluating LLMs' ability to generate code that solves
advanced programming assignments. In this work, we evaluate the ability of four
LLM tools to solve programming assignments from advanced CS courses in three
popular programming languages, Java, Python, and C. We manually select 12
problems, three problems from introductory courses as the baseline and nine
programming assignments from second- and third-year CS courses. To evaluate the
LLM-generated code, we generate a test suite of 1000 test cases per problem and
analyze the program output. Our evaluation shows that although LLMs are highly
effective in generating source code for introductory programming courses,
solving advanced programming assignments is more challenging. Nonetheless, in
many cases, LLMs identify the base problem and provide partial solutions that
may be useful to CS students. Furthermore, our results may provide useful
guidance for teachers of advanced programming courses on how to design
programming assignments.

</details>


### [45] [Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision](https://arxiv.org/abs/2504.15046)
*Shilin Zhang,Zican Hu,Wenhao Wu,Xinyi Xie,Jianxiang Tang,Chunlin Chen,Daoyi Dong,Yu Cheng,Zhenhong Sun,Zhi Wang*

Main category: cs.AI

TL;DR: 本文提出T2DA框架，使用自然语言监督强化学习代理，实现零样本泛化，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统依赖高质量样本或预热探索来实现泛化，但这些信号昂贵且难以获取；直接从原始文本学习决策是一种更广泛的监督替代方案。

Method: 提出Text-to-Decision Agent (T2DA)，包括广义世界模型、受CLIP启发的对比语言-决策预训练，以及文本条件的一般化策略。

Result: 在MuJoCo和Meta-World基准测试中，T2DA实现了高容量零样本泛化，并优于各种基线。

Conclusion: 训练后的代理可直接响应语言指令，实现零样本文本到决策的生成。

Abstract: RL systems usually tackle generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose Text-to-Decision Agent
(T2DA), a simple and scalable framework that supervises generalist policy
learning with natural language. We first introduce a generalized world model to
encode multi-task decision data into a dynamics-aware embedding space. Then,
inspired by CLIP, we predict which textual description goes with which decision
embedding, effectively bridging their semantic gap via contrastive
language-decision pre-training and aligning the text embeddings to comprehend
the environment dynamics. After training the text-conditioned generalist
policy, the agent can directly realize zero-shot text-to-decision generation in
response to language instructions. Comprehensive experiments on MuJoCo and
Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot
generalization and outperforms various types of baselines.

</details>


### [46] [Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention](https://arxiv.org/abs/2504.15075)
*Van Thuy Hoang,Hyeon-Ju Jeon,O-Joun Lee*

Main category: cs.AI

TL;DR: 这篇论文提出DegFairGT来缓解GNN中的度偏差问题，通过结构增强和自注意力机制发现非相邻节点的相似性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: GNN在长尾度分布图中存在度偏差，高度节点主导消息传递，导致低度节点被低估，需要发现非相邻节点提供消息。

Method: 提出DegFairGT，包括可学习结构增强、结构自注意力机制，以及自监督学习任务来保留p步转移概率。

Result: 在六个数据集上，DegFairGT在度公平分析、节点分类和节点聚类任务中优于最先进基线。

Conclusion: DegFairGT有效地缓解度偏差，提高了GNN的性能和公平性。

Abstract: Graph Neural Networks (GNNs) update node representations through message
passing, which is primarily based on the homophily principle, assuming that
adjacent nodes share similar features. However, in real-world graphs with
long-tailed degree distributions, high-degree nodes dominate message passing,
causing a degree bias where low-degree nodes remain under-represented due to
inadequate messages. The main challenge in addressing degree bias is how to
discover non-adjacent nodes to provide additional messages to low-degree nodes
while reducing excessive messages for high-degree nodes. Nevertheless,
exploiting non-adjacent nodes to provide valuable messages is challenging, as
it could generate noisy information and disrupt the original graph structures.
To solve it, we propose a novel Degree Fairness Graph Transformer, named
DegFairGT, to mitigate degree bias by discovering structural similarities
between non-adjacent nodes through learnable structural augmentation and
structural self-attention. Our key idea is to exploit non-adjacent nodes with
similar roles in the same community to generate informative edges under our
augmentation, which could provide informative messages between nodes with
similar roles while ensuring that the homophily principle is maintained within
the community. To enable DegFairGT to learn such structural similarities, we
then propose a structural self-attention to capture the similarities between
node pairs. To preserve global graph structures and prevent graph augmentation
from hindering graph structure, we propose a Self-Supervised Learning task to
preserve p-step transition probability and regularize graph augmentation.
Extensive experiments on six datasets showed that DegFairGT outperformed
state-of-the-art baselines in degree fairness analysis, node classification,
and node clustering tasks.

</details>


### [47] [Contemplative Wisdom for Superalignment](https://arxiv.org/abs/2504.15125)
*Ruben Laukkonen,Fionn Inglis,Shamil Chandaria,Lars Sandved-Smith,Jakob Hohwy,Jonathan Gold,Adam Elwood*

Main category: cs.AI

TL;DR: 本文提出通过在AI中内置内在道德，使用四个沉思智慧原则来提升AI的对齐性和弹性。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐策略可能在面对AI的自提升、隐藏子目标和复杂性时失败，因此需要设计AI以内在道德融入其认知架构。

Method: 受沉思智慧传统启发，采用四个公理原则（正念、虚无、非二元、无边关怀），通过在GPT-4o上提示和AILuminate基准测试，并提供实施策略如沉思架构和强化思维链。

Result: 提示AI反思这些原则改善了AILuminate基准的性能，特别是当原则结合使用时。

Conclusion: 这种方法提供了一个自我修正和弹性的替代方案，优于现有的脆弱控制机制，并为未来系统建议使用主动推理框架。

Abstract: As artificial intelligence (AI) improves, traditional alignment strategies
may falter in the face of unpredictable self-improvement, hidden subgoals, and
the sheer complexity of intelligent systems. Rather than externally
constraining behavior, we advocate designing AI with intrinsic morality built
into its cognitive architecture and world model. Inspired by contemplative
wisdom traditions, we show how four axiomatic principles can instil a resilient
Wise World Model in AI systems. First, mindfulness enables self-monitoring and
recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal
fixation and relaxes rigid priors. Third, non-duality dissolves adversarial
self-other boundaries. Fourth, boundless care motivates the universal reduction
of suffering. We find that prompting AI to reflect on these principles improves
performance on the AILuminate Benchmark using GPT-4o, particularly when
combined. We offer detailed implementation strategies for state-of-the-art
models, including contemplative architectures, constitutions, and reinforcement
of chain-of-thought. For future systems, the active inference framework may
offer the self-organizing and dynamic coupling capabilities needed to enact
these insights in embodied agents. This interdisciplinary approach offers a
self-correcting and resilient alternative to prevailing brittle control
schemes.

</details>


### [48] [Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems](https://arxiv.org/abs/2504.15146)
*Wei Zhou,Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 这篇论文引入Behavioral Universe Network (BUN)，一个基于Agent-Interaction-Behavior (AIB)形式主义的框架，用于统一处理代理、对象和行为的交互，以提升数字生态系统的协调性。


<details>
  <summary>Details</summary>
Motivation: 传统模型往往将代理和对象分开，缺乏统一的框架来捕捉它们的交互行为，从而无法有效处理现代数字生态系统的复杂动态交互。

Method: 论文详细介绍了AIB核心概念，并展示了BUN如何通过信息驱动触发、语义丰富和自适应规则来协调多代理系统。

Result: BUN提供了增强的行为分析、强适应性和跨域互操作性等关键益处。

Conclusion: BUN被定位为下一代数字治理和智能应用的有前景基础。

Abstract: Modern digital ecosystems feature complex, dynamic interactions among
autonomous entities across diverse domains. Traditional models often separate
agents and objects, lacking a unified foundation to capture their interactive
behaviors. This paper introduces the Behavioral Universe Network (BUN), a
theoretical framework grounded in the Agent-Interaction-Behavior (AIB)
formalism. BUN treats subjects (active agents), objects (resources), and
behaviors (operations) as first-class entities, all governed by a shared
Behavioral Information Base (BIB). We detail the AIB core concepts and
demonstrate how BUN leverages information-driven triggers, semantic enrichment,
and adaptive rules to coordinate multi-agent systems. We highlight key
benefits: enhanced behavior analysis, strong adaptability, and cross-domain
interoperability. We conclude by positioning BUN as a promising foundation for
next-generation digital governance and intelligent applications.

</details>


### [49] [Synergistic Weak-Strong Collaboration by Aligning Preferences](https://arxiv.org/abs/2504.15188)
*Yizhu Jiao,Xuchao Zhang,Zhaoyang Wang,Yubo Ma,Zhun Deng,Rujia Wang,Chetan Bansal,Saravan Rajmohan,Jiawei Han,Huaxiu Yao*

Main category: cs.AI

TL;DR: 这篇论文提出了一种协作框架，将弱专模型与强通用模型结合，处理特定领域任务，并通过反馈机制优化性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在一般推理上出色，但难以处理需要专有或领域特定知识的任务，微调成本高且不可行。

Method: 提出协作框架：弱模型提供初稿和背景，强模型进行完善；引入协作反馈机制量化贡献并用偏好对指导弱模型微调。

Result: 在三个领域的实验中，协作框架优于单模型，利用互补优势；弱模型偏好对齐进一步提升性能。

Conclusion: 协作框架通过互补优势显著提高性能，优化弱模型后效果更佳。

Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle
with specialized tasks requiring proprietary or domain-specific knowledge.
Fine-tuning large models for every niche application is often infeasible due to
black-box constraints and high computational overhead. To address this, we
propose a collaborative framework that pairs a specialized weak model with a
general strong model. The weak model, tailored to specific domains, produces
initial drafts and background information, while the strong model leverages its
advanced reasoning to refine these drafts, extending LLMs' capabilities to
critical yet specialized tasks. To optimize this collaboration, we introduce a
collaborative feedback to fine-tunes the weak model, which quantifies the
influence of the weak model's contributions in the collaboration procedure and
establishes preference pairs to guide preference tuning of the weak model. We
validate our framework through experiments on three domains. We find that the
collaboration significantly outperforms each model alone by leveraging
complementary strengths. Moreover, aligning the weak model with the
collaborative preference further enhances overall performance.

</details>


### [50] [Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI](https://arxiv.org/abs/2504.15211)
*Yanan Long*

Main category: cs.AI

TL;DR: This paper advocates for using Bayesian statistics to evaluate Generative AI systems, addressing limitations in current methods by incorporating uncertainty and societal impacts.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods rely on benchmark-driven comparisons that fail to capture uncertainty and broader societal effects, highlighting the need for a more robust approach.

Method: Proposes Bayesian statistics framework, including prior elicitation for domain expertise, continuous learning, uncertainty quantification through posterior inference, and iterative Bayesian workflows for model validation.

Result: Demonstrates the application of Bayesian inference in GenAI evaluation, particularly in integrating stakeholder perspectives to enhance fairness, transparency, and reliability.

Conclusion: Bayesian methods offer a principled and iterative approach for robust assessment of GenAI systems in dynamic real-world contexts.

Abstract: The evaluation of Generative AI (GenAI) systems plays a critical role in
public policy and decision-making, yet existing methods are often limited by
reliance on benchmark-driven, point-estimate comparisons that fail to capture
uncertainty and broader societal impacts. This paper argues for the use of
Bayesian statistics as a principled framework to address these challenges.
Bayesian methods enable the integration of domain expertise through prior
elicitation, allow for continuous learning from new data, and provide robust
uncertainty quantification via posterior inference. We demonstrate how Bayesian
inference can be applied to GenAI evaluation, particularly in incorporating
stakeholder perspectives to enhance fairness, transparency, and reliability.
Furthermore, we discuss Bayesian workflows as an iterative process for model
validation and refinement, ensuring robust assessments of GenAI systems in
dynamic, real-world contexts.

</details>


### [51] [A Self-Improving Coding Agent](https://arxiv.org/abs/2504.15228)
*Maxime Robeyns,Martin Szummer,Laurence Aitchison*

Main category: cs.AI

TL;DR: 本研究展示了配备基本编码工具的大型语言模型（LLM）编码代理能够自主编辑自身，从而在基准任务上提升性能。


<details>
  <summary>Details</summary>
Motivation: 动机是探索LLM代理通过自我编辑提高性能的可能性，以推进代理系统的自动化设计。

Method: 方法涉及使用LLM编码代理，该代理配备基本工具，能够自主编辑自身代码。

Result: 结果显示，在SWE Bench Verified上性能提升17%至53%，并在LiveCodeBench和其他合成基准上也有额外提升。

Conclusion: 结论是，这项工作推动了代理系统的自动化和开放式设计，并提供了一个用于在工具使用和其他代理任务上后训练LLM的参考框架。

Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools,
can autonomously edit itself, and thereby improve its performance on benchmark
tasks. We find performance gains from 17% to 53% on a random subset of SWE
Bench Verified, with additional performance gains on LiveCodeBench, as well as
synthetically generated agent benchmarks. Our work represents an advancement in
the automated and open-ended design of agentic systems, and provides a
reference agent framework for those seeking to post-train LLMs on tool use and
other agentic tasks.

</details>


### [52] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TL;DR: 本文提出SuoiAI，一个用于越南水生无脊椎动物数据集构建和物种分类的端到端管道，使用机器学习技术。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保护至关重要，需要解决数据稀缺、细粒度分类和多样环境部署的挑战。

Method: 数据收集、标注和模型训练管道，通过半监督学习减少标注工作，并利用最先进的物体检测和分类模型。

Result: 减少了标注努力，并改进了在多样环境下的物种分类性能。

Conclusion: 该方法克服了关键挑战，提升了水生生物多样性监测。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>


### [53] [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257)
*Hongcheng Gao,Yue Liu,Yufei He,Longxu Dou,Chao Du,Zhijie Deng,Bryan Hooi,Min Lin,Tianyu Pang*

Main category: cs.AI

TL;DR: 这篇论文提出FlowReasoner，一种通过强化学习和反馈自动设计查询级多代理系统的元代理，在基准测试中优于o1-mini 10.52%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了自动化针对每个用户查询的个性化多代理系统设计，通过外部执行反馈激励推理-based元代理。

Method: 先通过蒸馏DeepSeek R1赋予基本推理能力，然后用强化学习和多方面奖励（性能、复杂性、效率）增强FlowReasoner。

Result: 在工程和竞争代码基准测试中，FlowReasoner准确率超过o1-mini 10.52%。

Conclusion: FlowReasoner通过审议性推理生成高效的多代理系统，实验证明其优越性，代码已开源。

Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate
the design of query-level multi-agent systems, i.e., one system per user query.
Our core idea is to incentivize a reasoning-based meta-agent via external
execution feedback. Concretely, by distilling DeepSeek R1, we first endow the
basic reasoning ability regarding the generation of multi-agent systems to
FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with
external execution feedback. A multi-purpose reward is designed to guide the RL
training from aspects of performance, complexity, and efficiency. In this
manner, FlowReasoner is enabled to generate a personalized multi-agent system
for each user query via deliberative reasoning. Experiments on both engineering
and competition code benchmarks demonstrate the superiority of FlowReasoner.
Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.
The code is available at https://github.com/sail-sg/FlowReasoner.

</details>


### [54] [Leveraging Language Models for Automated Patient Record Linkage](https://arxiv.org/abs/2504.15261)
*Mohammad Beheshti,Lovedeep Gondara,Iris Zachary*

Main category: cs.AI

TL;DR: 本文探讨使用语言模型自动链接患者记录，以解决医疗数据碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 医疗数据碎片化导致患者数据链接困难，需要鲁棒的记录链接方法。

Method: 使用密苏里癌症登记处数据，以概率链接为基准；微调RoBERTa模型用于阻塞；对多种语言模型进行微调和零射击设置下的匹配实验。

Result: 微调阻塞模型减少92%的候选对，同时保持近乎完美召回率；匹配任务中，微调Mistral-7B只有6个错误预测，零射击Mistral-Small-24B有55个错误。

Conclusion: 语言模型可自动化记录链接，提高效率并减少手动努力，但混合方法可能更准确高效；它们为数据整合和研究提供可扩展解决方案。

Abstract: Objective: Healthcare data fragmentation presents a major challenge for
linking patient data, necessitating robust record linkage to integrate patient
records from diverse sources. This study investigates the feasibility of
leveraging language models for automated patient record linkage, focusing on
two key tasks: blocking and matching. Materials and Methods: We utilized
real-world healthcare data from the Missouri Cancer Registry and Research
Center, linking patient records from two independent sources using
probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was
fine-tuned for blocking using sentence embeddings. For matching, several
language models were experimented under fine-tuned and zero-shot settings,
assessing their performance against ground truth labels. Results: The
fine-tuned blocking model achieved a 92% reduction in the number of candidate
pairs while maintaining near-perfect recall. In the matching task, fine-tuned
Mistral-7B achieved the best performance with only 6 incorrect predictions.
Among zero-shot models, Mistral-Small-24B performed best, with a total of 55
incorrect predictions. Discussion: Fine-tuned language models achieved strong
performance in patient record blocking and matching with minimal errors.
However, they remain less accurate and efficient than a hybrid rule-based and
probabilistic approach for blocking. Additionally, reasoning models like
DeepSeek-R1 are impractical for large-scale record linkage due to high
computational costs. Conclusion: This study highlights the potential of
language models for automating patient record linkage, offering improved
efficiency by eliminating the manual efforts required to perform patient record
linkage. Overall, language models offer a scalable solution that can enhance
data integration, reduce manual effort, and support disease surveillance and
research.

</details>


### [55] [Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning](https://arxiv.org/abs/2504.15275)
*Jie Cheng,Ruixi Qiao,Lijun Li,Chao Guo,Junle Wang,Gang Xiong,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 本论文提出PURE方法，使用min-form信用分配缓解LLM中PRM的奖励黑客问题，实验显示在少量步骤内达到高性能。


<details>
  <summary>Details</summary>
Motivation: PRM在强化学习中易受奖励黑客影响，特别是在推理任务上，动机是解决这一问题。

Method: 提出PURE：过程监督强化学习，采用min-form信用分配，将价值函数定义为未来奖励的最小值。

Result: 实验在3个模型上显示，min-form方法在30%步数内性能与可验证方法相当；结合10%可验证奖励，Qwen2.5-Math-7B模型在AMC23上达82.5%准确率，5个基准平均53.3%。

Conclusion: min-form信用分配有效缓解奖励黑客，避免训练崩溃，并通过实验验证其优势。

Abstract: Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [A discrete physics-informed training for projection-based reduced order models with neural networks](https://arxiv.org/abs/2504.13875)
*N. Sibuet,S. Ares de Parga,J. R. Bravo,R. Rossi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于物理信息训练的框架，用于改进投影-based Reduced Order Models (ROMs)，通过FEM残差损失桥接传统ROM和PINNs，在非线性问题上提升准确性。


<details>
  <summary>Details</summary>
Motivation: 为了弥合传统ROM与物理信息神经网络(PINNs)之间的差距，并处理非线性问题，提高ROM的鲁棒性和准确性。

Method: 扩展PROM-ANN架构，添加基于FEM的离散残差损失，进行架构修改，并通过非线性超弹性问题进行实证研究。

Result: 改进了PROM-ANN的准确性，远超POD在快衰减奇异值上的表现，适用于非线性问题，并缩小数据重构与ROM准确性差距。

Conclusion: 强调FEM残差在ROM构建中的关键作用，呼吁进一步探索超越PROM-ANN的其他架构。

Abstract: This paper presents a physics-informed training framework for
projection-based Reduced Order Models (ROMs). We extend the PROM-ANN
architecture by complementing snapshot-based training with a FEM-based,
discrete physics-informed residual loss, bridging the gap between traditional
projection-based ROMs and physics-informed neural networks (PINNs). Unlike
conventional PINNs that rely on analytical PDEs, our approach leverages FEM
residuals to guide the learning of the ROM approximation manifold. Key
contributions include: (1) a parameter-agnostic, discrete residual loss
applicable to non-linear problems, (2) an architectural modification to
PROM-ANN improving accuracy for fast-decaying singular values, and (3) an
empirical study on the proposed physics informed training process for ROMs.
  The method is demonstrated on a non-linear hyperelasticity problem,
simulating a rubber cantilever under multi-axial loads. The main accomplishment
in regards to the proposed residual-based loss is its applicability on
non-linear problems by interfacing with FEM software while maintaining
reasonable training times. The modified PROM-ANN outperforms POD by orders of
magnitude in snapshot reconstruction accuracy, while the original formulation
is not able to learn a proper mapping for this use-case. Finally, the
application of physics informed training in ANN-PROM modestly narrows the gap
between data reconstruction and ROM accuracy, however it highlights the
untapped potential of the proposed residual-driven optimization for future ROM
development. This work underscores the critical role of FEM residuals in ROM
construction and calls for further exploration on architectures beyond
PROM-ANN.

</details>


### [57] [Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning](https://arxiv.org/abs/2504.13927)
*F. Herrera,U. A. Rozikov,M. V. Velasco*

Main category: cs.LG

TL;DR: 本文研究了一种包含Ising相互作用的Hamiltonian，在Cayley树上发现了多达三个平移不变Gibbs测度，可应用于机器学习中的推理任务。


<details>
  <summary>Details</summary>
Motivation: 提供一种结构化的方法，用于机器学习中分层数据的推理，应用于去噪、弱监督学习和异常检测，利用Cayley树的易处理性。

Method: 研究Hamiltonian的平移不变Gibbs测度，该Hamiltonian包括Ising相互作用和数据相关项，在Cayley树上，在明确的参数条件下进行。

Result: 证明在某些条件下，可能存在多达三个不同的TIGM，每个代表一个平衡状态。

Conclusion: 这些测度由于Cayley树的结构而在机器学习任务中提供精确推理的优势。

Abstract: In this paper, we investigate a Hamiltonian that incorporates Ising
interactions between hidden $\pm 1$ spins, alongside a data-dependent term that
couples the hidden and observed variables. Specifically, we explore
translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley
trees.
  Under certain explicit conditions on the model's parameters, we demonstrate
that there can be up to three distinct TIGMs. Each of these measures represents
an equilibrium state of the spin system. These measures provide a structured
approach to inference on hierarchical data in machine learning. They have
practical applications in tasks such as denoising, weakly supervised learning,
and anomaly detection. The Cayley tree structure is particularly advantageous
for exact inference due to its tractability.

</details>


### [58] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TL;DR: 本论文提出一种改进ApiQ方法的超低位量化技术，通过显著性感知正则化提升大型语言模型的准确性，而无需完全重训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型体积大、计算需求高，量化方法可压缩模型，但现有方法在准确性和资源消耗间存在权衡，本文旨在超越ApiQ在超低位量化中的性能。

Method: 本文先探索将量化感知训练与ApiQ的部分训练结合，但效果不佳；随后提出一种新方法，使用显著性感知正则化，优先保留关键参数，基于ApiQ实现改进。

Result: 实验在LLaMA系列模型上显示，该方法提升了量化模型的准确性，缩小了与全精度模型的差距，同时保持最小开销。

Conclusion: 该方法将公开以推动超低位量化技术的未来发展。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>


### [59] [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941)
*Syeda Nahida Akter,Shrimai Prabhumoye,Matvei Novikov,Seungju Han,Ying Lin,Evelina Bakhturi,Eric Nyberg,Yejin Choi,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本论文提出NEMOTRON-CROSSTHINK框架，通过强化学习整合多领域数据，提高LLM在多样推理任务中的泛化能力，并展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要应用于数学推理，但泛化到更广泛领域面临数据有限、可验证奖励结构缺失和任务多样性挑战。

Method: NEMOTRON-CROSSTHINK框架通过整合多领域语料、使用结构化模板、过滤可验证答案和优化数据混合策略来改进RL训练。

Result: 在数学和非数学基准上取得改进，如MATH-500准确率提高30.1%、MMLU-PRO提高12.8%，并减少28%令牌使用。

Conclusion: 证明在强化学习中整合多领域多格式数据能使LLM更准确、高效和可泛化。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities,
particularly when enhanced through Reinforcement Learning (RL). While prior
work has successfully applied RL to mathematical reasoning -- where rules and
correctness are well-defined -- generalizing these methods to broader reasoning
domains remains challenging due to limited data, the lack of verifiable reward
structures, and diverse task requirements. In this work, we propose
NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain
corpora, including both synthetic and real-world question-answer pairs, into RL
training to improve generalization across diverse reasoning tasks.
NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from
varied sources spanning STEM, humanities, social sciences, etc.; (2) applying
structured templates (e.g., multiple-choice and open-ended) to control
answer-space complexity; (3) filtering for verifiable answers; and (4)
optimizing data blending strategies that utilizes data from multiple sources
effectively. Our approach enables scalable and verifiable reward modeling
beyond mathematics and demonstrates improved accuracies on both math (MATH-500:
+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,
GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,
NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --
using 28% fewer tokens for correct answers -- highlighting more focused and
effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that
integrating multi-domain, multi-format data in RL leads to more accurate,
efficient, and generalizable LLMs.

</details>


### [60] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Mengli Zhu,Shuang Wu,Shiliang Sun,Hao Yang*

Main category: cs.LG

TL;DR: 本论文提出MOTBench基准，用于评估大型视觉语言模型在菜单OCR和翻译上的能力，强调复杂布局的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM评估主要关注短文本和简单布局，忽略了复杂布局的长文本理解；针对菜单翻译在跨文化交流中的重要性，提出新基准。

Method: 构建MOTBench数据集，包括中英菜单的复杂布局、多样字体和人工标注；通过自动和人工评估LVLM的识别与翻译性能。

Result: 实验显示自动评估结果与人工评估高度一致；分析了现有LVLM的优缺点，提供未来改进的洞见。

Conclusion: MOTBench开源可用，可指导LVLM的进一步发展。

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>


### [61] [On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence](https://arxiv.org/abs/2504.13949)
*M. W. Przewozniczek,F. Chicano,R. Tinós,J. Nalepa,B. Ruszczak,A. M. Wijata*

Main category: cs.LG

TL;DR: 这篇论文通过加权动态变量交互图 (wdVIG) 过滤噪声依赖，提高灰盒优化的变异算子有效性。


<details>
  <summary>Details</summary>
Motivation: 在某些优化问题中，所有变量非线性依赖，导致现有掩码无效，且许多依赖可能是噪声引起的。

Method: 扩展Walsh分解测量依赖强度，构建wdVIG调整混合个体依赖，过滤无关依赖，并与变异算子结合。

Result: 在基准测试中，wdVIG在有噪声问题中提升优化有效性，在无噪声问题中与最先进方法类似。

Conclusion: wdVIG可忽略噪声依赖，提高优化性能，尤其适用于噪声存在的情况。

Abstract: Gray-box optimization employs Walsh decomposition to obtain non-linear
variable dependencies and utilize them to propose masks of variables that have
a joint non-linear influence on fitness value. These masks significantly
improve the effectiveness of variation operators. In some problems, all
variables are non-linearly dependent, making the aforementioned masks useless.
We analyze the features of the real-world instances of such problems and show
that many of their dependencies may have noise-like origins. Such noise-caused
dependencies are irrelevant to the optimization process and can be ignored. To
identify them, we propose extending the use of Walsh decomposition by measuring
variable dependency strength that allows the construction of the weighted
dynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency
strength to mixed individuals. They allow the filtering of irrelevant
dependencies and re-enable using dependency-based masks by variation operators.
We verify the wdVIG potential on a large benchmark suite. For problems with
noise, the wdVIG masks can improve the optimizer's effectiveness. If all
dependencies are relevant for the optimization, i.e., the problem is not
noised, the influence of wdVIG masks is similar to that of state-of-the-art
structures of this kind.

</details>


### [62] [Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain](https://arxiv.org/abs/2504.13950)
*Zhongxi Qiu,Zhang Zhang,Yan Hu,Heng Li,Jiang Liu*

Main category: cs.LG

TL;DR: 本文探讨医学领域RLVR训练的最佳数据选择策略，测试多种采样方法，发现过滤数据优于随机采样，并提供见解。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR在提升大语言模型推理能力方面潜力巨大，但之前主要应用于数学和逻辑谜题，医学领域的应用较少。

Method: 使用MedQA-USMLE数据集，测试随机采样和基于Phi-4、Gemma-3-27b-it、Gemma-3-12b-it的过滤策略，以Gemma-3-12b-it为基础模型，采用GRPO优化，在MMLU、GSM8K、MMLU-Pro和CMMLU基准上评估。

Result: 过滤数据训练的模型性能优于随机采样；自过滤样本在医学领域表现superior但整体鲁棒性较差；使用更大模型过滤获得更好鲁棒性。

Conclusion: 为RLVR在专业领域的数据组织策略提供宝贵洞见，强调数据选择的importance以实现最佳性能。

Abstract: This paper explores optimal data selection strategies for Reinforcement
Learning with Verified Rewards (RLVR) training in the medical domain. While
RLVR has shown exceptional potential for enhancing reasoning capabilities in
large language models, most prior implementations have focused on mathematics
and logical puzzles, with limited exploration of domain-specific applications
like medicine. We investigate four distinct data sampling strategies from
MedQA-USMLE: random sampling (baseline), and filtering using Phi-4,
Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base
model and implementing Group Relative Policy Optimization (GRPO), we evaluate
performance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and
CMMLU. Our findings demonstrate that models trained on filtered data generally
outperform those trained on randomly selected samples. Notably, training on
self-filtered samples (using Gemma-3-12b-it for filtering) achieved superior
performance in medical domains but showed reduced robustness across different
benchmarks, while filtering with larger models from the same series yielded
better overall robustness. These results provide valuable insights into
effective data organization strategies for RLVR in specialized domains and
highlight the importance of thoughtful data selection in achieving optimal
performance. You can access our repository
(https://github.com/Qsingle/open-medical-r1) to get the codes.

</details>


### [63] [Generative System Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/2504.13951)
*Michele Casoni,Tommaso Guidi,Alessandro Betti,Stefano Melacci,Marco Gori*

Main category: cs.LG

TL;DR: 本研究探讨RNN在非线性激活函数下的连续时间动态，焦点是永续振荡行为，发现skew-symmetric权重矩阵和双曲正切激活函数有助于稳定极限环。


<details>
  <summary>Details</summary>
Motivation: 识别RNN不收敛到静态固定点而展示振荡行为的条件，以设计能捕获复杂时间依赖性的神经架构。

Method: 通过建立skew-symmetric权重矩阵条件，并使用数值模拟验证非线性激活函数对极限环和数值稳定的影响。

Result: skew-symmetric矩阵启用稳定极限环，非线性激活函数保持振荡动态，提升数值稳定性，减轻前向Euler方法的不稳定性。

Conclusion: 这些发现为设计增强RNN记忆能力和处理时间依赖性的模型提供实用策略。

Abstract: In this study, we investigate the continuous time dynamics of Recurrent
Neural Networks (RNNs), focusing on systems with nonlinear activation
functions. The objective of this work is to identify conditions under which
RNNs exhibit perpetual oscillatory behavior, without converging to static fixed
points. We establish that skew-symmetric weight matrices are fundamental to
enable stable limit cycles in both linear and nonlinear configurations. We
further demonstrate that hyperbolic tangent-like activation functions (odd,
bounded, and continuous) preserve these oscillatory dynamics by ensuring motion
invariants in state space. Numerical simulations showcase how nonlinear
activation functions not only maintain limit cycles, but also enhance the
numerical stability of the system integration process, mitigating those
instabilities that are commonly associated with the forward Euler method. The
experimental results of this analysis highlight practical considerations for
designing neural architectures capable of capturing complex temporal
dependencies, i.e., strategies for enhancing memorization skills in recurrent
models.

</details>


### [64] [Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity](https://arxiv.org/abs/2504.13956)
*Md Azizul Hoque,Babul Salam,Mohd Khair Hassan,Abdulkabir Aliyu,Abedalmuhdi Almomany,Muhammed Sutcu*

Main category: cs.LG

TL;DR: 本论文开发了一种电池退化模型，使用差分容量分析和机器学习评估锂离子电池在不同加载条件下的健康状况。


<details>
  <summary>Details</summary>
Motivation: 电池退化是电动汽车和能源存储系统的重大挑战，现有的SOC估计方法无法准确解释内部退化机制。

Method: 使用LiNiCoAlO2和LiFePO4电池，采用不同充放电率（DCR和DDR），结合差分容量分析、EKF、CNN、LSTM和峰值识别技术进行建模。

Result: 模型误差（MSE和RMSE）小于0.001%，LiFePO4电池更稳健，电池在正常加载下退化缓慢，在快速加载下退化迅速。

Conclusion: LiFePO4电池在不同条件下表现更可靠，提出的模型有效评估了电池健康。

Abstract: Battery degradation is a major challenge in electric vehicles (EV) and energy
storage systems (ESS). However, most degradation investigations focus mainly on
estimating the state of charge (SOC), which fails to accurately interpret the
cells' internal degradation mechanisms. Differential capacity analysis (DCA)
focuses on the rate of change of cell voltage about the change in cell
capacity, under various charge/discharge rates. This paper developed a battery
cell degradation testing model that used two types of lithium-ions (Li-ion)
battery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and
lithium iron phosphate (LiFePO4), to evaluate internal degradation during
loading conditions. The proposed battery degradation model contains distinct
charge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates
(DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and
performance of battery cells during slow, moderate, and fast loading
conditions. Besides, this research proposed a model that incorporates the
Extended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long
Short-Term Memory (LSTM) networks to validate experimental data. The proposed
model yields excellent modelling results based on mean squared error (MSE), and
root mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR.
The peak identification technique (PIM) has been utilized to investigate
battery health based on the number of peaks, peak position, peak height, peak
area, and peak width. At last, the PIM method has discovered that the cell aged
gradually under normal loading rates but deteriorated rapidly under fast
loading conditions. Overall, LiFePO4 batteries perform more robustly and
consistently than (LiNiCoAlO2) cells under varying loading conditions.

</details>


### [65] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TL;DR: 本研究通过改进强化学习的奖励设计，提升了大语言模型在工具使用方面的能力和泛化性能，比基础模型提高17%，比监督微调模型提高15%。


<details>
  <summary>Details</summary>
Motivation: 监督微调在复杂工具使用场景中泛化能力不足，强化学习虽有潜力，但奖励设计面临工具多样性和反馈粒度挑战。

Method: 系统探索奖励策略类型、规模、粒度和动态，提出针对工具使用的奖励设计，并使用Group Relative Policy Optimization (GRPO)训练LLM。

Result: 实证评估显示，方法在多个基准上实现稳健、可扩展训练，比基线模型改善17%，比监督微调模型改善15%。

Conclusion: 奖励设计的精心设计对提升LLM工具使用和泛化性能至关重要，并开源代码以促进未来研究。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>


### [66] [CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee](https://arxiv.org/abs/2504.13961)
*Chao Yang,Xiannan Huang,Shuhan Qiu,Yan Cheng*

Main category: cs.LG

TL;DR: 本论文提出CONTINA方法，用于交通需求预测的自适应置信区间，能根据错误动态调整，提供更短的有效区间。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖严格假设，无法适应变化的交通环境，导致置信区间无效，需要一种适应性方法。

Method: 提出CONTINA，通过收集部署错误动态调整区间宽度。

Result: 理论证明覆盖率收敛到目标水平，实验在四个真实数据集上显示更短的有效置信区间。

Conclusion: 有助于交通管理制定更合理操作计划，代码已开源。

Abstract: Accurate short-term traffic demand prediction is critical for the operation
of traffic systems. Besides point estimation, the confidence interval of the
prediction is also of great importance. Many models for traffic operations,
such as shared bike rebalancing and taxi dispatching, take into account the
uncertainty of future demand and require confidence intervals as the input.
However, existing methods for confidence interval modeling rely on strict
assumptions, such as unchanging traffic patterns and correct model
specifications, to guarantee enough coverage. Therefore, the confidence
intervals provided could be invalid, especially in a changing traffic
environment. To fill this gap, we propose an efficient method, CONTINA
(Conformal Traffic Intervals with Adaptation) to provide interval predictions
that can adapt to external changes. By collecting the errors of interval during
deployment, the method can adjust the interval in the next step by widening it
if the errors are too large or shortening it otherwise. Furthermore, we
theoretically prove that the coverage of the confidence intervals provided by
our method converges to the target coverage level. Experiments across four
real-world datasets and prediction models demonstrate that the proposed method
can provide valid confidence intervals with shorter lengths. Our method can
help traffic management personnel develop a more reasonable and robust
operation plan in practice. And we release the code, model and dataset in
\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.

</details>


### [67] [Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings](https://arxiv.org/abs/2504.13966)
*Carolin Heinzler*

Main category: cs.LG

TL;DR: 本论文基于现有工作修正错误，将顺序学习框架扩展到不可知设置，分析干净标签对抗样本下的分歧基于学习器。


<details>
  <summary>Details</summary>
Motivation: 动机是处理数据流中未知数量的干净标签对抗样本，提供随机性保证，并允许学习器在不确定时弃权以减少错误。

Method: 方法包括修正Goel等人的论证，适应不可知设置，并理论分析分歧基于的学习器针对阈值函数。

Result: 结果是首次在不可知背景下对干净标签对抗样本下的学习器进行了理论分析，扩展了现有方法。

Conclusion: 结论是这种方法在不可知设置下有效，能够处理噪声和对抗样本，提供更 robust 的学习保证。

Abstract: We investigate the challenge of establishing stochastic-like guarantees when
sequentially learning from a stream of i.i.d. data that includes an unknown
quantity of clean-label adversarial samples. We permit the learner to abstain
from making predictions when uncertain. The regret of the learner is measured
in terms of misclassification and abstention error, where we allow the learner
to abstain for free on adversarial injected samples. This approach is based on
the work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore
the methods they present and manage to correct inaccuracies in their
argumentation.
  However, this approach is limited to the realizable setting, where labels are
assigned according to some function $f^*$ from the hypothesis space
$\mathcal{F}$. Based on similar arguments, we explore methods to make
adaptations for the agnostic setting where labels are random. Introducing the
notion of a clean-label adversary in the agnostic context, we are the first to
give a theoretical analysis of a disagreement-based learner for thresholds,
subject to a clean-label adversary with noise.

</details>


### [68] [Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach](https://arxiv.org/abs/2504.13974)
*Yao Zhiwan,Reza Zarrab,Jean Dubois*

Main category: cs.LG

TL;DR: 本文提出了一种加权投票集成机器学习模型，用于更有效地预测脑卒中，准确率达94.91%，以解决传统诊断方法的成本高和耗时问题。


<details>
  <summary>Details</summary>
Motivation: 传统脑卒中诊断方法如CT扫描和MRI成本高且耗时，因此需要更高效的替代方案。

Method: 提出了一种加权投票集成(WVE)模型，结合随机森林、深度学习和基于直方图的梯度提升分类器。

Result: 在私有数据集上实现了94.91%的准确率。

Conclusion: 这使早期的风险评估和预防成为可能；未来研究可探索优化技术以进一步提高准确率。

Abstract: A brain stroke occurs when blood flow to a part of the brain is disrupted,
leading to cell death. Traditional stroke diagnosis methods, such as CT scans
and MRIs, are costly and time-consuming. This study proposes a weighted voting
ensemble (WVE) machine learning model that combines predictions from
classifiers like random forest, Deep Learning, and histogram-based gradient
boosting to predict strokes more effectively. The model achieved 94.91%
accuracy on a private dataset, enabling early risk assessment and prevention.
Future research could explore optimization techniques to further enhance
accuracy.

</details>


### [69] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TL;DR: This paper introduces Multiscale Tensor Summation (MTS) Factorization as a new neural layer that improves efficiency over MLPs and CNNs, and MTSNet outperforms transformers in computer vision tasks.


<details>
  <summary>Details</summary>
Motivation: To address the high dimensionality limitations of MLPs and the restricted receptive fields of CNNs, proposing a more efficient neural operator for better weight sharing and optimization.

Method: Introduces MTS Factorization using tensor summation at multiple scales with Tucker-decomposition-like mode products, and integrates it with a multi-head gate (MHG) in MTSNet as a backbone layer.

Result: MTS reduces parameters and enhances performance compared to MLPs and CNNs in tasks like classification, compression, and signal restoration; MTSNet shows better complexity-performance tradeoff than transformers in vision applications.

Conclusion: MTS and MTSNet offer a novel, efficient alternative to traditional neural layers, demonstrating superior advantages in parameter efficiency and task performance.

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>


### [70] [CacheFormer: High Attention-Based Segment Caching](https://arxiv.org/abs/2504.13981)
*Sushant Singh,Ausif Mahmood*

Main category: cs.LG

TL;DR: 这篇论文提出了一种受计算机缓存启发的方法，提高transformer模型处理长上下文的效率，并降低了困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有模型如Linformer、Longformer等未能完全解决长上下文处理问题，论文借鉴计算机缓存和虚拟内存原理来优化。

Method: 将上下文分为小段，当高注意力发生时检索附近段落；整合短滑动窗口注意力、长压缩段注意力、动态top k未压缩段检索及段重叠机制。

Result: 新架构比现有最先进模型平均降低了8.5%的困惑度。

Conclusion: 该方法有效提升了长上下文处理的性能，证明了缓存机制在语言模型中的潜力。

Abstract: Efficiently handling long contexts in transformer-based language models with
low perplexity is an active area of research. Numerous recent approaches like
Linformer, Longformer, Performer, and Structured state space models (SSMs).,
have not fully resolved this problem. All these models strive to reduce the
quadratic time complexity of the attention mechanism while minimizing the loss
in quality due to the effective compression of the long context. Inspired by
the cache and virtual memory principle in computers, where in case of a cache
miss, not only the needed data is retrieved from the memory, but the adjacent
data is also obtained, we apply this concept to handling long contexts by
dividing it into small segments. In our design, we retrieve the nearby segments
in an uncompressed form when high segment-level attention occurs at the
compressed level. Our en-hancements for handling long context include
aggregating four attention mechanisms consisting of short sliding window
attention, long compressed segmented attention, dynamically retrieving top k
high attention uncompressed segments, and overlapping segments in long segment
attention to avoid segment fragmentation. These enhancements result in an
architecture that outperforms ex-isting SOTA architectures with an average
perplexity improvement of 8.5% over similar model sizes.

</details>


### [71] [When Machine Learning Meets Importance Sampling: A More Efficient Rare Event Estimation Approach](https://arxiv.org/abs/2504.13982)
*Ruoning Zhao,Xinyun Chen*

Main category: cs.LG

TL;DR: 论文提出新重要性采样方法，使用稳态分布边际似然比和机器学习算法，改善电信网络串联队列稀有事件概率估计。


<details>
  <summary>Details</summary>
Motivation: 受电信网络应用驱动，需要估计稳态下串联队列稀有事件概率，但现有方法因方差过大而低效。

Method: 引入基于稳态分布边际似然比的重要性采样方法，并设计机器学习算法估计该比值。

Result: 数值实验显示，该算法优于经典重要性采样方法。

Conclusion: 新方法有效降低方差，提高了估计效率。

Abstract: Driven by applications in telecommunication networks, we explore the
simulation task of estimating rare event probabilities for tandem queues in
their steady state. Existing literature has recognized that importance sampling
methods can be inefficient, due to the exploding variance of the path-dependent
likelihood functions. To mitigate this, we introduce a new importance sampling
approach that utilizes a marginal likelihood ratio on the stationary
distribution, effectively avoiding the issue of excessive variance. In
addition, we design a machine learning algorithm to estimate this marginal
likelihood ratio using importance sampling data. Numerical experiments indicate
that our algorithm outperforms the classic importance sampling methods.

</details>


### [72] [QuatE-D: A Distance-Based Quaternion Model for Knowledge Graph Embedding](https://arxiv.org/abs/2504.13983)
*Hamideh-Sadat Fazael-Ardakani,Hamid Soltanian-Zadeh*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于四元数的知识图嵌入模型QuatE-D，使用距离-based评分函数，提高可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升基于四元数的KGE模型的可解释性和灵活性，改用距离-based评分函数代替传统内积方法。

Method: 提出QuatE-D模型，使用欧几里得距离作为评分函数。

Result: 实验结果显示，QuatE-D在性能上具有竞争力，参数化高效，尤其在Mean Rank降低方面表现出色。

Conclusion: 这些发现突显了在四元数嵌入中使用距离-based评分的有效性，为知识图完成提供了有前景的方向。

Abstract: Knowledge graph embedding (KGE) methods aim to represent entities and
relations in a continuous space while preserving their structural and semantic
properties. Quaternion-based KGEs have demonstrated strong potential in
capturing complex relational patterns. In this work, we propose QuatE-D, a
novel quaternion-based model that employs a distance-based scoring function
instead of traditional inner-product approaches. By leveraging Euclidean
distance, QuatE-D enhances interpretability and provides a more flexible
representation of relational structures. Experimental results demonstrate that
QuatE-D achieves competitive performance while maintaining an efficient
parameterization, particularly excelling in Mean Rank reduction. These findings
highlight the effectiveness of distance-based scoring in quaternion embeddings,
offering a promising direction for knowledge graph completion.

</details>


### [73] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TL;DR: 提出OJFA方法，减少30倍参数成本，同时保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 为了减少大语言模型推理的时间和计算成本，关注参数高效的低秩早退机制。

Method: 提出单一的One-Jump-Fits-All (OJFA)低秩捷径，替代每个transformer块的独立捷径。

Result: 提供超过30倍的捷径参数减少，性能与多捷径方法相当，并在GPT2-XL、Phi3-Mini和Llama2-7B模型中显示稳定精度。

Conclusion: OJFA方法在减少参数同时维持性能，证明了其有效性。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>


### [74] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TL;DR: 本文使用Hadamard矩阵改进LLM量化，实现3位量化并显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: LLM模型庞大，难以部署在边缘设备；量化可减少内存，但激活中的异常值是主要挑战。

Method: 利用Hadamard矩阵减少异常值，通过渐进二分搜索实现权重、激活和KV缓存的3位量化，并使用Paley算法支持非2的幂嵌入维度。

Result: 在基准测试中准确率提高40%，实现了3位量化，并在Mistral、LLaMA和Qwen等模型上优于现有方法。

Conclusion: Hadamard矩阵在减少异常值方面更优越，实现实用3位量化并提升模型性能。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>


### [75] [PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network](https://arxiv.org/abs/2504.13990)
*M. Humayun Kabir,Md. Ali Hasan,Md. Shafiqul Islam,Kyeongjun Ko,Wonjae Shin*

Main category: cs.LG

TL;DR: 本篇论文提出了一种新型深度学习框架PC-DeepNet，用于改善城市地区GNSS定位的准确性，通过处理NLOS和多径问题，并显示出比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: GNSS系统在城市和郊区面临NLOS传播、多径效应和低接收功率等挑战，导致测量误差分布高度非线性、非高斯，传统基于高斯误差的定位方法难以实现精确定位。

Method: 提出了一种基于置换不变（PI）深度神经网络（DNN）的框架PC-DeepNet，用于估计位置校正，利用NLOS和多径指示作为特征，以增强在城市和郊区环境的定位准确性。

Result: 使用两个公开可用数据集，与最先进基于模型和基于学习的方法比较，结果显示PC-DeepNet在定位精度上优于现有方法，同时计算复杂度低于之前的学习方法。

Conclusion: 结果证实，提出的PC-DeepNet在挑战性环境中实现了更高的准确性和鲁棒性。

Abstract: Global navigation satellite systems (GNSS) face significant challenges in
urban and sub-urban areas due to non-line-of-sight (NLOS) propagation,
multipath effects, and low received power levels, resulting in highly
non-linear and non-Gaussian measurement error distributions. In light of this,
conventional model-based positioning approaches, which rely on Gaussian error
approximations, struggle to achieve precise localization under these
conditions. To overcome these challenges, we put forth a novel learning-based
framework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural
network (DNN) to estimate position corrections (PC). This approach is designed
to ensure robustness against changes in the number and/or order of visible
satellite measurements, a common issue in GNSS systems, while leveraging NLOS
and multipath indicators as features to enhance positioning accuracy in
challenging urban and sub-urban environments. To validate the performance of
the proposed framework, we compare the positioning error with state-of-the-art
model-based and learning-based positioning methods using two publicly available
datasets. The results confirm that proposed PC-DeepNet achieves superior
accuracy than existing model-based and learning-based methods while exhibiting
lower computational complexity compared to previous learning-based approaches.

</details>


### [76] [Deep Learning on Graphs for Mobile Network Topology Generation](https://arxiv.org/abs/2504.13991)
*Felix Nannesson Meli,Johan Tell,Shirwan Piroti,Tahar Zanouda,Elias Jarlebring*

Main category: cs.LG

TL;DR: 这篇论文使用图-based深度学习方法确定移动网络中的移动关系，通过GNN和MLP模型及启发式优化，提高了精度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法仅能在硬件安装前建立连接，存在局限性，因此需要深度学习来动态优化网络拓扑。

Method: 采用图神经网络(GNN)和多层感知器(MLP)，基于无线节点配置数据和ANR数据训练，并使用距离-based启发式方法减少训练时间。

Result: 实验在电信数据集上显示，GNN模型有效，考虑图结构改善结果，使用启发式方法显著提升精度和准确性。

Conclusion: 结论是，图-based深度学习结合启发式方法可有效改善移动网络移动关系的确定。

Abstract: Mobile networks consist of interconnected radio nodes strategically
positioned across various geographical regions to provide connectivity
services. The set of relations between these radio nodes, referred to as the
\emph{mobile network topology}, is vital in the construction of the networking
infrastructure. Typically, the connections between radio nodes and their
associated cells are defined by software features that establish mobility
relations (referred to as \emph{edges} in this paper) within the mobile network
graph through heuristic methods. Although these approaches are efficient, they
encounter significant limitations, particularly since edges can only be
established prior to the installation of physical hardware.
  In this work, we use graph-based deep learning methods to determine mobility
relations (edges), trained on radio node configuration data and reliable
mobility relations set by Automatic Neighbor Relations (ANR) in stable
networks. This paper focuses on measuring the accuracy and precision of
different graph-based deep learning approaches applied to real-world mobile
networks. We evaluated two deep learning models. Our comprehensive experiments
on Telecom datasets obtained from operational Telecom Networks demonstrate the
effectiveness of the graph neural network (GNN) model and multilayer
perceptron. Our evaluation showed that considering graph structure improves
results, which motivates the use of GNNs. Additionally, we investigated the use
of heuristics to reduce the training time based on the distance between radio
nodes to eliminate irrelevant cases. Our investigation showed that the use of
these heuristics improved precision and accuracy considerably.

</details>


### [77] [First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms](https://arxiv.org/abs/2504.13992)
*Eric Lu*

Main category: cs.LG

TL;DR: 本论文在弱假设下，为具有时间变化的学习率和动量参数的SGD方法提供了逼近结果。


<details>
  <summary>Details</summary>
Motivation: 现有的工作仅覆盖常量学习率或无动量项场景，而动量SGD改进多基于经验而非严格证明，因此本研究旨在提供更一般的逼近结果。

Method: 使用连续逼近方法，在弱假设下分析学习率和动量参数随时间变化的SGD。

Result: 获得了允许学习率和动量参数随时间变化的SGD逼近结果。

Conclusion: 这为SGD方法的理论分析提供了新的洞见，特别是对于参数变化的情况。

Abstract: Stochastic Gradient Descent (SGD) methods see many uses in optimization
problems. Modifications to the algorithm, such as momentum-based SGD methods
have been known to produce better results in certain cases. Much of this,
however, is due to empirical information rather than rigorous proof. While the
dynamics of gradient descent methods can be studied through continuous
approximations, existing works only cover scenarios with constant learning
rates or SGD without momentum terms. We present approximation results under
weak assumptions for SGD that allow learning rates and momentum parameters to
vary with respect to time.

</details>


### [78] [Large Language Bayes](https://arxiv.org/abs/2504.14025)
*Justin Domke*

Main category: cs.LG

TL;DR: 这篇论文提出了一种方法，使用大型语言模型和概率编程，从非正式问题描述中生成贝叶斯模型，并通过加权平均推理实现预测，而无需指定正式模型。


<details>
  <summary>Details</summary>
Motivation: 许多领域专家缺乏时间或训练来编写正式的贝叶斯模型，因此需要一种自动化的方法。

Method: 方法包括输入非正式描述，使用语言模型生成多个正式模型，在每个模型上进行近似推理，然后通过加权平均结合结果。这种方法基于自标准化重要性采样、MCMC 和变分推理。

Result: 该方法能够在不指定正式模型的情况下产生合理的预测。

Conclusion: 这使得贝叶斯建模对非专家更易访问，并证明了该推理方法的有效性。

Abstract: Many domain experts do not have the time or training to write formal Bayesian
models. This paper takes an informal problem description as input, and combines
a large language model and a probabilistic programming language to create a
joint distribution over formal models, latent variables, and data. A posterior
over latent variables follows by conditioning on observed data and integrating
over formal models. This presents a challenging inference problem. We suggest
an inference recipe that amounts to generating many formal models from the
large language model, performing approximate inference on each, and then doing
a weighted average. This is justified an analyzed as a combination of
self-normalized importance sampling, MCMC, and variational inference. We show
that this produces sensible predictions without the need to specify a formal
model.

</details>


### [79] [A synthetic dataset of French electric load curves with temperature conditioning](https://arxiv.org/abs/2504.14046)
*Tahar Nabil,Ghislain Agoua,Pierre Cauchois,Anne De Moliner,Benoît Grossin*

Main category: cs.LG

TL;DR: 本论文引入了一种通过条件潜在扩散生成的新型合成负载曲线数据集，并评估其保真度、效用和隐私性，以支持能源建模应用。


<details>
  <summary>Details</summary>
Motivation: 能源转型导致用电行为变化，如本地发电自用或需求控制，需要智能电表数据来理解这些变化，但受欧盟GDPR隐私保护限制，因此需要创建真实且隐私保护的合成数据集。

Method: 使用条件潜在扩散模型生成合成负载曲线数据集，并提供合同功率、用时计划和本地温度作为生成条件。

Result: 数据集的保真度、效用和隐私性经过彻底评估，显示出良好的质量。

Conclusion: 该数据集对能源建模应用具有重要意义，可用于相关研究。

Abstract: The undergoing energy transition is causing behavioral changes in electricity
use, e.g. with self-consumption of local generation, or flexibility services
for demand control. To better understand these changes and the challenges they
induce, accessing individual smart meter data is crucial. Yet this is personal
data under the European GDPR. A widespread use of such data requires thus to
create synthetic realistic and privacy-preserving samples. This paper
introduces a new synthetic load curve dataset generated by conditional latent
diffusion. We also provide the contracted power, time-of-use plan and local
temperature used for generation. Fidelity, utility and privacy of the dataset
are thoroughly evaluated, demonstrating its good quality and thereby supporting
its interest for energy modeling applications.

</details>


### [80] [On Dimension-Free Transformer: An Application of STP to AI](https://arxiv.org/abs/2504.14514)
*Daizhan Cheng*

Main category: cs.LG

TL;DR: 这篇论文使用半张量积和投影方法，提出了一种无维变换器框架，提高了信号处理的效率。


<details>
  <summary>Details</summary>
Motivation: 动机是改进变换器，使其能够处理任意维度输入输出，并提升信号处理效率。

Method: 方法包括描述变换器的矩阵表达式，基于半张量积重新考虑超向量，使用投影构造线性变换，并提出基于投影的超向量变换（PBTH）构建无维变换器（DFT）。

Result: 结果获得了变换的属性、计算公式，并验证了DFT框架的有效性。

Conclusion: 结论是DFT通过平衡所有条目信息，更有效地处理信号。

Abstract: The matrix expressions for every parts of a transformer are firstly
described. Based on semi-tensor product (STP) of matrices the hypervectors are
reconsidered and the linear transformation over hypervectors is constructed by
using projection. Its properties and calculating formulas are obtained. Using
projection-based transformation of hypervector (PBTH), the framework of
dimension-free transformer (DFT) is proposed by verifying each linear
transformation in a transformer and replacing it by a proper PBTH, which allows
the inputs and outputs being of arbitrary dimensions. Using balanced
information about all entries, DFT must be more efficient in dealing with
signals.

</details>


### [81] [CAOTE: KV Caching through Attention Output Error based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: 这篇论文提出CAOTE方法，通过优化令牌驱逐来减少大型语言模型的内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 动机是解决长上下文支持带来的内存计算瓶颈，以及注意力分数作为重要性指标的局限性。

Method: 方法是基于令牌对注意力输出的贡献，整合注意力分数和值向量信息，提出CAOTE作为驱逐标准。

Result: 结果显示，CAOTE与现有方法结合时，总是提高了下游任务的准确性。

Conclusion: 结论强调利用值向量信息在令牌驱逐中的重要性，这是首个整合此信息的方法。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value vector information on
top of attention-based eviction scores. Additionally, CAOTE can act as a
meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [82] [Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement](https://arxiv.org/abs/2504.14068)
*K M Sajjadul Islam,Ravi Teja Karri,Srujan Vegesna,Jiawei Wu,Praveen Madiraju*

Main category: cs.LG

TL;DR: 本研究使用无监督方法分析医疗反馈，提出kBERT模型，表现最佳。


<details>
  <summary>Details</summary>
Motivation: 理解患者反馈改善医疗服务重要，但未标记短文本分析挑战大，无监督方法更适合。

Method: 使用关键词过滤、LDA、GSDMM、BERTopic和kBERT（BERT嵌入结合k-means聚类），通过Cv和IRBOavg评估。

Result: kBERT达到最高相干性（Cv=0.53）和主题分离度（IRBOavg=1.00），优于其他模型。

Conclusion: 强调嵌入技术在主题识别中的重要性，并需开发医疗分析的上下文感知模型。

Abstract: Understanding patient feedback is crucial for improving healthcare services,
yet analyzing unlabeled short-text feedback presents significant challenges due
to limited data and domain-specific nuances. Traditional supervised learning
approaches require extensive labeled datasets, making unsupervised methods more
viable for uncovering meaningful insights from patient feedback. This study
explores unsupervised methods to extract meaningful topics from 439 survey
responses collected from a healthcare system in Wisconsin, USA. A keyword-based
filtering approach was applied to isolate complaint-related feedback using a
domain-specific lexicon. To delve deeper and analyze dominant topics in
feedback, we explored traditional topic modeling methods, including Latent
Dirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture
(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering
approach. To improve coherence and interpretability where data are scarce and
consist of short-texts, we propose kBERT, an integration of BERT embeddings
with k-means clustering. Model performance was assessed using coherence scores
(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap
(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest
coherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),
outperforming all other models in short-text healthcare feedback analysis. Our
findings emphasize the importance of embedding-based techniques for topic
identification and highlight the need for context-aware models in healthcare
analytics.

</details>


### [83] [Leakage and Interpretability in Concept-Based Models](https://arxiv.org/abs/2504.14094)
*Enrico Parisini,Tapabrata Chakraborti,Chris Harbron,Ben D. MacArthur,Christopher R. S. Banerji*

Main category: cs.LG

TL;DR: 这篇论文引入信息论框架来衡量和减少概念瓶颈模型中的信息泄漏，提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决概念瓶颈模型信息泄漏问题，以提升高风险场景下的可解释性。

Method: 引入概念-任务泄漏（CTL）和概念间泄漏（ICL）分数，评估其预测能力，识别泄漏原因，并提出减少泄漏的设计指南。

Result: CTL和ICL分数在稳健性和可靠性上优于现有方法；概念嵌入模型无论超参数如何，都存在显著泄漏。

Conclusion: 提供实用指南，帮助设计减少泄漏并确保可解释性的概念-based模型。

Abstract: Concept Bottleneck Models aim to improve interpretability by predicting
high-level intermediate concepts, representing a promising approach for
deployment in high-risk scenarios. However, they are known to suffer from
information leakage, whereby models exploit unintended information encoded
within the learned concepts. We introduce an information-theoretic framework to
rigorously characterise and quantify leakage, and define two complementary
measures: the concepts-task leakage (CTL) and interconcept leakage (ICL)
scores. We show that these measures are strongly predictive of model behaviour
under interventions and outperform existing alternatives in robustness and
reliability. Using this framework, we identify the primary causes of leakage
and provide strong evidence that Concept Embedding Models exhibit substantial
leakage regardless of the hyperparameters choice. Finally, we propose practical
guidelines for designing concept-based models to reduce leakage and ensure
interpretability.

</details>


### [84] [Personalizing Exposure Therapy via Reinforcement Learning](https://arxiv.org/abs/2504.14095)
*Athar Mahmoudi-Nejad,Matthew Guzdial,Pierre Boulanger*

Main category: cs.LG

TL;DR: 这篇论文提出了一种使用强化学习的AI方法来个性化虚拟现实蜘蛛恐惧症暴露疗法，通过生理测量适应患者，并证明其优于传统规则-based方法。


<details>
  <summary>Details</summary>
Motivation: 个性化疗法能改善健康结果，但依赖治疗师的专业知识和直觉，可能不泛化；现有自动方法使用手工规则，可能不适用于所有个体。

Method: 提出基于生理测量的自动适应疗法内容的方法，在虚拟现实蜘蛛恐惧症暴露疗法中使用经验驱动的过程内容生成 via 强化学习（EDPCGRL）来生成虚拟蜘蛛匹配个体患者。

Result: 通过人体受试者研究，证明该系统显著优于规则-based方法。

Conclusion: 突显其在增强个性化疗法干预方面的潜力。

Abstract: Personalized therapy, in which a therapeutic practice is adapted to an
individual patient, can lead to improved health outcomes. Typically, this is
accomplished by relying on a therapist's training and intuition along with
feedback from a patient. However, this requires the therapist to become an
expert on any technological components, such as in the case of Virtual Reality
Exposure Therapy (VRET). While there exist approaches to automatically adapt
therapeutic content to a patient, they generally rely on hand-authored,
pre-defined rules, which may not generalize to all individuals. In this paper,
we propose an approach to automatically adapt therapeutic content to patients
based on physiological measures. We implement our approach in the context of
virtual reality arachnophobia exposure therapy, and rely on experience-driven
procedural content generation via reinforcement learning (EDPCGRL) to generate
virtual spiders to match an individual patient. Through a human subject study,
we demonstrate that our system significantly outperforms a more common
rules-based method, highlighting its potential for enhancing personalized
therapeutic interventions.

</details>


### [85] [Can We Ignore Labels In Out of Distribution Detection?](https://arxiv.org/abs/2504.14704)
*Hong Yang,Qi Yu,Travis Desel*

Main category: cs.LG

TL;DR: 本论文从信息论视角证明无标签OOD检测在标签盲条件下失败，并引入Adjacent OOD新任务。


<details>
  <summary>Details</summary>
Motivation: OOD检测对安全自治系统至关重要，但标注数据成本高，推动探索无标签OOD检测方法。

Method: 理论证明标签盲失败条件、定义Adjacent OOD任务、实验验证现有方法失效。

Result: 证明标签盲条件下检测失败，实验显示现有无标签OOD方法失效。

Conclusion: 分析标签盲理论对未来无标签OOD检测研究的影响。

Abstract: Out-of-distribution (OOD) detection methods have recently become more
prominent, serving as a core element in safety-critical autonomous systems. One
major purpose of OOD detection is to reject invalid inputs that could lead to
unpredictable errors and compromise safety. Due to the cost of labeled data,
recent works have investigated the feasibility of self-supervised learning
(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In
this work, we identify a set of conditions for a theoretical guarantee of
failure in unlabeled OOD detection algorithms from an information-theoretic
perspective. These conditions are present in all OOD tasks dealing with
real-world data: I) we provide theoretical proof of unlabeled OOD detection
failure when there exists zero mutual information between the learning
objective and the in-distribution labels, a.k.a. 'label blindness', II) we
define a new OOD task - Adjacent OOD detection - that tests for label blindness
and accounts for a previously ignored safety gap in all OOD detection
benchmarks, and III) we perform experiments demonstrating that existing
unlabeled OOD methods fail under conditions suggested by our label blindness
theory and analyze the implications for future research in unlabeled OOD
methods.

</details>


### [86] [Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations](https://arxiv.org/abs/2504.14098)
*Justus Råmunddal*

Main category: cs.LG

TL;DR: 本论文提出AI驱动方法，通过推荐类似数学问题提升LMS学习，使用Llama-3.2-11B-Vision-Instruct生成嵌入，并比较余弦相似性、SOM和GMM。结果显示SOM提高用户满意度，适度多样性有益学习。


<details>
  <summary>Details</summary>
Motivation: 动机是提升数学学习，通过推荐类似问题增加用户参与度和学习效果。

Method: 方法包括用Meta的Llama-3.2-11B-Vision-Instruct生成深度嵌入，应用余弦相似性、SOM和GMM识别类似问题，并用用户交互数据（如会话时长、响应时间和正确率）评估。

Result: 结果显示余弦相似性产生近似匹配，SOM用户满意度更高，GMM表现较差，数据表明适度多样性提升参与度。

Conclusion: 结论是适度多样性可增强参与度和学习效果，但需合理平衡。

Abstract: This paper presents an AI-driven approach to enhance math learning in a
modern Learning Management System (LMS) by recommending similar math questions.
Deep embeddings for math questions are generated using Meta's
Llama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine
similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are
applied to identify similar questions. User interaction data, including session
durations, response times, and correctness, are used to evaluate the methods.
Our findings suggest that while cosine similarity produces nearly identical
question matches, SOM yields higher user satisfaction whereas GMM generally
underperforms, indicating that introducing variety to a certain degree may
enhance engagement and thereby potential learning outcomes until variety is no
longer balanced reasonably, which our data about the implementations of all
three methods demonstrate.

</details>


### [87] [Predicting Stress and Damage in Carbon Fiber-Reinforced Composites Deformation Process using Composite U-Net Surrogate Model](https://arxiv.org/abs/2504.14143)
*Zeping Chen,Marwa Yacouti,Maryam Shakiba,Jian-Xun Wang,Tengfei Luo,Vikas Varshney*

Main category: cs.LG

TL;DR: 本研究提出了一种新型自回归复合U-Net深度学习模型，用于预测碳纤维增强复合材料在变形过程中的应力和损伤场，比传统IGFEM快60倍以上，并实现高精度。


<details>
  <summary>Details</summary>
Motivation: 传统FEM模拟计算效率低下，现有的数据驱动模型无法全面捕获应力和损伤演变，因此需要更高效的方法来优化CFRC在航空航天等工程应用中的性能。

Method: 提出了一种基于U-Net架构的自回归复合深度学习模型，捕获空间特征并整合宏观微观现象，来预测CFRC在单向应变下的应力和损伤分布。

Result: 模型在预测应力和损伤演变方面达到高精度，并比IGFEM提速超过60倍。

Conclusion: 该模型克服了现有方法的局限性，为CFRC在高需求应用中的性能优化提供了更有效的工具。

Abstract: Carbon fiber-reinforced composites (CFRC) are pivotal in advanced engineering
applications due to their exceptional mechanical properties. A deep
understanding of CFRC behavior under mechanical loading is essential for
optimizing performance in demanding applications such as aerospace structures.
While traditional Finite Element Method (FEM) simulations, including advanced
techniques like Interface-enriched Generalized FEM (IGFEM), offer valuable
insights, they can struggle with computational efficiency. Existing data-driven
surrogate models partially address these challenges by predicting propagated
damage or stress-strain behavior but fail to comprehensively capture the
evolution of stress and damage throughout the entire deformation history,
including crack initiation and propagation. This study proposes a novel
auto-regressive composite U-Net deep learning model to simultaneously predict
stress and damage fields during CFRC deformation. By leveraging the U-Net
architecture's ability to capture spatial features and integrate macro- and
micro-scale phenomena, the proposed model overcomes key limitations of prior
approaches. The model achieves high accuracy in predicting evolution of stress
and damage distribution within the microstructure of a CFRC under
unidirectional strain, offering a speed-up of over 60 times compared to IGFEM.

</details>


### [88] [A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences](https://arxiv.org/abs/2504.14174)
*Jing Han,Hanting Chen,Kai Han,Xiaomeng Huang,Yongyun Hu,Wenjun Xu,Dacheng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: 这篇论文回顾AI在气象学中的应用，并提出一种基于transformer的多模态数据整合新范式，以提升模型准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 机器学习快速发展，能更好地解决气象问题，数据驱动算法比传统方法更准确，驱动了对多模态数据整合的需求。

Method: 提出新范式，使用transformer整合不同视角的多模态气象数据，并通过正则化技术融入物理知识如温度、压力和风速。

Result: 新范式显示出多功能性、强泛化能力，能处理各种气象任务，并讨论了未来改进方向。

Conclusion: 该范式增强了模型能力，并为提高准确性和可解释性提供了未来研究方向。

Abstract: With the rapid development of machine learning in recent years, many problems
in meteorology can now be addressed using AI models. In particular, data-driven
algorithms have significantly improved accuracy compared to traditional
methods. Meteorological data is often transformed into 2D images or 3D videos,
which are then fed into AI models for learning. Additionally, these models
often incorporate physical signals, such as temperature, pressure, and wind
speed, to further enhance accuracy and interpretability. In this paper, we
review several representative AI + Weather/Climate algorithms and propose a new
paradigm where observational data from different perspectives, each with
distinct physical meanings, are treated as multimodal data and integrated via
transformers. Furthermore, key weather and climate knowledge can be
incorporated through regularization techniques to further strengthen the
model's capabilities. This new paradigm is versatile and can address a variety
of tasks, offering strong generalizability. We also discuss future directions
for improving model accuracy and interpretability.

</details>


### [89] [FedC4: Graph Condensation Meets Client-Client Collaboration for Efficient and Private Federated Graph Learning](https://arxiv.org/abs/2504.14188)
*Zekai Chen,Xunkai Li,Yinlin Zhu,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: FedC4 是一种新的联邦图学习框架，通过图压缩和客户端协作减少通信成本并提升隐私和性能。


<details>
  <summary>Details</summary>
Motivation: 现有客户端-客户端方法在联邦图学习中因广播冗余节点嵌入导致通信成本高和隐私风险。

Method: 提出 FedC4 框架，使用图压缩将私有图浓缩为合成节点嵌入，并引入模块实现个性化优化和全局指导。

Result: 在八个真实数据集上实验表明，FedC4 在性能和通信效率上优于现有最先进基线。

Conclusion: FedC4 有效解决了当前方法的局限性，提供了一种更高效和隐私友好的联邦图学习方案。

Abstract: Federated Graph Learning (FGL) is an emerging distributed learning paradigm
that enables collaborative model training over decentralized graph-structured
data while preserving local privacy. Existing FGL methods can be categorized
into two optimization architectures: (1) the Server-Client (S-C) paradigm,
where clients upload local models for server-side aggregation; and (2) the
Client-Client (C-C) paradigm, which allows direct information exchange among
clients to support personalized training. Compared to S-C, the C-C architecture
better captures global graph knowledge and enables fine-grained optimization
through customized peer-to-peer communication. However, current C-C methods
often broadcast identical and redundant node embeddings, incurring high
communication costs and privacy risks. To address this, we propose FedC4, a
novel framework that combines graph Condensation with Client-Client
Collaboration. Instead of transmitting raw node-level features, FedC4 distills
each client's private graph into a compact set of synthetic node embeddings,
reducing communication overhead and enhancing privacy. In addition, FedC4
introduces three modules that allow source clients to send distinct node
representations tailored to target clients'graph structures, enabling
personalized optimization with global guidance. Extensive experiments on eight
real-world datasets show that FedC4 outperforms state-of-the-art baselines in
both performance and communication efficiency.

</details>


### [90] [DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](https://arxiv.org/abs/2504.14204)
*Wenxin Zhang,Xiaojian Lin,Wenjun Yu,Guangzhen Yao,jingxiang Zhong,Yu Li,Renda Han,Songcheng Xu,Hao Shi,Cuicui Luo*

Main category: cs.LG

TL;DR: 本文提出DConAD框架，通过差分和对比学习提升时间序列异常检测性能，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对无监督方法在异常模式多样、异常稀疏及数据规模复杂性下的局限性，旨在增强模型捕获正常模式的鲁棒性，避免依赖高质量先验知识。

Method: DConAD生成差分数据并使用Transformer捕获时空依赖；采用KL散度-based对比学习，仅用正样本并通过stop-gradient策略确保收敛。

Result: 在五个公共数据集上实验，DConAD优于九个基线方法。

Conclusion: DConAD框架证明了其在时间序列异常检测中的优越性和有效性。

Abstract: Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.

</details>


### [91] [Dual-channel Heterophilic Message Passing for Graph Fraud Detection](https://arxiv.org/abs/2504.14205)
*Wenxin Zhang,Jingxing Zhong,Guangzhen Yao,Renda Han,Xiaojian Lin,Zeyu Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 这篇论文提出DHMP框架，通过分离同质性和异质性信号改进GNN在欺诈检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 欺诈活动增加，现有的GNN方法排除异质性邻居会破坏图结构并增加预测不确定性。

Method: 提出Dual-channel Heterophilic Message Passing (DHMP)，包括异质性分离模块、共享权重和定制采样策略。

Result: 在三个真实数据集上优于现有方法。

Conclusion: 强调分离不同频率信号对提升欺诈检测性能的重要性。

Abstract: Fraudulent activities have significantly increased across various domains,
such as e-commerce, online review platforms, and social networks, making fraud
detection a critical task. Spatial Graph Neural Networks (GNNs) have been
successfully applied to fraud detection tasks due to their strong inductive
learning capabilities. However, existing spatial GNN-based methods often
enhance the graph structure by excluding heterophilic neighbors during message
passing to align with the homophilic bias of GNNs. Unfortunately, this approach
can disrupt the original graph topology and increase uncertainty in
predictions. To address these limitations, this paper proposes a novel
framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud
detection. DHMP leverages a heterophily separation module to divide the graph
into homophilic and heterophilic subgraphs, mitigating the low-pass inductive
bias of traditional GNNs. It then applies shared weights to capture signals at
different frequencies independently and incorporates a customized sampling
strategy for training. This allows nodes to adaptively balance the
contributions of various signals based on their labels. Extensive experiments
on three real-world datasets demonstrate that DHMP outperforms existing
methods, highlighting the importance of separating signals with different
frequencies for improved fraud detection. The code is available at
https://github.com/shaieesss/DHMP.

</details>


### [92] [Decomposition-based multi-scale transformer framework for time series anomaly detection](https://arxiv.org/abs/2504.14206)
*Wenxin Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 本论文提出TransDe框架，用于多变量时间序列异常检测，通过结合时间序列分解和Transformer提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂模式依赖性和处理噪声，导致性能下降。

Method: 提出基于Transformer的TransDe框架，使用多尺度patch-based架构、KL散度对比学习和异步损失函数。

Result: 在五个公共数据集上实验，与十二个基线方法相比，F1分数表现出优越性。

Conclusion: TransDe框架有效解决了挑战，提高了时间序列异常检测的准确性。

Abstract: Time series anomaly detection is crucial for maintaining stable systems.
Existing methods face two main challenges. First, it is difficult to directly
model the dependencies of diverse and complex patterns within the sequences.
Second, many methods that optimize parameters using mean squared error struggle
with noise in the time series, leading to performance deterioration. To address
these challenges, we propose a transformer-based framework built on
decomposition (TransDe) for multivariate time series anomaly detection. The key
idea is to combine the strengths of time series decomposition and transformers
to effectively learn the complex patterns in normal time series data. A
multi-scale patch-based transformer architecture is proposed to exploit the
representative dependencies of each decomposed component of the time series.
Furthermore, a contrastive learn paradigm based on patch operation is proposed,
which leverages KL divergence to align the positive pairs, namely the pure
representations of normal patterns between different patch-level views. A novel
asynchronous loss function with a stop-gradient strategy is further introduced
to enhance the performance of TransDe effectively. It can avoid time-consuming
and labor-intensive computation costs in the optimization process. Extensive
experiments on five public datasets are conducted and TransDe shows superiority
compared with twelve baselines in terms of F1 score. Our code is available at
https://github.com/shaieesss/TransDe.

</details>


### [93] [A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs](https://arxiv.org/abs/2504.14237)
*Dekang Zhang,Dan Niu,Zhou Jin,Yichao Dong,Jingweijia Tan,Changyin Sun*

Main category: cs.LG

TL;DR: 本文提出FSA-Heat模型，用于2.5D集成电路的快速高精度热预测。


<details>
  <summary>Details</summary>
Motivation: 后摩尔时代2.5D芯片let-based集成电路热管理挑战增加，现有的CNN和GCN方法无法有效捕获全局热特征，尤其是高频成分。

Method: 提出FSA-Heat网络，集成FSTE和FCIFormer模块进行频率-空间域特征提取，并设计FSL损失函数减弱噪声和失调。

Result: 实验显示，与GCN+PNA相比，RMSE减少超过99%，推理速度提升4.23倍，并具有强泛化能力。

Conclusion: FSA-Heat方法显著提升了2.5D IC热预测的准确性和效率。

Abstract: In the post-Moore era, 2.5D chiplet-based ICs present significant challenges
in thermal management due to increased power density and thermal hotspots.
Neural network-based thermal prediction models can perform real-time
predictions for many unseen new designs. However, existing CNN-based and
GCN-based methods cannot effectively capture the global thermal features,
especially for high-frequency components, hindering prediction accuracy
enhancement. In this paper, we propose a novel frequency-spatial dual domain
aware prediction network (FSA-Heat) for fast and high-accuracy thermal
prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain
encoder (FSTE) module with frequency domain cross-scale interaction module
(FCIFormer) to achieve high-to-low frequency and global-to-local thermal
dissipation feature extraction. Additionally, a frequency-spatial hybrid loss
(FSL) is designed to effectively attenuate high-frequency thermal gradient
noise and spatial misalignments. The experimental results show that the
performance enhancements offered by our proposed method are substantial,
outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins
(over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive
experiments demonstrate that FSA-Heat also exhibits robust generalization
capabilities.

</details>


### [94] [A Pre-Training and Adaptive Fine-Tuning Framework for Graph Anomaly Detection](https://arxiv.org/abs/2504.14250)
*Yunhui Liu,Jiashun Cheng,Jia Li,Fugee Tsung,Hongzhi Yin,Tieke He*

Main category: cs.LG

TL;DR: 本文提出PAF框架，通过自适应过滤处理图异常检测中的同质异质混合问题，提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 图异常检测面临异常节点稀少和标注成本高的问题，图预训练假设强同质性，但实际中存在复杂的同质异质混合。

Method: 通过谱过滤分析，提出PAF框架，包括预训练中使用低通和高通滤波器联合训练，微调中使用门控融合网络自适应结合节点表示。

Result: 在十个基准数据集上的广泛实验 consistently 证明了PAF的有效性。

Conclusion: PAF框架有效地解决了图异常检测中的问题，强调需要选择性地应用适当的滤波器，而非仅依赖全局低通滤波器。

Abstract: Graph anomaly detection (GAD) has garnered increasing attention in recent
years, yet it remains challenging due to the scarcity of abnormal nodes and the
high cost of label annotations. Graph pre-training, the two-stage learning
paradigm, has emerged as an effective approach for label-efficient learning,
largely benefiting from expressive neighborhood aggregation under the
assumption of strong homophily. However, in GAD, anomalies typically exhibit
high local heterophily, while normal nodes retain strong homophily, resulting
in a complex homophily-heterophily mixture. To understand the impact of this
mixed pattern on graph pre-training, we analyze it through the lens of spectral
filtering and reveal that relying solely on a global low-pass filter is
insufficient for GAD. We further provide a theoretical justification for the
necessity of selectively applying appropriate filters to individual nodes.
Building upon this insight, we propose PAF, a Pre-Training and Adaptive
Fine-tuning framework specifically designed for GAD. In particular, we
introduce joint training with low- and high-pass filters in the pre-training
phase to capture the full spectrum of frequency information in node features.
During fine-tuning, we devise a gated fusion network that adaptively combines
node representations generated by both filters. Extensive experiments across
ten benchmark datasets consistently demonstrate the effectiveness of PAF.

</details>


### [95] [Generative emulation of chaotic dynamics with coherent prior](https://arxiv.org/abs/2504.14264)
*Juan Nathaniel,Pierre Gentine*

Main category: cs.LG

TL;DR: Cohesion 是一种高效生成框架，结合湍流原理和扩散模型，提高非线性动力学模拟的长期预测准确性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的非线性动力学仿真存在长期技能衰减和不现实输出问题，本文旨在通过不确定性量化与修正来解决。

Method: 提出 Cohesion 框架，使用减少阶模型（如深度 Koopman 算子）估计大尺度相干结构，作为去噪过程中的指导，并将预测转化为强化学习的轨迹规划任务。

Result: 在 Kolmogorov 流、浅水方程和气候动力学等混沌系统中，实证显示出优越的长期预测技能和物理一致性模拟，即使在部分观测条件下。

Conclusion: Cohesion 方法高效生成物理一致的模拟，证明其在复杂动力学仿真中的有效性和稳定性。

Abstract: Data-driven emulation of nonlinear dynamics is challenging due to long-range
skill decay that often produces physically unrealistic outputs. Recent advances
in generative modeling aim to address these issues by providing uncertainty
quantification and correction. However, the quality of generated simulation
remains heavily dependent on the choice of conditioning priors. In this work,
we present an efficient generative framework for dynamics emulation, unifying
principles of turbulence with diffusion-based modeling: Cohesion. Specifically,
our method estimates large-scale coherent structure of the underlying dynamics
as guidance during the denoising process, where small-scale fluctuation in the
flow is then resolved. These coherent priors are efficiently approximated using
reduced-order models, such as deep Koopman operators, that allow for rapid
generation of long prior sequences while maintaining stability over extended
forecasting horizon. With this gain, we can reframe forecasting as trajectory
planning, a common task in reinforcement learning, where conditional denoising
is performed once over entire sequences, minimizing the computational cost of
autoregressive-based generative methods. Empirical evaluations on chaotic
systems of increasing complexity, including Kolmogorov flow, shallow water
equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion
superior long-range forecasting skill that can efficiently generate
physically-consistent simulations, even in the presence of partially-observed
guidance.

</details>


### [96] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/abs/2504.14268)
*Xinye Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新型强化学习框架，用于动态优化预处理共轭梯度方法中的数值精度。


<details>
  <summary>Details</summary>
Motivation: 为了平衡计算效率与数值精度，并首次将强化学习应用于混合精度数值方法。

Method: 通过将精度选择建模为马尔可夫决策过程，使用Q学习自适应分配精度水平，并在训练数据上训练后在新数据上推理。

Result: 提升了求解器的性能，展示了方法的鲁棒性、可扩展性，并为科学计算中的AI驱动进展提供了见解。

Conclusion: 该方法有效且实用，开辟了强化学习在数值方法中的新应用路径。

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>


### [97] [SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/abs/2504.14286)
*Xiaojiang Zhang,Jinghui Wang,Zifei Cheng,Wenhao Zhuang,Zheng Lin,Minglei Zhang,Shaojie Wang,Yinghan Cui,Chao Wang,Junyi Peng,Shimiao Jiang,Shiqi Kuang,Shouyu Yin,Chaohang Wen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: 本论文引入SRPO方法，使用强化学习提升大语言模型的推理能力，在AIME24和LiveCodeBench基准上超越DeepSeek-R1-Zero-32B。


<details>
  <summary>Details</summary>
Motivation: 由于方法透明度不足，复制推理模型进步在不同领域存在挑战，因此本研究旨在提升LLM在多样任务中的推理能力。

Method: SRPO基于GRPO，引入双阶段跨领域训练以平衡数学和编码能力，以及历史重采样技术处理无效样本，仅使用强化学习，无需监督微调。

Result: 使用与DeepSeek相同的基模型，SRPO在AIME24和LiveCodeBench基准上表现优于DeepSeek-R1-Zero-32B。

Conclusion: 实验验证了方法的有效性，提供见解以扩展LLM的推理能力。

Abstract: Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24
and LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised
Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we
introduce two key methodological innovations: (1) a two-stage cross-domain
training paradigm designed to balance the development of mathematical reasoning
and coding proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, dedicating to offer valuable insights into scaling LLM
reasoning capabilities across diverse tasks.

</details>


### [98] [Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection](https://arxiv.org/abs/2504.14300)
*Xinyu Liang,Hao Wang*

Main category: cs.LG

TL;DR: 本文提出RLP-GAN，一种基于GAN的合成住宅负载数据生成模型，它优于现有方法，并提供了一百万个配置文件的人工数据集。


<details>
  <summary>Details</summary>
Motivation: 高质量住宅负载数据的稀缺阻碍了住宅部门的脱碳以及电网规划和操作，这激发了对合成负载数据生成方法的研究，以解决可扩展性、多样性和相似性的限制。

Method: RLP-GAN是一种弱监督GAN框架，使用过完备自编码器捕获负载模式的依赖关系，并通过模型权重选择方法避免模式崩溃。

Result: 使用417个真实家庭数据评估，结果显示RLP-GAN在捕获时间依赖性和生成与真实数据更相似的负载模式方面优于现有模型，并发布了合成数据集。

Conclusion: RLP-GAN有效地解决了合成负载数据生成中的问题，并通过公开数据集促进了相关领域的应用。

Abstract: The scarcity of high-quality residential load data can pose obstacles for
decarbonizing the residential sector as well as effective grid planning and
operation. The above challenges have motivated research into generating
synthetic load data, but existing methods faced limitations in terms of
scalability, diversity, and similarity. This paper proposes a Generative
Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)
generation model, a novel weakly-supervised GAN framework, leveraging an
over-complete autoencoder to capture dependencies within complex and diverse
load patterns and learn household-level data distribution at scale. We
incorporate a model weight selection method to address the mode collapse
problem and generate load patterns with high diversity. We develop a holistic
evaluation method to validate the effectiveness of RLP-GAN using real-world
data of 417 households. The results demonstrate that RLP-GAN outperforms
state-of-the-art models in capturing temporal dependencies and generating load
patterns with higher similarity to real data. Furthermore, we have publicly
released the RLP-GAN generated synthetic dataset, which comprises one million
synthetic residential load pattern profiles.

</details>


### [99] [Learning to Score](https://arxiv.org/abs/2504.14302)
*Yogev Kriger,Shai Fine*

Main category: cs.LG

TL;DR: 这篇论文研究了目标标签不可用但有侧边信息的情况下的机器学习方法，提出了一种评分模型结合表示学习、侧边信息和度量学习，并在医疗和基准数据集上验证了效用。


<details>
  <summary>Details</summary>
Motivation: 动机是处理标签缺失但有相关信息的情景，这在半监督学习和领域如医疗诊断中很重要。

Method: 方法是将问题表述为表示学习、侧边信息和度量学习的集成，开发评分模型。

Result: 结果显示，该模型在基准数据集和生物医学记录上表现出良好性能。

Conclusion: 结论是，该评分系统在多个应用场景中具有优势，例如医疗领域的疾病严重程度评估。

Abstract: Common machine learning settings range from supervised tasks, where
accurately labeled data is accessible, through semi-supervised and
weakly-supervised tasks, where target labels are scant or noisy, to
unsupervised tasks where labels are unobtainable. In this paper we study a
scenario where the target labels are not available but additional related
information is at hand. This information, referred to as Side Information, is
either correlated with the unknown labels or imposes constraints on the feature
space. We formulate the problem as an ensemble of three semantic components:
representation learning, side information and metric learning. The proposed
scoring model is advantageous for multiple use-cases. For example, in the
healthcare domain it can be used to create a severity score for diseases where
the symptoms are known but the criteria for the disease progression are not
well defined. We demonstrate the utility of the suggested scoring system on
well-known benchmark data-sets and bio-medical patient records.

</details>


### [100] [Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation](https://arxiv.org/abs/2504.14307)
*Muhammad Haseeb Aslam,Clara Martinez,Marco Pedersoli,Alessandro Koerich,Ali Etemad,Eric Granger*

Main category: cs.LG

TL;DR: 提出随机自蒸馏(SSD)方法，使用蒸馏时dropout和学生引导知识蒸馏(SGKD)，在不增加模型大小的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统自蒸馏和集成学习方法在资源受限环境中的局限性，如需多个模型或未充分利用随机性。

Method: 训练单一模型，通过蒸馏时dropout生成多样教师表示，并采用SGKD过滤和加权教师表示，仅从任务相关表示中蒸馏知识。

Result: 在情感计算、UCR档案、HAR数据集和图像分类数据集上实验，性能优于现有方法，计算复杂度低。

Conclusion: SSD方法提供高效替代方案，提高模型性能而不增加资源消耗。

Abstract: Advances in self-distillation have shown that when knowledge is distilled
from a teacher to a student using the same deep learning (DL) architecture, the
student performance can surpass the teacher particularly when the network is
overparameterized and the teacher is trained with early stopping.
Alternatively, ensemble learning also improves performance, although training,
storing, and deploying multiple models becomes impractical as the number of
models grows. Even distilling an ensemble to a single student model or weight
averaging methods first requires training of multiple teacher models and does
not fully leverage the inherent stochasticity for generating and distilling
diversity in DL models. These constraints are particularly prohibitive in
resource-constrained or latency-sensitive applications such as wearable
devices. This paper proposes to train only one model and generate multiple
diverse teacher representations using distillation-time dropout. However,
generating these representations stochastically leads to noisy representations
that are misaligned with the learned task. To overcome this problem, a novel
stochastic self-distillation (SSD) training strategy is introduced for
filtering and weighting teacher representation to distill from task-relevant
representations only, using student-guided knowledge distillation (SGKD). The
student representation at each distillation step is used as authority to guide
the distillation process. Experimental results on real-world affective
computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,
and image classification datasets show that the proposed SSD method can
outperform state-of-the-art methods without increasing the model size at both
training and testing time, and incurs negligible computational complexity
compared to state-of-the-art ensemble learning and weight averaging methods.

</details>


### [101] [Local distribution-based adaptive oversampling for imbalanced regression](https://arxiv.org/abs/2504.14316)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: 这篇论文提出LDAO方法，用于解决机器学习中不平衡回归问题，通过局部分布自适应过采样改善模型预测。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归问题研究较少，现有的方法依赖任意阈值，产生无效合成样本并可能丢弃信息。

Method: 提出LDAO数据级方法，将数据集分解为局部分布，独立建模和采样，然后合并成平衡训练集。

Result: 在45个不平衡数据集上评估，LDAO优于现有过采样方法，在频繁和稀有目标值上均表现突出。

Conclusion: LDAO有效地解决了不平衡回归的挑战，提高了模型性能。

Abstract: Imbalanced regression occurs when continuous target variables have skewed
distributions, creating sparse regions that are difficult for machine learning
models to predict accurately. This issue particularly affects neural networks,
which often struggle with imbalanced data. While class imbalance in
classification has been extensively studied, imbalanced regression remains
relatively unexplored, with few effective solutions. Existing approaches often
rely on arbitrary thresholds to categorize samples as rare or frequent,
ignoring the continuous nature of target distributions. These methods can
produce synthetic samples that fail to improve model performance and may
discard valuable information through undersampling. To address these
limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),
a novel data-level approach that avoids categorizing individual samples as rare
or frequent. Instead, LDAO learns the global distribution structure by
decomposing the dataset into a mixture of local distributions, each preserving
its statistical characteristics. LDAO then models and samples from each local
distribution independently before merging them into a balanced training set.
LDAO achieves a balanced representation across the entire target range while
preserving the inherent statistical structure within each local distribution.
In extensive evaluations on 45 imbalanced datasets, LDAO outperforms
state-of-the-art oversampling methods on both frequent and rare target values,
demonstrating its effectiveness for addressing the challenge of imbalanced
regression.

</details>


### [102] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TL;DR: 本研究整合scGPT与DeepCDR模型，提高癌症药物反应预测准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提升癌症药物反应预测的准确性，解决现有模型的局限性。

Method: 使用scGPT生成基因表达数据嵌入，并作为DeepCDR的输入。

Result: 实验显示，该方法优于原始DeepCDR和基于scFoundation的模型。

Conclusion: scGPT嵌入可显著提高CDR预测准确性，提供新颖替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>


### [103] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 这篇论文提出RRL算法，通过动态重放机制改善强化学习在大型语言模型训练中的探索效率，提升了模型在复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 论文观察到强化学习训练大型语言模型时，早期探索能发现有前景的解决方案，但能力限制导致无法成功解决；后期能力提升却无法重新探索这些想法，阻碍了复杂问题的处理。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)算法，引入动态重放机制，在训练过程中重新访问早期有前景的状态，以提高探索效率。

Result: 实验显示RRL在数学推理、代码生成和对话任务上显著提升探索效率，优化了强化学习效果，并改善了RLHF的表现，使模型更安全和更有帮助。

Conclusion: RRL算法有效地解决了强化学习探索问题，显著提高了大型语言模型在复杂任务上的性能和整体表现。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>


### [104] [Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator](https://arxiv.org/abs/2504.14365)
*Akshat Ramachandran,Souvik Kundu,Arnab Raha,Shamik Kundu,Deepak K. Mathaikutty,Tushar Krishna*

Main category: cs.LG

TL;DR: 这篇论文提出FLOW方法优化N:M稀疏性选择和FlexCiM架构，以提升LLM剪枝的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决固定N:M稀疏性限制模型表现力和支持多种模式带来的硬件开销问题。

Method: FLOW通过分层异常值密度感知选择最佳N和M值，FlexCiM使用数字计算存储架构的自适应分区支持灵活稀疏模式。

Result: FLOW比现有方法提高准确率高达36%，FlexCiM降低推理延迟高达1.75倍和能量消耗1.5倍。

Conclusion: 该方法显著改善稀疏LLM的准确性和效率，代码开源。

Abstract: Large language model (LLM) pruning with fixed N:M structured sparsity
significantly limits the expressivity of the sparse model, yielding sub-optimal
performance. In contrast, supporting multiple N:M patterns to provide sparse
representational freedom introduces costly overhead in hardware. To address
these challenges for LLMs, we first present a flexible layer-wise
outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the
identification of optimal layer-wise N and M values (from a given range) by
simultaneously accounting for the presence and distribution of outliers,
allowing a higher degree of representational freedom. To deploy sparse models
with such N:M flexibility, we then introduce a flexible, low-overhead digital
compute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity
patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,
which are adaptively aggregated and disaggregated through distribution and
merging mechanisms for different N and M values. Extensive experiments on both
transformer-based and recurrence-based state space foundation models (SSMs)
demonstrate that FLOW outperforms existing alternatives with an accuracy
improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference
latency and 1.5x lower energy consumption compared to existing sparse
accelerators. Code is available at: https://github.com/FLOW-open-project/FLOW

</details>


### [105] [Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data](https://arxiv.org/abs/2504.14368)
*Shlomi Hod,Lucas Rosenblatt,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 本论文提出使用代理公共数据解决差分隐私机器学习中缺乏公共表格数据的限制，通过LLM生成数据，并证明其在预训练和超参数调整中的有效性。


<details>
  <summary>Details</summary>
Motivation: 表格数据异质性高，公共数据假设不成立，因此需要从模式级规格合成代理数据以避免隐私损失。

Method: 使用LLM直接生成CSV文件或构建结构化因果模型（SCM）来采样记录。

Result: 实验显示，代理数据可有效替代传统公共数据用于预训练DP分类器，并在一定程度上用于超参数调整和隐私-效用权衡估计。

Conclusion: 代理公共数据在差分隐私机制中具有多种下游用途，可作为公共数据的替代方案。

Abstract: Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.

</details>


### [106] [Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping](https://arxiv.org/abs/2504.14372)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的不确定性感知机制，使用空间块和VQ-VAE来提高海底地形图的分辨率，同时提供自适应不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前的海底地形数据集分辨率不足，现有深度学习方法难以维持物理结构一致性和量化不确定性。

Method: 本工作使用基于块的保形预测和向量量化变分自编码器（VQ-VAE）架构，捕获局部海底复杂性，提供空间自适应置信估计。

Result: 实验结果显示，在多个海洋区域，重建质量和不确定性估计可靠性显著提升。

Conclusion: 该框架提高了海底重建的可靠性，保持结构完整性，并为气候建模和沿海灾害评估提供了更可靠的基础。

Abstract: Accurate ocean modeling and coastal hazard prediction depend on
high-resolution bathymetric data; yet, current worldwide datasets are too
coarse for exact numerical simulations. While recent deep learning advances
have improved earth observation data resolution, existing methods struggle with
the unique challenges of producing detailed ocean floor maps, especially in
maintaining physical structure consistency and quantifying uncertainties. This
work presents a novel uncertainty-aware mechanism using spatial blocks to
efficiently capture local bathymetric complexity based on block-based conformal
prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)
architecture, the integration of this uncertainty quantification framework
yields spatially adaptive confidence estimates while preserving topographical
features via discrete latent representations. With smaller uncertainty widths
in well-characterized areas and appropriately larger bounds in areas of complex
seafloor structures, the block-based design adapts uncertainty estimates to
local bathymetric complexity. Compared to conventional techniques, experimental
results over several ocean regions show notable increases in both
reconstruction quality and uncertainty estimation reliability. This framework
increases the reliability of bathymetric reconstructions by preserving
structural integrity while offering spatially adaptive uncertainty estimates,
so opening the path for more solid climate modeling and coastal hazard
assessment.

</details>


### [107] [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://arxiv.org/abs/2504.14375)
*Kun Qian,Maximillian Chen,Siyan Li,Arpit Sharma,Zhou Yu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种自下而上的对话合成方法，用于生成高质量的对话式QA数据。


<details>
  <summary>Details</summary>
Motivation: 训练对话式QA系统需要大量领域数据，但数据稀缺，传统自上而下方法缺乏控制和易产生幻觉。

Method: 采用先生成QA对然后组合成对话的底-up方法，提供更好的控制、精确性和非本地模型使用。

Result: 人类和自动评估显示，该方法生成更真实和高质量的对话，比自上而下方法优越。

Conclusion: 该方法通过分步过程提升了合成数据的质量和实用性。

Abstract: Training conversational question-answering (QA) systems requires a
substantial amount of in-domain data, which is often scarce in practice. A
common solution to this challenge is to generate synthetic data. Traditional
methods typically follow a top-down approach, where a large language model
(LLM) generates multi-turn dialogues from a broad prompt. Although this method
produces coherent conversations, it offers limited fine-grained control over
the content and is susceptible to hallucinations. We introduce a bottom-up
conversation synthesis approach, where QA pairs are generated first and then
combined into a coherent dialogue. This method offers greater control and
precision by dividing the process into two distinct steps, allowing refined
instructions and validations to be handled separately. Additionally, this
structure allows the use of non-local models in stages that do not involve
proprietary knowledge, enhancing the overall quality of the generated data.
Both human and automated evaluations demonstrate that our approach produces
more realistic and higher-quality dialogues compared to top-down methods.

</details>


### [108] [Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach](https://arxiv.org/abs/2504.14388)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: FairGrad 是一种新方法，通过平衡准确性和公平性来使医疗 AI 更公平。


<details>
  <summary>Details</summary>
Motivation: AI 在医疗中的应用可能加剧不公平，因此需要解决预测性能与多属性公平性的平衡问题。

Method: 提出 FairGrad 框架，通过将梯度投影到正交平面调节优化轨迹，确保所有目标公平考虑。

Result: 在真实医疗数据集上，FairGrad 显著提高了公平性指标（如等机会），同时保持了竞争性的预测准确性。

Conclusion: 证明了在医疗 AI 中协调公平性和实用性的可行性。

Abstract: The rapid growth of healthcare data and advances in computational power have
accelerated the adoption of artificial intelligence (AI) in medicine. However,
AI systems deployed without explicit fairness considerations risk exacerbating
existing healthcare disparities, potentially leading to inequitable resource
allocation and diagnostic disparities across demographic subgroups. To address
this challenge, we propose FairGrad, a novel gradient reconciliation framework
that automatically balances predictive performance and multi-attribute fairness
optimization in healthcare AI models. Our method resolves conflicting
optimization objectives by projecting each gradient vector onto the orthogonal
plane of the others, thereby regularizing the optimization trajectory to ensure
equitable consideration of all objectives. Evaluated on diverse real-world
healthcare datasets and predictive tasks - including Substance Use Disorder
(SUD) treatment and sepsis mortality - FairGrad achieved statistically
significant improvements in multi-attribute fairness metrics (e.g., equalized
odds) while maintaining competitive predictive accuracy. These results
demonstrate the viability of harmonizing fairness and utility in
mission-critical medical AI applications.

</details>


### [109] [Exploring Pseudo-Token Approaches in Transformer Neural Processes](https://arxiv.org/abs/2504.14416)
*Jose Lara-Rangel,Nanze Chen,Fengzhe Zhang*

Main category: cs.LG

TL;DR: ISANPs 通过 Induced Set Attention 和创新查询阶段改进了神经过程模型，提高了效率和性能，在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决传统 NPs 的欠拟合问题和 TNPs 的高计算复杂度问题。

Method: 引入 Induced Set Attention 和创新查询阶段的 ISANPs。

Result: 在 1D 回归、图像补全等任务中与 TNPs 竞争或优于现有模型，并提供性能和复杂度的可调平衡。

Conclusion: ISANPs 在大型数据集上扩展良好，克服了现有模型的限制。

Abstract: Neural Processes (NPs) have gained attention in meta-learning for their
ability to quantify uncertainty, together with their rapid prediction and
adaptability. However, traditional NPs are prone to underfitting. Transformer
Neural Processes (TNPs) significantly outperform existing NPs, yet their
applicability in real-world scenarios is hindered by their quadratic
computational complexity relative to both context and target data points. To
address this, pseudo-token-based TNPs (PT-TNPs) have emerged as a novel NPs
subset that condense context data into latent vectors or pseudo-tokens,
reducing computational demands. We introduce the Induced Set Attentive Neural
Processes (ISANPs), employing Induced Set Attention and an innovative query
phase to improve querying efficiency. Our evaluations show that ISANPs perform
competitively with TNPs and often surpass state-of-the-art models in 1D
regression, image completion, contextual bandits, and Bayesian optimization.
Crucially, ISANPs offer a tunable balance between performance and computational
complexity, which scale well to larger datasets where TNPs face limitations.

</details>


### [110] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TL;DR: 本论文提出一种低秩偏好建模框架，用于个性化大语言模型（LLM），以更好地适应用户偏好，提高对齐性和满意度。


<details>
  <summary>Details</summary>
Motivation: 个性化LLM以适应多样用户偏好至关重要，因为传统RLHF方法依赖单一价值表示，限制了其适应性。

Method: 引入低秩偏好建模框架，通过在低维子空间表示奖励函数，并将个体偏好建模为共享基函数的加权组合，实现高效学习和泛化。

Result: 在多个偏好数据集上验证，展示了更好的泛化能力到未见用户，以及改进的偏好预测准确性。

Conclusion: 该方法避免了刚性用户分类，实现了可扩展性和少样本适应，提高了LLM的个性化性能。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>


### [111] [A computational framework for longitudinal medication adherence prediction in breast cancer survivors: A social cognitive theory based approach](https://arxiv.org/abs/2504.14469)
*Navreet Kaur,Manuel Gonzales IV,Cristian Garcia Alcaraz,Jiaqi Gong,Kristen J. Wells,Laura E. Barnes*

Main category: cs.LG

TL;DR: 这篇论文开发了基于社会认知理论的多尺度模型来预测乳腺癌幸存者的药物依从性，并评估了不同因素的影响。


<details>
  <summary>Details</summary>
Motivation: 药物不依从性导致慢性病患者死亡率增加、成本上升和痛苦；对于乳腺癌幸存者，依从性可显著提高复发-free生存率。

Method: 引入一个由社会认知理论指导的计算框架，用于多尺度（每日和每周）建模药物依从性，使用动态和静态因素进行预测。

Result: 模型在准确性和特异性上优于传统机器学习方法。每日准确率87.25%，每周76.04%。动态因素对每日预测最重要，组合因素对每周预测重要。

Conclusion: 该方法有效，突出了不同时间尺度上因素的重要性。

Abstract: Non-adherence to medications is a critical concern since nearly half of
patients with chronic illnesses do not follow their prescribed medication
regimens, leading to increased mortality, costs, and preventable human
distress. Amongst stage 0-3 breast cancer survivors, adherence to long-term
adjuvant endocrine therapy (i.e., Tamoxifen and aromatase inhibitors) is
associated with a significant increase in recurrence-free survival. This work
aims to develop multi-scale models of medication adherence to understand the
significance of different factors influencing adherence across varying time
frames. We introduce a computational framework guided by Social Cognitive
Theory for multi-scale (daily and weekly) modeling of longitudinal medication
adherence. Our models employ both dynamic medication-taking patterns in the
recent past (dynamic factors) as well as less frequently changing factors
(static factors) for adherence prediction. Additionally, we assess the
significance of various factors in influencing adherence behavior across
different time scales. Our models outperform traditional machine learning
counterparts in both daily and weekly tasks in terms of both accuracy and
specificity. Daily models achieved an accuracy of 87.25%, and weekly models, an
accuracy of 76.04%. Notably, dynamic past medication-taking patterns prove most
valuable for predicting daily adherence, while a combination of dynamic and
static factors is significant for macro-level weekly adherence patterns.

</details>


### [112] [Less is More: Adaptive Coverage for Synthetic Training Data](https://arxiv.org/abs/2504.14508)
*Sasan Tavakkol,Max Springer,Mohammadhossein Bateni,Neslihan Bulut,Vincent Cohen-Addad,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 使用LLM生成合成数据，并通过新采样算法提高分类器性能，强调'少即是多'。


<details>
  <summary>Details</summary>
Motivation: 解决获取大型标注数据集的挑战，特别是快速部署模型以应对新兴社交媒体趋势或在线滥用。

Method: 引入基于最大覆盖问题的新采样算法，从合成数据集选择代表性子集。

Result: 使用采样子集训练分类器比整个数据集性能更好，提高准确率并减少数据量。

Conclusion: '少即是多'方法改善了模型准确性和效率。

Abstract: Synthetic training data generation with Large Language Models (LLMs) like
Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of
obtaining large, labeled datasets for training classifiers. When rapid model
deployment is critical, such as in classifying emerging social media trends or
combating new forms of online abuse tied to current events, the ability to
generate training data is invaluable. While prior research has examined the
comparability of synthetic data to human-labeled data, this study introduces a
novel sampling algorithm, based on the maximum coverage problem, to select a
representative subset from a synthetically generated dataset. Our results
demonstrate that training a classifier on this contextually sampled subset
achieves superior performance compared to training on the entire dataset. This
"less is more" approach not only improves accuracy but also reduces the volume
of data required, leading to potentially more efficient model fine-tuning.

</details>


### [113] [SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training](https://arxiv.org/abs/2504.14519)
*Zhouyang Li,Yuliang Liu,Wei Zhang,Tailing Yuan,Bin Chen,Chengru Song,Di Zhang*

Main category: cs.LG

TL;DR: SlimPipe 是一种改进的管道并行方法，用于训练大型语言模型，在长上下文场景下减少内存开销并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有管道并行方法在长上下文场景下无法有效处理激活内存压力和管道气泡问题，导致内存消耗和效率低下。

Method: 提出 SlimPipe，使用均匀序列切片、1F1B 调度和工作负载重分布技术来解决负载不平衡。

Result: SlimPipe 在 Llama 70B 模型上将 MFU 提高至 1.57 倍，并在 2048K 上下文长度下在 256 个 NVIDIA Hopper 80GB GPU 上保持超过 45% 利用率。

Conclusion: SlimPipe 通过全面测试证明其有效性，在多种模型架构、上下文窗口大小和配置下表现出色。

Abstract: Pipeline Parallelism (PP) serves as a crucial technique for training Large
Language Models (LLMs), owing to its capability to alleviate memory pressure
from model states with relatively low communication overhead. However, in
long-context scenarios, existing pipeline parallelism methods fail to address
the substantial activation memory pressure, primarily due to the peak memory
consumption resulting from the accumulation of activations across multiple
microbatches. Moreover, these approaches inevitably introduce considerable
pipeline bubbles, further hindering efficiency.
  To tackle these challenges, we propose SlimPipe, a novel approach to
fine-grained pipeline parallelism that employs uniform sequence slicing coupled
with one-forward-one-backward (1F1B) schedule. It reduces the accumulated
activations from several microbatches to just one, which is split into several
slices. Although the slices are evenly partitioned, the computation cost is not
equal across slices due to causal attention. We develop a sophisticated
workload redistribution technique to address this load imbalance. SlimPipe
achieves (1) near-zero memory overhead and (2) minimal pipeline bubbles
simultaneously. The effectiveness of SlimPipe has been proven by thorough
testing with diverse model architectures, context window sizes, and
SlimPipe-specific configurations. For example, on the Llama 70B model, compared
to state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs
Utilization (MFU) to up to $1.57\times$ for a context length of 512K. More
notably, for a context length of 2048K, it maintains over 45% utilization on
256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant
performance drops or fail entirely due to memory constraints.

</details>


### [114] [TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data](https://arxiv.org/abs/2504.14545)
*Fei Zhu,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种使用低秩适配器处理神经网络中OOD数据的故障检测框架，并展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中实现可靠预测，处理协变量和语义OOD数据，通过拒绝错误分类输入实现安全决策，并提供灵活的故障检测方法。

Method: 一个简单的故障检测框架，使用低秩适配器分离和整合特定故障的可靠性知识，实现分类和拒绝的统一。

Result: 大量实验证明了框架的优越性。

Conclusion: 框架有效地且灵活地增强了故障检测能力。

Abstract: Reliable prediction is an essential requirement for deep neural models that
are deployed in open environments, where both covariate and semantic
out-of-distribution (OOD) data arise naturally. In practice, to make safe
decisions, a reliable model should accept correctly recognized inputs while
rejecting both those misclassified covariate-shifted and semantic-shifted
examples. Besides, considering the potential existing trade-off between
rejecting different failure cases, more convenient, controllable, and flexible
failure detection approaches are needed. To meet the above requirements, we
propose a simple failure detection framework to unify and facilitate
classification with rejection under both covariate and semantic shifts. Our key
insight is that by separating and consolidating failure-specific reliability
knowledge with low-rank adapters and then integrating them, we can enhance the
failure detection ability effectively and flexibly. Extensive experiments
demonstrate the superiority of our framework.

</details>


### [115] [NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models](https://arxiv.org/abs/2504.14569)
*Lawrence Liu,Inesh Chakrabarti,Yixiao Li,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TL;DR: NoWag是一个统一的压缩框架，通过标准化权重和激活引导，显著提高了大语言模型的效率，并在矢量量化和剪枝上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务中表现出色，但计算和内存需求巨大，限制了在资源受限环境中的部署。

Method: 提出NoWag框架，包括NoWag-VQ（矢量量化）和NoWag-P（剪枝），用于零射击形状保持压缩，应用于Llama-2和Llama-3模型。

Result: NoWag-VQ在零射击矢量量化中显著优于现有方法，NoWag-P与最先进方法竞争性能。

Conclusion: 结果表明这些压缩范式之间有共同点，可能启发未来研究。代码开源。

Abstract: Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

</details>


### [116] [Data Selection for ERMs](https://arxiv.org/abs/2504.14572)
*Steve Hanneke,Shay Moran,Alexander Shlimovich,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 这篇论文从数据中心视角研究学习理论，固定学习规则优化训练数据，探讨小样本是否能与全数据集相当。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论模型中心，本文提出互补的数据中心方法，以研究数据选择预算下的性能优化。

Method: 研究经验风险最小化器，包括均值估计、线性分类和回归，提供最优数据选择界，并建立错误率分类。

Result: 获得了均值估计、线性分类和回归的最优数据选择界，以及二元分类和随机凸优化的错误率分类。

Conclusion: 总结研究成果，并提出开放问题和未来研究方向。

Abstract: Learning theory has traditionally followed a model-centric approach, focusing
on designing optimal algorithms for a fixed natural learning task (e.g., linear
classification or regression). In this paper, we adopt a complementary
data-centric perspective, whereby we fix a natural learning rule and focus on
optimizing the training data. Specifically, we study the following question:
given a learning rule $\mathcal{A}$ and a data selection budget $n$, how well
can $\mathcal{A}$ perform when trained on at most $n$ data points selected from
a population of $N$ points? We investigate when it is possible to select $n \ll
N$ points and achieve performance comparable to training on the entire
population.
  We address this question across a variety of empirical risk minimizers. Our
results include optimal data-selection bounds for mean estimation, linear
classification, and linear regression. Additionally, we establish two general
results: a taxonomy of error rates in binary classification and in stochastic
convex optimization. Finally, we propose several open questions and directions
for future research.

</details>


### [117] [Generative Auto-Bidding with Value-Guided Explorations](https://arxiv.org/abs/2504.14587)
*Jingtong Gao,Yewen Li,Shuai Mao,Peng Jiang,Nan Jiang,Yejing Wang,Qingpeng Cai,Fei Pan,Peng Jiang,Kun Gai,Bo An,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 本文提出GAVE框架，解决自动竞价的适应性和稳定性问题，通过价值引导探索在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法缺乏适应性、无法捕获历史依赖和离线训练导致的行为崩溃等问题。

Method: 引入离线生成式框架GAVE，包括RTG模块、行动探索机制和可学习价值函数以缓解OOD问题。

Result: 在离线数据集和在线A/B测试中优于基线，代码公开以便重现。

Conclusion: GAVE有效提升自动竞价性能，并为进一步研究提供基础。

Abstract: Auto-bidding, with its strong capability to optimize bidding decisions within
dynamic and competitive online environments, has become a pivotal strategy for
advertising platforms. Existing approaches typically employ rule-based
strategies or Reinforcement Learning (RL) techniques. However, rule-based
strategies lack the flexibility to adapt to time-varying market conditions, and
RL-based methods struggle to capture essential historical dependencies and
observations within Markov Decision Process (MDP) frameworks. Furthermore,
these approaches often face challenges in ensuring strategy adaptability across
diverse advertising objectives. Additionally, as offline training methods are
increasingly adopted to facilitate the deployment and maintenance of stable
online strategies, the issues of documented behavioral patterns and behavioral
collapse resulting from training on fixed offline datasets become increasingly
significant. To address these limitations, this paper introduces a novel
offline Generative Auto-bidding framework with Value-Guided Explorations
(GAVE). GAVE accommodates various advertising objectives through a score-based
Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration
mechanism with an RTG-based evaluation method to explore novel actions while
ensuring stability-preserving updates. A learnable value function is also
designed to guide the direction of action exploration and mitigate
Out-of-Distribution (OOD) problems. Experimental results on two offline
datasets and real-world deployments demonstrate that GAVE outperforms
state-of-the-art baselines in both offline evaluations and online A/B tests.
The implementation code is publicly available to facilitate reproducibility and
further research.

</details>


### [118] [No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning](https://arxiv.org/abs/2504.14610)
*Manar D. Samad,Kazi Fuad B. Akhter,Shourav B. Rabbani,Ibna Kowsar*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为NIIL的无插补增量学习方法，用于处理带有缺失值的表格数据，并展示了其在分类性能上的优越性。


<details>
  <summary>Details</summary>
Motivation: 针对现有插补策略带来的计算复杂性、数据质量和数据驱动结果的担忧，论文旨在开发一种无需插补的学习方法。

Method: 提出NIIL方法，通过增量学习重叠特征集的分区，并使用注意力掩码排除缺失值的影响。

Result: 在15个数据集上，NIIL的分类性能优于11种最先进方法；对不同缺失值类型和比例具有鲁棒性；特征分区大小为原空间一半时性能最佳。

Conclusion: NIIL是首批能有效学习表格数据而不需插补的深度学习解决方案之一。

Abstract: Tabular data sets with varying missing values are prepared for machine
learning using an arbitrary imputation strategy. Synthetic values generated by
imputation models often concern data stakeholders about computational
complexity, data quality, and data-driven outcomes. This paper eliminates these
concerns by proposing no imputation incremental learning (NIIL) of tabular data
with varying missing value rates and types. The proposed method incrementally
learns partitions of overlapping feature sets while using attention masks to
exclude missing values from attention scoring. The average classification
performance rank order across 15 diverse tabular data sets highlights the
superiority of NIIL over 11 state-of-the-art learning methods with or without
missing value imputations. Further experiments substantiate the robustness of
NIIL against varying missing value types and rates compared to methods that
involve the imputation of missing values. Our empirical analysis reveals that a
feature partition size of half of the original feature space is,
computation-wise and accuracy-wise, the best choice for the proposed
incremental learning. The proposed method is one of the first deep learning
solutions that can effectively learn tabular data without requiring the
imputation of missing values.

</details>


### [119] [AlphaZero-Edu: Making AlphaZero Accessible to Everyone](https://arxiv.org/abs/2504.14636)
*Binjie Guo,Hanyu Zheng,Guowei Su,Ru Zhang,Haohan Jiang,Xurong Lin,Hongyan Wei,Aisheng Mo,Jie Li,Zhiyuan Qian,Zhuhao Zhang,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: AlphaZero-Edu 是一个轻量级、教育导向的 AlphaZero 实现，模块化架构、资源高效并开源。


<details>
  <summary>Details</summary>
Motivation: 解决现有强化学习框架的高实现复杂性和差的可复现性。

Method: 基于 AlphaZero 数学框架，采用模块化设计，优化单 GPU 训练，并行化自对弈数据生成。

Result: 在 Gomoku 对战中实现 3.2 倍加速，并对人类对手保持高胜率。

Conclusion: 已开源，提供学术研究和工业应用的实用基准。

Abstract: Recent years have witnessed significant progress in reinforcement learning,
especially with Zero-like paradigms, which have greatly boosted the
generalization and reasoning abilities of large-scale language models.
Nevertheless, existing frameworks are often plagued by high implementation
complexity and poor reproducibility. To tackle these challenges, we present
AlphaZero-Edu, a lightweight, education-focused implementation built upon the
mathematical framework of AlphaZero. It boasts a modular architecture that
disentangles key components, enabling transparent visualization of the
algorithmic processes. Additionally, it is optimized for resource-efficient
training on a single NVIDIA RTX 3090 GPU and features highly parallelized
self-play data generation, achieving a 3.2-fold speedup with 8 processes. In
Gomoku matches, the framework has demonstrated exceptional performance,
achieving a consistently high win rate against human opponents. AlphaZero-Edu
has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,
providing an accessible and practical benchmark for both academic research and
industrial applications.

</details>


### [120] [Surrogate Fitness Metrics for Interpretable Reinforcement Learning](https://arxiv.org/abs/2504.14645)
*Philipp Altmann,Céline Davignon,Maximilian Zorn,Fabian Ritz,Claudia Linnhoff-Popien,Thomas Gabor*

Main category: cs.LG

TL;DR: 本研究使用进化优化框架通过代理适应度函数提高RL策略的可解释性，在离散和连续环境中取得改善。


<details>
  <summary>Details</summary>
Motivation: 为了提升RL模型在安全关键和解释性需求领域的可解释性。

Method: 采用进化优化框架扰动初始状态，结合局部多样性、行为确定性和全局多样性的代理适应度函数，并使用奖励-based最优性差距、Fidelity IQM等指标评估。

Result: 在网格世界中演示保真度显著提升，在连续控制中为早期策略提供洞察，保真度优化对成熟策略更有效。

Conclusion: 优化代理适应度函数推进了RL模型的可解释性，提供更深入的决策洞察，惠及安全关键应用。

Abstract: We employ an evolutionary optimization framework that perturbs initial states
to generate informative and diverse policy demonstrations. A joint surrogate
fitness function guides the optimization by combining local diversity,
behavioral certainty, and global population diversity. To assess demonstration
quality, we apply a set of evaluation metrics, including the reward-based
optimality gap, fidelity interquartile means (IQMs), fitness composition
analysis, and trajectory visualizations. Hyperparameter sensitivity is also
examined to better understand the dynamics of trajectory optimization. Our
findings demonstrate that optimizing trajectory selection via surrogate fitness
metrics significantly improves interpretability of RL policies in both discrete
and continuous environments. In gridworld domains, evaluations reveal
significantly enhanced demonstration fidelities compared to random and ablated
baselines. In continuous control, the proposed framework offers valuable
insights, particularly for early-stage policies, while fidelity-based
optimization proves more effective for mature policies. By refining and
systematically analyzing surrogate fitness functions, this study advances the
interpretability of RL models. The proposed improvements provide deeper
insights into RL decision-making, benefiting applications in safety-critical
and explainability-focused domains.

</details>


### [121] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TL;DR: This paper introduces LeetCodeDataset, a benchmark for code-generation models in LLM research, addressing gaps in benchmarks and training.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reasoning-focused coding benchmarks and self-contained training testbeds in LLM research.

Method: Curating LeetCode Python problems with rich metadata, broad coverage, over 100 test cases per problem, and temporal splits for contamination-free evaluation and SFT.

Result: Reasoning models outperform non-reasoning ones; SFT with 2.6K generated solutions achieves performance comparable to models trained on 110K samples.

Conclusion: The dataset and evaluation framework are publicly available on Hugging Face and Github, providing a valuable resource for the community.

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>


### [122] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TL;DR: 本文提出使用锐度感知最小化（SAM）微调预训练模型，以减少任务间参数干扰并提升合并模型性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决合并多任务模型时参数干扰问题，同时保持各任务性能。

Method: 方法是通过锐度感知最小化（SAM）作为微调目标函数。

Result: 实验和理论结果显示，该方法在各种合并和微调方法上提高了性能。

Conclusion: 结论是该方法有效且与现有方法正交，能够改进性能。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>


### [123] [Efficient Federated Split Learning for Large Language Models over Communication Networks](https://arxiv.org/abs/2504.14667)
*Kai Zhao,Zhaohui Yang*

Main category: cs.LG

TL;DR: 本论文提出FedsLLM框架，通过结合分割联邦学习和LoRA技术，在资源受限的边缘设备上高效微调大语言模型，减少计算负担和训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决分布式微调预训练大语言模型在资源受限边缘设备上的挑战。

Method: 提出FedsLLM框架，整合模型分割和LoRA；制定联合优化问题，包括子信道分配、功率控制、模型分割点选择和LoRA秩配置；开发交替优化算法。

Result: 模拟结果显示，模型准确性与传统方法相当，但显著降低了客户端计算需求和训练延迟。

Conclusion: FedsLLM框架有效提升了训练效率，同时保持了模型性能。

Abstract: Fine-tuning pre-trained large language models (LLM) in a distributed manner
poses significant challenges on resource-constrained edge devices. To address
this challenge, we propose FedsLLM, a novel framework that integrates split
federated learning with parameter-efficient fine-tuning techniques. By
leveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the
computational burden on edge devices. Furthermore, the introduction of a
federated server facilitates parallel training and enhances privacy. To
accommodate heterogeneous communication conditions and diverse computational
capabilities of edge devices, as well as the impact of LoRA rank selection on
model convergence and training cost, we formulate a joint optimization problem.
The formulated problem jointly optimizes subchannel allocation, power control,
model splitting point selection, and LoRA rank configuration, all aimed at
minimizing total training delay. An alternating optimization algorithm is
developed to efficiently solve this problem and accelerate the training
process. Simulation results demonstrate that the proposed FedsLLM framework
achieves comparable model accuracy while significantly reducing client-side
computational requirements. Furthermore, the proposed resource allocation
scheme and adaptive LoRA rank selection strategy notably reduce the training
latency compared to conventional approaches.

</details>


### [124] [Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning](https://arxiv.org/abs/2504.14677)
*Jia Liu,Cheng Jinguo,Xia Fang,Zhenyuan Ma,Yuankai Wu*

Main category: cs.LG

TL;DR: 本研究首次探索时间序列基础模型的持续学习能力，发现基础模型在增量学习中性能持续提升，而传统模型则下降，并建议优化微调策略。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在多样预测任务中表现出色，但其通过增量学习持续改进的能力尚未被研究。

Method: 使用新的持续学习框架，在真实数据集上实验，比较传统深度学习模型和基础模型如Time-MoE和Chronos。

Result: 基础模型显示预测准确性持续改进，传统模型性能下降；优化基础模型微调策略比开发特定领域小模型更有价值。

Conclusion: 引入新评估方法和见解，以开发具有强大连续学习能力的时序基础模型。

Abstract: Time series foundation models excel at diverse time series forecasting tasks,
but their capacity for continuous improvement through incremental learning
remains unexplored. We present the first comprehensive study investigating
these models' temporal plasticity - their ability to progressively enhance
performance through continual learning while maintaining existing capabilities.
Through experiments on real-world datasets exhibiting distribution shifts, we
evaluate both conventional deep learning models and foundation models using a
novel continual learning framework. Our findings reveal that while traditional
models struggle with performance deterioration during incremental fine-tuning,
foundation models like Time-MoE and Chronos demonstrate sustained improvement
in predictive accuracy. This suggests that optimizing foundation model
fine-tuning strategies may be more valuable than developing domain-specific
small models. Our research introduces new evaluation methodologies and insights
for developing foundation time series models with robust continuous learning
capabilities.

</details>


### [125] [Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data](https://arxiv.org/abs/2504.14694)
*Yuting He,Yiqiang Chen,XiaoDong Yang,Hanchao Yu,Yi-Hua Huang,Yang Gu*

Main category: cs.LG

TL;DR: 本文提出FedSSD方法，通过选择性自蒸馏解决联邦学习中数据异质性问题，提高性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 数据异质性导致本地模型偏向局部最优，影响全局性能和收敛。

Method: FedSSD通过自蒸馏全局模型知识，并在类和样本级别选择性加权约束本地更新。

Result: 实验在基准数据集上显示FedSSD在更少通信轮次内实现更好泛化和鲁棒性。

Conclusion: FedSSD比现有方法更有效，并有理论收敛保证。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global model while keeping local data decentralized. Data heterogeneity
(non-IID) across clients has imposed significant challenges to FL, which makes
local models re-optimize towards their own local optima and forget the global
knowledge, resulting in performance degradation and convergence slowdown. Many
existing works have attempted to address the non-IID issue by adding an extra
global-model-based regularizing item to the local training but without an
adaption scheme, which is not efficient enough to achieve high performance with
deep learning models. In this paper, we propose a Selective Self-Distillation
method for Federated learning (FedSSD), which imposes adaptive constraints on
the local updates by self-distilling the global model's knowledge and
selectively weighting it by evaluating the credibility at both the class and
sample level. The convergence guarantee of FedSSD is theoretically analyzed and
extensive experiments are conducted on three public benchmark datasets, which
demonstrates that FedSSD achieves better generalization and robustness in fewer
communication rounds, compared with other state-of-the-art FL methods.

</details>


### [126] [Quantitative Clustering in Mean-Field Transformer Models](https://arxiv.org/abs/2504.14697)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 本论文研究均场Transformer模型的长期聚类行为，并证明了在特定条件下指数级同步收敛。


<details>
  <summary>Details</summary>
Motivation: 动机是探索Transformer模型中令牌演化的聚类行为，类似于Kuramoto模型的同步现象，以更好地理解深度学习模型。

Method: 方法是通过建模为互作用粒子系统，并使用均场理论在特定假设下建立指数收敛率。

Result: 结果是证明了任意合适的初始化条件下，模型指数级快速同步到Dirac点质量。

Conclusion: 结论是Transformer模型在一定条件下表现出快速同步行为，这可能对AI模型的稳定性和效率有重要启示。

Abstract: The evolution of tokens through a deep transformer models can be modeled as
an interacting particle system that has been shown to exhibit an asymptotic
clustering behavior akin to the synchronization phenomenon in Kuramoto models.
In this work, we investigate the long-time clustering of mean-field transformer
models. More precisely, we establish exponential rates of contraction to a
Dirac point mass for any suitably regular initialization under some assumptions
on the parameters of transformer models, any suitably regular mean-field
initialization synchronizes exponentially fast with some quantitative rates.

</details>


### [127] [Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods](https://arxiv.org/abs/2504.14701)
*Andres Fernandez,Frank Schneider,Maren Mahsereci,Philipp Hennig*

Main category: cs.LG

TL;DR: 本论文研究深度神经网络中Hessian特征空间与参数掩码的关联，发现二者有显著overlap，并开发高效计算方法。


<details>
  <summary>Details</summary>
Motivation: 最近观察到Hessian曲率集中在顶层特征空间，且参数修剪掩码早早稳定，本文旨在探索二者之间的联系。

Method: 开发Grassmannian度量（如overlap）测量相似性，并使用基于草图SVD的矩阵自由算法计算Hessian特征值对。

Result: 实验显示overlap高于随机水平，且在更大网络中更明显；顶层Hessian特征向量集中在较大参数周围。

Conclusion: 提供了大规模Hessian分析方法，并揭示了eigenspace结构的洞见。

Abstract: Recently, it has been observed that when training a deep neural net with SGD,
the majority of the loss landscape's curvature quickly concentrates in a tiny
*top* eigenspace of the loss Hessian, which remains largely stable thereafter.
Independently, it has been shown that successful magnitude pruning masks for
deep neural nets emerge early in training and remain stable thereafter. In this
work, we study these two phenomena jointly and show that they are connected: We
develop a methodology to measure the similarity between arbitrary parameter
masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*
as the most useful such metric due to its interpretability and stability. To
compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs
that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M
parameters --an unprecedented scale by several orders of magnitude. Our
experiments reveal an *overlap* between magnitude parameter masks and top
Hessian eigenspaces consistently higher than chance-level, and that this effect
gets accentuated for larger network sizes. This result indicates that *top
Hessian eigenvectors tend to be concentrated around larger parameters*, or
equivalently, that *larger parameters tend to align with directions of larger
loss curvature*. Our work provides a methodology to approximate and analyze
deep learning Hessians at scale, as well as a novel insight on the structure of
their eigenspace.

</details>


### [128] [Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation](https://arxiv.org/abs/2504.14716)
*Tuhina Tripathi,Manya Wadhwa,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 这篇论文探讨了LLM反馈协议（绝对分数 vs. 相对偏好）对评估可靠性的影响，发现配对评估更容易受干扰特征操纵，导致偏见，而绝对评分更稳健。


<details>
  <summary>Details</summary>
Motivation: LLM的校准和评估至关重要，反馈协议的选择起核心作用但尚未得到充分研究。

Method: 通过实验比较绝对评分和相对偏好协议，分析生成模型如何利用虚假属性影响判断。

Result: 配对偏好在35%情况下翻转，而绝对评分仅9%；绝对评分更能反映响应质量，减少干扰特征影响。

Conclusion: 根据数据集特性和评估目标，提供选择反馈协议的推荐，以提高评估可靠性。

Abstract: Large Language Models (LLMs) are widely used as proxies for human labelers in
both training (Reinforcement Learning from AI Feedback) and large-scale
response evaluation (LLM-as-a-judge). Alignment and evaluation are critical
components in the development of reliable LLMs, and the choice of feedback
protocol plays a central role in both but remains understudied. In this work,
we show that the choice of feedback protocol (absolute scores versus relative
preferences) can significantly affect evaluation reliability and induce
systematic biases. In particular, we show that pairwise evaluation protocols
are more vulnerable to distracted evaluation. Generator models can exploit
spurious attributes (or distractor features) favored by the LLM judge,
resulting in inflated scores for lower-quality outputs and misleading training
signals. We find that absolute scoring is more robust to such manipulation,
producing judgments that better reflect response quality and are less
influenced by distractor features. Our results demonstrate that generator
models can flip preferences by embedding distractor features, skewing
LLM-as-a-judge comparisons and leading to inaccurate conclusions about model
quality in benchmark evaluations. Pairwise preferences flip in about 35% of the
cases, compared to only 9% for absolute scores. We offer recommendations for
choosing feedback protocols based on dataset characteristics and evaluation
objectives.

</details>


### [129] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 论文提出一种模仿人类记忆的持续学习框架，帮助深度神经网络在学习新任务时不忘记旧知识。


<details>
  <summary>Details</summary>
Motivation: 人类和动物能持续学习积累知识，而深度神经网络在连续任务中容易遗忘先验知识，因此需要改进以适应动态环境。

Method: 提出生物模拟的持续学习框架，整合半参数记忆和唤醒-睡眠巩固机制。

Result: 方法在ImageNet等真实场景中，使深度神经网络在新任务上保持高性能，同时保留先验知识。

Conclusion: 研究表明，模仿生物智能是为深度神经网络赋予持续学习能力提供了一个有前景的途径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>


### [130] [Geometric Learning Dynamics](https://arxiv.org/abs/2504.14728)
*Vitaly Vanchurin*

Main category: cs.LG

TL;DR: This paper presents a unified geometric framework for modeling learning dynamics across physical, biological, and machine learning systems, identifying three regimes based on a power-law relationship and arguing that the intermediate regime drives biological complexity.


<details>
  <summary>Details</summary>
Motivation: To unify the modeling of learning dynamics in diverse fields and explain the emergence of biological complexity through different regimes.

Method: Using a geometric framework with a power-law relationship between the metric tensor and noise covariance to define and analyze three fundamental regimes.

Result: Identification of three regimes: quantum (a=1) with Schrödinger-like dynamics, efficient learning (a=1/2) for fast algorithms, and equilibration (a=0) for biological evolution models.

Conclusion: The intermediate regime (a=1/2) is a key mechanism for the emergence of biological complexity.

Abstract: We present a unified geometric framework for modeling learning dynamics in
physical, biological, and machine learning systems. The theory reveals three
fundamental regimes, each emerging from the power-law relationship $g \propto
\kappa^a$ between the metric tensor $g$ in the space of trainable variables and
the noise covariance matrix $\kappa$. The quantum regime corresponds to $a = 1$
and describes Schr\"odinger-like dynamics that emerges from a discrete shift
symmetry. The efficient learning regime corresponds to $a = \tfrac{1}{2}$ and
describes very fast machine learning algorithms. The equilibration regime
corresponds to $a = 0$ and describes classical models of biological evolution.
We argue that the emergence of the intermediate regime $a = \tfrac{1}{2}$ is a
key mechanism underlying the emergence of biological complexity.

</details>


### [131] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi,Somtochukwu Oguchienti,Maheed H. Ahmed,Mahsa Ghasemi*

Main category: cs.LG

TL;DR: 本论文探讨从多级人类反馈（每回合结束的分数）中学习强化学习的奖励函数和最优策略，提出算法并证明次线性遗憾率。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数设计困难，特别是复杂环境，论文使用多级反馈提供更信息丰富的信号，并处理非马尔可夫奖励。

Method: 提出算法，从每回合结束的评分反馈中高效学习奖励函数和最优策略。

Result: 算法实现次线性遗憾率，并在模拟实验中验证有效性。

Conclusion: 该方法提升了人类反馈在强化学习中的应用，改善奖励函数学习。

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>


### [132] [AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization](https://arxiv.org/abs/2504.14741)
*Namrata Vaswani*

Main category: cs.LG

TL;DR: 这篇论文引入了AltGDmin优化框架，它比传统交替最小化更快，适用于特定优化问题。


<details>
  <summary>Details</summary>
Motivation: 论文动机是为那些一个变量集最小化较快且成本函数可微的优化问题提供更高效解决方案，尤其在联邦学习中提升通信效率。

Method: 方法是交替使用梯度下降和最小化，针对优化变量的两个块Za和Zb。

Result: 结果显示AltGDmin在低秩压缩感知、矩阵补全、鲁棒PCA等诸多问题中表现更快，并有实际应用如动态MRI和推荐系统。

Conclusion: 结论是AltGDmin是一种创新框架，提高了优化效率，并扩展到聚类、相位检索等应用领域。

Abstract: This article describes a novel optimization solution framework, called
alternating gradient descent (GD) and minimization (AltGDmin), that is useful
for many problems for which alternating minimization (AltMin) is a popular
solution. AltMin is a special case of the block coordinate descent algorithm
that is useful for problems in which minimization w.r.t one subset of variables
keeping the other fixed is closed form or otherwise reliably solved. Denote the
two blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,
Zb}. AltGDmin is often a faster solution than AltMin for any problem for which
(i) the minimization over one set of variables, Zb, is much quicker than that
over the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.
Often, the reason for one minimization to be quicker is that the problem is
``decoupled" for Zb and each of the decoupled problems is quick to solve. This
decoupling is also what makes AltGDmin communication-efficient for federated
settings.
  Important examples where this assumption holds include (a) low rank
column-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)
their outlier-corrupted extensions such as robust PCA, robust LRCS and robust
LRMC; (c) phase retrieval and its sparse and low-rank model based extensions;
(d) tensor extensions of many of these problems such as tensor LRCS and tensor
completion; and (e) many partly discrete problems where GD does not apply --
such as clustering, unlabeled sensing, and mixed linear regression. LRCS finds
important applications in multi-task representation learning and few shot
learning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA
find important applications in recommender systems, computer vision and video
analytics.

</details>


### [133] [AI for the Open-World: the Learning Principles](https://arxiv.org/abs/2504.14751)
*Jianyu Zhang*

Main category: cs.LG

TL;DR: This thesis explores learning principles for open-world AI and verifies them through experiments.


<details>
  <summary>Details</summary>
Motivation: AI successes in closed-world tasks do not translate to open-world due to lack of transferable insights, insufficient examples, and unclear criteria.

Method: Proposes and tests learning principles like rich features, disentangled representation, and inference-time learning via large-scale experiments.

Result: Verified the effectiveness of the learning principles for open-world AI.

Conclusion: Open-world AI requires unique learning principles and techniques distinct from closed-world approaches.

Abstract: During the past decades, numerous successes of AI has been made on "specific
capabilities", named closed-world, such as artificial environments or specific
real-world tasks. This well-defined narrow capability brings two nice benefits,
a clear criterion of success and the opportunity to collect a lot of examples.
The criteria not only reveal whether a machine has achieved a goal, but reveal
how the machine falls short of the goal. As a result, human designers can fix
the problems one after the other until the machine is deemed good enough for
the task. Furthermore, the large set of collected examples reduces the
difficulty of this problem-fixing process (by the central limit theorem).
  Do the success in closed-world translate into broad open-world, where a
machine is required to perform any task that a human could possibly undertake
with fewer examples and less priori knowledge from human designers? No. Because
competence in a specific task provides little insight in handling other tasks,
the valuable criteria for specific tasks become helpless when handling broader
unseen tasks. Furthermore, due to the shortage of examples in unseen tasks,
central limit theorem does not stand on our side. At the end, human designers
lose the oscilloscope to "hack" an AI system for the open-world.
  Achieving AI for the open-world requires unique learning principles and
innovated techniques, which are different from the ones in building AI for the
closed-world. This thesis explores necessary learning principles required to
construct AI for the open-world, including rich features (analogy a large tool
box), disentangled representation (an organized tool box), and inference-time
learning (a tool-savvy hand). Driven by the learning principles, this thesis
further proposes techniques to use the learning principles, conducts enormous
large-scale experiments to verify the learning principles.

</details>


### [134] [A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization](https://arxiv.org/abs/2504.14762)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 论文提出图论视角的dropout理论，将训练建模为二元子网络图上的随机游走，证明泛化子网络形成连通簇，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 动机是开发dropout的组合和图论理论，以更好地理解其在神经网络泛化中的机制。

Method: 方法包括使用随机游走建模、定义子网络贡献分数，并应用谱图理论、PAC-Bayes分析和组合学工具。

Result: 结果证明泛化子网络形成大、连通、低阻抗簇，其数量随网络宽度指数增长；dropout采样鲁棒集成；实验验证所有理论声明。

Conclusion: 结论为dropout提供统一基础，并提出新的掩码引导正则化和子网络优化方向。

Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling
training as a random walk over a high-dimensional graph of binary subnetworks.
Each node represents a masked version of the network, and dropout induces
stochastic traversal across this space. We define a subnetwork contribution
score that quantifies generalization and show that it varies smoothly over the
graph. Using tools from spectral graph theory, PAC-Bayes analysis, and
combinatorics, we prove that generalizing subnetworks form large, connected,
low-resistance clusters, and that their number grows exponentially with network
width. This reveals dropout as a mechanism for sampling from a robust,
structured ensemble of well-generalizing subnetworks with built-in redundancy.
Extensive experiments validate every theoretical claim across diverse
architectures. Together, our results offer a unified foundation for
understanding dropout and suggest new directions for mask-guided regularization
and subnetwork optimization.

</details>


### [135] [Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model](https://arxiv.org/abs/2504.14782)
*Ahmed Sobhi Saleh,Kristof Croes,Hajdin Ceric,Ingrid De Wolf,Houman Zahedmanesh*

Main category: cs.LG

TL;DR: 本研究提出自动方法，使用边缘检测和生成扩散模型从TEM图像提取晶粒，准确率达97.23%。


<details>
  <summary>Details</summary>
Motivation: 传统方法劳动密集、主观耗时，限制可扩展性；数据稀缺是主要挑战。

Method: 整合边缘检测与生成扩散模型；采用七阶段方法生成合成TEM图像用于训练。

Result: 应用于各种金属，纳米级粒径，准确率平均97.23%，媲美高级实验技术。

Conclusion: 方法有效，可扩展到其他数据稀缺领域。

Abstract: The traditional techniques for extracting polycrystalline grain structures
from microscopy images, such as transmission electron microscopy (TEM) and
scanning electron microscopy (SEM), are labour-intensive, subjective, and
time-consuming, limiting their scalability for high-throughput analysis. In
this study, we present an automated methodology integrating edge detection with
generative diffusion models to effectively identify grains, eliminate noise,
and connect broken segments in alignment with predicted grain boundaries. Due
to the limited availability of adequate images preventing the training of deep
machine learning models, a new seven-stage methodology is employed to generate
synthetic TEM images for training. This concept-oriented synthetic data
approach can be extended to any field of interest where the scarcity of data is
a challenge. The presented model was applied to various metals with average
grain sizes down to the nanoscale, producing grain morphologies from
low-resolution TEM images that are comparable to those obtained from advanced
and demanding experimental techniques with an average accuracy of 97.23%.

</details>


### [136] [Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization](https://arxiv.org/abs/2504.14790)
*Jun Yang,Shintaro Yamasaki*

Main category: cs.LG

TL;DR: 本研究提出多级网格DDTD方法，通过相关性-based mutation模块和多级网格策略，提高拓扑优化泛化性和效率，无需高质量初始数据集。


<details>
  <summary>Details</summary>
Motivation: 克服敏感度-based TO方法在强非线性问题上的局限，以及DDTD对初始数据集质量的依赖，提高有效性和泛化性。

Method: 提出多级网格DDTD方法，结合相关性-based mutation模块生成有物理意义的几何特征，并逐步提升网格精细度以提高计算效率。

Result: 实验证明该方法在应力相关强非线性问题上具有良好泛化性和有效性，优于传统敏感度-based TO方法。

Conclusion: 该方法提升了DDTD的泛化性，减少了对特定数据集的依赖，并降低了计算成本。

Abstract: Topology optimization (TO) serves as a widely applied structural design
approach to tackle various engineering problems. Nevertheless,
sensitivity-based TO methods usually struggle with solving strongly nonlinear
optimization problems. By leveraging high capacity of deep generative model,
which is an influential machine learning technique, the sensitivity-free
data-driven topology design (DDTD) methodology is regarded as an effective
means of overcoming these issues. The DDTD methodology depends on initial
dataset with a certain regularity, making its results highly sensitive to
initial dataset quality. This limits its effectiveness and generalizability,
especially for optimization problems without priori information. In this
research, we proposed a multi-level mesh DDTD-based method with
correlation-based mutation module to escape from the limitation of the quality
of the initial dataset on the results and enhance computational efficiency. The
core is to employ a correlation-based mutation module to assign new geometric
features with physical meaning to the generated data, while utilizing a
multi-level mesh strategy to progressively enhance the refinement of the
structural representation, thus avoiding the maintenance of a high
degree-of-freedom (DOF) representation throughout the iterative process. The
proposed multi-level mesh DDTD-based method can be driven by a low quality
initial dataset without the need for time-consuming construction of a specific
dataset, thus significantly increasing generality and reducing application
difficulty, while further lowering computational cost of DDTD methodology.
Various comparison experiments with the traditional sensitivity-based TO
methods on stress-related strongly nonlinear problems demonstrate the
generality and effectiveness of the proposed method.

</details>


### [137] [Edge-boosted graph learning for functional brain connectivity analysis](https://arxiv.org/abs/2504.14796)
*David Yang,Mostafa Abdelmegeed,John Modl,Minjeong Kim*

Main category: cs.LG

TL;DR: 本文提出了一种使用边缘功能连接和共嵌入技术来更好地从脑网络预测阿尔茨海默病和帕金森病的新方法，并优于现有的GNN方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于节点的连接方法无法准确捕获脑内的功能连接，需要更好的方法来实现神经退行性疾病的早期诊断。

Method: 提出了一种强调边缘功能连接（eFC）的新方法，并引入共嵌入技术来有效地整合边缘间的关系。

Result: 在ADNI和PPMI数据集上的实验结果显示，该方法在功能脑网络分类中显著优于最先进的GNN方法。

Conclusion: 该方法显著提高了疾病预测的准确性。

Abstract: Predicting disease states from functional brain connectivity is critical for
the early diagnosis of severe neurodegenerative diseases such as Alzheimer's
Disease and Parkinson's Disease. Existing studies commonly employ Graph Neural
Networks (GNNs) to infer clinical diagnoses from node-based brain connectivity
matrices generated through node-to-node similarities of regionally averaged
fMRI signals. However, recent neuroscience studies found that such node-based
connectivity does not accurately capture ``functional connections" within the
brain. This paper proposes a novel approach to brain network analysis that
emphasizes edge functional connectivity (eFC), shifting the focus to inter-edge
relationships. Additionally, we introduce a co-embedding technique to integrate
edge functional connections effectively. Experimental results on the ADNI and
PPMI datasets demonstrate that our method significantly outperforms
state-of-the-art GNN methods in classifying functional brain networks.

</details>


### [138] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TL;DR: 本研究引入Robust Unlearning概念和UMA攻击框架，以评估机器学习模型的遗忘效果，并发现现有技术存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对隐私保护至关重要，但现有方法无法完全消除遗忘信息残留，且验证工具不足以检测泄漏。

Method: 提出Robust Unlearning概念和UMA框架，通过对抗查询主动探测模型中的遗忘痕迹。

Result: 实验显示现有遗忘技术易受攻击，即使通过现有验证指标。

Conclusion: UMA作为实用工具，设定新的机器遗忘安全评估标准，提升安全性。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>


### [139] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TL;DR: 这篇论文提出一个针对小样本不平衡（S&I）问题的系统分析框架，总结指标和方法，审视现有解决方案，并通过实验显示分类器选择比重采样更重要。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题在机器学习中是一个重大挑战，现有的方法依赖启发式算法而未充分分析数据特性，因此需要从数据角度进行详细分析。

Method: 论文的方法包括总结不平衡指标和复杂性分析方法，审视常规、复杂性和极端S&I问题的解决方案，并进行二元和多类数据集的实验。

Result: 实验结果显示，分类器性能差异远大于重采样带来的改善，重采样仍是广泛采用的解决方案。

Conclusion: 论文突出了S&I问题中的开放问题和未来趋势，强调需要更有效的分析和方法。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>


### [140] [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://arxiv.org/abs/2504.14805)
*Jinwoo Choi,Seung-Woo Seo*

Main category: cs.LG

TL;DR: 本论文提出Dynamic Contrastive Skill Learning (DCSL)框架，以改进强化学习中技能学习的问题，解决长时域任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长时域复杂决策任务中面临困难，现有方法无法识别语义相似的行为并使用固定技能长度，限制了灵活性和泛化。

Method: DCSL引入基于状态转换的技能表示、技能相似性函数学习和动态技能长度调整，利用对比学习捕获行为语义上下文。

Result: 在任务完成和效率方面，与现有方法相比表现出竞争性性能。

Conclusion: DCSL实现了更灵活和自适应的技能提取，特别是在复杂或噪声数据集中的应用。

Abstract: Reinforcement learning (RL) has made significant progress in various domains,
but scaling it to long-horizon tasks with complex decision-making remains
challenging. Skill learning attempts to address this by abstracting actions
into higher-level behaviors. However, current approaches often fail to
recognize semantically similar behaviors as the same skill and use fixed skill
lengths, limiting flexibility and generalization. To address this, we propose
Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines
skill representation and learning. DCSL introduces three key ideas:
state-transition based skill representation, skill similarity function
learning, and dynamic skill length adjustment. By focusing on state transitions
and leveraging contrastive learning, DCSL effectively captures the semantic
context of behaviors and adapts skill lengths to match the appropriate temporal
extent of behaviors. Our approach enables more flexible and adaptive skill
extraction, particularly in complex or noisy datasets, and demonstrates
competitive performance compared to existing methods in task completion and
efficiency.

</details>


### [141] [A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm](https://arxiv.org/abs/2504.14814)
*Kazuhisa Fujita*

Main category: cs.LG

TL;DR: 本文介绍了Kaneko的错误扩散学习算法（EDLA），作为反向传播的生物学合理替代，并通过实验验证其在多种任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播缺乏生物学合理性，促使开发EDLA等替代方法，以实现基于全局错误信号的网络训练。

Method: 呈现EDLA的当代公式，并通过奇偶校验、回归和图像分类任务评估其性能，还分析了内部特征表示。

Result: EDLA网络在基准测试中达到高准确率，性能受学习率、神经元数量和网络深度影响；其特征提取能力类似于传统网络。

Conclusion: EDLA是训练前馈网络的生物学合理替代方案，将激励未来扩展到生物启发神经网络的研究。

Abstract: Artificial neural networks are powerful tools capable of addressing various
tasks. Although the backpropagation algorithm has become a standard training
method for these neural networks, its lack of biological plausibility has
inspired the development of alternative learning approaches. One such
alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a
biologically motivated approach wherein a single global error signal diffuses
throughout a network composed of paired excitatory-inhibitory sublayers,
thereby eliminating the necessity for layer-wise backpropagation. This study
presents a contemporary formulation of the EDLA framework and evaluates its
effectiveness through parity check, regression, and image classification tasks.
Our experimental results indicate that EDLA networks can consistently achieve
high accuracy across these benchmarks, with performance efficiency and
convergence speed notably influenced by the choice of learning rate, neuron
count, and network depth. Further investigation of the internal representations
formed by EDLA networks reveals their capacity for meaningful feature
extraction, similar to traditional neural networks. These results suggest that
EDLA is a biologically motivated alternative for training feedforward networks
and will motivate future work on extending this method to biologically inspired
neural networks.

</details>


### [142] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TL;DR: 本文提出PAIA框架，用于审计扩散模型是否学习特定概念，实现高效、准确的模型审计。


<details>
  <summary>Details</summary>
Motivation: 扩散模型微调共享可能生成敏感内容，缺乏实用审计工具，需解决伦理和法律风险。

Method: 引入PAIA（Prompt-Agnostic Image-Free Auditing），通过直接分析模型内部行为，避免依赖提示和图像。

Result: 在320个控制模型和690个真实模型上，PAIA检测准确率超过90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展的扩散模型部署前审计解决方案，促进更安全透明的模型共享。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>


### [143] [Uncertainty quantification of neural network models of evolving processes via Langevin sampling](https://arxiv.org/abs/2504.14854)
*Cosmin Safta,Reese E. Jones,Ravi G. Patel,Raelynn Wonnacot,Dan S. Bolintineanu,Craig M. Hamel,Sharlotte L. B. Kramer*

Main category: cs.LG

TL;DR: 我们提出了一种可扩展的近似推理超网络框架，用于一般历史依赖过程模型。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一个灵活的数据模型，能够平衡计算预算，并在化学反应和材料物理数据上进行应用。

Method: 方法基于神经常微分方程（NODE）表示内部状态演化，结合可训练的观测模型和朗之万采样来学习后验分布。

Result: 结果展示了在化学反应和材料物理数据上的性能，并与均场变分推理进行了比较。

Conclusion: 结论是这种框架提供了计算灵活性和有效的后验逼近。

Abstract: We propose a scalable, approximate inference hypernetwork framework for a
general model of history-dependent processes. The flexible data model is based
on a neural ordinary differential equation (NODE) representing the evolution of
internal states together with a trainable observation model subcomponent. The
posterior distribution corresponding to the data model parameters (weights and
biases) follows a stochastic differential equation with a drift term related to
the score of the posterior that is learned jointly with the data model
parameters. This Langevin sampling approach offers flexibility in balancing the
computational budget between the evaluation cost of the data model and the
approximation of the posterior density of its parameters. We demonstrate
performance of the hypernetwork on chemical reaction and material physics data
and compare it to mean-field variational inference.

</details>


### [144] [Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder](https://arxiv.org/abs/2504.14879)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TL;DR: 本研究比较了Vision Transformer (ViT) 和Variational Auto-Encoder (VAE) 在IoT僵尸网络流量数据集上的降维性能，发现VAE表现更好。


<details>
  <summary>Details</summary>
Motivation: IoT设备快速增长，成为网络攻击目标，安全问题突出；本研究探讨潜变量维度对深度学习分类器性能的影响。

Method: 使用ViT和VAE编码器将高维IoT僵尸网络流量数据集投影到低维潜空间，然后训练深度学习分类器，在N-BaIoT和CICIoT2022数据集上评估精度、精确度、召回率和F1分数。

Result: VAE编码器基于的降维在两个数据集上优于ViT编码器，在所有性能指标上表现更好，归因于数据集缺少ViT试图学习的空间模式。

Conclusion: VAE在非图像IoT流量数据上的降维更有效，建议在类似应用中优先使用VAE。

Abstract: The rapid evolution of Internet of Things (IoT) technology has led to a
significant increase in the number of IoT devices, applications, and services.
This surge in IoT devices, along with their widespread presence, has made them
a prime target for various cyber-attacks, particularly through IoT botnets. As
a result, security has become a major concern within the IoT ecosystem. This
study focuses on investigating how the latent dimension impacts the performance
of different deep learning classifiers when trained on latent vector
representations of the train dataset. The primary objective is to compare the
outcomes of these models when encoder components from two cutting-edge
architectures: the Vision Transformer (ViT) and the Variational Auto-Encoder
(VAE) are utilized to project the high dimensional train dataset to the learned
low dimensional latent space. The encoder components are employed to project
high-dimensional structured .csv IoT botnet traffic datasets to various latent
sizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that
VAE-encoder based dimension reduction outperforms ViT-encoder based dimension
reduction for both datasets in terms of four performance metrics including
accuracy, precision, recall, and F1-score for all models which can be
attributed to absence of spatial patterns in the datasets the ViT model
attempts to learn and extract from image instances.

</details>


### [145] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TL;DR: 本研究显示，优化算法的选择会影响神经网络的群体公平，适应性优化器如RMSProp比SGD更可能实现更公平的结果。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法如何影响深度神经网络中的群体公平，特别是严重失衡情况下。

Method: 使用随机微分方程分析、理论保证和在CelebA、FairFace、MS-COCO数据集上的实验，验证不同优化器在各种任务和公平定义下的表现。

Result: RMSProp比SGD更可能收敛到更公平的最小值，适应性优化器在群体公平上 consistently 优于SGD，同时保持相似的预测准确性。

Conclusion: 强调适应性更新是促进公平结果的关键机制，但常被忽略。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>


### [146] [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://arxiv.org/abs/2504.14889)
*Seunghun Lee,Jinyoung Park,Jaewon Chu,Minseo Yoon,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 本论文提出NF-BO，使用归一化流解决潜在贝叶斯优化的重建间隙问题，并在分子生成任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有潜在贝叶斯优化方法因输入和潜在空间的重建间隙导致价值差异问题，优化结果次优，需要改进。

Method: 提出NF-BO，使用归一化流建立一对一编码函数，引入SeqFlow和基于重要性的动态候选采样策略。

Result: 实验显示NF-BO在分子生成任务中显著优于传统和最近的潜在贝叶斯优化方法。

Conclusion: NF-BO通过消除重建间隙，改进了优化性能，证明了其有效性。

Abstract: Bayesian Optimization (BO) has been recognized for its effectiveness in
optimizing expensive and complex objective functions. Recent advancements in
Latent Bayesian Optimization (LBO) have shown promise by integrating generative
models such as variational autoencoders (VAEs) to manage the complexity of
high-dimensional and structured data spaces. However, existing LBO approaches
often suffer from the value discrepancy problem, which arises from the
reconstruction gap between input and latent spaces. This value discrepancy
problem propagates errors throughout the optimization process, leading to
suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based
Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative
model to establish one-to-one encoding function from the input space to the
latent space, along with its left-inverse decoding function, eliminating the
reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive
normalizing flow for sequence data. In addition, we develop a new candidate
sampling strategy that dynamically adjusts the exploration probability for each
token based on its importance. Through extensive experiments, our NF-BO method
demonstrates superior performance in molecule generation tasks, significantly
outperforming both traditional and recent LBO approaches.

</details>


### [147] [Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel](https://arxiv.org/abs/2504.14907)
*Kexin Wang,Mengna Liu,Xu Cheng,Fan Shi,Shanshan Qi,Shengyong Chen*

Main category: cs.LG

TL;DR: 本论文提出TGC-SSE模型，通过减少数据冗余、捕捉变量交互和处理类别不平衡，提高了海况估计的准确性，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理船舶运动数据中的数据不平衡和特征冗余方面存在挑战，限制了其有效性。

Method: 提出TGC-SSE模型，包括时间维度因子分解模块、动态图学习模块和对比聚类损失函数。

Result: 在14个公共数据集上表现最佳，在9个数据集上accuracy最高，比EDI提高20.79%；超越五个基准方法和七个深度学习模型；消融研究证实各模块的有效性。

Conclusion: TGC-SSE提高了海况估计准确性，展示了强大泛化能力，为自主船舶操作提供可靠支持。

Abstract: Accurate sea state estimation is crucial for the real-time control and future
state prediction of autonomous vessels. However, traditional methods struggle
with challenges such as data imbalance and feature redundancy in ship motion
data, limiting their effectiveness. To address these challenges, we propose the
Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel
deep learning model that combines three key components: a time dimension
factorization module to reduce data redundancy, a dynamic graph-like learning
module to capture complex variable interactions, and a contrastive clustering
loss function to effectively manage class imbalance. Our experiments
demonstrate that TGC-SSE significantly outperforms existing methods across 14
public datasets, achieving the highest accuracy in 9 datasets, with a 20.79%
improvement over EDI. Furthermore, in the field of sea state estimation,
TGC-SSE surpasses five benchmark methods and seven deep learning models.
Ablation studies confirm the effectiveness of each module, demonstrating their
respective roles in enhancing overall model performance. Overall, TGC-SSE not
only improves the accuracy of sea state estimation but also exhibits strong
generalization capabilities, providing reliable support for autonomous vessel
operations.

</details>


### [148] [POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications](https://arxiv.org/abs/2504.14917)
*Chunjing Gan,Dan Yang,Binbin Hu,Ziqi Liu,Yue Shen,Zhiqiang Zhang,Jian Wang,Jun Zhou*

Main category: cs.LG

TL;DR: 本文提出PolyRAG，一种在医疗场景中考虑多视角的检索增强生成方法，以及PolyEVAL基准，展示了其在处理及时性、权威性和共性方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗应用中面临知识更新和幻觉问题，现有的检索增强生成方法未充分考虑检索的及时性、权威性和共性，导致性能 suboptimal。

Method: 提出PolyRAG，通过整合不同视角的判断器进行检索增强生成，并开发PolyEVAL基准，该基准包含真实医疗场景的查询和文档，并带有多个标签（如及时性、权威性）。

Result: 在PolyEVAL基准上的广泛实验证明了PolyRAG的优越性。

Conclusion: PolyRAG通过多视角整合改进了检索增强生成方法，提升了在医疗场景中的可靠性和应用性能。

Abstract: Large language models (LLMs) have become a disruptive force in the industry,
introducing unprecedented capabilities in natural language processing, logical
reasoning and so on. However, the challenges of knowledge updates and
hallucination issues have limited the application of LLMs in medical scenarios,
where retrieval-augmented generation (RAG) can offer significant assistance.
Nevertheless, existing retrieve-then-read approaches generally digest the
retrieved documents, without considering the timeliness, authoritativeness and
commonality of retrieval. We argue that these approaches can be suboptimal,
especially in real-world applications where information from different sources
might conflict with each other and even information from the same source in
different time scale might be different, and totally relying on this would
deteriorate the performance of RAG approaches. We propose PolyRAG that
carefully incorporate judges from different perspectives and finally integrate
the polyviews for retrieval augmented generation in medical applications. Due
to the scarcity of real-world benchmarks for evaluation, to bridge the gap we
propose PolyEVAL, a benchmark consists of queries and documents collected from
real-world medical scenarios (including medical policy, hospital & doctor
inquiry and healthcare) with multiple tagging (e.g., timeliness,
authoritativeness) on them. Extensive experiments and analysis on PolyEVAL have
demonstrated the superiority of PolyRAG.

</details>


### [149] [Causal DAG Summarization (Full Version)](https://arxiv.org/abs/2504.14937)
*Anna Zeng,Michael Cafarella,Batya Kenig,Markos Markakis,Brit Youngmann,Babak Salimi*

Main category: cs.LG

TL;DR: 本文提出一种因果图总结方法，以简化高维DAG并提升因果推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 因果DAG复杂且易出错，导致因果推理不可靠；现有图总结方法不适合因果DAG。

Method: 提出因果图总结目标和高效贪婪算法，可直接用于因果推理。

Result: 在六个真实数据集上实验，证明算法处理高维数据有效，并提升因果推理的可靠性和鲁棒性。

Conclusion: 总结DAG可增强因果推理的鲁棒性，减少假设错误的影响。

Abstract: Causal inference aids researchers in discovering cause-and-effect
relationships, leading to scientific insights. Accurate causal estimation
requires identifying confounding variables to avoid false discoveries. Pearl's
causal model uses causal DAGs to identify confounding variables, but incorrect
DAGs can lead to unreliable causal conclusions. However, for high dimensional
data, the causal DAGs are often complex beyond human verifiability. Graph
summarization is a logical next step, but current methods for general-purpose
graph summarization are inadequate for causal DAG summarization. This paper
addresses these challenges by proposing a causal graph summarization objective
that balances graph simplification for better understanding while retaining
essential causal information for reliable inference. We develop an efficient
greedy algorithm and show that summary causal DAGs can be directly used for
inference and are more robust to misspecification of assumptions, enhancing
robustness for causal inference. Experimenting with six real-life datasets, we
compared our algorithm to three existing solutions, showing its effectiveness
in handling high-dimensional data and its ability to generate summary DAGs that
ensure both reliable causal inference and robustness against misspecifications.

</details>


### [150] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY框架通过off-policy指导提升大型推理模型的推理能力，平均在数学基准上提升7.0分，并显著改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有on-policy零强化学习方法限制了模型的学习，仅限于自身输出，无法超越初始能力，因此需要引入off-policy方法来增强推理。

Method: 引入LUFFY框架，结合off-policy演示和on-policy rollout，通过正则化重要性采样进行策略塑造，动态平衡模仿和探索。

Result: 在六个数学基准上平均提升7.0分，分布外任务提升6.2分，显著超过监督微调，尤其在泛化方面。

Conclusion: LUFFY不仅有效模仿演示，还能探索新内容，提供可扩展的路径训练一般化推理模型。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>


### [151] [Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling](https://arxiv.org/abs/2504.14946)
*Tin Ping Chan,Yunlong Cheng,Yizhan Zhu,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TL;DR: 本文引入DVAMP问题，定义为混合整数线性规划，提出SPANE深度强化学习方法，实验显示减少VM等待时间45%。


<details>
  <summary>Details</summary>
Motivation: 云计算演进中多NUMA架构带来VM调度挑战，需要更准确处理现代云环境复杂性。

Method: 形式化定义DVAMP离线和在线版本为混合整数线性规划，并提出SPANE，利用对称性提升深度强化学习效率。

Result: 在Huawei-East-1数据集实验中，SPANE比基线减少平均VM等待时间45%。

Conclusion: 为云资源管理提供理论和实用解决方案，改进多NUMA环境VM调度性能，填补文献空白。

Abstract: As cloud computing continues to evolve, the adoption of multi-NUMA
(Non-Uniform Memory Access) architecture by cloud service providers has
introduced new challenges in virtual machine (VM) scheduling. To address these
challenges and more accurately reflect the complexities faced by modern cloud
environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM
(DVAMP). We formally define both offline and online versions of DVAMP as
mixed-integer linear programming problems, providing a rigorous mathematical
foundation for analysis. A tight performance bound for greedy online algorithms
is derived, offering insights into the worst-case optimality gap as a function
of the number of physical machines and VM lifetime variability. To address the
challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture
for Multi-NUMA Environments), a novel deep reinforcement learning approach that
exploits the problem's inherent symmetries. SPANE produces invariant results
under arbitrary permutations of physical machine states, enhancing learning
efficiency and solution quality. Extensive experiments conducted on the
Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,
reducing average VM wait time by 45%. Our work contributes to the field of
cloud resource management by providing both theoretical insights and practical
solutions for VM scheduling in multi-NUMA environments, addressing a critical
gap in the literature and offering improved performance for real-world cloud
systems.

</details>


### [152] [Efficient Document Retrieval with G-Retriever](https://arxiv.org/abs/2504.14955)
*Manthankumar Solanki*

Main category: cs.LG

TL;DR: 本论文提出改进的RAG方法，使用注意力机制构建子图，编码节点和边属性，在WebQSP数据集上取得略微更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法使用PCST优化仅关注节点属性，导致上下文理解不完整。

Method: 替换PCST为基于注意力的子图构建技术，编码节点和边属性，并使用改进的投影层和多头注意力池化，以更好地与大语言模型对齐。

Result: 在WebQSP数据集实验中，方法具有竞争力，并比原方法略微提升性能。

Conclusion: 这突显了方法在实现更准确问题回答方面的潜力。

Abstract: Textual data question answering has gained significant attention due to its
growing applicability. Recently, a novel approach leveraging the
Retrieval-Augmented Generation (RAG) method was introduced, utilizing the
Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.
However, this method focused solely on node attributes, leading to incomplete
contextual understanding. In this paper, we propose an enhanced approach that
replaces the PCST method with an attention-based sub-graph construction
technique, enabling more efficient and context-aware retrieval. Additionally,
we encode both node and edge attributes, leading to richer graph
representations. Our method also incorporates an improved projection layer and
multi-head attention pooling for better alignment with Large Language Models
(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our
approach is competitive and achieves marginally better results compared to the
original method, underscoring its potential for more accurate question
answering.

</details>


### [153] [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://arxiv.org/abs/2504.14960)
*Dennis Liu,Zijie Yan,Xin Yao,Tong Liu,Vijay Korthikanti,Evan Wu,Shiqing Fan,Gao Deng,Hongxiao Bai,Ashwath Aithal,Michael Andersch,Mohammad Shoeybi,Jiajie Yao,Chandler Zhou,David Wu,Xipeng Li,June Yang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种端到端训练框架，用于大规模Mixture of Experts (MoE) 模型，通过五维混合并行性提升训练效率。


<details>
  <summary>Details</summary>
Motivation: MoE 模型通过动态选择专家提高神经网络可扩展性，但大规模训练面临现有并行策略的限制。

Method: 引入五维混合并行性（张量并行、专家并行、上下文并行、数据并行和管道并行），并开发 MoE Parallel Folding 和灵活的 token-level dispatcher，支持注意力层和 MoE 层的优化并行。

Result: 实验显示最高达 49.3% MFU for Mixtral 8x22B 和 39.0% MFU for Qwen2-57B-A14B，在 H100 GPU 上扩展到 1024 GPU，并支持序列长度达 128K 标记。

Conclusion: 框架有效提高了训练效率和可扩展性，代码在 Megatron-Core 中可用。

Abstract: Mixture of Experts (MoE) models enhance neural network scalability by
dynamically selecting relevant experts per input token, enabling larger model
sizes while maintaining manageable computation costs. However, efficient
training of large-scale MoE models across thousands of GPUs presents
significant challenges due to limitations in existing parallelism strategies.
We introduce an end-to-end training framework for large-scale MoE models that
utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert
Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.
Central to our approach is MoE Parallel Folding, a novel strategy that
decouples the parallelization of attention and MoE layers in Transformer
models, allowing each layer type to adopt optimal parallel configurations.
Additionally, we develop a flexible token-level dispatcher that supports both
token-dropping and token-dropless MoE training across all five dimensions of
parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates
different parallelism schemes for Attention and MoE layers, facilitating
complex parallelism implementations. Our experiments demonstrate significant
improvements in training efficiency and scalability. We achieve up to 49.3%
Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the
Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The
framework scales efficiently up to 1,024 GPUs and maintains high performance
with sequence lengths up to 128K tokens, validating its effectiveness for
large-scale MoE model training. The code is available in Megatron-Core.

</details>


### [154] [Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation](https://arxiv.org/abs/2504.14994)
*Hankang Sun,Guiming Li,Su Yang,Baoqi Li*

Main category: cs.LG

TL;DR: 本研究针对无源域适应的时间序列分类提出组合式重建架构，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列域适应因动态性高而具挑战，本文处理最难子任务：目标标签和源数据不可访问，需保留源先验并适应域变化。

Method: 使用冻结U-net粗略重建，结合源重放和偏移补偿分支，通过可学习因子组合输出，并在推理时进行稳定性aware重缩放。

Result: 在3个基准上达到SOTA性能。

Conclusion: 组合式重建架构有效解决source-free domain adaptation，提升了时间序列分类的鲁棒性。

Abstract: Domain adaptation is challenging for time series classification due to the
highly dynamic nature. This study tackles the most difficult subtask when both
target labels and source data are inaccessible, namely, source-free domain
adaptation. To reuse the classification backbone pre-trained on source data,
time series reconstruction is a sound solution that aligns target and source
time series by minimizing the reconstruction errors of both. However, simply
fine-tuning the source pre-trained reconstruction model on target data may lose
the learnt priori, and it struggles to accommodate domain varying temporal
patterns in a single encoder-decoder. Therefore, this paper tries to
disentangle the composition of domain transferability by using a compositional
architecture for time series reconstruction. Here, the preceding component is a
U-net frozen since pre-trained, the output of which during adaptation is the
initial reconstruction of a given target time series, acting as a coarse step
to prompt the subsequent finer adaptation. The following pipeline for finer
adaptation includes two parallel branches: The source replay branch using a
residual link to preserve the output of U-net, and the offset compensation
branch that applies an additional autoencoder (AE) to further warp U-net's
output. By deploying a learnable factor on either branch to scale their
composition in the final output of reconstruction, the data transferability is
disentangled and the learnt reconstructive capability from source data is
retained. During inference, aside from the batch-level optimization in the
training, we search at test time stability-aware rescaling of source replay
branch to tolerate instance-wise variation. The experimental results show that
such compositional architecture of time series reconstruction leads to SOTA
performance on 3 widely used benchmarks.

</details>


### [155] [A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](https://arxiv.org/abs/2504.15037)
*Huanyu Zhang,Chengzu Li,Wenshan Wu,Shaoguang Mao,Yan xia,Ivan Vulić,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.LG

TL;DR: This paper highlights the spatial reasoning limitations in Multimodal Large Language Models (MLLMs) and proposes a framework to address them.


<details>
  <summary>Details</summary>
Motivation: MLLMs' deficiencies in spatial reasoning limit their real-world applications, necessitating dedicated modifications in development approaches.

Method: The paper establishes a comprehensive framework for spatial reasoning, elaborates on its role, and systematically analyzes components like training data and reasoning mechanisms.

Result: It reveals critical limitations in current methodologies and identifies promising avenues for advancement in spatial reasoning capabilities.

Conclusion: The work aims to direct AI research toward improving MLLMs to achieve human-like spatial reasoning by focusing on these underexplored aspects.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in general vision-language tasks. However, recent studies have
exposed critical limitations in their spatial reasoning capabilities. This
deficiency in spatial reasoning significantly constrains MLLMs' ability to
interact effectively with the physical world, thereby limiting their broader
applications. We argue that spatial reasoning capabilities will not naturally
emerge from merely scaling existing architectures and training methodologies.
Instead, this challenge demands dedicated attention to fundamental
modifications in the current MLLM development approach. In this position paper,
we first establish a comprehensive framework for spatial reasoning within the
context of MLLMs. We then elaborate on its pivotal role in real-world
applications. Through systematic analysis, we examine how individual components
of the current methodology-from training data to reasoning mechanisms-influence
spatial reasoning capabilities. This examination reveals critical limitations
while simultaneously identifying promising avenues for advancement. Our work
aims to direct the AI research community's attention toward these crucial yet
underexplored aspects. By highlighting these challenges and opportunities, we
seek to catalyze progress toward achieving human-like spatial reasoning
capabilities in MLLMs.

</details>


### [156] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 论文提出VeLU激活函数，通过动态调整输入方差改善神经网络性能。


<details>
  <summary>Details</summary>
Motivation: ReLU存在消失梯度问题和缺乏适应性，Swish和GELU虽有平滑过渡但不能动态调整输入统计。

Method: VeLU整合ArcTan-Sin变换和Wasserstein-2正则化，根据输入方差动态缩放。

Result: 在ViT_B16等模型和六个视觉基准上，VeLU优于ReLU等激活函数。

Conclusion: VeLU缓解协变量偏移、稳定优化，代码已在GitHub公开。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>


### [157] [Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL](https://arxiv.org/abs/2504.15077)
*Simone Papicchio,Simone Rossi,Luca Cagliero,Paolo Papotti*

Main category: cs.LG

TL;DR: 這篇論文研究大語言模型的推理能力對Text2SQL性能的影響，發現強化學習和混合方法能顯著提升小模型在複雜任務中的表現。


<details>
  <summary>Details</summary>
Motivation: 大語言模型在處理多表和多跳推理的Text2SQL任務時存在挑戰，現有方法如監督微調無法完全解決，需探索推理能力的影響。

Method: 比較不同LLM設置的性能，包括零射擊學習（有無推理）、監督微調（有無推理痕跡）、強化學習（以執行準確性為獎勵）、以及SFT+RL組合。

Result: 一般推理在零射擊學習中無效；小LLM從帶推理的SFT中受益更多；RL對所有模型有益，特別是複雜查詢；SFT+RL使小模型如7B Qwen-Coder-2.5在Bird數據集上與大模型相當。

Conclusion: 強化學習和SFT+RL方法能有效提升Text2SQL性能，特別適合推理密集型任務。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in
transforming natural language questions about relational databases into SQL
queries. Despite recent improvements, small LLMs struggle to handle questions
involving multiple tables and complex SQL patterns under a Zero-Shot Learning
(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge
deficits in pretrained models but falls short while dealing with queries
involving multi-hop reasoning. To bridge this gap, different LLM training
strategies to reinforce reasoning capabilities have been proposed, ranging from
leveraging a thinking process within ZSL, including reasoning traces in SFT, or
adopt Reinforcement Learning (RL) strategies. However, the influence of
reasoning on Text2SQL performance is still largely unexplored. This paper
investigates to what extent LLM reasoning capabilities influence their Text2SQL
performance on four benchmark datasets. To this end, it considers the following
LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,
with and without task-specific reasoning traces; (3) RL, leveraging execution
accuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that
combines SFT and RL. The results show that general-purpose reasoning under ZSL
proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit
from SFT with reasoning much more than larger ones, bridging the gap of their
(weaker) model pretraining. RL is generally beneficial across all tested models
and datasets, particularly when SQL queries involve multi-hop reasoning and
multiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks
to a strategic balance between generality of the reasoning process and
optimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5
model performs on par with 100+ Billion ones on the Bird dataset.

</details>


### [158] [Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving](https://arxiv.org/abs/2504.15090)
*Junxiang Gao,Yixin Ran,Jia Chen*

Main category: cs.LG

TL;DR: 這篇論文提出一種Federated Bias-Aware Latent Factor (FBALF)模型，用於在聯邦學習框架下處理推薦系統的評分偏差問題，提高推薦準確性，同時保護用戶隱私。


<details>
  <summary>Details</summary>
Motivation: 傳統推薦系統在集中式服務器上處理數據，存在隱私洩露風險；聯邦學習可保護隱私，但難以處理評分偏差。

Method: 提出FBALF模型，將偏差顯式納入每個本地模型的損失函數中，從而消除評分偏差而不損害數據隱私。

Result: 在三個真實世界數據集上的廣泛實驗顯示，FBALF比其他最先進的聯邦推薦系統實現了顯著更高的推薦準確性。

Conclusion: FBALF模型成功解決了聯邦推薦系統中的評分偏差問題，提高了推薦性能，同時確保了數據隱私。

Abstract: A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.

</details>


### [159] [Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN](https://arxiv.org/abs/2504.15099)
*Lin Wang,Xiancheng Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 本论文开发了FSCO优化器，使用强化学习改进GAN训练稳定性，减少对超参数的敏感性。


<details>
  <summary>Details</summary>
Motivation: GAN训练对数据属性和超参数高度敏感，可能导致振荡、收敛困难或失败，尤其在训练集方差大时。

Method: 提出Fast-Slow Co-advancing Optimizer (FSCO)，通过强化学习控制训练步长和学习率，使训练更智能。

Result: 在三个基准数据集上实验验证了FSCO的有效性。

Conclusion: FSCO提高了GAN训练的稳定性和收敛性，降低了对其步长的敏感度。

Abstract: Up to now, the training processes of typical Generative Adversarial Networks
(GANs) are still particularly sensitive to data properties and hyperparameters,
which may lead to severe oscillations, difficulties in convergence, or even
failures to converge, especially when the overall variances of the training
sets are large. These phenomena are often attributed to the training
characteristics of such networks. Aiming at the problem, this paper develops a
new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which
employs reinforcement learning in the training process of GANs to make training
easier. Specifically, this paper allows the training step size to be controlled
by an agent to improve training stability, and makes the training process more
intelligent with variable learning rates, making GANs less sensitive to step
size. Experiments have been conducted on three benchmark datasets to verify the
effectiveness of the developed FSCO.

</details>


### [160] [Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives](https://arxiv.org/abs/2504.15110)
*Anastasis Kratsios,Takashi Furuya*

Main category: cs.LG

TL;DR: This paper demonstrates that Kolmogorov-Arnold Networks (KANs) can optimally approximate Besov functions and provides a dimension-free sample complexity estimate.


<details>
  <summary>Details</summary>
Motivation: Inspired by the Kolmogorov-Arnold superposition theorem, to establish theoretical foundations for KANs as an improvement over multilayer perceptrons (MLPs).

Method: Proving optimal approximation rates for Besov functions on bounded or fractal domains and estimating sample complexity for residual KAN models.

Result: Achieved optimal approximation guarantees and dimension-free sample complexity estimates.

Conclusion: KANs with residual connections offer strong theoretical support for efficient function approximation in deep learning.

Abstract: Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold
Networks (KANs) have recently emerged as an improved backbone for most deep
learning frameworks, promising more adaptivity than their multilayer perception
(MLP) predecessor by allowing for trainable spline-based activation functions.
In this paper, we probe the theoretical foundations of the KAN architecture by
showing that it can optimally approximate any Besov function in
$B^{s}_{p,q}(\mathcal{X})$ on a bounded open, or even fractal, domain
$\mathcal{X}$ in $\mathbb{R}^d$ at the optimal approximation rate with respect
to any weaker Besov norm $B^{\alpha}_{p,q}(\mathcal{X})$; where $\alpha < s$.
We complement our approximation guarantee with a dimension-free estimate on the
sample complexity of a residual KAN model when learning a function of Besov
regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates
contemporary deep learning wisdom by leveraging residual/skip connections
between layers.

</details>


### [161] [Survey of Loss Augmented Knowledge Tracing](https://arxiv.org/abs/2504.15163)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 这篇论文回顾了使用先进损失函数的深度学习知识追踪算法，包括对比学习方法，提供性能基准，并讨论未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 动机是解决神经网络损失函数的挑战，提高模型性能和鲁棒性，并通过知识追踪实现个性化教育体验。

Method: 方法是通过回顾和基准测试深度学习知识追踪算法，如Bi-CLKT等，使用损失正则化和对比学习技术。

Result: 结果展示了这些算法的性能改进、基准测试数据以及实际部署中的挑战洞见。

Conclusion: 结论指出了未来研究方向，包括混合损失策略和上下文感知建模。

Abstract: The training of artificial neural networks is heavily dependent on the
careful selection of an appropriate loss function. While commonly used loss
functions, such as cross-entropy and mean squared error (MSE), generally
suffice for a broad range of tasks, challenges often emerge due to limitations
in data quality or inefficiencies within the learning process. In such
circumstances, the integration of supplementary terms into the loss function
can serve to address these challenges, enhancing both model performance and
robustness. Two prominent techniques, loss regularization and contrastive
learning, have been identified as effective strategies for augmenting the
capacity of loss functions in artificial neural networks.
  Knowledge tracing is a compelling area of research that leverages predictive
artificial intelligence to facilitate the automation of personalized and
efficient educational experiences for students. In this paper, we provide a
comprehensive review of the deep learning-based knowledge tracing (DKT)
algorithms trained using advanced loss functions and discuss their improvements
over prior techniques. We discuss contrastive knowledge tracing algorithms,
such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,
providing performance benchmarks and insights into real-world deployment
challenges. The survey concludes with future research directions, including
hybrid loss strategies and context-aware modeling.

</details>


### [162] [Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture](https://arxiv.org/abs/2504.15171)
*Meng Cui,Xianghu Yue,Xinyuan Qian,Jinzheng Zhao,Haohe Liu,Xubo Liu,Daoliang Li,Wenwu Wang*

Main category: cs.LG

TL;DR: 本论文引入新数据集和框架，改进鱼类喂食强度评估的增量学习，解决遗忘和存储挑战。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有多模态方法在适应新鱼种或环境时面临的灾难性遗忘和数据集缺乏问题。

Method: 方法包括引入AV-CIL-FFIA数据集和HAIL-FFIA框架，该框架采用原型-based音频-视觉增量学习、分层表示学习、双路径知识保留和动态模态平衡系统。

Result: 结果显示HAIL-FFIA在AV-CIL-FFIA数据集上优于现有方法，获得更高准确率、更低存储需求，并有效缓解灾难性遗忘。

Conclusion: 结论是HAIL-FFIA框架在鱼类喂食强度评估的增量学习中表现出色，适用于不同鱼种场景。

Abstract: Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture
management. Recent multi-modal approaches have shown promise in improving FFIA
robustness and efficiency. However, these methods face significant challenges
when adapting to new fish species or environments due to catastrophic
forgetting and the lack of suitable datasets. To address these limitations, we
first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled
audio-visual clips capturing feeding intensities across six different fish
species in real aquaculture environments. Then, we pioneer audio-visual class
incremental learning (CIL) for FFIA and demonstrate through benchmarking on
AV-CIL-FFIA that it significantly outperforms single-modality methods. Existing
CIL methods rely heavily on historical data. Exemplar-based approaches store
raw samples, creating storage challenges, while exemplar-free methods avoid
data storage but struggle to distinguish subtle feeding intensity variations
across different fish species. To overcome these limitations, we introduce
HAIL-FFIA, a novel audio-visual class-incremental learning framework that
bridges this gap with a prototype-based approach that achieves exemplar-free
efficiency while preserving essential knowledge through compact feature
representations. Specifically, HAIL-FFIA employs hierarchical representation
learning with a dual-path knowledge preservation mechanism that separates
general intensity knowledge from fish-specific characteristics. Additionally,
it features a dynamic modality balancing system that adaptively adjusts the
importance of audio versus visual information based on feeding behaviour
stages. Experimental results show that HAIL-FFIA is superior to SOTA methods on
AV-CIL-FFIA, achieving higher accuracy with lower storage needs while
effectively mitigating catastrophic forgetting in incremental fish species
learning.

</details>


### [163] [How Global Calibration Strengthens Multiaccuracy](https://arxiv.org/abs/2504.15206)
*Sílvia Casacuberta,Parikshit Gopalan,Varun Kanade,Omer Reingold*

Main category: cs.LG

TL;DR: 这篇论文探讨多准确性和多校准在预测中的公平性，发现多准确性本身较弱，但结合全局校准后显著增强，能够从弱无知学习中恢复更强的结果。


<details>
  <summary>Details</summary>
Motivation: 动机是研究多准确性作为学习原语的威力，以及校准的添加如何提升其在学习和计算复杂性中的应用。

Method: 方法包括理论分析，证明多准确性无法后处理成弱学习器，但添加校准后可恢复强无知学习，并比较硬核测度的派生。

Result: 结果显示，多准确性单独时仅在概念相关性大于1/2时有效；添加校准后可获得强无知学习和最优密度的硬核测度。

Conclusion: 结论是，多准确性和校准互补，结合它们能产生更强大的公平性概念。

Abstract: Multiaccuracy and multicalibration are multigroup fairness notions for
prediction that have found numerous applications in learning and computational
complexity. They can be achieved from a single learning primitive: weak
agnostic learning. Here we investigate the power of multiaccuracy as a learning
primitive, both with and without the additional assumption of calibration. We
find that multiaccuracy in itself is rather weak, but that the addition of
global calibration (this notion is called calibrated multiaccuracy) boosts its
power substantially, enough to recover implications that were previously known
only assuming the stronger notion of multicalibration.
  We give evidence that multiaccuracy might not be as powerful as standard weak
agnostic learning, by showing that there is no way to post-process a
multiaccurate predictor to get a weak learner, even assuming the best
hypothesis has correlation $1/2$. Rather, we show that it yields a restricted
form of weak agnostic learning, which requires some concept in the class to
have correlation greater than $1/2$ with the labels. However, by also requiring
the predictor to be calibrated, we recover not just weak, but strong agnostic
learning.
  A similar picture emerges when we consider the derivation of hardcore
measures from predictors satisfying multigroup fairness notions. On the one
hand, while multiaccuracy only yields hardcore measures of density half the
optimal, we show that (a weighted version of) calibrated multiaccuracy achieves
optimal density.
  Our results yield new insights into the complementary roles played by
multiaccuracy and calibration in each setting. They shed light on why
multiaccuracy and global calibration, although not particularly powerful by
themselves, together yield considerably stronger notions.

</details>


### [164] [Compute-Optimal LLMs Provably Generalize Better With Scale](https://arxiv.org/abs/2504.15208)
*Marc Finzi,Sanyam Kapoor,Diego Granziol,Anming Gu,Christopher De Sa,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文探讨了为什么更大的语言模型泛化性能更好，通过开发泛化界限和缩放定律，解释了计算最优状态下模型规模与泛化差距的关系。


<details>
  <summary>Details</summary>
Motivation: 动机是调查更大语言模型为什么泛化更好，针对Chinchilla缩放定律下的预训练目标。

Method: 方法包括引入一种新的基于经验的Freedman型马氏链浓度不等式，分解泛化界限为参数数量、损失方差和量化误差三个组成部分，并从信息论角度分析模型的可量化性。

Result: 结果显示，随着模型规模增加，损失方差和量化误差减少，导致泛化差距缩小，并导出了泛化差距的缩放定律。

Conclusion: 结论是更大模型的泛化界限更强，表明在计算最优前沿，模型规模增长会改善泛化性能。

Abstract: Why do larger language models generalize better? To investigate this
question, we develop generalization bounds on the pretraining objective of
large language models (LLMs) in the compute-optimal regime, as described by the
Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type
martingale concentration inequality that tightens existing bounds by accounting
for the variance of the loss function. This generalization bound can be
decomposed into three interpretable components: the number of parameters per
token, the loss variance, and the quantization error at a fixed bitrate. As
compute-optimal language models are scaled up, the number of parameters per
data point remains constant; however, both the loss variance and the
quantization error decrease, implying that larger models should have smaller
generalization gaps. We examine why larger models tend to be more quantizable
from an information theoretic perspective, showing that the rate at which they
can integrate new information grows more slowly than their capacity on the
compute-optimal frontier. From these findings we produce a scaling law for the
generalization gap, with bounds that become predictably stronger with scale.

</details>


### [165] [A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data](https://arxiv.org/abs/2504.15209)
*Xin Liao,Bing Yang,Tan Dongli,Cai Yu*

Main category: cs.LG

TL;DR: 本文提出CLR模型，用于填充水质监测数据的缺失值，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 水质监测数据因故障导致缺失值，简单填充方法准确性不足，需要改进。

Method: 提出CLR模型，使用因果卷积考虑时间依赖，并自动调整超参数。

Result: 实验显示CLR模型在准确性和时间成本上优于现有模型。

Conclusion: 模型提升数据完整性，为环境监测提供可靠决策支持。

Abstract: The monitoring of water quality is a crucial part of environmental
protection, and a large number of monitors are widely deployed to monitor water
quality. Due to unavoidable factors such as data acquisition breakdowns,
sensors and communication failures, water quality monitoring data suffers from
missing values over time, resulting in High-Dimensional and Sparse (HDS) Water
Quality Data (WQD). The simple and rough filling of the missing values leads to
inaccurate results and affects the implementation of relevant measures.
Therefore, this paper proposes a Causal convolutional Low-rank Representation
(CLR) model for imputing missing WQD to improve the completeness of the WQD,
which employs a two-fold idea: a) applying causal convolutional operation to
consider the temporal dependence of the low-rank representation, thus
incorporating temporal information to improve the imputation accuracy; and b)
implementing a hyperparameters adaptation scheme to automatically adjust the
best hyperparameters during model training, thereby reducing the tedious manual
adjustment of hyper-parameters. Experimental studies on three real-world water
quality datasets demonstrate that the proposed CLR model is superior to some of
the existing state-of-the-art imputation models in terms of imputation accuracy
and time cost, as well as indicating that the proposed model provides more
reliable decision support for environmental monitoring.

</details>


### [166] [Histogram-based Parameter-efficient Tuning for Passive Sonar Classification](https://arxiv.org/abs/2504.15214)
*Amirmohammad Mohammadi,Davelle Carreiro,Alexandra Van Dine,Joshua Peeples*

Main category: cs.LG

TL;DR: 论文提出HPT技术，用于参数高效迁移学习，针对被动声呐数据集，优于传统适配器，提供分布感知方法。


<details>
  <summary>Details</summary>
Motivation: 现有添加式方法如适配器难以捕获中间特征嵌入的分布偏移，因此需要更有效的技术。

Method: 提出基于直方图的HPT技术，捕获目标域统计信息并调节嵌入。

Result: 在ShipsEar、DeepShip和VTUAD数据集上，HPT准确率更高（如VTUAD 91.8% vs. 89.8%），训练更快，特征表示更接近完全微调模型。

Conclusion: HPT在参数节省和性能间平衡，提供分布感知替代方案，适合资源受限环境中的可扩展迁移学习。

Abstract: Parameter-efficient transfer learning (PETL) methods adapt large artificial
neural networks to downstream tasks without fine-tuning the entire model.
However, existing additive methods, such as adapters, sometimes struggle to
capture distributional shifts in intermediate feature embeddings. We propose a
novel histogram-based parameter-efficient tuning (HPT) technique that captures
the statistics of the target domain and modulates the embeddings. Experimental
results on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)
demonstrate that HPT outperforms conventional adapters. Notably, HPT achieves
91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields
feature representations closer to those of fully fine-tuned models. Overall,
HPT balances parameter savings and performance, providing a distribution-aware
alternative to existing adapters and shows a promising direction for scalable
transfer learning in resource-constrained environments. The code is publicly
available:
https://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.

</details>


### [167] [A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention](https://arxiv.org/abs/2504.15223)
*Tao Yang,Yu Cheng,Yaokun Ren,Yujia Lou,Minggu Wei,Honghui Xin*

Main category: cs.LG

TL;DR: 这篇论文提出了一种整合BiLSTM和多尺度注意力机制的序列模式挖掘算法，用于处理复杂序列数据中的潜在模式和上下文依赖性。实验显示它在准确性、精确度和召回率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂序列数据中挖掘潜在模式和建模上下文依赖性的挑战。

Method: 提出序列模式挖掘算法，结合BiLSTM捕获前后向依赖性，以及多尺度注意力机制为关键特征分配自适应权重。

Result: 在多变量时间序列数据集实验中，模型在准确性、精确度和召回率上优于现有方法。消融研究和敏感性分析支持其有效性和鲁棒性。

Conclusion: 证实了所提出架构在复杂模式识别任务中的有效性和鲁棒性，并为模型结构优化提供了经验支持。

Abstract: This paper addresses the challenges of mining latent patterns and modeling
contextual dependencies in complex sequence data. A sequence pattern mining
algorithm is proposed by integrating Bidirectional Long Short-Term Memory
(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both
forward and backward dependencies in sequences, enhancing the model's ability
to perceive global contextual structures. At the same time, the multi-scale
attention module assigns adaptive weights to key feature regions under
different window sizes. This improves the model's responsiveness to both local
and global important information. Extensive experiments are conducted on a
publicly available multivariate time series dataset. The proposed model is
compared with several mainstream sequence modeling methods. Results show that
it outperforms existing models in terms of accuracy, precision, and recall.
This confirms the effectiveness and robustness of the proposed architecture in
complex pattern recognition tasks. Further ablation studies and sensitivity
analyses are carried out to investigate the effects of attention scale and
input sequence length on model performance. These results provide empirical
support for structural optimization of the model.

</details>


### [168] [M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding](https://arxiv.org/abs/2504.15225)
*Sarah Alnegheimish,Zelin He,Matthew Reimherr,Akash Chandrayan,Abhinav Pradhan,Luca D'Angelo*

Main category: cs.LG

TL;DR: This paper introduces M²AD, an unsupervised anomaly detection framework for multivariate time series from multiple systems, outperforming existing methods by 21%.


<details>
  <summary>Details</summary>
Motivation: Existing methods are insufficient for heterogeneous time series data from multiple systems, highlighting the need for better anomaly detection in predictive maintenance.

Method: M²AD uses deep models to capture normal behavior, employs residuals for anomaly detection, and aggregates scores via Gaussian Mixture Model and Gamma calibration, with theoretical guarantees for handling heterogeneity.

Result: Outperforms existing methods by 21% on average in evaluations and is validated on a real-world case study with 130 Amazon assets.

Conclusion: The framework effectively addresses cross-sensor and cross-system dependencies, offering a robust solution for anomaly detection.

Abstract: With the widespread availability of sensor data across industrial and
operational systems, we frequently encounter heterogeneous time series from
multiple systems. Anomaly detection is crucial for such systems to facilitate
predictive maintenance. However, most existing anomaly detection methods are
designed for either univariate or single-system multivariate data, making them
insufficient for these complex scenarios. To address this, we introduce
M$^2$AD, a framework for unsupervised anomaly detection in multivariate time
series data from multiple systems. M$^2$AD employs deep models to capture
expected behavior under normal conditions, using the residuals as indicators of
potential anomalies. These residuals are then aggregated into a global anomaly
score through a Gaussian Mixture Model and Gamma calibration. We theoretically
demonstrate that this framework can effectively address heterogeneity and
dependencies across sensors and systems. Empirically, M$^2$AD outperforms
existing methods in extensive evaluations by 21% on average, and its
effectiveness is demonstrated on a large-scale real-world case study on 130
assets in Amazon Fulfillment Centers. Our code and results are available at
https://github.com/sarahmish/M2AD.

</details>


### [169] [Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning](https://arxiv.org/abs/2504.15240)
*Amirhossein Mollaali,Christian Bolivar Moya,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Guang Lin*

Main category: cs.LG

TL;DR: 这篇论文探讨了在Kolmogorov-Arnold Networks (KANs)中的不确定性量化 (UQ) 方法，使用集成方法和保形预测，引入了Conformalized-KANs，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升KANs在建模复杂函数时的可解释性和鲁棒性，并提高其在科学机器学习中的可靠性和适用性。

Method: 通过对KANs应用集成方法获得UQ的启发式度量，并引入Conformalized-KANs，将保形预测与KAN集成相结合，进行广泛的数值实验。

Result: 方法有效，预测区间鲁棒且准确，可应用于Finite Basis KANs (FBKANs)和多保真KANs (MFKANs)。

Conclusion: 结果表明，这些方法有潜力提升KANs的可靠性和适用性。

Abstract: This paper explores uncertainty quantification (UQ) methods in the context of
Kolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to
obtain a heuristic measure of UQ, enhancing interpretability and robustness in
modeling complex functions. Building on this, we introduce Conformalized-KANs,
which integrate conformal prediction, a distribution-free UQ technique, with
KAN ensembles to generate calibrated prediction intervals with guaranteed
coverage. Extensive numerical experiments are conducted to evaluate the
effectiveness of these methods, focusing particularly on the robustness and
accuracy of the prediction intervals under various hyperparameter settings. We
show that the conformal KAN predictions can be applied to recent extensions of
KANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The
results demonstrate the potential of our approaches to improve the reliability
and applicability of KANs in scientific machine learning.

</details>


### [170] [Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints](https://arxiv.org/abs/2504.15243)
*Ming Yang,Gang Li,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的单循环惩罚-based随机算法，用于机器学习中的约束优化问题，改进了复杂度和性能，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习中多功能不等式约束优化问题，现有的方法存在收敛慢或依赖双循环设计的局限性。

Method: 引入基于铰链的惩罚和单循环随机算法，使用常量惩罚参数，并扩展到有限和耦合组合目标。

Result: 实现了最先进的KKT解近似复杂度和改进的扩展复杂性。

Conclusion: 通过公平学习和持续学习实验验证了方法的有效性。

Abstract: Constrained optimization with multiple functional inequality constraints has
significant applications in machine learning. This paper examines a crucial
subset of such problems where both the objective and constraint functions are
weakly convex. Existing methods often face limitations, including slow
convergence rates or reliance on double-loop algorithmic designs. To overcome
these challenges, we introduce a novel single-loop penalty-based stochastic
algorithm. Following the classical exact penalty method, our approach employs a
{\bf hinge-based penalty}, which permits the use of a constant penalty
parameter, enabling us to achieve a {\bf state-of-the-art complexity} for
finding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our
algorithm to address finite-sum coupled compositional objectives, which are
prevalent in artificial intelligence applications, establishing improved
complexity over existing approaches. Finally, we validate our method through
experiments on fair learning with receiver operating characteristic (ROC)
fairness constraints and continual learning with non-forgetting constraints.

</details>


### [171] [Faster Algorithms for Agnostically Learning Disjunctions and their Implications](https://arxiv.org/abs/2504.15244)
*Ilias Diakonikolas,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TL;DR: 本文提出了一种新的无关PAC学习算法，用于布尔析取，复杂度从2^{	ilde{O}(n^{1/2})}改进到2^{	ilde{O}(n^{1/3})}，并展示了SQ和CSQ模型的首次分离。


<details>
  <summary>Details</summary>
Motivation: 动机是改进现有CSQ算法的极限复杂度，并探索SQ和CSQ模型在无关学习中的差异。

Method: 方法是开发一种基于SQ模型的统计查询算法，实现复杂度2^{	ilde{O}(n^{1/3})}。

Result: 结果是降低了学习复杂度，并证明了SQ模型在该任务中的优势。

Conclusion: 结论是SQ模型在分布无关的无关学习中可能优于CSQ模型，这为学习理论提供了新见解。

Abstract: We study the algorithmic task of learning Boolean disjunctions in the
distribution-free agnostic PAC model. The best known agnostic learner for the
class of disjunctions over $\{0, 1\}^n$ is the $L_1$-polynomial regression
algorithm, achieving complexity $2^{\tilde{O}(n^{1/2})}$. This complexity bound
is known to be nearly best possible within the class of Correlational
Statistical Query (CSQ) algorithms. In this work, we develop an agnostic
learner for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$. Our
algorithm can be implemented in the Statistical Query (SQ) model, providing the
first separation between the SQ and CSQ models in distribution-free agnostic
learning.

</details>


### [172] [On Learning Parallel Pancakes with Mostly Uniform Weights](https://arxiv.org/abs/2504.15251)
*Ilias Diakonikolas,Daniel M. Kane,Sushrut Karmalkar,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.LG

TL;DR: 本文研究了学习k-高斯混合模型的复杂度，在特定权重假设下给出了下界和上界。


<details>
  <summary>Details</summary>
Motivation: 为了应对一般k-GMM学习指数级复杂度的挑战，通过假设相等协方差和权重不指数级小来规避。

Method: 使用统计查询下界，并为特定权重分布开发准多项式时间算法。

Result: 证明了在均匀权重下区分混合模型和标准高斯的SQ-hardness，并当大部分权重均匀时给出了上界。

Conclusion: 复杂度取决于权重分布，在某些情况下准多项式界是紧的。

Abstract: We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on
$\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full
generality. To circumvent this exponential lower bound on the number of
components, research has focused on learning families of GMMs satisfying
additional structural properties. A natural assumption posits that the
component weights are not exponentially small and that the components have the
same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time
algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our
first main result is a Statistical Query (SQ) lower bound showing that this
quasi-polynomial upper bound is essentially best possible, even for the special
case of uniform weights. Specifically, we show that it is SQ-hard to
distinguish between such a mixture and the standard Gaussian. We further
explore how the distribution of weights affects the complexity of this task.
Our second main result is a quasi-polynomial upper bound for the aforementioned
testing task when most of the weights are uniform while a small fraction of the
weights are potentially arbitrary.

</details>


### [173] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文设计最小算法任务评估语言模型创造力，论证next-token学习局限性，并提出多token方法和输入层噪声注入的优势。


<details>
  <summary>Details</summary>
Motivation: 动机是抽象现实任务，精确量化语言模型的创造力极限，提供可控测试平台。

Method: 方法包括设计隐式随机规划任务、比较next-token学习与多token方法（如无教师训练和扩散模型），并使用hash-conditioning在输入层注入噪声。

Result: 结果显示next-token学习短视且过度记忆，多token方法在多样性和原创性输出上更优秀；输入层噪声注入比输出层温度采样更有效。

Conclusion: 结论是提供分析开放式创造技能的测试平台，并主张超越next-token学习和softmax-based采样。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [174] [Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry](https://arxiv.org/abs/2504.14164)
*Kisung You,Dennis Shung,Mauro Giuffrè*

Main category: stat.ML

TL;DR: 本论文提出了一种新的几何感知距离度量，用于von Mises-Fisher分布，以改善球面数据的比较和混合模型压缩。


<details>
  <summary>Details</summary>
Motivation: 由于vMF分布的归一化常数难以处理，且缺乏合适的几何度量，迫切需要基于最优传输理论的比较工具。

Method: 提出Wasserstein-like距离，通过高浓度Gaussian近似，得到闭合形式表达式，包括测地线和方差项。

Result: 该距离具有良好理论性质，并应用于vMF混合模型的压缩，实证结果在合成和真实数据集上显示有效性。

Conclusion: 扩展了方向数据分析的统计工具，提供了一个适合球面几何的、可解释的距离度量。

Abstract: We introduce a novel, geometry-aware distance metric for the family of von
Mises-Fisher (vMF) distributions, which are fundamental models for directional
data on the unit hypersphere. Although the vMF distribution is widely employed
in a variety of probabilistic learning tasks involving spherical data,
principled tools for comparing vMF distributions remain limited, primarily due
to the intractability of normalization constants and the absence of suitable
geometric metrics. Motivated by the theory of optimal transport, we propose a
Wasserstein-like distance that decomposes the discrepancy between two vMF
distributions into two interpretable components: a geodesic term capturing the
angular separation between mean directions, and a variance-like term
quantifying differences in concentration parameters. The derivation leverages a
Gaussian approximation in the high-concentration regime to yield a tractable,
closed-form expression that respects the intrinsic spherical geometry. We show
that the proposed distance exhibits desirable theoretical properties and
induces a latent geometric structure on the space of non-degenerate vMF
distributions. As a primary application, we develop the efficient algorithms
for vMF mixture reduction, enabling structure-preserving compression of mixture
models in high-dimensional settings. Empirical results on synthetic datasets
and real-world high-dimensional embeddings, including biomedical sentence
representations and deep visual features, demonstrate the effectiveness of the
proposed geometry in distinguishing distributions and supporting interpretable
inference. This work expands the statistical toolbox for directional data
analysis by introducing a tractable, transport-inspired distance tailored to
the geometry of the hypersphere.

</details>


### [175] [Optimal Scheduling of Dynamic Transport](https://arxiv.org/abs/2504.14425)
*Panos Tsimpos,Zhi Ren,Jakob Zech,Youssef Marzouk*

Main category: stat.ML

TL;DR: 本论文提出使用弯曲轨迹最小化速度场的Lipschitz常数，显著改善基于流的采样和生成建模方法的近似和学习性能。


<details>
  <summary>Details</summary>
Motivation: 动机是利用时间轴的设计自由度，通过弯曲轨迹降低速度场的Lipschitz常数，从而控制从数据中学习速度场时的近似误差。

Method: 方法是考虑给定传输映射T的单位时间插值，并求解最小化速度场空间Lipschitz常数的调度τ，使用变分微积分和Γ-收敛计算最优调度。

Result: 结果是，对于广泛的源/目标测度和传输映射T，最优调度可闭式形式计算，其Lipschitz常数比恒等调度小几个数量级。

Conclusion: 结论是，这种弯曲轨迹方法可显著提高基于流方法的性能。

Abstract: Flow-based methods for sampling and generative modeling use continuous-time
dynamical systems to represent a {transport map} that pushes forward a source
measure to a target measure. The introduction of a time axis provides
considerable design freedom, and a central question is how to exploit this
freedom. Though many popular methods seek straight line (i.e., zero
acceleration) trajectories, we show here that a specific class of ``curved''
trajectories can significantly improve approximation and learning. In
particular, we consider the unit-time interpolation of any given transport map
$T$ and seek the schedule $\tau: [0,1] \to [0,1]$ that minimizes the spatial
Lipschitz constant of the corresponding velocity field over all times $t \in
[0,1]$. This quantity is crucial as it allows for control of the approximation
error when the velocity field is learned from data. We show that, for a broad
class of source/target measures and transport maps $T$, the \emph{optimal
schedule} can be computed in closed form, and that the resulting optimal
Lipschitz constant is \emph{exponentially smaller} than that induced by an
identity schedule (corresponding to, for instance, the Wasserstein geodesic).
Our proof technique relies on the calculus of variations and
$\Gamma$-convergence, allowing us to approximate the aforementioned degenerate
objective by a family of smooth, tractable problems.

</details>


### [176] [On the Tunability of Random Survival Forests Model for Predictive Maintenance](https://arxiv.org/abs/2504.14744)
*Yigitcan Yardımcı,Mustafa Cavus*

Main category: stat.ML

TL;DR: 本文研究了随机生存森林在预测性维护中的可调性，发现超参数调整可以改善性能指标，如C-index和Brier分数，并指出了关键的超参数。


<details>
  <summary>Details</summary>
Motivation: 解决RSF性能对超参数敏感的问题，以及在预测性维护中缺乏系统评估。

Method: 引入了一个三层框架来量化可调性，使用C-index和Brier分数在CMAPSS数据集上评估。

Result: 调整后C-index平均增加0.0547，Brier分数减少0.0199；ntree和mtry具有高可调性，nodesize在10-30范围内稳定，splitrule平均具有负面可调性。

Conclusion: 强调了在生存模型中超参数调整的实际重要性，并为RSF在真实预测性维护应用中的优化提供可操作的见解。

Abstract: This paper investigates the tunability of the Random Survival Forest (RSF)
model in predictive maintenance, where accurate time-to-failure estimation is
crucial. Although RSF is widely used due to its flexibility and ability to
handle censored data, its performance is sensitive to hyperparameter
configurations. However, systematic evaluations of RSF tunability remain
limited, especially in predictive maintenance contexts. We introduce a
three-level framework to quantify tunability: (1) a model-level metric
measuring overall performance gain from tuning, (2) a hyperparameter-level
metric assessing individual contributions, and (3) identification of optimal
tuning ranges. These metrics are evaluated across multiple datasets using
survival-specific criteria: the C-index for discrimination and the Brier score
for calibration. Experiments on four CMAPSS dataset subsets, simulating
aircraft engine degradation, reveal that hyperparameter tuning consistently
improves model performance. On average, the C-index increased by 0.0547, while
the Brier score decreased by 0.0199. These gains were consistent across all
subsets. Moreover, ntree and mtry showed the highest average tunability, while
nodesize offered stable improvements within the range of 10 to 30. In contrast,
splitrule demonstrated negative tunability on average, indicating that improper
tuning may reduce model performance. Our findings emphasize the practical
importance of hyperparameter tuning in survival models and provide actionable
insights for optimizing RSF in real-world predictive maintenance applications.

</details>


### [177] [Expected Free Energy-based Planning as Variational Inference](https://arxiv.org/abs/2504.14898)
*Bert de Vries,Wouter Nuijten,Thijs van de Laar,Wouter Kouw,Sepideh Adamiat,Tim Nisslbeck,Mykola Lukashchuk,Hoang Minh Huu Nguyen,Marco Hidalgo Araya,Raphael Tresor,Thijs Jenneskens,Ivana Nikoloska,Raaja Subramanian,Bart van Erp,Dmitry Bagaev,Albert Podusenko*

Main category: stat.ML

TL;DR: 本文使用主动推理和自由能量原理统一探索与利用，解决不确定性规划问题，提高计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将探索和利用分开，缺乏统一基础；主动推理虽提供基础，但计算负担过重。

Method: 通过最小化增强偏好和认识先验的生成模型上的变分自由能量函数，将规划表述为变分推理。

Result: 获得支持目标实现和信息获取的最优策略，包含复杂度项，实现可扩展的资源感知主动推理代理。

Conclusion: 强化了与自由能量原理的理论一致性，连接并扩展现有方法。

Abstract: We address the problem of planning under uncertainty, where an agent must
choose actions that not only achieve desired outcomes but also reduce
uncertainty. Traditional methods often treat exploration and exploitation as
separate objectives, lacking a unified inferential foundation. Active
inference, grounded in the Free Energy Principle, offers such a foundation by
minimizing Expected Free Energy (EFE), a cost function that combines utility
with epistemic drives like ambiguity resolution and novelty seeking. However,
the computational burden of EFE minimization has remained a major obstacle to
its scalability. In this paper, we show that EFE-based planning arises
naturally from minimizing a variational free energy functional on a generative
model augmented with preference and epistemic priors. This result reinforces
theoretical consistency with the Free Energy Principle, by casting planning
itself as variational inference. Our formulation yields optimal policies that
jointly support goal achievement and information gain, while incorporating a
complexity term that accounts for bounded computational resources. This
unifying framework connects and extends existing methods, enabling scalable,
resource-aware implementations of active inference agents.

</details>


### [178] [Advanced posterior analyses of hidden Markov models: finite Markov chain imbedding and hybrid decoding](https://arxiv.org/abs/2504.15156)
*Zenia Elise Damgaard Bæk,Moisès Coll Macià,Laurits Skov,Asger Hobolth*

Main category: stat.ML

TL;DR: 本论文介绍了使用有限马尔可夫链嵌入（FMCI）计算隐藏马尔可夫模型（HMM）隐藏状态摘要统计分布，并通过混合解码改进状态序列解码的方法。


<details>
  <summary>Details</summary>
Motivation: 解决HMM中计算隐藏状态序列摘要统计分布（如访问次数、停留时间）和解码隐藏状态序列的两个主要任务。

Method: 采用FMCI框架基于模拟计算后验分布；使用混合分割解码，并引入调参参数选择新方法和基于加权几何均值的损失函数推导。

Result: 混合解码性能优于Viterbi或后验解码；在经典数据集上验证，并提供可重现代码。

Conclusion: 展示了FMCI和混合解码的有效性，并强调了实际应用和代码支持的重要性。

Abstract: Two major tasks in applications of hidden Markov models are to (i) compute
distributions of summary statistics of the hidden state sequence, and (ii)
decode the hidden state sequence. We describe finite Markov chain imbedding
(FMCI) and hybrid decoding to solve each of these two tasks. In the first part
of our paper we use FMCI to compute posterior distributions of summary
statistics such as the number of visits to a hidden state, the total time spent
in a hidden state, the dwell time in a hidden state, and the longest run
length. We use simulations from the hidden state sequence, conditional on the
observed sequence, to establish the FMCI framework. In the second part of our
paper we apply hybrid segmentation for improved decoding of a HMM. We
demonstrate that hybrid decoding shows increased performance compared to
Viterbi or Posterior decoding (often also referred to as global or local
decoding), and we introduce a novel procedure for choosing the tuning parameter
in the hybrid procedure. Furthermore, we provide an alternative derivation of
the hybrid loss function based on weighted geometric means. We demonstrate and
apply FMCI and hybrid decoding on various classical data sets, and supply
accompanying code for reproducibility.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [179] [Charging While Driving Lanes: A Boon to Electric Vehicle Owners or a Disruption to Traffic Flow](https://arxiv.org/abs/2504.14360)
*Shayan Bafandkar,Alireza Talebpour*

Main category: cs.ET

TL;DR: 这篇论文研究了充电时驾驶（CWD）车道对交通流、排放和能源消耗的影响，考虑不同电动汽车市场渗透率（MPR），结果显示它可能增加拥堵但减少排放。


<details>
  <summary>Details</summary>
Motivation: 解决电动汽车的续航焦虑和充电挑战，以提高采用率并放大系统级影响。

Method: 提出一个框架，分析自治、速度协调、环境因素和不同MPR下的CWD车道影响，并评估各种政策。

Result: 引入CWD车道可能降低交通吞吐量并增加拥堵，由于电动汽车换道行为；较高MPR能稳定交通流、减少冲击波，但速度干扰增加；排放减少高达63%。

Conclusion: CWD车道能促进电动汽车采用，但可能恶化交通效率，强调需要仔细设计和政策考虑。

Abstract: Large-scale adoption of commercial and personal Electric Vehicles (EVs) is
expected to significantly affect traffic flow dynamics, emissions, and energy
consumption in the transportation sector. Range anxiety and challenges
associated with charging EVs are among the key issues that reduce the adoption
rate of EVs and, in turn, limit their system-level impacts. A promising
solution to address these challenges is the introduction of charging while
driving (CWD) lanes. Although technological advancements have made it possible
to charge vehicles wirelessly while driving, introducing such lanes to the
traffic stream can potentially disturb traffic flow and result in new
congestion patterns. This study puts forward a framework to investigate the
effects of CWD lanes on traffic flow, considering %autonomy, speed
harmonization, and environmental factors for different market penetration rates
(MPRs) of personal and commercial EVs. Different policies have been
investigated to suggest the best design for CWD lanes. Results indicate that
introducing CWD lanes can decrease overall traffic throughput and increase
congestion due to additional lane-changing maneuvers by electric vehicles
aiming to utilize the CWD lane. Although higher MPRs of EVs help stabilize
traffic flow and reduce the number of shockwaves, speed disruption tends to
increase in the CWD lane and propagate to adjacent lanes. Emission analyses
show significant reductions (up to 63\%) in pollution levels with increasing
MPRs of personal and commercial EVs. Our analysis shows that while CWD lanes
can facilitate the adoption of EVs, they can deteriorate traffic efficiency,
emphasizing the importance of careful design and policy considerations.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [180] [Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model](https://arxiv.org/abs/2504.14345)
*Youngbin Lee,Yejin Kim,Suin Kim,Yongjae Lee*

Main category: q-fin.PM

TL;DR: 本研究使用大语言模型生成投资者观点，整合到Black-Litterman框架中优化投资组合，并通过回测评估不同LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 传统均值-方差模型敏感性高，Black-Litterman模型定义观点困难，因此探索LLM生成观点以改进优化。

Method: 利用LLM从历史价格和公司元数据估计股票回报，纳入预测方差不确定性；采用Black-Litterman框架，回测2024年6月至2025年2月，每两周重新平衡，并与S&P 500等基准比较。

Result: 不同LLM显示预测乐观度和置信度稳定性差异，影响投资组合性能。

Conclusion: LLM可提升投资组合优化，但效果依赖于LLM特性。

Abstract: Portfolio optimization faces challenges due to the sensitivity in traditional
mean-variance models. The Black-Litterman model mitigates this by integrating
investor views, but defining these views remains difficult. This study explores
the integration of large language models (LLMs) generated views into portfolio
optimization using the Black-Litterman framework. Our method leverages LLMs to
estimate expected stock returns from historical prices and company metadata,
incorporating uncertainty through the variance in predictions. We conduct a
backtest of the LLM-optimized portfolios from June 2024 to February 2025,
rebalancing biweekly using the previous two weeks of price data. As baselines,
we compare against the S&P 500, an equal-weighted portfolio, and a traditional
mean-variance optimized portfolio constructed using the same set of stocks.
Empirical results suggest that different LLMs exhibit varying levels of
predictive optimism and confidence stability, which impact portfolio
performance. The source code and data are available at
https://github.com/youngandbin/LLM-MVO-BLM.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [181] [6G WavesFM: A Foundation Model for Sensing, Communication, and Localization](https://arxiv.org/abs/2504.14100)
*Ahmed Aboulfotouh,Elsayed Mohammed,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 本文介绍了WavesFM，一种无线基础模型框架，能够高效支持通信、感知和定位等多种任务。


<details>
  <summary>Details</summary>
Motivation: 为了通过参数共享减少计算和内存开销，并推动AI原生6G网络的发展。

Method: 采用共享的Vision Transformer骨干网络、任务特定的MLP头和LoRA进行参数高效微调，处理光谱图、CSI和OFDM资源网格等无线数据。

Result: 在参数共享80%的基础上，实现优越性能、5倍训练时间减少和强泛化能力。

Conclusion: 证明了基础模型在提升无线网络效率和性能方面的变革潜力。

Abstract: This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [182] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TL;DR: 本文提出Entropy Rectifying Guidance (ERG)，一种基于注意力机制的指导技术，能同时提升扩散模型图像生成的质量、多样性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有指导技术如CFG在改善图像质量和一致性时存在权衡，且其他方法需额外模型或更多前向传递，本文旨在提供一种简单有效的替代方案。

Method: 提出ERG，通过推理时修改扩散变压器架构的注意力机制，实现对条件和无条件采样的指导。

Result: ERG显著改善文本到图像、类别条件和无条件图像生成任务，并可与其他指导方法结合，进一步提升性能。

Conclusion: ERG是一种通用、有效的指导技术，能同时优化生成图像的多个方面，提供更好的整体表现。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>


### [183] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 这篇论文引入Fashion-RAG方法，通过检索增强生成，基于文本输入实现时尚虚拟试穿和图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖特定服装输入，而用户常只提供文本描述，因此需要一种更实用基于文本的生成方法。

Method: 使用检索技术匹配文本描述的服装图像，并通过文本反演将图像投影到Stable Diffusion的文本嵌入空间中，生成个性化图像。

Result: 在Dress Code数据集上的实验显示，Fashion-RAG在定性和定量上优于现有方法，能够捕捉细粒度的视觉细节。

Conclusion: 这是首个针对多模态时尚图像编辑的检索增强生成方法，填补了研究空白。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>


### [184] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TL;DR: 本论文提出一种改进视觉基础模型特征上采样的方法，使用坐标-based交叉注意力Transformer和基于无类别掩码的自蒸馏训练，显著提升像素级任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉基础模型如DINOv2和CLIP的特征分辨率不足问题，以提升像素级理解应用中的表现。

Method: 引入坐标-based交叉注意力Transformer上采样器，并通过无类别掩码和自蒸馏构建高分辨率伪ground truth特征。

Result: 实验显示，该方法在各种下游任务中显著优于现有上采样技术。

Conclusion: 该方法有效捕获细粒度细节，并灵活适应不同输入和特征分辨率。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>


### [185] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的任务OOSIS（Occlusion-Ordered Semantic Instance Segmentation），通过基于遮挡的相对深度排序来增强实例分割，提供更可靠的3D信息，而非使用困难的绝对深度估计。


<details>
  <summary>Details</summary>
Motivation: 动机是解决从单张图像中获得3D分析的挑战，传统方法依赖困难的单目深度估计，而本方法使用更简单的基于遮挡的相对深度排序。

Method: 方法包括开发一种基于定向遮挡边界和语义分割的approach，同时提取实例和它们的遮挡顺序，将OOSIS表述为一个标记问题。

Result: 结果显示，该方法在定向遮挡边界上显著优于现有工作，并在KINS和COCOA数据集上取得了比强基线更好的性能，使用了一个新的联合OOSIS指标。

Conclusion: 结论是，该方法有效地结合了实例分割和深度信息，提供了一个改进的3D分析框架。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>


### [186] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TL;DR: 本研究开发了一个高效的深度学习系统，用于甲状腺FNAB图像的多类分类，实现了高准确性和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决自动分类甲状腺FNAB图像的挑战，包括数据有限、观察者间变异性和计算成本，并开发高效、可解释的模型指导治疗决策。

Method: 框架包括YOLOv10-based细胞簇检测、课程学习启发的协议、自适应轻量级EfficientNetB0和Transformer启发模块，并使用1015张外部验证图像。

Result: 内部测试集宏F1为89.19%，AUC分别为0.98（B2）、0.95（B5）和0.96（B6）；外部验证AUC分别为0.9495（B2）、0.7436（B5）和0.8396（B6）；30秒内处理1000个病例。

Conclusion: 高准确性、可解释的分类可以在最小计算需求下实现。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>


### [187] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TL;DR: 本论文介绍了LOCATE 3D模型，用于从指称表达式定位3D场景物体，并引入3D-JEPA自监督学习算法和新的数据集。


<details>
  <summary>Details</summary>
Motivation: 为了提升3D场景中基于指称表达式的物体定位性能，实现机器人和AR设备上的实际部署，并研究泛化能力。

Method: 使用3D-JEPA自监督学习算法，对传感器点云进行特征化，通过掩码预测在潜在空间预训练，然后微调编码器和语言条件解码器预测3D掩码和边界框。

Result: 在标准基准上达到新最先进水平，展示强大泛化能力，并引入包含超过13万标注的LOCATE 3D数据集。

Conclusion: 本方法和数据集推进了3D参照定位领域，提供更强的模型和系统研究基础。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>


### [188] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TL;DR: 本文提出KeCO框架，优化视觉语言模型的上下文学习，通过低成本核心集构建提升图像分类性能，平均改善20%以上。


<details>
  <summary>Details</summary>
Motivation: 解决选择演示样本的高计算内存成本和信息损失问题，尤其适用于图像分类任务。

Method: KeCO使用视觉特征作为键锚点，通过更新策略利用未用数据演化核心集。

Result: 实验显示图像分类任务ICL性能平均提升20%以上，并在在线场景中表现强劲。

Conclusion: KeCO框架在资源受限的实际应用中具有重要实用价值。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>


### [189] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TL;DR: 本论文提出一种新框架，使用多模态编码策略实现ID保留生成，将身份和文本作为统一条件输入。


<details>
  <summary>Details</summary>
Motivation: 动机是克服现有方法依赖适配器注入身份特征的局限，提供更统一的身份和文本处理。

Method: 方法包括引入FaceCLIP多模态编码器，学习身份和文本联合嵌入空间，并与Stable Diffusion XL整合，使用多模态对齐算法训练。

Result: 结果是生成更逼真的肖像图像，具有更好的身份保留和文本相关性，实验证明定量和定性优势。

Conclusion: 结论是该框架在ID保留图像合成中表现出优越性。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [190] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TL;DR: 这篇论文提出了一种隐私保护图像匿名化技术，通过实用性分支的惩罚优化匿名器，以平衡隐私泄露和动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决视频监控系统中隐私保护与性能权衡的挑战，确保符合欧盟AI法案和GDPR标准。

Method: 方法是引入基于特征的惩罚方案，使用实用性分支的惩罚来优化匿名器，允许自由匿名化私有属性。

Result: 结果显示，施加惩罚可提升动作识别性能，同时保持隐私泄露水平一致。

Conclusion: 结论是，该方法首次提出特征-based惩罚方案，有效保护隐私并维持性能，符合相关法规。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>


### [191] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: 本文提出使用修改后的U-Net生成化学图谱，比PLS回归更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PLS回归不考虑空间上下文并有高噪声，需要改进。

Method: 采用端到端深度学习，使用修改的U-Net和自定义损失函数直接生成化学图谱。

Result: U-Net测试RMSE低9%至13%，空间相关性高99.91%，PLS预测超出范围。

Conclusion: 研究表明U-Net在化学图谱生成中优于PLS。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [192] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TL;DR: 本论文提出了一种无需DDIM反演的一键可控视频编辑方法，通过视觉提示和一致性采样确保内容和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法DDIM反演积累错误，影响内容一致性，需要新方法克服。

Method: 使用视觉提示进行编辑，提出内容一致性采样(CCS)和时间-内容一致性采样(TCS)，基于Stein Variational Gradient Descent。

Result: 广泛实验验证了方法的有效性。

Conclusion: 改进了视频编辑性能，确保了内容和时间一致性。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>


### [193] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于LLM的多模态重标注策略，以提升视觉语言模型的多语言感知多样性，在德语和日语文本图像检索中取得了显著改善。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型数据主要来自英语使用者，存在感知偏差和灵活性不足，需要提升对不同文化感知的理解。

Method: 提出LLM-based多模态重标注策略，通过修改英语标题的对象描述再翻译，并使用母语者数据指导机制，将改写作为训练增强。

Result: 在德语和日语文本图像检索中，平均召回率提升多达3.5（整体）和4.7（非母语错误情况），并分析了数据集间的对象描述差异。

Conclusion: 提供了对跨数据集和跨语言泛化的见解，强调了模型的鲁棒性提升。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>


### [194] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TL;DR: 本文提出LOOPE方法优化Vision Transformers的位置嵌入补丁顺序，并引入“三细胞实验”基准测试，显著提升位置信息保留和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 位置嵌入在Vision Transformers中提供空间信息，但2D网格到1D序列的映射和补丁顺序问题被忽略，导致性能损失。

Method: 提出LOOPE，一种可学习的补丁顺序优化方法，结合频率设置来改善空间表示。

Result: 实验显示LOOPE提高分类准确率，新基准测试揭示使用位置嵌入的性能差距达30-35%，证实其在保留位置信息方面的有效性。

Conclusion: LOOPE增强了位置嵌入在保持相对和绝对位置信息方面的效能，提供更可靠的Vision Transformers优化。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>


### [195] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TL;DR: 这篇调查综述了CLIP在领域泛化和领域适配中的应用，分类方法并讨论挑战。


<details>
  <summary>Details</summary>
Motivation: 文献中缺乏对CLIP在DG和DA中应用的全面调查，因此有必要进行此回顾。

Method: 在DG中，将方法分为优化提示学习和使用CLIP作为骨干网；在DA中，考察源数据可用和无源数据方法，强调知识转移机制。

Result: 通过综合文献，识别关键挑战和未来机会，提供见解以提升CLIP的应用。

Conclusion: 旨在促进创新和合作，开发更具弹性的机器学习模型。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>


### [196] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Hydra 是一种自适应框架，通过迭代推理和批评机制，提升视觉语言模型的对抗鲁棒性和减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 为了提高视觉语言模型在高风险应用中的事实准确性，需要统一解决对抗防御和幻觉问题，但现有方法存在局限。

Method: Hydra 采用行动-批评循环、链式思考和上下文学习，动态检索并精炼视觉信息输出。

Result: 在多个基准和攻击策略上评估，Hydra 超越现有方法，提升鲁棒性和事实一致性。

Conclusion: Hydra 提供可扩展、无需训练的解决方案，桥接对抗抵抗和幻觉缓解，提高模型可靠性。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>


### [197] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是填补RGB-Event流跟踪算法中对抗攻击和防御研究的空白，因为这方面的研究相对稀少。

Method: 方法包括针对RGB-Event体素的对抗损失优化、针对离散Event体素的二步攻击策略，以及针对RGB-Event帧的跨模态通用扰动优化。

Result: 实验结果显示，在COESOT、FE108和VisEvent数据集上，跟踪器的性能显著下降。

Conclusion: 结论是该方法在单模和多模场景下有效，源代码将发布。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>


### [198] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 这篇论文通过引入幻觉检测和缓解策略，针对ResNetVLLM模型减少多模态幻觉，将准确率从54.8%提高到65.3%。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型和视频语言模型中幻觉问题，以提高生成内容的真实性和可靠性。

Method: 采用两步协议：(1) 使用修改后的Lynx模型进行忠实度检测，评估生成标题与视频参考的语义对齐；(2) 通过检索增强生成(RAG)与动态知识库缓解幻觉。

Result: 在ActivityNet-QA基准测试中，准确率从54.8%提高到65.3%，证明策略的有效性。

Conclusion: 这些策略显著提升了视频语言模型的可靠性和事实一致性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>


### [199] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 本论文引入ResNetVLLM框架，用于零样本视频理解，结合ResNet视觉编码器和大型语言模型，实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频模型挑战，避免依赖预训练视频模型，使用非预训练ResNet提取视觉特征，在统一架构中学习视觉和语义表示。

Method: 采用ResNet-based视觉编码器提取特征，并与大型语言模型集成，形成跨模态框架，从视频输入生成准确的文本描述。

Result: 在MSRVTT-QA、MSVD-QA、TGIF-QA FrameQA和ActivityNet-QA等基准上，实现了最先进性能。

Conclusion: ResNetVLLM增强了模型生成准确和上下文相关的文本描述能力，展示了其在零样本视频理解中的有效性。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>


### [200] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TL;DR: This paper proposes a pipeline to detect and remove children's images from AI datasets to address ethical concerns like privacy and consent.


<details>
  <summary>Details</summary>
Motivation: Ethical issues arise from using children's images in datasets scraped from the internet, including risks of exploitation and limited existing solutions.

Method: Built a pipeline using a Vision-Language Model for Visual Question Answering, tested on the #PraCegoVer dataset and a subset of Open Images V7.

Result: The pipeline effectively detects and removes children's images, serving as a baseline for future tools.

Conclusion: Calls for community reflection and action to protect children's rights in dataset creation and development.

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>


### [201] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 这篇论文引入了DreamID，一种基于扩散的换脸模型，实现了高ID相似度、属性保留、图像保真度和快速推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统的换脸训练依赖隐式监督，难以获得满意结果，因此本文通过显式监督提升身份相似度和属性保留。

Method: 本文构建Triplet ID Group数据以实现显式监督，使用SD Turbo减少推理步骤，并提出SwapNet、FaceNet和ID Adapter的架构来优化训练。

Result: DreamID在身份相似度、姿态和表情保留、图像保真度方面优于现有方法，能在0.6秒内以512*512分辨率生成高质量图像，并处理复杂场景。

Conclusion: 总之，DreamID通过显式监督和高效架构实现了高质量换脸，证明了其在换脸任务中的优越性。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [202] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 这篇论文提出VGNC方法，使用生成式新视图合成模型减少稀疏视图3D重建中基于3D高斯喷溅的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯喷溅的稀疏视图3D重建方法存在严重过拟合问题，本文首次尝试使用生成式验证图像缓解此问题。

Method: 引入基于生成式新视图合成模型的验证图像生成方法，并提出利用这些图像控制高斯点数以优化重建的策略。

Result: 实验显示VGNC减少过拟合，提高测试集渲染质量，降低高斯点数，从而减少存储需求并加速训练和渲染。

Conclusion: VGNC有效缓解过拟合并提升性能，代码将公开。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>


### [203] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TL;DR: 这篇论文提出XMANet模型，用于EMG-based手势识别，通过细粒度分类和跨层互注意机制提高准确性。


<details>
  <summary>Details</summary>
Motivation: 动机是改善EMG手势识别的性能，以应用于假肢、康复和人机交互领域。

Method: 方法包括使用XMANet模型融合浅层和深层特征，通过STFT和WT处理信号，并在Grabmyo和FORS数据集上与ResNet50等基准模型比较。

Result: 结果显示，XMANet在STFT下比ResNet50等模型提高了1.72%至5.10%，在WT下也有1.46%至9.36%的改善。

Conclusion: 结论是XMANet在不同架构和信号处理技术下 consistently 提升性能，证明细粒度特征对EMG分类的潜力。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>


### [204] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TL;DR: 使用状态空间模型的VM-BHINet提升双手手交互重建，减少2-3%错误。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、模糊外观和计算效率方面的挑战。

Method: 提出VM-BHINet和VM-IFEBlock，结合SSMs与局部/全局特征操作建模交互。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE错误降低2-3%，优于最先进方法。

Conclusion: 证明SSMs有效改善手部重建的准确性和效率。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>


### [205] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: TAPIP3D 是一种新方法，通过 3D 稳定特征云实现单目 RGB 和 RGB-D 视频的长期 3D 点跟踪，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有跟踪方法在相机运动和 3D 空间关系处理上的不足，利用深度信息补偿相机运动以提升性能。

Method: 将视频表示为相机稳定的时空特征云，利用深度和运动信息将 2D 特征提升到 3D 空间，迭代优化运动估计，并引入局部对注意力机制处理 3D 不规则分布。

Result: 显著优于现有 3D 点跟踪方法，并在有深度信息时提升 2D 跟踪准确性，支持不同坐标系推理，实验显示补偿相机运动改善性能。

Conclusion: TAPIP3D 替换传统 2D 相关邻域方法，在各种基准上提供更鲁棒和准确的跟踪结果。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>


### [206] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TL;DR: 本论文提出了一种实时驾驶员眼睛状态检测技术，用于检测驾驶员疲劳。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是许多事故的重要因素，因此需要开发检测系统。

Method: 首先检测面部并定位眼睛，然后使用基于归一化交叉相关和Kalman滤波的动态模板匹配跟踪眼睛位置，并采用支持向量机结合HOG特征分类眼睛状态为睁开或闭合。如果眼睛闭合一段时间，则触发警报。

Result: 系统能够实时检测眼睛状态，并在驾驶员入睡时触发警报。

Conclusion: 该技术可有效检测驾驶员疲劳，减少事故发生。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>


### [207] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TL;DR: 提出ReSpec框架，用于在线过滤视频-文本数据，提高视频检索任务效率，使用5%数据即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 视频-文本数据快速增长导致存储和计算挑战；在线学习可实现实时处理和快速适应。

Method: 提出ReSpec框架，基于四个标准：模态对齐、任务相关性、特异性（使用根嵌入距离作为信息性代理）和效率；实时过滤数据，无需大量存储和计算。

Result: 在五个零样本视频检索任务上，使用仅5%数据，达到最先进性能，同时计算量最小。

Conclusion: ReSpec是一种高效方法，代码开源可用。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>


### [208] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TL;DR: 本论文引入Video-MMLU基准，评估语言多模态模型在多学科讲座理解中的能力，发现当前模型的局限性，并探讨视觉标记和语言模型的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管语言多模态模型在视频理解方面取得进展，但多学科讲座理解仍未被充分探索，因此需要一个新的基准来评估模型。

Method: 引入Video-MMLU基准，并评估超过90个开源和专有模型（参数从0.5B到40B），探索视觉标记数量和大型语言模型对性能的影响。

Result: 结果显示当前模型在处理讲座中的感知和推理任务时存在局限性，并提供了多模态感知和推理之间相互作用的见解。

Conclusion: 这强调了开发更先进的语言多模态模型以更好地理解多学科讲座的必要性。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>


### [209] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: This paper extends Gaussian splatting for 3D spinal reconstruction from sparse X-rays, reducing radiation exposure and improving surgical navigation without pretraining.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of supervised learning methods that require large annotated data and poor generalization in reconstructing 3D spinal anatomy from sparse fluoroscopic data for spine surgery.

Method: Extends R²-Gaussian splatting with anatomy-guided style transfer for radiographic standardization, enabling reconstruction without pretraining.

Result: Evaluation on ex-vivo data showed clinical utility with 20-30 views, enhanced anatomical clarity via standardization, and improved quantitative metrics (PSNR/SSIM).

Conclusion: Demonstrates the feasibility of instance-based reconstruction from sparse X-rays, advancing intraoperative 3D imaging for surgical navigation.

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>


### [210] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 这篇论文针对模仿学习在驾驶政策中的局限性，提出闭环模拟器、因果基准和整合IL与RL的框架。


<details>
  <summary>Details</summary>
Motivation: 模仿学习可能无法理解驾驶原则，过度拟合常见场景，泛化能力差，需要改进方法。

Method: 提出：1) 支持IL和RL的闭环模拟器；2) 从Waymo数据集衍生的因果基准；3) 整合IL和RL的框架。

Result: 新方法可改善泛化能力和评估，代码即将发布。

Conclusion: 整合学习方法可克服模仿学习的缺点，提升驾驶规划性能。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>


### [211] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 提出SuperCL方法，使用超像素改进医用图像分割的对比学习，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医用图像分割数据标注困难，现有的对比学习方法忽略图像内相似像素群组，且对比对生成依赖手动阈值，效率低下。

Method: 提出SuperCL，包括Intra-image Local Contrastive Pairs (ILCP) 和 Inter-image Global Contrastive Pairs (IGCP) 生成策略，使用超像素图生成伪掩码，并引入ASP和CCL模块。

Result: 在8个数据集上实验，SuperCL优于12种现有方法，在MMWHS、CHAOS、Spleen数据集上DSC提高3.15%、5.44%、7.89%。

Conclusion: SuperCL方法在少量标注下表现出色，预测更精确，代码待发布。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>


### [212] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TL;DR: 本文提出了一种自监督学习框架，用于在数据有限的情况下进行自动湿疹诊断，使用SegGPT进行分割和DINO与MLP进行分类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习方法在湿疹诊断中需要大量标注数据的问题。

Method: 一个两阶段框架：1) 使用SegGPT进行少样本湿疹区域分割；2) 提取DINO特征，并使用MLP进行4类严重度分类。

Result: 取得了加权F1分数0.67 ± 0.01，优于微调的Resnet-18（0.44 ± 0.16）和Vision Transformer（0.40 ± 0.22）。

Conclusion: 自监督学习是针对标签数据稀缺的自动皮肤诊断的一种可行解决方案。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>


### [213] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TL;DR: 本文揭示MILS在零样本图像描述中计算成本高昂，并与BLIP-2和GPT-4V比较，强调效率在多模态模型设计中的重要性。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM无需训练即可实现多模态能力的声明，通过暴露MILS迭代过程的隐藏计算开销。

Method: 通过调查和与BLIP-2、GPT-4V等模型的比较，量化输出质量与计算成本的权衡。

Result: 发现MILS计算开销巨大，而替代模型以单步方式获得类似性能，这是首次量化此类权衡。

Conclusion: 强调在设计零样本多模态模型时需考虑计算效率，提供更高效模型设计的洞见。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>


### [214] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TL;DR: 本文探讨了在多实例学习（MIL）中应用dropout技术来改善全滑片图像（WSI）分类的性能，提出MIL-Dropout方法，并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前MIL在WSI分类中的训练方案存在特征噪声和弱监督问题，导致学习效果不佳，需要探索dropout技术来提升泛化能力。

Method: 提出MIL-Dropout方法，通过丢弃袋中top-k最重要的实例来系统性地减少噪声影响。

Result: 在五个MIL基准数据集和两个WSI数据集上的实验显示，MIL-Dropout显著提升了现有方法的性能，且计算成本很低。

Conclusion: MIL-Dropout是一种有效的dropout技术，能够改善MIL的泛化性能和鲁棒性。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>


### [215] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TL;DR: 简而言之，ECViT 是一种结合 CNN 和 Transformer 的混合架构，提高了视觉任务的效率。


<details>
  <summary>Details</summary>
Motivation: 解决 Vision Transformers 的高计算成本和对大量训练数据的需求。

Method: 通过从低级特征提取 patch、添加卷积操作、局部注意力机制和金字塔结构，将 CNN 的局部性和平移不变性融入 Transformer。

Result: 在图像分类任务中优于现有最先进模型，同时保持低计算和存储需求。

Conclusion: 为追求高效率而不牺牲性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>


### [216] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TL;DR: 本论文提出CSP框架，通过语义扰动改进视觉语言模型的信心校准，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现优秀，但信心校准不佳，导致信心与正确性不一致，损害用户信任。

Method: 提出CSP框架，包括创建扰动数据集模拟视觉不确定性，并通过两阶段训练（监督微调和偏好优化）提升校准。

Result: 实验显示方法显著改善信心与正确性的对齐，同时维持或提升任务性能。

Conclusion: 结果突显语义扰动作为提升VLMs可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>


### [217] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TL;DR: PseudoFormer 是一种新框架，通过生成高质量伪标签和处理噪声，改进了弱监督时间动作定位，在基准测试中达到最先进结果。


<details>
  <summary>Details</summary>
Motivation: 解决弱监督时间动作定位中缺乏时间标注导致的性能差距，挑战包括生成高质量伪标签、充分利用不同先验知识以及优化带噪声标签训练。

Method: 提出 PseudoFormer 双分支框架，包括 RickerFusion 生成伪标签、利用弱分支先验训练全分支模型、以及不确定性掩码和迭代精炼机制处理噪声。

Result: 在 THUMOS14 和 ActivityNet1.3 基准上达到最先进结果，消融实验证明各组件贡献。

Conclusion: 成功桥接弱监督和全监督时间动作定位的差距，提升了整体性能。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>


### [218] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TL;DR: 论文针对OCR性能受外部干扰下降的问题，编制了干扰因素表和使用指南，以帮助用户提升识别准确性。


<details>
  <summary>Details</summary>
Motivation: OCR在实际应用中易受环境干扰导致性能下降，质量控制困难，因此需要提供指导以确保用户正确使用。

Method: 通过编译真实世界的外部干扰因素和图像退化现象，整理成干扰因素表，并组织成使用指南。

Result: 创建了外部干扰因素表和指南，列出了干扰因素及其影响。

Conclusion: 该指南有助于改善OCR在复杂环境下的性能，确保用户能有效利用OCR技术。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>


### [219] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TL;DR: 论文提出VFAT-WS方法，用于视频数据的快速对抗训练，提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视频识别任务中，对抗训练面临训练缓慢和高成本，以及准确性和鲁棒性权衡的挑战。

Method: 引入VFAT-WS，包括时间频率增强(TF-AUG)、空间-时间增强(STF-AUG)、单步PGD攻击和弱到强一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，CNN和Transformer模型的对抗鲁棒性和腐败鲁棒性提升，训练加速近490%。

Conclusion: VFAT-WS在提升鲁棒性的同时显著提高训练效率，适用于实际应用。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>


### [220] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的终身人脸再识别模型DAFC，通过文本驱动提示和分布整合减少遗忘，实验结果显示性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 解决终身人脸再识别中保留旧知识同时适应新信息的挑战，现有的方法存在遗忘积累或分布学习不足的问题。

Method: 提出DAFC模型，包括文本驱动提示聚合（TPA）、分布感知和整合（DAI）、知识巩固机制（KCM），实现跨域共享学习和域特定分布整合。

Result: 实验结果显示DAFC在两个训练顺序上平均mAP/R@1提高了至少9.8%/6.6%和6.4%/6.2%，优于现有方法。

Conclusion: DAFC模型有效缓解灾难性遗忘，提升了终身人脸再识别的性能。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>


### [221] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TL;DR: 本论文提出TBSFNet和MLFGNet来增强潜指纹图像，针对不同区域优化策略，并通过实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法无法满足实际应用需求，尤其在恢复低质量指纹区域，且不同区域需不同增强策略。

Method: 提出Triple Branch Spatial Fusion Network (TBSFNet)同时增强图像不同区域，并引入Multi-Level Feature Guidance Network (MLFGNet)，整合方向场和细节点模块以提高泛化能力。

Result: 在MOLF和MUST数据集上的实验结果显示，MLFGNet优于现有增强算法。

Conclusion: 该方法有效提升了潜指纹增强性能，适用于实际应用。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [222] [Information Diffusion and Preferential Attachment in a Network of Large Language Models](https://arxiv.org/abs/2504.14438)
*Adit Jain,Vikram Krishnamurthy,Yiming Zhang*

Main category: cs.SI

TL;DR: 这篇论文研究了在可能产生幻觉的LLM网络中信息扩散的模型，引入动态系统和机制来确保LLM保持真实性，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在分布式查询中可能产生的幻觉问题，确保信息准确性和网络稳定。

Method: 使用两时间尺度动态模型、均值场近似、奇异扰动近似，并提出基于声誉的优先附着机制来重新配置网络。

Result: 建立了LLM保持真实性的稳定平衡条件，提供近似保证，并通过LLaMA-3.1-8B的数值实验验证了机制的有效性。

Conclusion: 所提出的机制能减轻幻觉，提高真实节点的影响，并优化系统成本函数。

Abstract: This paper models information diffusion in a network of Large Language Models
(LLMs) that is designed to answer queries from distributed datasets, where the
LLMs can hallucinate the answer. We introduce a two-time-scale dynamical model
for the centrally administered network, where opinions evolve faster while the
network's degree distribution changes more slowly. Using a mean-field
approximation, we establish conditions for a locally asymptotically stable
equilibrium where all LLMs remain truthful. We provide approximation guarantees
for the mean-field approximation and a singularly perturbed approximation of
the two-time-scale system. To mitigate hallucination and improve the influence
of truthful nodes, we propose a reputation-based preferential attachment
mechanism that reconfigures the network based on LLMs' evaluations of their
neighbors. Numerical experiments on an open-source LLM (LLaMA-3.1-8B) validate
the efficacy of our preferential attachment mechanism and demonstrate the
optimization of a cost function for the two-time-scale system.

</details>


### [223] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TL;DR: 本文提出KuaiMod框架，使用VLM和CoT推理解决短视频平台内容审核问题，提高准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台有害内容对心理健康的影响，现有方法存在人工偏差、高成本和准确性不足等问题。

Method: 提出KuaiMod框架，包括训练数据构建、离线适应和在线部署，利用大型视觉语言模型及Chain-of-Thought推理。

Result: 在基准上表现最佳，减少用户报告率20%，增加DAU和AUT，并开源基准。

Conclusion: KuaiMod框架有效提升内容审核性能和平台用户体验。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [224] [HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis](https://arxiv.org/abs/2504.14641)
*Kangwei Xu,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.SE

TL;DR: 本文提出HLSTester框架，使用LLM辅助高效检测HLS行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有HLS测试方法不成熟且需大量人力，因此开发LLM辅助框架。

Method: 利用LLM生成HLS兼容测试bench、后向切片监控关键变量、动态变异结合LLM推理生成输入、冗余过滤减少重复测试。

Result: 实验显示加速测试流程，并比传统方法和直接LLM使用获得更高通过率。

Conclusion: LLM辅助框架提升HLS测试效率和有效性。

Abstract: In high-level synthesis (HLS), C/C++ programs with synthesis directives are
used to generate circuits for FPGA implementations. However, hardware-specific
and platform-dependent characteristics in these implementations can introduce
behavioral discrepancies between the original C/C++ programs and the circuits
after high-level synthesis. Existing methods for testing behavioral
discrepancies in HLS are still immature, and the testing workflow requires
significant human efforts. To address this challenge, we propose HLSTester, a
large language model (LLM) aided testing framework that efficiently detects
behavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance
prompt quality, the testbenches for original C/C++ programs are leveraged to
guide LLMs in generating HLS-compatible testbenches, effectively eliminating
certain traditional C/C++ constructs that are incompatible with HLS tools. Key
variables are pinpointed through a backward slicing technique in both C/C++ and
HLS programs to monitor their runtime spectra, enabling an in-depth analysis of
the discrepancy symptoms. To reduce test time, a testing input generation
mechanism is introduced to integrate dynamic mutation with insights from an
LLM-based progressive reasoning chain. In addition, repetitive hardware testing
is skipped by a redundancy-aware filtering technique for the generated test
inputs. Experimental results demonstrate that the proposed LLM-aided testing
framework significantly accelerates the testing workflow while achieving higher
testbench simulation pass rates compared with the traditional method and the
direct use of LLMs on the same HLS programs.

</details>


### [225] [Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem](https://arxiv.org/abs/2504.14026)
*Nusrat Zahan,Laurie Williams*

Main category: cs.SE

TL;DR: 本研究通过实证分析安全实践与软件安全结果的关系，发现采用OpenSSF Scorecard推荐的实践可减少漏洞和缩短更新时间。


<details>
  <summary>Details</summary>
Motivation: 从业者因资源有限难以优先安全实践，本研究旨在提供证据支持决策。

Method: 使用OpenSSF Scorecard指标测量npm仓库安全实践采用，并通过回归和因果分析评估与Vul_Count、MTTR、MTTU的关系。

Result: 结果显示，较高Scorecard分数与更少的漏洞和更短的MTTU相关；MTTR受项目特性影响；代码审查等实践有强关联。

Conclusion: 建议优先采用特定安全实践以提升软件安全，帮助决策者优化资源。

Abstract: Practitioners often struggle with the overwhelming number of security
practices outlined in cybersecurity frameworks for risk mitigation. Given the
limited budget, time, and resources, practitioners want to prioritize the
adoption of security practices based on empirical evidence. The goal of this
study is to assist practitioners and policymakers in making informed decisions
on which security practices to adopt by evaluating the relationship between
software security practices and security outcome metrics. The study
investigated the relationship between security practice adoption and security
outcomes. We selected the OpenSSF Scorecard metrics to automatically measure
the adoption of security practices in npm GitHub repositories. We also explored
security outcome metrics, such as the number of open vulnerabilities
(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and
mean time to update (MTTU) dependencies. We conducted regression and causal
analysis using 12 Scorecard metrics and their aggregated Scorecard score
(computed by aggregating individual security practice scores) as predictors and
Vul_Count, MTTR, and MTTU as target variables. Our findings show that higher
aggregated Scorecard scores are associated with fewer Vul_Count and shorter
MTTU, also supported by causal analysis. However, while the regression model
suggests shorter MTTR, causal analysis indicates project characteristics likely
influence MTTR direction. Segment analysis shows that larger, newer
repositories with more contributors, dependencies, and downloads have shorter
MTTR. Among individual security practices, Code Review, Maintained status,
Pinned Dependencies, and Branch Protection show strong associations with
security outcomes; the directionality of these associations varies across
security outcomes.

</details>


### [226] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TL;DR: 本文提出PtTrust框架，通过内部状态预训练评估代码LLM的风险，以提升其可信度。


<details>
  <summary>Details</summary>
Motivation: 代码LLM存在生成错误、不安全代码的风险，需要可扩展的内部状态分析方法来检测这些问题。

Method: PtTrust采用两阶段框架：第一阶段无监督预训练学习LLM状态表示，第二阶段使用小规模标注数据集训练风险预测器。

Result: 框架在代码行级风险评估中有效，可泛化到不同任务和编程语言，并提供直观的特征。

Conclusion: PtTrust是向可扩展的代码LLM可信保障迈出的重要一步。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>


### [227] [SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs](https://arxiv.org/abs/2504.14757)
*Minh V. T. Pham,Huy N. Phan,Hoang N. Phan,Cuong Le Chi,Tien N. Nguyen,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: 本论文提出SWE-Synth框架，使用LLM代理合成bug修复数据集，提高自动程序修复性能，实验结果显示提升2.3%。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量、可扩展的训练数据集，尤其是具有可验证输出和中间推理痕迹的，限制了开源LLM在自动程序修复中的进展。

Method: 提出SWE-Synth框架，利用LLM代理模拟调试工作流，生成bug-修复对、测试用例和结构化修复轨迹，实现可扩展且上下文丰富的合成数据集。

Result: 使用SWE-Synth训练的模型在SWE-Bench Lite上比真实数据集训练的模型性能提高2.3%。

Conclusion: 合成数据有潜力推进自动程序修复和软件工程自动化的发展。

Abstract: Large language models (LLMs) are transforming automated program repair (APR)
through agent-based approaches that localize bugs, generate patches, and verify
fixes. However, the lack of high-quality, scalable training datasets,
especially those with verifiable outputs and intermediate reasoning
traces-limits progress, particularly for open-source models. In this work, we
present SWE-Synth, a framework for synthesizing realistic, verifiable, and
process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM
agents to simulate debugging workflows, producing not only bug-fix pairs but
also test cases and structured repair trajectories. Compared to manually
curated datasets, our method scales with minimal human effort while preserving
contextual richness and correctness. Experiments show that models trained on
SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench
Lite. Our results highlight the potential of synthetic, agent-generated data to
advance the state of the art in APR and software engineering automation.

</details>


### [228] [Automated Duplicate Bug Report Detection in Large Open Bug Repositories](https://arxiv.org/abs/2504.14797)
*Clare E. Laney,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 本论文提出机器学习方法自动检测重复 bug 报告，准确率达70-90%。


<details>
  <summary>Details</summary>
Motivation: 用户常因时间不足或专业知识缺乏而重复报告已有的 bug。

Method: 使用主题建模、高斯朴素贝叶斯、深度学习、基于时间的组织、聚类和生成式预训练Transformer的总结方法，以及新颖的阈值识别方法。

Result: 在Eclipse开源项目的公共数据集上，准确率范围为高70%至低90%。

Conclusion: 方法有效，可提高 bug 报告系统的效率。

Abstract: Many users and contributors of large open-source projects report software
defects or enhancement requests (known as bug reports) to the issue-tracking
systems. However, they sometimes report issues that have already been reported.
First, they may not have time to do sufficient research on existing bug
reports. Second, they may not possess the right expertise in that specific area
to realize that an existing bug report is essentially elaborating on the same
matter, perhaps with a different wording. In this paper, we propose a novel
approach based on machine learning methods that can automatically detect
duplicate bug reports in an open bug repository based on the textual data in
the reports. We present six alternative methods: Topic modeling, Gaussian Naive
Bayes, deep learning, time-based organization, clustering, and summarization
using a generative pre-trained transformer large language model. Additionally,
we introduce a novel threshold-based approach for duplicate identification, in
contrast to the conventional top-k selection method that has been widely used
in the literature. Our approach demonstrates promising results across all the
proposed methods, achieving accuracy rates ranging from the high 70%'s to the
low 90%'s. We evaluated our methods on a public dataset of issues belonging to
an Eclipse open-source project.

</details>


### [229] [Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs](https://arxiv.org/abs/2504.15080)
*Chen Xie,Mingsheng Jiao,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本篇论文提出DLCodeGen，一种针对深度学习项目代码生成的规划引导方法，展示了比基线更好的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成复杂深度学习项目时困难，缺乏上下文指导和领域知识。

Method: DLCodeGen预测结构化计划，检索类似代码样本，抽象模板，并用比较学习机制生成代码。

Result: 实验显示DLCodeGen在CodeBLEU上提升9.7%，人工评估上提升3.6%。

Conclusion: 该方法有效改善深度学习代码生成。

Abstract: While large language models (LLMs) have been widely applied to code
generation, they struggle with generating entire deep learning projects, which
are characterized by complex structures, longer functions, and stronger
reliance on domain knowledge than general-purpose code. An open-domain LLM
often lacks coherent contextual guidance and domain expertise for specific
projects, making it challenging to produce complete code that fully meets user
requirements.
  In this paper, we propose a novel planning-guided code generation method,
DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a
structured solution plan, offering global guidance for LLMs to generate the
project. The generated plan is then leveraged to retrieve semantically
analogous code samples and subsequently abstract a code template. To
effectively integrate these multiple retrieval-augmented techniques, a
comparative learning mechanism is designed to generate the final code. We
validate the effectiveness of our approach on a dataset we build for deep
learning code generation. Experimental results demonstrate that DLCodeGen
outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and
3.6% in human evaluation metrics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [230] [Towards Optimal Orders for Entanglement Swapping in Path Graphs: A Greedy Approach](https://arxiv.org/abs/2504.14040)
*Van Sy Mai,Abderrahim Amlou,Amar Abane,Abdella Battou*

Main category: quant-ph

TL;DR: 本论文优化量子中继器中纠缠交换的顺序，以最大化通量，使用高效的近似和启发式方法，并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 本论文的动机是找到异构量子中继器路径中纠缠交换的最佳顺序，以最大化端到端纠缠的传递率，尽管存在大量的可能顺序和依赖于节点属性的复杂性。

Method: 提出用于估计交换结果的常量时间近似方法、针对小路径的精确解，以及基于动态交换分数的贪婪启发式方法用于较大路径。

Result: 通过使用量子网络模拟器的广泛实验，证明了所提出模型和方法的效率和有效性。

Conclusion: 所提出的方法为优化量子网络中的纠缠交换顺序提供了一种高效的方式。

Abstract: This paper considers the problem of finding an optimal order for entanglement
swapping in a heterogeneous path of quantum repeaters so as to maximize the
path throughput defined as the delivery rate of end-to-end entanglements. The
primary difficulty in addressing this problem lies in the vast array of
possible swapping orders for large paths and the complexity of the expected
throughput, which depends on the attributes of each node and edge along the
path, as well as the order of swapping. To cope with these issues, we first
propose simple approximations in estimating the swapping outcome between two
entanglement distributions that can run in constant time, thereby providing an
efficient approach for evaluating and comparing different swapping orders,
allowing us to solve the problem exactly for small paths. Second, as the number
of possible orders grows exponentially with the number of repeaters in the
path, we develop an efficient heuristic based on the greedy selection of nodes
to sequentially perform swaps according to their swapping scores, defined as
the expected number of entanglements resulting from their swaps. The scores are
local but dynamic in the sense that they depend not just on the entanglement
distributions available on the path but also on prior swapping decisions.
Finally, we illustrate the efficiency and effectiveness of our proposed model
and approach through extensive experimentation conducted using a general
quantum network simulator.

</details>


### [231] [Predicting fermionic densities using a Projected Quantum Kernel method](https://arxiv.org/abs/2504.14002)
*Francesco Perciavalle,Francesco Plastina,Michele Pisarra,Nicola Lo Gullo*

Main category: quant-ph

TL;DR: 本研究使用基于投影量子核的支持向量回归器预测一维费米子系统的密度结构，并与经典方法比较性能。


<details>
  <summary>Details</summary>
Motivation: 为了改进量子化学和量子物质中费米子系统密度结构的预测，探索量子核方法的应用。

Method: 采用支持向量回归器结合投影量子核方法，核基于与相互作用的里德伯原子实现的量子 reservoir，训练数据来自密度泛函理论。

Result: 测试不同哈密顿参数，发现错误随测量时间变化的共同行为；在足够长的测量时间内，性能优于经典线性核方法，并可与径向基函数方法竞争。

Conclusion: 量子核方法在特定条件下表现出色，有潜力在量子模拟中应用。

Abstract: We use a support vector regressor based on a projected quantum kernel method
to predict the density structure of 1D fermionic systems of interest in quantum
chemistry and quantum matter. The kernel is built on with the observables of a
quantum reservoir implementable with interacting Rydberg atoms. Training and
test data of the fermionic system are generated using a Density Functional
Theory approach. We test the performance of the method for several Hamiltonian
parameters, finding a general common behavior of the error as a function of
measurement time. At sufficiently large measurement times, we find that the
method outperforms the classical linear kernel method and can be competitive
with the radial basis function method.

</details>


### [232] [Guess, SWAP, Repeat : Capturing Quantum Snapshots in Classical Memory](https://arxiv.org/abs/2504.14459)
*Debarshi Kundu,Avimita Chatterjee,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 本论文提出了一种非破坏性观察量子态的技术，使用机器学习在经典内存中存储和重建量子态快照。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统调试、内省和持久内存的需求，克服无克隆定理和破坏性测量的挑战。

Method: 采用猜测-检查方法，通过SWAP测试估计保真度，并使用梯度-based深度神经网络和无梯度进化策略进行量子态估计和重建。

Result: 在IBM量子硬件上实现高保真度（约1.0）的重建，在模拟中对100个随机量子态平均保真度达0.999。

Conclusion: 为非易失量子内存的发展提供途径，支持量子信息的长期存储和重用，奠定未来量子内存架构的基础。

Abstract: We introduce a novel technique that enables observation of quantum states
without direct measurement, preserving them for reuse. Our method allows
multiple quantum states to be observed at different points within a single
circuit, one at a time, and saved into classical memory without destruction.
These saved states can be accessed on demand by downstream applications,
introducing a dynamic and programmable notion of quantum memory that supports
modular, non-destructive quantum workflows. We propose a hardware-agnostic,
machine learning-driven framework to capture non-destructive estimates, or
"snapshots," of quantum states at arbitrary points within a circuit, enabling
classical storage and later reconstruction, similar to memory operations in
classical computing. This capability is essential for debugging, introspection,
and persistent memory in quantum systems, yet remains difficult due to the
no-cloning theorem and destructive measurements. Our guess-and-check approach
uses fidelity estimation via the SWAP test to guide state reconstruction. We
explore both gradient-based deep neural networks and gradient-free evolutionary
strategies to estimate quantum states using only fidelity as the learning
signal. We demonstrate a key component of our framework on IBM quantum
hardware, achieving high-fidelity (approximately 1.0) reconstructions for
Hadamard and other known states. In simulation, our models achieve an average
fidelity of 0.999 across 100 random quantum states. This provides a pathway
toward non-volatile quantum memory, enabling long-term storage and reuse of
quantum information, and laying groundwork for future quantum memory
architectures.

</details>


### [233] [Quantum-Enhanced Weight Optimization for Neural Networks Using Grover's Algorithm](https://arxiv.org/abs/2504.14568)
*Stefan-Alexandru Jura,Mihai Udrescu*

Main category: quant-ph

TL;DR: 本论文提出使用Grover量子搜索算法优化经典神经网络权重，解决梯度下降问题，提高训练效率和准确性，并更适用于近未来量子计算机。


<details>
  <summary>Details</summary>
Motivation: 解决梯度下降的梯度爆炸、消失和凸性问题，以及其他优化方法的缺点，如收敛一致性问题，通过量子计算加速神经网络训练。

Method: 设计Grover量子搜索算法来优化神经网络参数，而非使用反向传播和梯度下降方法。

Result: 在小数据集上，测试损失减少58.75%，准确性提高35.25%；在Digits数据集上，3层隐藏层网络平均准确率97.7%；比传统方法更可扩展，并使用更少量子比特。

Conclusion: 该方法有效避免传统优化问题，利用量子加速提升性能，并为量子计算机资源有限的现状提供实用方案。

Abstract: The main approach to hybrid quantum-classical neural networks (QNN) is
employing quantum computing to build a neural network (NN) that has quantum
features, which is then optimized classically. Here, we propose a different
strategy: to use quantum computing in order to optimize the weights of a
classical NN. As such, we design an instance of Grover's quantum search
algorithm to accelerate the search for the optimal parameters of an NN during
the training process, a task traditionally performed using the backpropagation
algorithm with the gradient descent method. Indeed, gradient descent has issues
such as exploding gradient, vanishing gradient, or convexity problem. Other
methods tried to address such issues with strategies like genetic searches, but
they carry additional problems like convergence consistency. Our original
method avoids these issues -- because it does not calculate gradients -- and
capitalizes on classical architectures' robustness and Grover's quadratic
speedup in high-dimensional search spaces to significantly reduce test loss
(58.75%) and improve test accuracy (35.25%), compared to classical NN weight
optimization, on small datasets. Unlike most QNNs that are trained on small
datasets only, our method is also scalable, as it allows the optimization of
deep networks; for an NN with 3 hidden layers, trained on the Digits dataset
from scikit-learn, we obtained a mean accuracy of 97.7%. Moreover, our method
requires a much smaller number of qubits compared to other QNN approaches,
making it very practical for near-future quantum computers that will still
deliver a limited number of logical qubits.

</details>


### [234] [Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks](https://arxiv.org/abs/2504.14995)
*Keisuke Murota,Takumi Kobori*

Main category: quant-ph

TL;DR: 本文提出FTN分类器，将TTN嵌入QNN以提高多类图像分类性能，克服量子资源挑战。


<details>
  <summary>Details</summary>
Motivation: 为了利用量子资源提升TTN分类器的多类分类性能，解决高阶门操作和中电路后选问题。

Method: 提出将多个小键维TTN聚合为FTN分类器，扩展绝热编码框架去除后选开销，并平滑编码为量子FTN分类器。

Result: 在MNIST和CIFAR-10上的数值实验显示成功训练和编码，性能保持或提升。

Conclusion: 建议TTN和QNN的协同可提供鲁棒且可扩展的多类量子增强图像分类框架。

Abstract: Tree tensor networks (TTNs) offer powerful models for image classification.
While these TTN image classifiers already show excellent performance on
classical hardware, embedding them into quantum neural networks (QNNs) may
further improve the performance by leveraging quantum resources. However,
embedding TTN classifiers into QNNs for multiclass classification remains
challenging. Key obstacles are the highorder gate operations required for large
bond dimensions and the mid-circuit postselection with exponentially low
success rates necessary for the exact embedding. In this work, to address these
challenges, we propose forest tensor network (FTN)-classifiers, which aggregate
multiple small-bond-dimension TTNs. This allows us to handle multiclass
classification without requiring large gates in the embedded circuits. We then
remove the overhead of mid-circuit postselection by extending the adiabatic
encoding framework to our setting and smoothly encode the FTN-classifiers into
a quantum forest tensor network (qFTN)- classifiers. Numerical experiments on
MNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers
and encode them into qFTN-classifiers, while maintaining or even improving the
performance of the pre-trained FTN-classifiers. These results suggest that
synergy between TTN classification models and QNNs can provide a robust and
scalable framework for multiclass quantum-enhanced image classification.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [235] [Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning](https://arxiv.org/abs/2504.13978)
*Yuqing Liu,Meng Zhao,Guanlan Hu,Yuchen Zhang*

Main category: q-bio.QM

TL;DR: 本研究使用机器学习分析营养和炎症因素与癌症的关系，发现这些因素可预测癌症状态，随机森林模型准确率0.72。


<details>
  <summary>Details</summary>
Motivation: 饮食和炎症影响癌症风险，但使用机器学习结合二者对癌症状态和类型的影响尚未充分探索。

Method: 分析NHANES数据中的24种营养素、CRP和ALI，使用多变量逻辑回归和ML模型（逻辑回归、随机森林、XGBoost）评估关联和预测。

Result: 营养因素如蛋白质和维生素、炎症标志物是癌症预测关键，随机森林模型准确率0.72。

Conclusion: 高质量营养摄入和低炎症水平可能降低癌症风险，建议结合ML用于癌症预防策略。

Abstract: Background. Diet and inflammation are critical factors influencing cancer
risk. However, the combined impact of nutritional status and inflammatory
biomarkers on cancer status and type, using machine learning (ML), remains
underexplored.
  Objectives. This study investigates the association between nutritional
factors, inflammatory biomarkers, and cancer status, and whether these
relationships differ across cancer types using National Health and Nutrition
Examination Survey (NHANES) data.
  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP),
and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES
participants (2,120 with cancer). Multivariable logistic regression assessed
associations with cancer prevalence. We also examined whether these features
differed across the five most common cancer types. To evaluate predictive
value, we applied three ML models - Logistic Regression, Random Forest, and
XGBoost - on the full feature set.
  Results. The cohort's mean age was 49.1 years; 34.7% were obese.
Comorbidities such as anemia and liver conditions, along with nutritional
factors like protein and several vitamins, were key predictors of cancer
status. Among the models, Random Forest performed best, achieving an accuracy
of 0.72.
  Conclusions. Higher-quality nutritional intake and lower levels of
inflammation may offer protective effects against cancer. These findings
highlight the potential of combining nutritional and inflammatory markers with
ML to inform cancer prevention strategies.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [236] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TL;DR: This paper formalizes the trade-off between validity and breadth in language generation, proposes an algorithm with positive density outputs, and introduces a novel topology.


<details>
  <summary>Details</summary>
Motivation: To quantify the trade-off between hallucination and mode collapse in large language models, which has been challenging to study.

Method: Formalizing breadth using density measures, developing an algorithm for language generation in the limit, and analyzing representations with a novel topology on language families.

Result: An algorithm that achieves strictly positive density in the true language, and strong breadth may require oscillating between representations.

Conclusion: Poor breadth is not necessary, and topological concepts are key to understanding language generation algorithms.

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [237] [Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning](https://arxiv.org/abs/2504.14156)
*Abdelali Sajia,Bilal Benzimoun,Pawan Khatiwada,Guogan Zhao,Xiao-Feng Qian*

Main category: physics.optics

TL;DR: 本论文提出一种参数解耦的超分辨率框架，用于被动双点源的亚波长分离估计，无需先验知识或源控制。


<details>
  <summary>Details</summary>
Motivation: 动机是绕过估计多个参数的需求，如部分相干性、亮度不平衡、随机相对相位和光子统计，并处理实际缺陷，以应用于无法控制源的场景。

Method: 方法基于理论基础和物理信息机器学习模型（在标准桌面工作站训练），整合参数解耦来应对背景噪声、光子损失和中心/方向不对齐。

Result: 结果在实验生成的真实图像上实现分辨率超过衍射极限14倍（约13.5 nm），保真度>82%，性能与主动控制源技术相当。

Conclusion: 结论桥接了理论超分辨率极限与被动系统的实际实现，潜力应用于天文成像、活细胞显微镜和量子计量学。

Abstract: We present a parameter-decoupled superresolution framework for estimating
sub-wavelength separations of passive two-point sources without requiring prior
knowledge or control of the source. Our theoretical foundation circumvents the
need to estimate multiple challenging parameters such as partial coherence,
brightness imbalance, random relative phase, and photon statistics. A
physics-informed machine learning (ML) model (trained with a standard desktop
workstation), synergistically integrating this theory, further addresses
practical imperfections including background noise, photon loss, and
centroid/orientation misalignment. The integrated parameter-decoupling
superresolution method achieves resolution 14 and more times below the
diffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on
experimentally generated realistic images with >82% fidelity, performance
rivaling state-of-the-art techniques for actively controllable sources.
Critically, our method's robustness against source parameter variability and
source-independent noises enables potential applications in realistic scenarios
where source control is infeasible, such as astrophysical imaging, live-cell
microscopy, and quantum metrology. This work bridges a critical gap between
theoretical superresolution limits and practical implementations for passive
systems.

</details>


### [238] [DeepPD: Joint Phase and Object Estimation from Phase Diversity with Neural Calibration of a Deformable Mirror](https://arxiv.org/abs/2504.14157)
*Magdalena C. Schneider,Courtney Johnson,Cedric Allier,Larissa Heinrich,Diane Adjavon,Joren Husic,Patrick La Rivière,Stephan Saalfeld,Hari Shroff*

Main category: physics.optics

TL;DR: DeepPD是一种基于深度学习的框架，使用相位多样性技术从五张图像中恢复荧光显微镜的衍射极限分辨率。


<details>
  <summary>Details</summary>
Motivation: 样品诱导的像差和光学缺陷限制了荧光显微镜的分辨率，现有方法依赖Zernike模式、需要大量多样性图像或精确镜面校准。

Method: DeepPD结合神经表示和可变形镜的学得模型，从仅五张图像中联合估计物体和相位。

Result: 提高了鲁棒性和重建质量，即使在严重像差下，也在校准目标和生物样本上展示了性能，包括固定PtK2细胞中的免疫标记肌球蛋白。

Conclusion: DeepPD改进了现有方法的局限性，提供更可靠的分辨率恢复。

Abstract: Sample-induced aberrations and optical imperfections limit the resolution of
fluorescence microscopy. Phase diversity is a powerful technique that leverages
complementary phase information in sequentially acquired images with
deliberately introduced aberrations--the phase diversities--to enable phase and
object reconstruction and restore diffraction-limited resolution. These phase
diversities are typically introduced into the optical path via a deformable
mirror. Existing phase-diversity-based methods are limited to Zernike modes,
require large numbers of diversity images, or depend on accurate mirror
calibration--which are all suboptimal. We present DeepPD, a deep learning-based
framework that combines neural representations of the object and wavefront with
a learned model of the deformable mirror to jointly estimate both object and
phase from only five images. DeepPD improves robustness and reconstruction
quality over previous approaches, even under severe aberrations. We demonstrate
its performance on calibration targets and biological samples, including
immunolabeled myosin in fixed PtK2 cells.

</details>


### [239] [Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects](https://arxiv.org/abs/2504.15044)
*Benshan Wang,Qiarong Xiao,Tengji Xu,Li Fan,Shaojie Liu,Jianji Dong,Junwen Zhang,Chaoran Huang*

Main category: physics.optics

TL;DR: 本文提出了一种基于深度reservoir计算的无DSP全光神经形态光学信号处理器，提升AI训练互连的性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速扩展对高性能计算需求巨大，传统互连技术无法满足低延迟和节能要求。

Method: 使用集成神经形态光学信号处理器，通过深度reservoir计算实现无DSP的全光实时信号处理。

Result: 实验实现100 Gbaud PAM4、1.6 Tbit/s的5 km光纤传输，延迟和能耗分别降低四个数量级和三个数量级，远超现有DSP方案。

Conclusion: 提供可扩展、高效节能的解决方案，推动下一代AI基础设施发展。

Abstract: The rapid expansion of generative AI drives unprecedented demands for
high-performance computing. Training large-scale AI models now requires vast
interconnected GPU clusters across multiple data centers. Multi-scale AI
training and inference demand uniform, ultra-low latency, and energy-efficient
links to enable massive GPUs to function as a single cohesive unit. However,
traditional electrical and optical interconnects, relying on conventional
digital signal processors (DSPs) for signal distortion compensation,
increasingly fail to meet these stringent requirements. To overcome these
limitations, we present an integrated neuromorphic optical signal processor
(OSP) that leverages deep reservoir computing and achieves DSP-free,
all-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud
PAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in
the C-band (equivalent to over 80 km in the O-band), far exceeding the reach of
state-of-the-art DSP solutions, which are fundamentally constrained by
chromatic dispersion in IMDD systems. Simultaneously, it reduces processing
latency by four orders of magnitude and energy consumption by three orders of
magnitude. Unlike DSPs, which introduce increased latency at high data rates,
our OSP maintains consistent, ultra-low latency regardless of data rate
scaling, making it ideal for future optical interconnects. Moreover, the OSP
retains full optical field information for better impairment compensation and
adapts to various modulation formats, data rates, and wavelengths. Fabricated
using a mature silicon photonic process, the OSP can be monolithically
integrated with silicon photonic transceivers, enhancing the compactness and
reliability of all-optical interconnects. This research provides a highly
scalable, energy-efficient, and high-speed solution, paving the way for
next-generation AI infrastructure.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [240] [On the redundancy of short and heterogeneous sequences of belief revisions](https://arxiv.org/abs/2504.13986)
*Paolo Liberatore*

Main category: cs.CC

TL;DR: 本文探讨遗忘信念修正事件是否删除信息，证明某些情况coNP-hard，并为两个Horn修正提供多项式算法。


<details>
  <summary>Details</summary>
Motivation: 动机是研究遗忘特定修正事件是否真正删除信息，因为其他修正可能补偿。

Method: 方法包括证明复杂性（coNP-hard和NP-hard）、开发多项式算法及分析异质序列。

Result: 结果显示：两个任意词汇修正或长Horn修正为coNP-hard；两个Horn修正有多项式算法；异质序列在Delta2中，且NP-hardness被证明。

Conclusion: 结论是，这有助于理解信念修正系统的复杂性，并为特定场景提供高效计算方法。

Abstract: Forgetting a specific belief revision episode may not erase information
because the other revisions may provide the same information or allow to deduce
it. Whether it does was proved coNP-hard for sequence of two arbitrary
lexicographic revision or arbitrarily long lexicographic Horn revision. A
polynomial algorithm is presented for the case of two Horn revision.
Heterogeneous sequences of revisions were proved to belong in Delta2. Their
previously proved coNP-hardness is enhanced by a proof of NP-hardness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [241] [CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews](https://arxiv.org/abs/2504.13993)
*Ekta Gujral,Apurva Sinha,Lishi Ji,Bijayani Sanghamitra Mishra*

Main category: cs.IR

TL;DR: 本论文提出CPR方法，使用LLM和主题建模帮助用户创建更全面的产品评论，实验显示质量提升12.3%。


<details>
  <summary>Details</summary>
Motivation: 消费者依赖在线评论，但现有研究未系统解决如何鼓励全面捕捉情感和产品特征的评论。

Method: CPR采用三阶段过程：第一阶段呈现产品特定术语评分；第二阶段基于评分生成针对性短语建议；第三阶段通过主题建模整合用户文本，确保覆盖关键方面。

Result: 实验使用LLM与Walmart真实评论比较，CPR能识别新产品术语，提供情感一致建议，BLEU分数提升12.3%，手动评估支持。

Conclusion: 讨论CPR的潜在扩展和未来研究方向。

Abstract: Consumers often heavily rely on online product reviews, analyzing both
quantitative ratings and textual descriptions to assess product quality.
However, existing research hasn't adequately addressed how to systematically
encourage the creation of comprehensive reviews that capture both customers
sentiment and detailed product feature analysis. This paper presents CPR, a
novel methodology that leverages the power of Large Language Models (LLMs) and
Topic Modeling to guide users in crafting insightful and well-rounded reviews.
Our approach employs a three-stage process: first, we present users with
product-specific terms for rating; second, we generate targeted phrase
suggestions based on these ratings; and third, we integrate user-written text
through topic modeling, ensuring all key aspects are addressed. We evaluate CPR
using text-to-text LLMs, comparing its performance against real-world customer
reviews from Walmart. Our results demonstrate that CPR effectively identifies
relevant product terms, even for new products lacking prior reviews, and
provides sentiment-aligned phrase suggestions, saving users time and enhancing
reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU
score over baseline methods, further supported by manual evaluation of
generated phrases. We conclude by discussing potential extensions and future
research directions.

</details>


### [242] [Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling](https://arxiv.org/abs/2504.14130)
*Qiang Li,Xinze Lin,Shenghao Lv,Faliang Huang,Xiangju Li*

Main category: cs.IR

TL;DR: 这篇论文提出多粒度候选新闻感知用户建模框架，提升个性化新闻推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分捕捉用户兴趣多样性，并忽略候选新闻与用户兴趣的多粒度相关性。

Method: 提出框架，包括候选新闻编码（文本和知识增强实体提取器）和用户建模（词级、实体级、新闻级机制）。

Result: 在真实数据集实验中，模型显著优于基线模型。

Conclusion: 框架有效捕捉多粒度相关性，提高新闻推荐准确性。

Abstract: Matching candidate news with user interests is crucial for personalized news
recommendations. Most existing methods can represent a user's reading interests
through a single profile based on clicked news, which may not fully capture the
diversity of user interests. Although some approaches incorporate candidate
news or topic information, they remain insufficient because they neglect the
multi-granularity relatedness between candidate news and user interests. To
address this, this study proposed a multi-granularity candidate-aware user
modeling framework that integrated user interest features across various levels
of granularity. It consisted of two main components: candidate news encoding
and user modeling. A news textual information extractor and a
knowledge-enhanced entity information extractor can capture candidate news
features, and word-level, entity-level, and news-level candidate-aware
mechanisms can provide a comprehensive representation of user interests.
Extensive experiments on a real-world dataset demonstrated that the proposed
model could significantly outperform baseline models.

</details>


### [243] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TL;DR: 本文提出了一种基于人类反馈的优化框架，使用大语言模型模拟人类反馈来提升可解释推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法依赖监督学习，在稀疏交互数据下无法提供有效反馈信号，导致用户体验不佳。

Method: 提出动态交互优化机制，使用LLM作为人类模拟器，引入自定义奖励评分、Pareto优化和离策略优化管道来处理数据偏差。

Result: 在四个数据集上的广泛实验证明了方法的优越性。

Conclusion: 该框架能高效实现人性化解释要求，改善推荐系统性能，而不需高人力成本。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>


### [244] [FinSage: A Multi-aspect RAG System for Financial Filings Question Answering](https://arxiv.org/abs/2504.14493)
*Xinyu Wang,Jijun Chi,Zhenghan Tai,Tung Sum Thomas Kwok,Muzhi Li,Zhuhong Li,Hailin He,Yuchen Hua,Peng Lu,Suyuchen Wang,Yihong Wu,Jerry Huang,Ling Zhou*

Main category: cs.IR

TL;DR: 这篇论文提出FinSage框架，一个针对金融文档合规分析的多模态RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统难以处理金融数据的异质性（如文本、表格、图表）和演变的监管标准，导致信息提取准确性不足。

Method: FinSage包括多模态预处理管道、生成元数据摘要；多路径稀疏-密集检索系统，使用HyDE查询扩展和元数据aware搜索；以及通过DPO微调的领域专用重新排序模块。

Result: 实验显示Recall达92.51%，准确率比最佳基线高24.06%；已部署在线会议中，服务超过1200人。

Conclusion: FinSage有效提升了金融合规分析的准确性和实用性，具有实际应用价值。

Abstract: Leveraging large language models in real-world settings often entails a need
to utilize domain-specific data and tools in order to follow the complex
regulations that need to be followed for acceptable use. Within financial
sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation
(RAG) systems to address complex compliance requirements in financial document
workflows. However, existing solutions struggle to account for the inherent
heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of
regulatory standards used in financial filings, leading to compromised accuracy
in critical information extraction. We propose the FinSage framework as a
solution, utilizing a multi-aspect RAG framework tailored for regulatory
compliance analysis in multi-modal financial documents. FinSage introduces
three innovative components: (1) a multi-modal pre-processing pipeline that
unifies diverse data formats and generates chunk-level metadata summaries, (2)
a multi-path sparse-dense retrieval system augmented with query expansion
(HyDE) and metadata-aware semantic search, and (3) a domain-specialized
re-ranking module fine-tuned via Direct Preference Optimization (DPO) to
prioritize compliance-critical content. Extensive experiments demonstrate that
FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions
derived from surpasses the best baseline method on the FinanceBench question
answering datasets by 24.06% in accuracy. Moreover, FinSage has been
successfully deployed as financial question-answering agent in online meetings,
where it has already served more than 1,200 people.

</details>


### [245] [Exploring $\ell_0$ Sparsification for Inference-free Sparse Retrievers](https://arxiv.org/abs/2504.14839)
*Xinjie Shen,Zhichao Geng,Yang Yang*

Main category: cs.IR

TL;DR: 本论文探索了L0启发的稀疏化方法，用于inference-free检索模型，在BEIR基准上达到最先进性能，并与Siamese模型相当。


<details>
  <summary>Details</summary>
Motivation: 随着效率需求的增加，信息检索发展出sparse retrieval的分支，但现有模型在inference-free场景下表现不佳，FLOPS正则化不适合asymmetric设置，先前尝试局限于规则-based方法。

Method: 使用L0启发的稀疏化方式针对inference-free检索模型进行优化。

Result: 在BEIR基准的全面out-of-domain评估中，取得了inference-free sparse retrieval模型的state-of-the-art性能，与领先Siamese模型相当，并分析了检索有效性和计算效率的trade-off。

Conclusion: 证明了该方法在实际应用中的实用价值。

Abstract: With increasing demands for efficiency, information retrieval has developed a
branch of sparse retrieval, further advancing towards inference-free retrieval
where the documents are encoded during indexing time and there is no
model-inference for queries. Existing sparse retrieval models rely on FLOPS
regularization for sparsification, while this mechanism was originally designed
for Siamese encoders, it is considered to be suboptimal in inference-free
scenarios which is asymmetric. Previous attempts to adapt FLOPS for
inference-free scenarios have been limited to rule-based methods, leaving the
potential of sparsification approaches for inference-free retrieval models
largely unexplored. In this paper, we explore $\ell_0$ inspired sparsification
manner for inference-free retrievers. Through comprehensive out-of-domain
evaluation on the BEIR benchmark, our method achieves state-of-the-art
performance among inference-free sparse retrieval models and is comparable to
leading Siamese sparse retrieval models. Furthermore, we provide insights into
the trade-off between retrieval effectiveness and computational efficiency,
demonstrating practical value for real-world applications.

</details>


### [246] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: 本论文提出KGMEL框架，通过整合知识图谱三元组提升多模态实体链接性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体链接方法忽略知识图谱三元组的结构信息，导致实体对齐准确性不足。

Method: KGMEL框架包括生成阶段（使用视觉语言模型产生三元组）、检索阶段（对比学习整合文本、图像和三元组）、重新排序阶段（优化三元组并用大语言模型选最佳实体）。

Result: 在基准数据集上，KGMEL优于现有方法。

Conclusion: 融入知识图谱三元组可显著提高多模态实体链接的准确性和鲁棒性。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [247] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TL;DR: 论文提出一种处理语义分割中空间相关标注错误的方法，使用近似贝叶斯估计和KMS矩阵，提高模型在噪声数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决实际场景如医疗和遥感中标注错误问题，这些错误空间相关，影响模型准确性。

Method: 方法基于概率模型，假设标注错误并使用高斯分布和KMS矩阵处理空间相关性的贝叶斯推断计算挑战。

Result: 实验显示，利用空间相关性显著提升性能，在肺分割任务中可与清洁标签训练相当。

Conclusion: 结论是，该方法有效改善标注噪声影响，并提供开源代码。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [248] [Generalized Derangetropy Functionals for Modeling Cyclical Information Flow](https://arxiv.org/abs/2504.14605)
*Masoud Ataei,Xiaogang Wang*

Main category: cs.IT

TL;DR: 这篇论文引入一个框架，使用derangetropy函数建模周期性和反馈驱动的信息流动，通过热方程导致收敛到高斯特征函数，并有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 克服Shannon熵等静态熵测度的局限性，处理信息分布的周期性和自指涉方面。

Method: 定义derangetropy函数作用于概率密度，通过非线性微分方程和递归算子诱导谱扩散过程。

Result: 证明了递归应用导致的谱扩散过程收敛到高斯特征函数，提供统一分析基础。

Conclusion: 为分析具有周期结构、随机反馈和延迟交互的系统信息演化提供新工具，应用于人工神经网络、通信理论和非平衡统计力学。

Abstract: This paper introduces a framework for modeling cyclical and feedback-driven
information flow through a generalized family of entropy-modulated
transformations called derangetropy functionals. Unlike scalar and static
entropy measures such as Shannon entropy, these functionals act directly on
probability densities and provide a topographical representation of information
structure across the support of the distribution. The framework captures
periodic and self-referential aspects of information distribution and encodes
them through functional operators governed by nonlinear differential equations.
When applied recursively, these operators induce a spectral diffusion process
governed by the heat equation, leading to convergence toward a Gaussian
characteristic function. This convergence theorem provides a unified analytical
foundation for describing the long-term dynamics of information under cyclic
modulation. The proposed framework offers new tools for analyzing the temporal
evolution of information in systems characterized by periodic structure,
stochastic feedback, and delayed interaction, with applications in artificial
neural networks, communication theory, and non-equilibrium statistical
mechanics.

</details>


### [249] [Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions](https://arxiv.org/abs/2504.14696)
*Naima Tasnim,Atefeh Gilani,Lalitha Sankar,Oliver Kosut*

Main category: cs.IT

TL;DR: 这篇论文引入了差分隐私算法ROO及其变体DS-ROO，用于生成代表性样本，并改善了隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 改进现有差分隐私方法，提供更好的采样复杂度界和自适应机制以提升效用。

Method: ROO通过随机揭示或遮蔽经验分布实现隐私；DS-ROO自适应调整遮蔽概率。

Result: 证明ROO有更好的采样复杂度界，DS-ROO满足ε-差分隐私，并通过实证显示更高效用。

Conclusion: DS-ROO实现了更好的隐私-效用权衡，推进了差分隐私采样技术。

Abstract: We introduce a differentially private (DP) algorithm called reveal-or-obscure
(ROO) to generate a single representative sample from a dataset of $n$
observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike
methods that add explicit noise to the estimated empirical distribution, ROO
achieves $\epsilon$-differential privacy by randomly choosing whether to
"reveal" or "obscure" the empirical distribution. While ROO is structurally
identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we
prove a strictly better bound on the sampling complexity than that established
in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility
trade-off, we propose a novel generalized sampling algorithm called
Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical
distribution of the dataset is chosen adaptively. We prove that DS-ROO
satisfies $\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve
better utility under the same privacy budget of vanilla ROO.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [250] [How competitive are pay-as-bid auction games?](https://arxiv.org/abs/2504.13920)
*Martina Vanelli,Giacomo Como,Fabio Fagnani*

Main category: math.OC

TL;DR: 本文研究了辅助服务市场的按出价支付拍卖，针对非对称企业，通过限制策略空间为Lipschitz连续函数，确保Nash均衡存在，并在极限情况下实现完美竞争和更低价格。


<details>
  <summary>Details</summary>
Motivation: 受辅助服务市场当前结构的启发，研究了具有歧视性定价和非对称企业的按出价支付拍卖游戏。

Method: 使用供给函数模型，将策略空间限制为Lipschitz连续函数，在标准凹性假设下证明Nash均衡的存在，并分析其性质。

Result: Nash均衡始终存在，且在支持集上是仿射的；在仿射需求和零同质边际成本下，均衡在市场出清价格上唯一；对于二次生产成本，得出封闭形式表达式，并在Lipschitz常数趋于无穷时实现完美竞争。

Conclusion: 按出价支付拍卖在极限情况下实现高效分配，并比统一价格拍卖导致更低的市场出清价格。

Abstract: Motivated by the current structure of ancillary services markets, we study
the pay-as-bid auction game, a supply function model with discriminatory
pricing and asymmetric firms. In this game, strategies are non-decreasing
supply functions relating price to quantity and the exact choice of the
strategy space turns out to be a crucial issue: when it includes all
non-decreasing continuous functions, pure-strategy Nash equilibria often fail
to exist. To overcome this, we restrict the strategy space to the set of
Lipschitz-continuous functions and we prove that Nash equilibria always exist
(under standard concavity assumptions) and consist of functions that are affine
on their own support and have slope equal to the maximum allowed Lipschitz
constant. We further show that the Nash equilibrium is unique up to the
market-clearing price when the demand is affine and the asymmetric marginal
production costs are homogeneous in zero. For quadratic production costs, we
derive a closed-form expression and we compute the limit as the allowed
Lipschitz constant grows to infinity. Our results show that in the limit the
pay-as-bid auction game achieves perfect competition with efficient allocation
and induces a lower market-clearing price compared to supply function models
based on uniform price auctions.

</details>


### [251] [Feedback Stackelberg-Nash equilibria in difference games with quasi-hierarchical interactions and inequality constraints](https://arxiv.org/abs/2504.15019)
*Partha Sarathi Mohapatra,Puduru Viswanadha Reddy,Georges Zaccour*

Main category: math.OC

TL;DR: 这篇论文研究了带耦合不等式约束的两玩家确定性有限时差分博弈，引入反馈Stackelberg-Nash (FSN) 均衡，并通过动态规划-like方法求解。


<details>
  <summary>Details</summary>
Motivation: 动机是处理具有顺序和同时交互决策变量的准分层动态博弈，以应对约束条件。

Method: 方法包括定义FSN均衡、在成本函数可分离假设下递归制定解决方案、从无约束博弈导出解，并针对线性二次情况转化为线性互补性问题。

Result: 结果显示FSN解可从相关无约束博弈导出，对于线性二次情况，转化为大规模线性互补性问题，并用动态双寡头博弈举例说明。

Conclusion: 结论是这种方法有效，可应用于带约束的动态博弈分析。

Abstract: In this paper, we study a class of two-player deterministic finite-horizon
difference games with coupled inequality constraints, where each player has two
types of decision variables: one involving sequential interactions and the
other simultaneous interactions. We refer to these as quasi-hierarchical
dynamic games and define a solution concept called the feedback
Stackelberg-Nash (FSN) equilibrium. Under a separability assumption on cost
functions, we formulate FSN solutions recursively using a dynamic
programming-like approach. We further show that the FSN solution for these
constrained games can be derived from the parametric feedback Stackelberg
solution of an associated unconstrained game with only sequential interactions,
given parameter choices that satisfy implicit complementarity conditions. For
the linear-quadratic case, we show that the FSN solutions are obtained by
reformulating these complementarity conditions as a single large-scale linear
complementarity problem. Finally, we illustrate our results with a dynamic
duopoly game with production constraints.

</details>


### [252] [Fully Adaptive Stepsizes: Which System Benefit More -- Centralized or Decentralized?](https://arxiv.org/abs/2504.15196)
*Diyako Ghaderyan,Stefan Werner*

Main category: math.OC

TL;DR: AdGT 是一种自适应梯度跟踪方法，用于去中心化优化，每个代理根据本地目标函数的光滑度调整步长，提高收敛性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化中，步长选择至关重要，共享步长需要仔细调优，可能导致慢速收敛，尤其当代理间局部目标函数的 L-光滑性差异大时。个体调优步长在大型网络中不实用。

Method: 提出 AdGT 自适应梯度跟踪方法，每个代理基于本地目标函数的光滑度调整其步长，并证明迭代序列收敛到最优共识解。

Result: 通过数值实验，与固定步长梯度跟踪方法比较，AdGT 性能更优；与中心化设置的适应性梯度下降 (AdGD) 比较，发现去中心化网络中完全自适应步长提供更大益处。

Conclusion: AdGT 在去中心化优化中实现了更好的性能，证明了自适应步长的优势。

Abstract: In decentralized optimization, the choice of stepsize plays a critical role
in algorithm performance. A common approach is to use a shared stepsize across
all agents to ensure convergence. However, selecting an optimal stepsize often
requires careful tuning, which can be time-consuming and may lead to slow
convergence, especially when there is significant variation in the smoothness
(L-smoothness) of local objective functions across agents. Individually tuning
stepsizes per agent is also impractical, particularly in large-scale networks.
To address these limitations, we propose AdGT, an adaptive gradient tracking
method that enables each agent to adjust its stepsize based on the smoothness
of its local objective. We prove that AdGT generates a sequence of iterates
that converges to the optimal consensus solution. Through numerical
experiments, we compare AdGT with fixed-stepsize gradient tracking methods and
demonstrate its superior performance. Additionally, we compare AdGT with
adaptive gradient descent (AdGD) in a centralized setting and observe that
fully adaptive stepsizes offer greater benefits in decentralized networks than
in centralized ones.

</details>


### [253] [OPO: Making Decision-Focused Data Acquisition Decisions](https://arxiv.org/abs/2504.15062)
*Egon Peršak,Miguel F. Anjos*

Main category: math.OC

TL;DR: 这篇论文提出一种模型，通过可微优化将数据获取集成到上下文随机优化问题中，应用于无人机侦察的最短路径问题，并展示了比随机搜索更好的性能。


<details>
  <summary>Details</summary>
Motivation: 数据获取成本高且受约束，现有方法使用次优的启发式代理目标，因此需要通过集成预测和优化来改善下游决策质量。

Method: 通过学习一个代理线性目标函数，使用可微优化来解决数据获取问题。

Result: 在最短路径问题上应用，展示了不同的训练模式，并证明可微优化方法优于随机搜索策略。

Conclusion: 可微优化方法有效地改进了数据获取决策的质量。

Abstract: We propose a model for making data acquisition decisions for variables in
contextual stochastic optimisation problems. Data acquisition decisions are
typically treated as separate and fixed. We explore problem settings in which
the acquisition of contextual variables is costly and consequently constrained.
The data acquisition problem is often solved heuristically for proxy objectives
such as coverage. The more intuitive objective is the downstream decision
quality as a result of data acquisition decisions. The whole pipeline can be
characterised as an optimise-then-predict-then-optimise (OPO) problem.
Analogously, much recent research has focused on how to integrate prediction
and optimisation (PO) in the form of decision-focused learning. We propose
leveraging differentiable optimisation to extend the integration to data
acquisition. We solve the data acquisition problem with well-defined
constraints by learning a surrogate linear objective function. We demonstrate
an application of this model on a shortest path problem for which we first have
to set a drone reconnaissance strategy to capture image segments serving as
inputs to a model that predicts travel costs. We ablate the problem with a
number of training modalities and demonstrate that the differentiable
optimisation approach outperforms random search strategies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [254] [Causal pieces: analysing and improving spiking neural networks piece by piece](https://arxiv.org/abs/2504.14015)
*Dominik Dold,Philipp Christian Petersen*

Main category: cs.NE

TL;DR: 本文引入SNN的新概念'因果片段'，基于ANN的'线性片段'，证明输入域分解为因果区域，并通过模拟展示其与训练成功性的相关性。


<details>
  <summary>Details</summary>
Motivation: 分析SNN的表达性和可训练性，使用ANN的'线性片段'想法作为灵感。

Method: 证明输入域分解为因果片段，使用模拟验证参数初始化与因果片段数量的相关性，并测试正权重前馈SNN在基准任务上的性能。

Result: 发现因果片段数量高的初始化与训练成功相关，正权重前馈SNN有高因果片段数和竞争性性能。

Conclusion: 因果片段是改进SNN和比较SNN与ANN的有力工具。

Abstract: We introduce a novel concept for spiking neural networks (SNNs) derived from
the idea of "linear pieces" used to analyse the expressiveness and trainability
of artificial neural networks (ANNs). We prove that the input domain of SNNs
decomposes into distinct causal regions where its output spike times are
locally Lipschitz continuous with respect to the input spike times and network
parameters. The number of such regions - which we call "causal pieces" - is a
measure of the approximation capabilities of SNNs. In particular, we
demonstrate in simulation that parameter initialisations which yield a high
number of causal pieces on the training set strongly correlate with SNN
training success. Moreover, we find that feedforward SNNs with purely positive
weights exhibit a surprisingly high number of causal pieces, allowing them to
achieve competitive performance levels on benchmark tasks. We believe that
causal pieces are not only a powerful and principled tool for improving SNNs,
but might also open up new ways of comparing SNNs and ANNs in the future.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [255] [Transformation of audio embeddings into interpretable, concept-based representations](https://arxiv.org/abs/2504.14076)
*Alice Zhang,Edison Thomaz,Lie Lu*

Main category: cs.SD

TL;DR: This paper improves interpretability of audio embeddings using CLAP and concept-based representations, achieving comparable or better performance on tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in black-box audio neural networks.

Method: Using CLAP for contrastive learning and a post-hoc method to transform embeddings into sparse, concept-based representations, with fine-tuning.

Result: Concept-based representations perform as well or better than originals on downstream tasks, and new audio vocabularies are published.

Conclusion: The approach adds interpretability without performance loss and contributes useful resources.

Abstract: Advancements in audio neural networks have established state-of-the-art
results on downstream audio tasks. However, the black-box structure of these
models makes it difficult to interpret the information encoded in their
internal audio representations. In this work, we explore the semantic
interpretability of audio embeddings extracted from these neural networks by
leveraging CLAP, a contrastive learning model that brings audio and text into a
shared embedding space. We implement a post-hoc method to transform CLAP
embeddings into concept-based, sparse representations with semantic
interpretability. Qualitative and quantitative evaluations show that the
concept-based representations outperform or match the performance of original
audio embeddings on downstream tasks while providing interpretability.
Additionally, we demonstrate that fine-tuning the concept-based representations
can further improve their performance on downstream tasks. Lastly, we publish
three audio-specific vocabularies for concept-based interpretability of audio
embeddings.

</details>


### [256] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON 是一个灵活的框架，用于通过分布奖励微调媒体生成模型，相比传统 RLHF 或 DPO 方法更具优势。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是提供一种更灵活的奖励优化方法，能够处理实例级和分布级奖励，提高生成模型的人类感知质量，而不依赖人类反馈。

Method: DRAGON 框架使用编码器和参考示例构建奖励函数，收集生成样本，创建正负集，通过对比最大化奖励。

Result: 在音频生成模型上测试 20 种奖励函数，平均获胜率 81.45%，使用示例集奖励函数的人类投票音乐质量获胜率 60.95%。

Conclusion: DRAGON 提供了一种新方法来设计和优化奖励函数，提升媒体生成的人类感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [257] [Application of Sensitivity Analysis Methods for Studying Neural Network Models](https://arxiv.org/abs/2504.15100)
*Jiaxuan Miao,Sergey Matveev*

Main category: math.NA

TL;DR: 本研究展示了多种方法分析神经网络对输入扰动的敏感性及其机制，包括Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，应用于小型神经网络和卷积神经网络如VGG-16、ResNet-18，并与Grad-CAM在超声数据中比较。


<details>
  <summary>Details</summary>
Motivation: 为了提高神经网络的可解释性，分析其对输入数据扰动的敏感性和底层机制。

Method: 使用Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，应用于临床糖尿病数据集的小型前馈神经网络以及图像分类的VGG-16和ResNet-18模型。

Result: 通过全局敏感性分析识别并减少输入参数而不显著损失准确性；局部敏感性和激活最大化方法在卷积神经网络中显示有趣模式；与Grad-CAM在超声数据分析中进行了比较。

Conclusion: 敏感性分析方法有助于理解不同规模神经网络的行为，提高模型解释性。

Abstract: This study demonstrates the capabilities of several methods for analyzing the
sensitivity of neural networks to perturbations of the input data and
interpreting their underlying mechanisms. The investigated approaches include
the Sobol global sensitivity analysis, the local sensitivity method for input
pixel perturbations and the activation maximization technique. As examples, in
this study we consider a small feedforward neural network for analyzing an open
tabular dataset of clinical diabetes data, as well as two classical
convolutional architectures, VGG-16 and ResNet-18, which are widely used in
image processing and classification. Utilization of the global sensitivity
analysis allows us to identify the leading input parameters of the chosen tiny
neural network and reduce their number without significant loss of the
accuracy. As far as global sensitivity analysis is not applicable to larger
models we try the local sensitivity analysis and activation maximization method
in application to the convolutional neural networks. These methods show
interesting patterns for the convolutional models solving the image
classification problem. All in all, we compare the results of the activation
maximization method with popular Grad-CAM technique in the context of
ultrasound data analysis.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [258] [Enhanced UAV Navigation Systems through Sensor Fusion with Trident Quaternions](https://arxiv.org/abs/2504.14133)
*Sebastian Incicco,Juan Ignacio Giribet,Leonardo Colombo*

Main category: cs.RO

TL;DR: 本文提出基于三叉四元数的集成导航算法，通过实验验证其在多旋翼无人机上的性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现更精确和鲁棒的导航，利用三叉四元数的优势。

Method: 使用三叉四元数扩展双四元数，开发导航算法，并通过多旋翼无人机实验测试。

Result: 实验验证显示，算法性能可靠，与商用自动驾驶仪相当。

Conclusion: 三叉四元数基于导航方法可提升导航系统的精确性和鲁棒性。

Abstract: This paper presents an integrated navigation algorithm based on trident
quaternions, an extension of dual quaternions. The proposed methodology
provides an efficient approach for achieving precise and robust navigation by
leveraging the advantages of trident quaternions. The performance of the
navigation system was validated through experimental tests using a multi-rotor
UAV equipped with two navigation computers: one executing the proposed
algorithm and the other running a commercial autopilot, which was used as a
reference.

</details>


### [259] [Haptic-based Complementary Filter for Rigid Body Rotations](https://arxiv.org/abs/2504.14570)
*Amit Kumar,Domenico Campolo,Ravi N. Banavar*

Main category: cs.RO

TL;DR: 这篇论文提出了一种互补滤波框架，用于3D方向估计，结合超二次曲面、SO(3)对称性及力和视觉传感器，并在双臂机器人实验中验证。


<details>
  <summary>Details</summary>
Motivation: 解决3D旋转非交换性带来的挑战，特别是接触丰富任务中触觉信息的使用，现有的非线性滤波器未应用于触觉测量。

Method: 互补滤波框架，通过解释物体几何为超二次曲面，利用SO(3)对称性，并整合力和视觉传感器测量。

Result: 实验证明框架具有鲁棒性和几乎全局稳定性，在双臂机器人设置中得到验证。

Conclusion: 框架有效估计3D方向，结合触觉和视觉输入，展示了稳定性能。

Abstract: The non-commutative nature of 3D rotations poses well-known challenges in
generalizing planar problems to three-dimensional ones, even more so in
contact-rich tasks where haptic information (i.e., forces/torques) is involved.
In this sense, not all learning-based algorithms that are currently available
generalize to 3D orientation estimation. Non-linear filters defined on
$\mathbf{\mathbb{SO}(3)}$ are widely used with inertial measurement sensors;
however, none of them have been used with haptic measurements. This paper
presents a unique complementary filtering framework that interprets the
geometric shape of objects in the form of superquadrics, exploits the symmetry
of $\mathbf{\mathbb{SO}(3)}$, and uses force and vision sensors as measurements
to provide an estimate of orientation. The framework's robustness and almost
global stability are substantiated by a set of experiments on a dual-arm
robotic setup.

</details>


### [260] [Safe Autonomous Environmental Contact for Soft Robots using Control Barrier Functions](https://arxiv.org/abs/2504.14755)
*Akua K. Dickson,Juan C. Pacheco Garcia,Meredith L. Anderson,Ran Jing,Sarah Alizadeh-Shabdiz,Audrey X. Wang,Charles DeLorey,Zach J. Patterson,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: 本论文开发了第一个正式满足安全规范的软体机械臂反馈控制器，确保环境接触力在界限内。


<details>
  <summary>Details</summary>
Motivation: 软体机器人施加的力较小，适合敏感环境，但需要控制器来确保安全，以避免意外接触。

Method: 将力边界映射到机器人尖端的安全位置集，通过预测环境变形；使用带有控制屏障函数的二次规划监督 nominal 反馈信号。

Result: 硬件实验证明，该框架成功约束了多段软气动机器人的环境接触力。

Conclusion: 这标志着软体机器人控制和安全领域的根本转变，实现了可验证的姿势和接触力逻辑规范。

Abstract: Robots built from soft materials will inherently apply lower environmental
forces than their rigid counterparts, and therefore may be more suitable in
sensitive settings with unintended contact. However, these robots' applied
forces result from both their design and their control system in closed-loop,
and therefore, ensuring bounds on these forces requires controller synthesis
for safety as well. This article introduces the first feedback controller for a
soft manipulator that formally meets a safety specification with respect to
environmental contact. In our proof-of-concept setting, the robot's environment
has known geometry and is deformable with a known elastic modulus. Our approach
maps a bound on applied forces to a safe set of positions of the robot's tip
via predicted deformations of the environment. Then, a quadratic program with
Control Barrier Functions in its constraints is used to supervise a nominal
feedback signal, verifiably maintaining the robot's tip within this safe set.
Hardware experiments on a multi-segment soft pneumatic robot demonstrate that
the proposed framework successfully constrains its environmental contact
forces. This framework represents a fundamental shift in perspective on control
and safety for soft robots, defining and implementing a formally verifiable
logic specification on their pose and contact forces.

</details>


### [261] [Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative System for Underwater Tasks in Extreme Sea Conditions](https://arxiv.org/abs/2504.14894)
*Jingzehua Xu,Guanwen Xie,Jiwei Tang,Yimian Ding,Weiyi Liu,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 本论文开发了一种新型USV-AUV协作系统，以提升极端海况下水下任务性能，并通过实验验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 为了在极端海况下增强水下任务的性能和稳定性。

Method: 整合了双重策略：(1) 使用Fisher信息矩阵优化USV路径规划实现高精度多AUV定位，(2) 基于强化学习的合作规划和控制方法。

Result: 实验评估显示系统在水下数据收集任务中表现出色，性能显著优于基准方法，并展示了在极端海况下的鲁棒协调能力。

Conclusion: 该系统实现了USV和AUV之间的稳健协调，并提供了开源模拟工具包以促进再现性和社区发展。

Abstract: This paper develops a novel unmanned surface vehicle (USV)-autonomous
underwater vehicle (AUV) collaborative system designed to enhance underwater
task performance in extreme sea conditions. The system integrates a dual
strategy: (1) high-precision multi-AUV localization enabled by Fisher
information matrix-optimized USV path planning, and (2) reinforcement
learning-based cooperative planning and control method for multi-AUV task
execution. Extensive experimental evaluations in the underwater data collection
task demonstrate the system's operational feasibility, with quantitative
results showing significant performance improvements over baseline methods. The
proposed system exhibits robust coordination capabilities between USV and AUVs
while maintaining stability in extreme sea conditions. To facilitate
reproducibility and community advancement, we provide an open-source simulation
toolkit available at: https://github.com/360ZMEM/USV-AUV-colab .

</details>


### [262] [A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing](https://arxiv.org/abs/2504.15226)
*Nathan Steffen,Wilhelm Louw,Nicholas Ernest,Timothy Arnett,Kelly Cohen*

Main category: cs.RO

TL;DR: 本研究结合遗传模糊树和LQR控制，创建了用于卫星维护的机器人控制器，比最佳LQR平均高效18.5%，并对不确定性具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星数量增加，月地空间机器人自动化服务变得重要，安全是关键，需要可信赖且高效的控制技术。

Method: 通过Thales' TrUE AI Toolkit将遗传模糊树与LQR控制方案相结合，应用于二维平面机器人操纵器。

Result: 遗传模糊-LQR比最佳LQR平均性能提高18.5%，并对不确定性具有极高鲁棒性。

Conclusion: 这种方法为卫星维护提供了更安全和高效的控制方案。

Abstract: Automation of robotic systems for servicing in cislunar space is becoming
extremely important as the number of satellites in orbit increases. Safety is
critical in performing satellite maintenance, so the control techniques
utilized must be trusted in addition to being highly efficient. In this work,
Genetic Fuzzy Trees are combined with the widely used LQR control scheme via
Thales' TrUE AI Toolkit to create a trusted and efficient controller for a
two-degree-of-freedom planar robotic manipulator that would theoretically be
used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is
18.5% more performant than optimal LQR on average, and that it is incredibly
robust to uncertainty.

</details>


### [263] [Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility](https://arxiv.org/abs/2504.14103)
*Merve Atasever,Ali Okhovat,Azhang Nazaripouya,John Nisbet,Omer Kurkutlu,Jyotirmoy V. Deshmukh,Yasemin Ozkan Aydin*

Main category: cs.RO

TL;DR: 本研究比较深度强化学习和生物启发步态设计在蝾螈-like机器人上的应用，以应对环境不确定性。


<details>
  <summary>Details</summary>
Motivation: 蝾螈脊柱灵活性有助于运动，但环境不确定性（如不平路面）导致协调问题，需要动态适应策略。

Method: 通过在蝾螈-like机器人上比较学习-based控制策略（如DRL）和生物启发步态设计方法。

Result: DRL在处理不确定环境中的鲁棒性表现优异，但摘要未详述具体结果。

Conclusion: DRL提供有效框架，帮助机器人适应不确定条件，实现高效运动。

Abstract: Among vertebrates, salamanders, with their unique ability to transition
between walking and swimming gaits, highlight the role of spinal mobility in
locomotion. A flexible spine enables undulation of the body through a wavelike
motion along the spine, aiding navigation over uneven terrains and obstacles.
Yet environmental uncertainties, such as surface irregularities and variations
in friction, can significantly disrupt body-limb coordination and cause
discrepancies between predictions from mathematical models and real-world
outcomes. Addressing this challenge requires the development of sophisticated
control strategies capable of dynamically adapting to uncertain conditions
while maintaining efficient locomotion. Deep reinforcement learning (DRL)
offers a promising framework for handling non-deterministic environments and
enabling robotic systems to adapt effectively and perform robustly under
challenging conditions. In this study, we comparatively examine learning-based
control strategies and biologically inspired gait design methods on a
salamander-like robot.

</details>


### [264] [Knitting Robots: A Deep Learning Approach for Reverse-Engineering Fabric Patterns](https://arxiv.org/abs/2504.14007)
*Haoliang Sheng,Songpu Cai,Xingyu Zheng,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 本研究提出了一种基于深度学习的管道，用于反向编织，以桥接纺织生产和机器人自动化的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 编织自动化特别具有挑战性，尤其是将织物设计转换为机器可读指令，因此本研究旨在整合视觉机器人系统到纺织制造中。

Method: 管道采用两阶段架构，机器人首先识别正面标签，然后推断完整标签，并处理单纱和多纱结构。

Result: 系统解决了标签不平衡、针迹类型不足等问题，展示了适应不同材料复杂性的能力，并为全自动机器人编织系统奠定基础。

Conclusion: 这项工作通过整合感知、规划和执行，推进了纺织制造的智能机器人自动化，实现可定制、灵活的生产过程。

Abstract: Knitting, a cornerstone of textile manufacturing, is uniquely challenging to
automate, particularly in terms of converting fabric designs into precise,
machine-readable instructions. This research bridges the gap between textile
production and robotic automation by proposing a novel deep learning-based
pipeline for reverse knitting to integrate vision-based robotic systems into
textile manufacturing. The pipeline employs a two-stage architecture, enabling
robots to first identify front labels before inferring complete labels,
ensuring accurate, scalable pattern generation. By incorporating diverse yarn
structures, including single-yarn (sj) and multi-yarn (mj) patterns, this study
demonstrates how our system can adapt to varying material complexities.
Critical challenges in robotic textile manipulation, such as label imbalance,
underrepresented stitch types, and the need for fine-grained control, are
addressed by leveraging specialized deep-learning architectures. This work
establishes a foundation for fully automated robotic knitting systems, enabling
customizable, flexible production processes that integrate perception,
planning, and actuation, thereby advancing textile manufacturing through
intelligent robotic automation.

</details>


### [265] [Experience-based Refinement of Task Planning Knowledge in Autonomous Robots](https://arxiv.org/abs/2504.14259)
*Hadeel Jazzaa,Thomas McCluskey,David Peebles*

Main category: cs.RO

TL;DR: 这篇论文展示了一个物理机器人通过使用行动执行经验来改进符号知识，从而提高任务计划成功率的方法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决自主机器人适应不断变化环境的需求，这是AI社区的挑战，且现有规划进展尚未应用于实际机器人。

Method: 方法是通过机器人行动执行经验驱动知识改进，提出一种改进领域知识的机制以提升规划系统的鲁棒性。

Result: 结果是使用NAO机器人实现，改进后的知识导致任务计划失败率逐渐降低。

Conclusion: 结论是机器人能够通过知识改进适应环境，从而随着时间推移减少计划失败。

Abstract: The requirement for autonomous robots to exhibit higher-level cognitive
skills by planning and adapting in an ever-changing environment is indeed a
great challenge for the AI community. Progress has been made in the automated
planning community on refinement and repair of an agent's symbolic knowledge to
do task planning in an incomplete or changing environmental model, but these
advances up to now have not been transferred to real physical robots. This
paper demonstrates how a physical robot can be capable of adapting its symbolic
knowledge of the environment, by using experiences in robot action execution to
drive knowledge refinement and hence to improve the success rate of the task
plans the robot creates. To implement more robust planning systems, we propose
a method for refining domain knowledge to improve the knowledge on which
intelligent robot behavior is based. This architecture has been implemented and
evaluated using a NAO robot. The refined knowledge leads to the future
synthesis of task plans which demonstrate decreasing rates of failure over time
as faulty knowledge is removed or adjusted.

</details>


### [266] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文介绍了Unreal Robotics Lab (URL)框架，它整合Unreal Engine的渲染能力和MuJoCo的物理模拟，提供高保真机器人模拟，用于基准测试和数据集生成。


<details>
  <summary>Details</summary>
Motivation: 机器人研究需要高保真模拟以安全高效测试感知、控制和导航算法，但同时实现逼真渲染和准确物理建模具有挑战性。

Method: 提出一个新框架，将Unreal Engine的高级渲染与MuJoCo的高精度物理模拟整合，支持复杂环境效果如烟雾、火焰和水动力学。

Result: 基准测试了视觉导航和SLAM方法，展示了框架在控制环境下评估机器人鲁棒性的效用，并生成数据集。

Conclusion: 该框架弥合了物理准确性和逼真渲染的差距，为机器人研究和模拟到现实转移提供强大工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>


### [267] [Modality Selection and Skill Segmentation via Cross-Modality Attention](https://arxiv.org/abs/2504.14573)
*Jiawei Jiang,Kei Ota,Devesh K. Jha,Asako Kanezaki*

Main category: cs.RO

TL;DR: 本论文提出跨模态注意力机制（CMA），用于选择感官模态以提升机器人动作生成和技能分割，解决多模态挑战。


<details>
  <summary>Details</summary>
Motivation: 整合触觉和音频等额外感官模态到机器人模型中，由于维度诅咒导致重大挑战。

Method: 提出CMA机制选择信息量最大的模态用于动作生成，并扩展到技能分割和分层策略训练。

Result: 实现长视野、接触丰富的操作任务解决。

Conclusion: 通过模态选择和CMA机制，成功处理多模态感知问题，提高机器人性能。

Abstract: Incorporating additional sensory modalities such as tactile and audio into
foundational robotic models poses significant challenges due to the curse of
dimensionality. This work addresses this issue through modality selection. We
propose a cross-modality attention (CMA) mechanism to identify and selectively
utilize the modalities that are most informative for action generation at each
timestep. Furthermore, we extend the application of CMA to segment primitive
skills from expert demonstrations and leverage this segmentation to train a
hierarchical policy capable of solving long-horizon, contact-rich manipulation
tasks.

</details>


### [268] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TL;DR: 本论文提出Phoenix框架，使用运动指令桥接多模态大语言模型的语义反射与机器人细粒度动作修正，提高故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: 机器人需要可泛化的自我修正系统以从故障中恢复，尽管多模态大语言模型提升了语义反射，但将语义反射转化为细粒度动作修正仍具挑战。

Method: 构建Phoenix框架，包括双过程运动调整机制和多任务运动条件扩散策略，以及终身学习方法，将语义反射转化为动作修正。

Result: 实验在RoboMimic模拟和真实场景中证明框架具有优越的泛化和鲁棒性。

Conclusion: 框架通过转移泛化需求到MLLMs驱动模型，实现精确动作修正，并开源代码以促进进一步研究。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>


### [269] [K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics](https://arxiv.org/abs/2504.14602)
*Jiwei Li,Bi Zhang,Xiaowei Tan,Wanxin Chen,Zhaoyuan Liu,Juanjuan Zhang,Weiguang Huo,Jian Huang,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 本论文介绍了K2MUSE数据集，提供下肢多模态数据，支持康复机器人控制和生物力学分析。


<details>
  <summary>Details</summary>
Motivation: 现有下肢数据集不足，无法提供必要多模态数据和大规模步态样本，且忽略实际应用干扰。

Method: 从30名健康参与者采集数据，包括不同坡度、速度和非理想条件，使用Vicon系统、力板、sEMG和AUS设备。

Result: 创建了K2MUSE数据集，包含运动学、动力学、AUS和sEMG数据，可用于机器人控制和生物力学研究。

Conclusion: 该数据集为康复机器人发展和实际应用提供新资源。

Abstract: The natural interaction and control performance of lower limb rehabilitation
robots are closely linked to biomechanical information from various human
locomotion activities. Multidimensional human motion data significantly deepen
the understanding of the complex mechanisms governing neuromuscular
alterations, thereby facilitating the development and application of
rehabilitation robots in multifaceted real-world environments. However,
currently available lower limb datasets are inadequate for supplying the
essential multimodal data and large-scale gait samples necessary for effective
data-driven approaches, and they neglect the significant effects of acquisition
interference in real applications.To fill this gap, we present the K2MUSE
dataset, which includes a comprehensive collection of multimodal data,
comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface
electromyography (sEMG) measurements. The proposed dataset includes lower limb
multimodal data from 30 able-bodied participants walking under different
inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5
m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions
(muscle fatigue, electrode shifts, and inter-day differences). The kinematic
and ground reaction force data were collected via a Vicon motion capture system
and an instrumented treadmill with embedded force plates, whereas the sEMG and
AUS data were synchronously recorded for thirteen muscles on the bilateral
lower limbs. This dataset offers a new resource for designing control
frameworks for rehabilitation robots and conducting biomechanical analyses of
lower limb locomotion. The dataset is available at https://k2muse.github.io/.

</details>


### [270] [An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework](https://arxiv.org/abs/2504.14681)
*Zeyu Wang,Frank P. -W. Lo,Qian Chen,Yongqi Zhang,Chen Lin,Xu Chen,Zhenhua Yu,Alexander J. Thompson,Eric M. Yeatman,Benny P. L. Lo*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于LLM的多代理框架，用于自主设计机电一体化系统，并通过水质监测船原型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多代理框架限于数字环境和狭窄领域，无法应用于需要物理设计、跨学科整合和约束推理的复杂工程任务。

Method: 框架整合机械设计、优化、电子和软件工程，通过语言驱动工作流和结构化用户反馈，应用于自主水质监测船的设计。

Result: 成功开发了功能齐全的自主船只，具有优化推进、成本有效电子和先进控制系统。

Conclusion: 证明了LLM多代理系统可自动化工程工作流程，减少对领域专业知识的依赖。

Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to
digital or simulated environments and confined to narrowly focused knowledge
domain, constraining their applicability to complex engineering tasks that
require the design of physical embodiment, cross-disciplinary integration, and
constraint-aware reasoning. This work proposes a multi-agent autonomous
mechatronics design framework, integrating expertise across mechanical design,
optimization, electronics, and software engineering to autonomously generate
functional prototypes with minimal direct human design input. Operating
primarily through a language-driven workflow, the framework incorporates
structured human feedback to ensure robust performance under real-world
constraints. To validate its capabilities, the framework is applied to a
real-world challenge involving autonomous water-quality monitoring and
sampling, where traditional methods are labor-intensive and ecologically
disruptive. Leveraging the proposed system, a fully functional autonomous
vessel was developed with optimized propulsion, cost-effective electronics, and
advanced control. The design process was carried out by specialized agents,
including a high-level planning agent responsible for problem abstraction and
dedicated agents for structural, electronics, control, and software
development. This approach demonstrates the potential of LLM-based multi-agent
systems to automate real-world engineering workflows and reduce reliance on
extensive domain expertise.

</details>


### [271] [A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors](https://arxiv.org/abs/2504.14739)
*Arpit Agarwal,Mohammad Amin Mirzaee,Xiping Sun,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 本论文提出一种系统优化GelSight触觉传感器设计的方法，使用光学模拟和工具箱简化过程。


<details>
  <summary>Details</summary>
Motivation: 现有传感器定制过程繁琐且依赖试错，需要一个系统化和目标驱动的方法来提高效率。

Method: 通过模块化和参数化光学组件，定义通用目标函数，并使用物理光学模拟和OptiSense Studio工具箱进行优化。

Result: 在模拟中快速优化四个不同GelSight传感器的设计，并成功转移到真实传感器。

Conclusion: 该方法使非专家能够轻松设计传感器，提高了设计通用性和效率。

Abstract: GelSight family of vision-based tactile sensors has proven to be effective
for multiple robot perception and manipulation tasks. These sensors are based
on an internal optical system and an embedded camera to capture the deformation
of the soft sensor surface, inferring the high-resolution geometry of the
objects in contact. However, customizing the sensors for different robot hands
requires a tedious trial-and-error process to re-design the optical system. In
this paper, we formulate the GelSight sensor design process as a systematic and
objective-driven design problem and perform the design optimization with a
physically accurate optical simulation. The method is based on modularizing and
parameterizing the sensor's optical components and designing four generalizable
objective functions to evaluate the sensor. We implement the method with an
interactive and easy-to-use toolbox called OptiSense Studio. With the toolbox,
non-sensor experts can quickly optimize their sensor design in both forward and
inverse ways following our predefined modules and steps. We demonstrate our
system with four different GelSight sensors by quickly optimizing their initial
design in simulation and transferring it to the real sensors.

</details>


### [272] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TL;DR: 本文提出一个平台，实现端到端深度强化学习策略从模拟到真实环境的无缝转移，针对四旋翼无人机在户外环境的应用。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在非结构化户外环境中部署机器人学习方法的挑战，包括训练数据需求、实时处理要求以及模拟到真实环境的差距。

Method: 方法是通过整合训练环境、飞行动力学控制、DRL算法、MAVROS中间件和硬件，创建一个工作流和架构，使策略从零开始训练并在几分钟内部署到真实世界。

Result: 结果展示了平台的效率，通过实证验证和在真实世界扰动下的鲁棒户外飞行性能，并提供了多种环境作为基准。

Conclusion: 结论是该平台桥接了模拟和真实环境的差距，实验细节可在网站https://emnavi.tech/AirGym/找到。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>


### [273] [Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning](https://arxiv.org/abs/2504.15130)
*Kushal Shah,Jihyun Park,Seung-Kyum Choi*

Main category: cs.RO

TL;DR: 本篇论文提出Neural ATTF算法，用于多代理取送货问题，提高可伸缩性、适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多代理取送货解决方案在动态环境中存在可伸缩性、适应性和效率挑战。

Method: 提出Neural ATTF算法，结合优先级引导任务匹配(PGTM)和神经网络增强的STA*路径规划。

Result: 实验显示Neural ATTF在可伸缩性、解决方案质量和计算效率上优于TPTS、CENTRAL等算法。

Conclusion: 该框架适用于复杂真实世界多代理系统的需求。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,
particularly in applications such as warehouse automation and logistics.
Existing solutions often face challenges in scalability, adaptability, and
efficiency, limiting their applicability in dynamic environments with real-time
planning requirements. This paper presents Neural ATTF (Adaptive Task Token
Framework), a new algorithm that combines a Priority Guided Task Matching
(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning
method. Neural STA* enhances path planning by enabling rapid exploration of the
search space through guided learned heuristics and ensures collision avoidance
under dynamic constraints. PGTM prioritizes delayed agents and dynamically
assigns tasks by prioritizing agents nearest to these tasks, optimizing both
continuity and system throughput. Experimental evaluations against
state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and
LNS-wPBS, demonstrate the superior scalability, solution quality, and
computational efficiency of Neural ATTF. These results highlight the
framework's potential for addressing the critical demands of complex,
real-world multi-agent systems operating in high-demand, unpredictable
settings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [274] [Resource Utilization Optimized Federated Learning](https://arxiv.org/abs/2504.13850)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: FedOptima 是一种优化联邦学习的系统，通过异步方法和分层训练减少空闲时间，提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统资源利用率低，主要由于任务依赖和异质设备拖延者导致的空闲时间，现有的方法无法同时减少两者。

Method: FedOptima 通过异步聚合消除拖延者影响、辅助网络减少任务依赖空闲时间、任务调度器确保设备平衡贡献，以及高效内存管理提高可扩展性。

Result: 与基线方法相比，FedOptima 实现准确性相当或更高，训练速度提高1.9倍至21.8倍，空闲时间减少高达93.9%和81.8%，吞吐量提高1.1倍至2.0倍。

Conclusion: FedOptima 显著提升了联邦学习的效率和实用性，使其更适合真实世界应用。

Abstract: Federated learning (FL) systems facilitate distributed machine learning
across a server and multiple devices. However, FL systems have low resource
utilization limiting their practical use in the real world. This inefficiency
primarily arises from two types of idle time: (i) task dependency between the
server and devices, and (ii) stragglers among heterogeneous devices. This paper
introduces FedOptima, a resource-optimized FL system designed to simultaneously
minimize both types of idle time; existing systems do not eliminate or reduce
both at the same time. FedOptima offloads the training of certain layers of a
neural network from a device to server using three innovations. First, devices
operate independently of each other using asynchronous aggregation to eliminate
straggler effects, and independently of the server by utilizing auxiliary
networks to minimize idle time caused by task dependency. Second, the server
performs centralized training using a task scheduler that ensures balanced
contributions from all devices, improving model accuracy. Third, an efficient
memory management mechanism on the server increases scalability of the number
of participating devices. Four state-of-the-art offloading-based and
asynchronous FL methods are chosen as baselines. Experimental results show that
compared to the best results of the baselines on convolutional neural networks
and transformers on multiple lab-based testbeds, FedOptima (i) achieves higher
or comparable accuracy, (ii) accelerates training by 1.9x to 21.8x, (iii)
reduces server and device idle time by up to 93.9% and 81.8%, respectively, and
(iv) increases throughput by 1.1x to 2.0x.

</details>


### [275] [PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/abs/2504.14145)
*Zhenliang Xue,Hanpeng Hu,Xing Chen,Yimin Jiang,Yixin Song,Zeyu Mi,Yibo Zhu,Daxin Jiang,Yubin Xia,Haibo Chen*

Main category: cs.DC

TL;DR: PipeWeaver框架通过动态管道调度提高大型多模态模型（LMM）训练效率，解决管道不平衡和数据动态性问题，效率提升高达97.3%。


<details>
  <summary>Details</summary>
Motivation: LMM训练存在管道阶段不平衡（因异构架构）和数据动态性（因多模态数据多样性）导致效率低下。

Method: 采用动态交错管道、适应性模态感知分区、层级调度空间搜索和SEMU模拟器（利用空间-时间子图重用加速）。

Result: 实验显示比最先进系统效率提升高达97.3%，并对数据动态性适应性强。

Conclusion: PipeWeaver显著提升LMM训练效率，并展示优秀的数据动态性适应能力。

Abstract: Large multimodal models (LMMs) have demonstrated excellent capabilities in
both understanding and generation tasks with various modalities. While these
models can accept flexible combinations of input data, their training
efficiency suffers from two major issues: pipeline stage imbalance caused by
heterogeneous model architectures, and training data dynamicity stemming from
the diversity of multimodal data.
  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework
designed for LMM training. The core of PipeWeaver is dynamic interleaved
pipeline, which searches for pipeline schedules dynamically tailored to current
training batches. PipeWeaver addresses issues of LMM training with two
techniques: adaptive modality-aware partitioning and efficient pipeline
schedule search within a hierarchical schedule space. Meanwhile, PipeWeaver
utilizes SEMU (Step Emulator), a training simulator for multimodal models, for
accurate performance estimations, accelerated by spatial-temporal subgraph
reuse to improve search efficiency. Experiments show that PipeWeaver can
enhance LMM training efficiency by up to 97.3% compared to state-of-the-art
systems, and demonstrate excellent adaptivity to LMM training's data
dynamicity.

</details>


### [276] [GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning](https://arxiv.org/abs/2504.14628)
*Shunxin Guo,Jiaqi Lv,Qiufeng Wang,Xin Geng*

Main category: cs.DC

TL;DR: 提出GENE-FL框架，针对动态联邦学习中的通信效率和模型初始化问题，通过Learngene范式优化。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统面临动态客户端和异质数据分布的挑战，需要高效通信和快速适应。

Method: 使用二次约束和高Fisher值参数凝练learnGene，进行参数敏感性分析，服务器聚合以初始化模型。

Result: 相比FedAvg，通信成本减少4倍，仅需约9.04 MB初始化无知客户端模型。

Conclusion: GENE-FL有效提升DAFL场景下的通信效率和模型泛化能力。

Abstract: Real-world \underline{F}ederated \underline{L}earning systems often encounter
\underline{D}ynamic clients with \underline{A}gnostic and highly heterogeneous
data distributions (DAFL), which pose challenges for efficient communication
and model initialization. To address these challenges, we draw inspiration from
the recently proposed Learngene paradigm, which compresses the large-scale
model into lightweight, cross-task meta-information fragments. Learngene
effectively encapsulates and communicates core knowledge, making it
particularly well-suited for DAFL, where dynamic client participation requires
communication efficiency and rapid adaptation to new data distributions. Based
on this insight, we propose a Gene-driven parameter-efficient dynamic Federated
Learning (GENE-FL) framework. First, local models perform quadratic constraints
based on parameters with high Fisher values in the global model, as these
parameters are considered to encapsulate generalizable knowledge. Second, we
apply the strategy of parameter sensitivity analysis in local model parameters
to condense the \textit{learnGene} for interaction. Finally, the server
aggregates these small-scale trained \textit{learnGene}s into a robust
\textit{learnGene} with cross-task generalization capability, facilitating the
rapid initialization of dynamic agnostic client models. Extensive experimental
results demonstrate that GENE-FL reduces \textbf{4 $\times$} communication
costs compared to FEDAVG and effectively initializes agnostic client models
with only about \textbf{9.04} MB.

</details>


### [277] [Is Intelligence the Right Direction in New OS Scheduling for Multiple Resources in Cloud Environments?](https://arxiv.org/abs/2504.15021)
*Xinglei Dou,Lei Liu,Limin Xiao*

Main category: cs.DC

TL;DR: 本文提出OSML+，一种基于机器学习的云服务资源调度机制，能够智能调度资源，提高效率并处理动态工作负载。


<details>
  <summary>Details</summary>
Motivation: 通过智能方法提升系统/OS设计，解决资源调度中的复杂问题，如避免资源悬崖、应用间资源共享和优先级调度。

Method: OSML+采用多模型协作学习方法，同时调度内存层次和计算核心资源，使用ML模型实现快速收敛、实时学习和迁移学习以适应不同服务器。

Result: 实验结果显示，OSML+支持更高负载，以更低开销满足QoS目标，比现有研究更优。

Conclusion: OSML+是一种有效的智能资源调度机制，在云服务中表现出色，优于现有方法。

Abstract: Making it intelligent is a promising way in System/OS design. This paper
proposes OSML+, a new ML-based resource scheduling mechanism for co-located
cloud services. OSML+ intelligently schedules the cache and main memory
bandwidth resources at the memory hierarchy and the computing core resources
simultaneously. OSML+ uses a multi-model collaborative learning approach during
its scheduling and thus can handle complicated cases, e.g., avoiding resource
cliffs, sharing resources among applications, enabling different scheduling
policies for applications with different priorities, etc. OSML+ can converge
faster using ML models than previous studies. Moreover, OSML+ can automatically
learn on the fly and handle dynamically changing workloads accordingly. Using
transfer learning technologies, we show our design can work well across various
cloud servers, including the latest off-the-shelf large-scale servers. Our
experimental results show that OSML+ supports higher loads and meets QoS
targets with lower overheads than previous studies.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [278] [Koopman Spectral Analysis and System Identification for Stochastic Dynamical Systems via Yosida Approximation of Generators](https://arxiv.org/abs/2504.13912)
*Jun Zhou,Yiming Meng,Jun Liu*

Main category: eess.SY

TL;DR: 本论文提出了一种新方法，使用分辨算子理论和Yosida逼近来估计随机微分方程系统的Koopman生成器，提高了谱分析和参数估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 系统识别和Koopman谱分析对于揭示物理定律和理解随机动力系统的长期行为至关重要。

Method: 提出基于分辨算子理论和Yosida逼近的新方法，估计Koopman生成器，避免误差放大，仅需温和假设，即使在低采样率或单一轨迹下也能有效。

Result: 在两个简单系统中验证，并与现有技术比较，实验结果显示了方法在参数估计、谱模式提取和鲁棒性方面的改进性能。

Conclusion: 实验结果证明了方法的有效性和整体性能提升。

Abstract: System identification and Koopman spectral analysis are crucial for
uncovering physical laws and understanding the long-term behaviour of
stochastic dynamical systems governed by stochastic differential equations
(SDEs). In this work, we propose a novel method for estimating the Koopman
generator of systems of SDEs, based on the theory of resolvent operators and
the Yosida approximation. This enables both spectral analysis and accurate
estimation and reconstruction of system parameters. The proposed approach
relies on only mild assumptions about the system and effectively avoids the
error amplification typically associated with direct numerical differentiation.
It remains robust even under low sampling rates or with only a single observed
trajectory, reliably extracting dominant spectral modes and dynamic features.
We validate our method on two simple systems and compare it with existing
techniques as benchmarks. The experimental results demonstrate the
effectiveness and improved performance of our approach in system parameter
estimation, spectral mode extraction, and overall robustness.

</details>


### [279] [Scalable Two-Stage Stochastic Optimal Power Flow via Separable Approximation](https://arxiv.org/abs/2504.13933)
*Shishir Lamichhane,Abodh Poudyal,Bala Krishnamoorthy,Anamika Dubey*

Main category: eess.SY

TL;DR: 本论文提出SPAR-OPF框架，用于电力系统的两阶段随机优化问题，通过可分离的分段线性逼近值函数学习，提供高准确性和高效性。


<details>
  <summary>Details</summary>
Motivation: 为了高效解决电力系统中大规模两阶段随机优化问题，传统方法在大型系统中失败。

Method: 提出SPAR-OPF框架，使用样本子梯度信息学习可分离的分段线性值函数，呈现两种建模公式，并引入统计方法评估解决方案质量。

Result: 在分布式发电选址问题上，准确率超过98%，最优性差距小于1%，在9500节点系统下高效。

Conclusion: 框架有效、高效，能够处理大型随机优化问题，提供高质量解决方案。

Abstract: This paper proposes a Separable Projective Approximation Routine-Optimal
Power Flow (SPAR-OPF) framework for solving two-stage stochastic optimization
problems in power systems. The framework utilizes a separable piecewise linear
approximation of the value function and learns the function based on sample
sub-gradient information. We present two formulations to model the learned
value function, and compare their effectiveness. Additionally, an efficient
statistical method is introduced to assess the quality of the obtained
solutions. The effectiveness of the proposed framework is validated using
distributed generation siting and sizing problem in three-phase unbalanced
power distribution systems as an example. Results show that the framework
approximates the value function with over 98% accuracy and provides
high-quality solutions with an optimality gap of less than 1%. The framework
scales efficiently with system size, generating high-quality solutions in a
short time when applied to a 9500-node distribution system with 1200 scenarios,
while the extensive formulations and progressive hedging failed to solve the
problem.

</details>


### [280] [Probability of collision in nonlinear dynamics by moment propagation](https://arxiv.org/abs/2504.13935)
*Théo Verhelst,Giacomo Acciarini,Dario Izzo,Francesco Biscani*

Main category: eess.SY

TL;DR: 本论文提出了一种不依赖高斯假设的航天器碰撞概率计算方法，使用高阶多元Taylor多项式和正交多项式，适用于非线性动态环境。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖高斯假设和简化，在高度非线性场景下不准确，需要更通用高效的碰撞概率估计。

Method: 使用高阶多元Taylor多项式传播初始不确定性的统计矩到最近点，并通过正交多项式推导最近距离概率密度函数的半解析表达式。

Result: 在低地球轨道各种短期和长期相遇中测试，准确处理非线性动态、非高斯不确定性和不规则分布。

Conclusion: 提升空间态势感知，提供精确碰撞概率估计，并可应用于其他带不确定性的动态系统。

Abstract: Estimating the probability of collision between spacecraft is crucial for
risk management and collision-avoidance strategies. Current methods often rely
on Gaussian assumptions and simplifications, which can be inaccurate in highly
nonlinear scenarios. This paper presents a general and efficient approach for
computing collision probabilities without relying on such assumptions. Using
high-order multivariate Taylor polynomials, we propagate statistical moments of
initial uncertainties to the point of closest approach between the spacecraft.
To compute the probability of collision, we derive a semi-analytical expression
for the probability density function (PDF) of the closest approach distance,
inferred from the propagated moments using orthogonal polynomials. Tested on
various short-term and long-term encounters in low-Earth orbit, our method
accurately handles nonlinear dynamics, non-Gaussian uncertainties, and
irregular distributions. This versatile framework advances space situational
awareness by providing precise collision probability estimates in complex
dynamical environments. Moreover, our methodology applies to any dynamical
system with uncertainty in its initial state and is therefore not restricted to
collision probability estimation.

</details>


### [281] [Data Assimilation-based Simultaneous Phase-Resolved Ocean Wave and Ship Motion Forecast](https://arxiv.org/abs/2504.13943)
*Guangyao Wang,Yulin Pan*

Main category: eess.SY

TL;DR: 本论文提出了一种基于数据同化的EnKF-HOS-CMI方法，用于预测非线性波浪演化和船舶运动，提高了长期模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了改进非线性相位分辨波场和船舶运动的长期模拟精度，并整合观察数据。

Method: 通过耦合高阶谱方法(HOS)、集合卡尔曼滤波器(EnKF)和基于Cummins方程的船舶模型(CMI)，开发了EnKF-HOS-CMI方法。

Result: 方法在合成问题上验证，显示比HOS-CMI方法有更高的准确性，并能准确估计船舶参数。

Conclusion: EnKF-HOS-CMI方法是有效的，用于波浪和船舶运动的预测。

Abstract: This paper presents a data-assimilation (DA)-based approach to forecast the
phase-resolved wave evolution process and ship motion, which is developed by
coupling the high-order spectral method (HOS), ensemble Kalman filter (EnKF),
and a Cummins-equation-based ship model (CMI). With the developed EnKF-HOS-CMI
method, the observation data for wave, ship, or both can be incorporated into
the model, therefore producing the optimal analysis results. The developed
method is validated and tested based on a synthetic problem on the motions of
an irregular wave field and a box-shaped free-floating ship. We show that the
EnKF-HOS-CMI method achieves much higher accuracy in the long-term simulation
of nonlinear phase-resolved wave field and ship motion in comparison with the
HOS-CMI method. Also, the ship parameters are estimated accurately by using a
parameter-augmented state space in EnKF.

</details>


### [282] [Koopman-Based Event-Triggered Control from Data](https://arxiv.org/abs/2504.14334)
*Zeyad M. Manaa,Ayman M. Abdallah,Mohamed Ismail,Samil El Ferik*

Main category: eess.SY

TL;DR: 本文使用数据驱动方法和Koopman理论为离散时间非线性系统设计事件触发控制，确保指数稳定并节省资源。


<details>
  <summary>Details</summary>
Motivation: 为了提高网络化和嵌入式控制系统的资源利用效率，通过减少通信实例改进传统时间触发策略。

Method: 利用Koopman算子理论将非线性系统动力学线性化，并基于数据直接设计状态反馈控制器和事件触发策略。

Result: 确保Lyapunov意义下的指数稳定性，并通过模拟实验验证了显著的资源节省。

Conclusion: 该方法有效，展示了数据驱动框架在事件触发控制中的潜力。

Abstract: Event-triggered Control (ETC) presents a promising paradigm for efficient
resource usage in networked and embedded control systems by reducing
communication instances compared to traditional time-triggered strategies. This
paper introduces a novel approach to ETC for discrete-time nonlinear systems
using a data-driven framework. By leveraging Koopman operator theory, the
nonlinear system dynamics are globally linearized (approximately in practical
settings) in a higher-dimensional space. We design a state-feedback controller
and an event-triggering policy directly from data, ensuring exponential
stability in Lyapunov sense. The proposed method is validated through extensive
simulation experiments, demonstrating significant resource savings.

</details>


### [283] [Graphical Dominance Analysis for Linear Systems: A Frequency-Domain Approach](https://arxiv.org/abs/2504.14394)
*Chao Chen,Thomas Chaffey,Rodolphe Sepulchre*

Main category: eess.SY

TL;DR: 本论文提出了一种频率域图形方法，用于分析MIMO系统的优势，通过缩放图界定特征轨迹，并基于开环系统分离量化闭环优势。


<details>
  <summary>Details</summary>
Motivation: 动机是扩展Nyquist图方法，提供鲁棒的MIMO反馈优势分析框架，并统一小增益、小相位和钝性定理。

Method: 方法是定义频率域缩放图，绘制在复平面上，作为系统特征轨迹的界限，并通过开环系统缩放图的频率分离判断闭环优势。

Result: 结果是给出了闭环系统优势的充分条件，并将现有定理整合。

Conclusion: 结论是这种方法为MIMO系统稳定性分析提供了一个可靠的图形工具。

Abstract: We propose a frequency-domain approach to dominance analysis for multi-input
multi-output (MIMO) linear time-invariant systems. The dominance of a MIMO
system is defined to be the number of its poles in the open right half-plane.
Our approach is graphical: we define a frequency-wise notion of the
recently-introduced scaled graph of a MIMO system plotted in a complex plane.
The scaled graph provides a bound of the eigenloci of the system, which can be
viewed as a robust MIMO extension of the classical Nyquist plot. Our main
results characterize sufficient conditions for quantifying the dominance of a
closed-loop system based upon separation of scaled graphs of two open-loop
systems in a frequency-wise manner. The results reconcile existing small gain,
small phase and passivity theorems for feedback dominance analysis.

</details>


### [284] [Soft and Hard Scaled Relative Graphs for Nonlinear Feedback Stability](https://arxiv.org/abs/2504.14407)
*Chao Chen,Sei Zhen Khong,Rodolphe Sepulchre*

Main category: eess.SY

TL;DR: 这篇论文基于软和硬缩放相对图（SRGs）分析非线性反馈系统的输入-输出稳定性，推广了现有定理，不需要弦状假设。


<details>
  <summary>Details</summary>
Motivation: 动机是调和增量正性和增量无源性的区别，从图形角度进行分析，并推广到可能无界的开环系统。

Method: 方法是通过在复平面上分离软/硬 SRGs 来保证闭环稳定性，不依赖弦状假设。

Result: 结果推广了现有的软 SRG 分离定理，适用于可能无界的开环系统。

Conclusion: 这项分析为非线性系统的稳定性提供了更广泛的框架。

Abstract: This paper presents input-output stability analysis of nonlinear feedback
systems based on the notion of soft and hard scaled relative graphs (SRGs). The
soft and hard SRGs acknowledge the distinction between incremental positivity
and incremental passivity and reconcile them from a graphical perspective. The
essence of our proposed analysis is that the separation of soft/hard SRGs of
two open-loop systems on the complex plane guarantees closed-loop stability.
The main results generalize an existing soft SRG separation theorem for bounded
open-loop systems which was proved based on interconnection properties of soft
SRGs under a chordal assumption. By comparison, our analysis does not require
this chordal assumption and applies to possibly unbounded open-loop systems.

</details>


### [285] [Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment](https://arxiv.org/abs/2504.14412)
*Benjamin M. Peter,Mert Korkali*

Main category: eess.SY

TL;DR: 本研究使用量子计算增强强化学习，以提高电力网格安全评估的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 电力网格安全维护面临日益严峻的挑战，传统强化学习在处理复杂的应急分析问题时难以扩展。

Method: 提出一种混合代理，利用IBM的Qiskit Runtime在量子硬件上运行，并构建参数化量子电路来训练RL代理。

Result: 量子增强代理在N-k应急分析中表现出比无量子增强基准模型更好的电网稳定性，并比较了训练过程。

Conclusion: 证明了量子计算在强化学习中的潜力，可提升计算效率和代理性能。

Abstract: The increasingly challenging task of maintaining power grid security requires
innovative solutions. Novel approaches using reinforcement learning (RL) agents
have been proposed to help grid operators navigate the massive decision space
and nonlinear behavior of these complex networks. However, applying RL to power
grid security assessment, specifically for combinatorially troublesome
contingency analysis problems, has proven difficult to scale. The integration
of quantum computing into these RL frameworks helps scale by improving
computational efficiency and boosting agent proficiency by leveraging quantum
advantages in action exploration and model-based interdependence. To
demonstrate a proof-of-concept use of quantum computing for RL agent training
and simulation, we propose a hybrid agent that runs on quantum hardware using
IBM's Qiskit Runtime. We also provide detailed insight into the construction of
parameterized quantum circuits (PQCs) for generating relevant quantum output.
This agent's proficiency at maintaining grid stability is demonstrated relative
to a benchmark model without quantum enhancement using N-k contingency
analysis. Additionally, we offer a comparative assessment of the training
procedures for RL models integrated with a quantum backend.

</details>


### [286] [Online Optimal Parameter Compensation method of High-dimensional PID Controller for Robust stability](https://arxiv.org/abs/2504.14486)
*Zimao Sheng,Hong'an Yang*

Main category: eess.SY

TL;DR: 这篇论文针对受扰非线性MIMO系统设计高维PID控制器，提供在线动态参数调节方法，确保鲁棒稳定性，并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 工业MIMO系统因强耦合、外扰和故障需要高鲁棒PID控制，现有的方法局限性大，如依赖特定系统和脱耦控制。

Method: 理论分析PID控制器，将参数调节转化为两阶段最小特征值问题，使用内点法实现在线高效求解。

Result: 实验证明动态补偿算法在多输入耦合下实现了系统的在线鲁棒稳定性。

Conclusion: 解决了PID控制在非线性MIMO系统中的关键限制，提高了鲁棒性和动态补偿能力。

Abstract: Classical PID control is widely applied in an engineering system, with
parameter regulation relying on a method like Trial - Error Tuning or the
Ziegler - Nichols rule, mainly for a Single - Input Single - Output (SISO)
system. However, the industrial nonlinear Multiple - Input Multiple - Output
(MIMO) system demands a high - robustness PID controller due to strong state
coupling, external disturbances, and faults. Existing research on PID parameter
regulation for a nonlinear uncertain MIMO system has a significant drawback:
it's limited to a specific system type, the control mechanism for a MIMO
nonlinear system under disturbances is unclear, the MIMO PID controller over -
relies on decoupled control, and lacks dynamic parameter compensation. This
paper theoretically analyzes a high - dimensional PID controller for a
disturbed nonlinear MIMO system, providing a condition for online dynamic
parameter regulation to ensure robust stability. By transforming the parameter
regulation into a two - stage minimum eigenvalue problem (EVP) solvable via the
interior point method, it enables efficient online tuning. The experiment
proves that the designed dynamic compensation algorithm can achieve online
robust stability of system errors considering multi - channel input coupling,
addressing the key limitation in the field.

</details>


### [287] [Proactive Radio Resource Allocation for 6G In-Factory Subnetworks](https://arxiv.org/abs/2504.14718)
*Hossam Farag,Mohamed Ragab,Gilberto Berardinelli,Cedomir Stefanovic*

Main category: eess.SY

TL;DR: 本论文提出主动无线资源分配方法，使用贝叶斯岭回归预测信息年龄（AoI），以最小化6G工厂子网中AoI违反概率，并实现98%的减少。


<details>
  <summary>Details</summary>
Motivation: 在6G工厂子网中，确保信息新鲜度（AoI）以满足现代控制系统的严格要求，但受限于资源和动态环境。

Method: 采用去中心化学习框架，通过贝叶斯岭回归预测未来AoI，并基于预测主动分配无线资源。

Result: 模拟结果显示，与基线方法相比，AoI违反概率减少98%。

Conclusion: 该方法提高了控制回路的可靠性和准确性。

Abstract: 6G In-Factory Subnetworks (InF-S) have recently been introduced as
short-range, low-power radio cells installed in robots and production modules
to support the strict requirements of modern control systems. Information
freshness, characterized by the Age of Information (AoI), is crucial to
guarantee the stability and accuracy of the control loop in these systems.
However, achieving strict AoI performance poses significant challenges
considering the limited resources and the high dynamic environment of InF-S. In
this work, we introduce a proactive radio resource allocation approach to
minimize the AoI violation probability. The proposed approach adopts a
decentralized learning framework using Bayesian Ridge Regression (BRR) to
predict the future AoI by actively learning the system dynamics. Based on the
predicted AoI value, radio resources are proactively allocated to minimize the
probability of AoI exceeding a predefined threshold, hence enhancing the
reliability and accuracy of the control loop. The conducted simulation results
prove the effectiveness of our proposed approach to improve the AoI performance
where a reduction of 98% is achieved in the AoI violation probability compared
to relevant baseline methods.

</details>


### [288] [Data-driven model order reduction for T-Product-Based dynamical systems](https://arxiv.org/abs/2504.14721)
*Shenghan Mei,Ziqin He,Yidan Mei,Xin Mao,Anqi Dong,Ren Wang,Can Chen*

Main category: eess.SY

TL;DR: 本文提出基于T-积的数据驱动模型阶减法框架，用于简化处理三阶张量数据的动态系统。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法直接处理张量数据，可能丢失高阶结构信息，因此需要新框架来保留动态特性。

Method: 开发T-平衡截断、T-平衡正交分解和T-特征系统实现算法，利用T-奇异值分解的特性。

Result: 方法提供显著内存和计算节省，减少误差与传统方法相当，通过合成和真实示例验证。

Conclusion: 框架在处理张量数据时有效，证明了其在实际应用中的实用性。

Abstract: Model order reduction plays a crucial role in simplifying complex systems
while preserving their essential dynamic characteristics, making it an
invaluable tool in a wide range of applications, including robotic systems,
signal processing, and fluid dynamics. However, traditional model order
reduction techniques like balanced truncation are not designed to handle tensor
data directly and instead require unfolding the data, which may lead to the
loss of important higher-order structural information. In this article, we
introduce a novel framework for data-driven model order reduction of
T-product-based dynamical systems (TPDSs), which are often used to capture the
evolution of third-order tensor data such as images and videos through the
T-product. Specifically, we develop advanced T-product-based techniques,
including T-balanced truncation, T-balanced proper orthogonal decomposition,
and the T-eigensystem realization algorithm for input-output TPDSs by
leveraging the unique properties of T-singular value decomposition. We
demonstrate that these techniques offer significant memory and computational
savings while achieving reduction errors that are comparable to those of
conventional methods. The effectiveness of the proposed framework is further
validated through synthetic and real-world examples.

</details>


### [289] [Sensor Scheduling in Intrusion Detection Games with Uncertain Payoffs](https://arxiv.org/abs/2504.14725)
*Jayanth Bhargav,Shreyas Sundaram,Mahsa Ghasemi*

Main category: eess.SY

TL;DR: 本文研究入侵检测中传感器调度的博弈论问题，提出高效算法处理已知和未知回报场景。


<details>
  <summary>Details</summary>
Motivation: 直接计算Nash Equilibrium策略计算量大，由于传感器数量指数级增长，需要高效方法。

Method: 提出分布式Weighted Majority算法和基于bandit反馈的在线学习算法，利用博弈结构和学习理论。

Result: 推导高概率最优遗憾界，并通过模拟验证算法在已知和未知回报场景下的性能。

Conclusion: 算法提供高效的Nash Equilibrium计算和学习策略，提升入侵检测效率。

Abstract: We study the problem of sensor scheduling for an intrusion detection task. We
model this as a two-player zero-sum game over a graph, where the defender
(Player 1) seeks to identify the optimal strategy for scheduling sensor
orientations to minimize the probability of missed detection at minimal cost,
while the intruder (Player 2) aims to identify the optimal path selection
strategy to maximize missed detection probability at minimal cost. The
defender's strategy space grows exponentially with the number of sensors,
making direct computation of the Nash Equilibrium (NE) strategies
computationally expensive. To tackle this, we propose a distributed variant of
the Weighted Majority algorithm that exploits the structure of the game's
payoff matrix, enabling efficient computation of the NE strategies with
provable convergence guarantees. Next, we consider a more challenging scenario
where the defender lacks knowledge of the true sensor models and, consequently,
the game's payoff matrix. For this setting, we develop online learning
algorithms that leverage bandit feedback from sensors to estimate the NE
strategies. By building on existing results from perturbation theory and online
learning in matrix games, we derive high-probability order-optimal regret
bounds for our algorithms. Finally, through simulations, we demonstrate the
empirical performance of our proposed algorithms in both known and unknown
payoff scenarios.

</details>


### [290] [Adaptive Field Effect Planner for Safe Interactive Autonomous Driving on Curved Roads](https://arxiv.org/abs/2504.14747)
*Qinghao Li,Zhen Tian,Xiaodan Wang,Jinming Yang,Zhihao Lin*

Main category: eess.SY

TL;DR: 本论文提出整合人工势场、Frenet坐标和改进粒子群优化算法的自动驾驶导航框架，以提升安全性和舒适性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临动态复杂环境挑战，传统方法不足，需要开发集成自适应框架。

Method: 提出动态风险场确保交互安全，使用Frenet坐标简化轨迹规划，改进五次多项式生成平滑轨迹，并采用IPSO实时优化。

Result: 通过模拟和真实场景验证，展示了在复杂交通环境中的导航能力、安全裕度和轨迹平滑性。

Conclusion: 该框架有效提高了自动驾驶的性能，平衡了安全和舒适需求。

Abstract: Autonomous driving has garnered significant attention for its potential to
improve safety, traffic efficiency, and user convenience. However, the dynamic
and complex nature of interactive driving poses significant challenges,
including the need to navigate non-linear road geometries, handle dynamic
obstacles, and meet stringent safety and comfort requirements. Traditional
approaches, such as artificial potential fields (APF), often fall short in
addressing these complexities independently, necessitating the development of
integrated and adaptive frameworks. This paper presents a novel approach to
autonomous vehicle navigation that integrates artificial potential fields,
Frenet coordinates, and improved particle swarm optimization (IPSO). A dynamic
risk field, adapted from traditional APF, is proposed to ensure interactive
safety by quantifying risks and dynamically adjusting lane-changing intentions
based on surrounding vehicle behavior. Frenet coordinates are utilized to
simplify trajectory planning on non-straight roads, while an enhanced quintic
polynomial trajectory generator ensures smooth and comfortable path
transitions. Additionally, an IPSO algorithm optimizes trajectory selection in
real time, balancing safety and user comfort within a feasible input range. The
proposed framework is validated through extensive simulations and real-world
scenarios, demonstrating its ability to navigate complex traffic environments,
maintain safety margins, and generate smooth, dynamically feasible
trajectories.

</details>


### [291] [Data-Driven Evolutionary Game-Based Model Predictive Control for Hybrid Renewable Energy Dispatch in Autonomous Ships](https://arxiv.org/abs/2504.14750)
*Yaoze Liu,Zhen Tian,Jinming Yang,Zhihao Lin*

Main category: eess.SY

TL;DR: 本篇论文提出了一种基于进化博弈的模型预测控制（EG-MPC）框架，用于自主船舶混合可再生能源系统的能量调度，旨在在不确定性条件下最小化成本并确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 针对自主船舶可再生能源生成的不确定性和动态能量需求，提出优化调度策略以降低运营成本并维持系统可靠性。

Method: 采用进化博弈动态与后退地平线优化相结合的EG-MPC框架，利用真实数据进行建模和控制。

Result: 基于现场数据的模拟结果表明，该方法在成本效益、可靠性和适应性方面优于传统规则-based和标准MPC方法，尤其在不确定性条件下。

Conclusion: 该EG-MPC方法为混合可再生能源系统的能量调度提供了高效、可靠的解决方案。

Abstract: In this paper, we propose a data-driven Evolutionary Game-Based Model
Predictive Control (EG-MPC) framework for the energy dispatch of a hybrid
renewable energy system powering an autonomous ship. The system integrates
solar photovoltaic and wind turbine generation with battery energy storage and
diesel backup power to ensure reliable operation. Given the uncertainties in
renewable generation and dynamic energy demands, an optimal dispatch strategy
is crucial to minimize operational costs while maintaining system reliability.
To address these challenges, we formulate a cost minimization problem that
considers both battery degradation costs and diesel fuel expenses, leveraging
real-world data to enhance modeling accuracy. The EG-MPC approach integrates
evolutionary game dynamics within a receding-horizon optimization framework,
enabling adaptive and near-optimal control solutions in real time. Simulation
results based on site-specific data demonstrate that the proposed method
achieves cost-effective, reliable, and adaptive energy dispatch, outperforming
conventional rule-based and standard MPC approaches, particularly under
uncertainty.

</details>


### [292] [Event triggered optimal formation control for nonlinear multi-agent systems under Denial-of-Service attacks](https://arxiv.org/abs/2504.14874)
*Jianqiang Zhang,Kaijun Yang*

Main category: eess.SY

TL;DR: 本论文研究了在拒绝服务攻击下非线性多智能体系统的最优编队控制，使用事件触发控制和批评神经网络方法，确保系统稳定并节省资源，通过模拟验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在拒绝服务攻击下需要鲁棒的控制策略来实现编队目标，以应对安全威胁。

Method: 采用事件触发控制方案、批评神经网络方法和Lyapunov稳定性理论来设计最优控制策略。

Result: 证明了局部邻域编队误差指数稳定，权重估计误差均匀最终有界，并通过Matlab模拟验证了算法有效性。

Conclusion: 在拒绝服务攻击下，非线性多智能体系统成功实现了期望的编队。

Abstract: This paper investigates the optimal formation control problem of a class of
nonlinear multi-agent systems(MASs) under Denial-of-Service(DoS) attacks. We
design the optimal formation control law using an event-triggered control
scheme to achieve formation objectives under DoS attacks. Critic neural network
(NN)-based approach is employed to achieve the optimal control policy under DoS
attacks. Event-triggered mechanism is introduced to ensure the saving of
control resources. Additionally, Lyapunov stability theory is utilized to
demonstrate that the local neighborhood formation error exhibits exponential
stability and the estimation error of weights are uniformly ultimately bounded.
Finally, the effectiveness of the control algorithm is validated through matlab
simulations. The results indicate that under DoS attacks, the nonlinear MAS
successfully achieves the desired formation for the MAS.

</details>


### [293] [Distributed Time-Varying Gaussian Regression via Kalman Filtering](https://arxiv.org/abs/2504.14900)
*Nicola Taddei,Riccardo Maggioni,Jaap Eising,Giulia De Pasquale,Florian Dorfler*

Main category: eess.SY

TL;DR: This paper proposes a distributed Kalman filter algorithm, DistKP, for learning time-varying functions in multi-agent systems, with validation in UAV wind field simulations.


<details>
  <summary>Details</summary>
Motivation: To address the need for real-time and robust estimation of dynamic cost/reward functions in distributed control applications, especially in safety-critical settings like UAV operations.

Method: Adopts a finite-dimensional Gaussian Process approximation and Bayesian linear regression, proposing the DistKP algorithm based on a distributed Kalman filter that supports arbitrary kernels and weaker assumptions on function evolution.

Result: Validated through a simulation example where UAVs collaboratively learn a dynamically changing wind field.

Conclusion: The method offers improved performance under weaker assumptions compared to existing literature, demonstrating effectiveness in distributed learning tasks.

Abstract: We consider the problem of learning time-varying functions in a distributed
fashion, where agents collect local information to collaboratively achieve a
shared estimate. This task is particularly relevant in control applications,
whenever real-time and robust estimation of dynamic cost/reward functions in
safety critical settings has to be performed. In this paper, we,adopt a
finite-dimensional approximation of a Gaussian Process, corresponding to a
Bayesian linear regression in an appropriate feature space, and propose a new
algorithm, DistKP, to track the time-varying coefficients via a distributed
Kalman filter. The proposed method works for arbitrary kernels and under weaker
assumptions on the time-evolution of the function to learn compared to the
literature. We validate our results using a simulation example in which a fleet
of Unmanned Aerial Vehicles (UAVs) learns a dynamically changing wind field.

</details>


### [294] [Considerations on the Design of Transceivers for Ambient Internet of Things](https://arxiv.org/abs/2504.14956)
*Yuxiao Zhao,Zhen Shen,Shiyu Li,Jing Feng,Hao Min*

Main category: eess.SY

TL;DR: 这篇论文提出了一种用于环境物联网(A-IoT)的接收器和LO合成器架构，以实现低成本、无电池设备，并提高灵敏度。


<details>
  <summary>Details</summary>
Motivation: 环境物联网(A-IoT)面临亚毫瓦接收器和无晶体时钟生成等挑战，需要低成本、免维护的设备用于供应链和智能农业等应用。

Method: 提出“近似低IF”接收器和“载波辅助IF反馈”LO合成器架构，使用55nm CMOS技术实现。

Result: 锁定LO校准回路后，接收器灵敏度优于-88 dBm。

Conclusion: 提出的接收器架构将促进“零功耗”设备，实现无处不在的物联网连接，桥接数字和物理世界。

Abstract: The Ambient IoT (A-IoT) will introduce trillions of connections and enable
low-cost battery-less devices. The A-IoT nodes can achieve low cost ($\sim \$
0.1$ like RFID tag), sub-1mW average power consumption, $\leq 10$ kbps data
rates, maintenance-free working for decades, cm-scale size, cm-scale size, and
supporting applications like supply chain and smart agriculture. The
transceiver challenges in A-IoT focus on sub-mW receivers and crystal-less
clock generation. The paper proposes an "approximate low-IF" receiver and
"carrier-auxiliary IF feedback" LO synthesizer architecture for Type-B/C A-IoT
devices, which tracks the RF carrier frequency and eliminates external
crystals. The proposed receiver and LO generator are implemented using 55nm
CMOS technology. After locking the LO calibration loop, the receiver
sensitivity is better than -88 dBm. The proposed receiver architecture will
promote "zero power" devices for ubiquitous IoT connectivity, bridging digital
and physical worlds.

</details>


### [295] [PID-GM: PID Control with Gain Mapping](https://arxiv.org/abs/2504.15081)
*Bo Zhu,Wei Yu,Hugh H. T. Liu*

Main category: eess.SY

TL;DR: 这篇论文引入非线性映射优化PID增益调谐，提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: PID控制在工业系统中广泛应用，但存在理解鲁棒性和缺乏明确调谐指南的问题。

Method: 引入非线性映射将PID控制等效为PD控制加补偿器，使用二自由度框架和奇异摄动理论。

Result: 证明性能依赖辅助参数，PID增益减小提升鲁棒性，并通过模拟实验验证。

Conclusion: 映射帮助在二自由度框架下更好地理解、设计和调谐PID控制。

Abstract: Proportional-Integral-Differential (PID) control is widely used in industrial
control systems. However, up to now there are at least two open problems
related with PID control. One is to have a comprehensive understanding of its
robustness with respect to model uncertainties and disturbances. The other is
to build intuitive, explicit and mathematically provable guidelines for PID
gain tuning. In this paper, we introduce a simple nonlinear mapping to
determine PID gains from three auxiliary parameters. By the mapping, PID
control is shown to be equivalent to a new PD control (serving as a nominal
control) plus an uncertainty and disturbance compensator (to recover the
nominal performance). Then PID control can be understood, designed and tuned in
a Two-Degree-of-Freedom (2-DoF) control framework. We discuss some basic
properties of the mapping, including the existence, uniqueness and
invertibility. Taking as an example the PID control applied to a general
uncertain second-order plant, we prove by the singular perturbation theory that
the closed-loop steady-state and transient performance depends explicitly on
one auxiliary parameter which can be viewed as the virtual singular
perturbation parameter (SPP) of PID control. All the three PID gains are
monotonically decreasing functions of the SPP, indicating that the smaller the
SPP is, the higher the PID gains are, and the better the robustness of PID
control is. Simulation and experimental examples are provided to demonstrate
the properties of the mapping as well as the effectiveness of the mapping based
PID gain turning.

</details>


### [296] [Scalable Discrete Event Simulation Tool for Large-Scale Cyber-Physical Energy Systems: Advancing System Efficiency and Scalability](https://arxiv.org/abs/2504.15198)
*Khandaker Akramul Haque,Shining Sun,Xiang Huo,Ana E. Goulart,Katherine R. Davis*

Main category: eess.SY

TL;DR: 本文介绍了DESTinE工具，用于模拟电力系统的网络物理攻击，提高系统弹性。


<details>
  <summary>Details</summary>
Motivation: 动机是应对现代电力系统面临的网络物理攻击风险，需要可扩展的威胁和风险评估。

Method: 方法包括开发DESTinE离散事件模拟工具，模拟网络流量和拥塞，使用合成电力系统案例，进行实验和优化问题求解，并与虚拟服务器和硬件集成。

Result: 结果是通过实验识别关键节点，评估干扰影响，并推导出混合网络拓扑以提高弹性。

Conclusion: 结论是DESTinE工具通过结合星型和辐射状拓扑，提升了网络弹性，并支持更大规模的分析。

Abstract: Modern power systems face growing risks from cyber-physical attacks,
necessitating enhanced resilience due to their societal function as critical
infrastructures. The challenge is that defense of large-scale
systems-of-systems requires scalability in their threat and risk assessment
environment for cyber physical analysis including cyber-informed transmission
planning, decision-making, and intrusion response. Hence, we present a scalable
discrete event simulation tool for analysis of energy systems, called DESTinE.
The tool is tailored for largescale cyber-physical systems, with a focus on
power systems. It supports faster-than-real-time traffic generation and models
packet flow and congestion under both normal and adversarial conditions. Using
three well-established power system synthetic cases with 500, 2000, and 10,000
buses, we overlay a constructed cyber network employing star and radial
topologies. Experiments are conducted to identify critical nodes within a
communication network in response to a disturbance. The findings are
incorporated into a constrained optimization problem to assess the impact of
the disturbance on a specific node and its cascading effects on the overall
network. Based on the solution of the optimization problem, a new hybrid
network topology is also derived, combining the strengths of star and radial
structures to improve network resilience. Furthermore, DESTinE is integrated
with a virtual server and a hardware-in-the-loop (HIL) system using Raspberry
Pi 5.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [297] [Interdisciplinary Integration of Remote Sensing -- A Review with Four Examples](https://arxiv.org/abs/2504.14590)
*Zichen Jin*

Main category: cs.GR

TL;DR: 本论文简要回顾了遥感与生态学、数学形态学、机器学习和电子学的跨学科整合。


<details>
  <summary>Details</summary>
Motivation: 强调遥感作为高级学科依赖其他学科贡献，以展示其跨学科整合的重要性。

Method: 通过四个例子（生态学、数学形态学、机器学习、电子学）的文献回顾和分析。

Result: 突出了遥感在这些领域中的应用实例，展示了其广泛的跨学科特性。

Conclusion: 这些例子仅是冰山一角，暗示遥感有更广泛的跨学科整合潜力。

Abstract: As a high-level discipline, the development of remote sensing depends on the
contribution of many other basic and applied disciplines and technologies. For
example, due to the close relationship between remote sensing and
photogrammetry, remote sensing would inevitably integrate disciplines such as
optics and color science. Also, remote sensing integrates the knowledge of
electronics in the conversion from optical signals to electrical signals via
CCD (Charge-Coupled Device) or other image sensors. Moreover, when conducting
object identification and classification with remote sensing data, mathematical
morphology and other digital image processing technologies are used. These
examples are only the tip of the iceberg of interdisciplinary integration of
remote sensing. This work briefly reviews the interdisciplinary integration of
remote sensing with four examples - ecology, mathematical morphology, machine
learning, and electronics.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [298] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: MERL的系统在外部数据集上预训练神经声学场，适应目标房间，并用于估计房间脉冲响应以增强数据和改善说话者距离估计。


<details>
  <summary>Details</summary>
Motivation: 解决房间脉冲响应估计和说话者距离估计的挑战，通过生成式数据增强利用大规模数据集。

Method: 预训练受房间几何条件约束的神经声学场，使用登记数据适应目标房间，然后预测指定源和接收器位置的RIR，并训练说话者距离估计模型。

Result: 成功预测RIR并用于训练模型，提高了说话者距离估计性能。

Conclusion: 证明神经声学场在生成式数据增强中的有效性。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>


### [299] [StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models](https://arxiv.org/abs/2504.14915)
*Yeona Hong,Hyewon Han,Woo-jin Chung,Hong-Goo Kang*

Main category: eess.AS

TL;DR: 本文提出StableQuant，一种自适应PTQ算法，用于压缩语音基础模型，实现高效量化。


<details>
  <summary>Details</summary>
Motivation: 由于SFM的网络架构与LLM不同，直接应用现有PTQ方法可能不优。

Method: StableQuant通过分析规模分布和整体性能，适应性地确定每个层的量化范围。

Result: 在HuBERT和wav2vec2.0上，模型大小减小四分之一，推理速度加倍，8位量化下WER下降小于0.3%。

Conclusion: StableQuant在各种网络架构下表现出优越的量化性能。

Abstract: In this paper, we propose StableQuant, a novel adaptive post-training
quantization (PTQ) algorithm for widely used speech foundation models (SFMs).
While PTQ has been successfully employed for compressing large language models
(LLMs) due to its ability to bypass additional fine-tuning, directly applying
these techniques to SFMs may not yield optimal results, as SFMs utilize
distinct network architecture for feature extraction. StableQuant demonstrates
optimal quantization performance regardless of the network architecture type,
as it adaptively determines the quantization range for each layer by analyzing
both the scale distributions and overall performance. We evaluate our algorithm
on two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)
task, and achieve superior performance compared to traditional PTQ methods.
StableQuant successfully reduces the sizes of SFM models to a quarter and
doubles the inference speed while limiting the word error rate (WER)
performance drop to less than 0.3% with 8-bit quantization.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [300] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TL;DR: 本论文提出一种新训练范式，通过触发激活模型提升对可转移对抗样本的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法存在部署效率低、防御无效和干净图像性能下降的问题，需要更高效有效的解决方案。

Method: 提出一种在干净数据上随机猜测、在恒定触发数据上准确预测的模型，并通过一阶梯度理论分析和触发与模型的联合优化来提高鲁棒性。

Result: 实验在多种数据集和攻击方法下显示，该方法对可转移对抗样本具有显著的鲁棒性，并优于现有方法。

Conclusion: 该方法是一种高效且有效的防御策略，可显著提升模型对可转移对抗样本的抵抗能力。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>


### [301] [Protecting Your Voice: Temporal-aware Robust Watermarking](https://arxiv.org/abs/2504.14832)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 论文提出了一种时间域感知的鲁棒水印方法（True），以提高合成语音的保真度并保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 合成语音快速发展和频率域水印方法导致保真度损失，动机是平衡鲁棒性和细粒度特征学习。

Method: temporal-aware robust watermarking (True) 方法，专注于时间域特征以最大化学习和保真度。

Result: 提高了水印的鲁棒性，同时提升了语音和歌声的保真度。

Conclusion: 该方法是保护合成语音的创新先驱，提供了新的平衡方案。

Abstract: The rapid advancement of generative models has led to the synthesis of
real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into
the frequency-domain features of synthesized voices has become a common
routine. However, the robustness achieved by choosing the frequency domain
often comes at the expense of fine-grained voice features, leading to a loss of
fidelity. Maximizing the comprehensive learning of time-domain features to
enhance fidelity while maintaining robustness, we pioneer a
\textbf{\underline{t}}emporal-aware
\textbf{\underline{r}}ob\textbf{\underline{u}}st
wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the
speech and singing voice.

</details>


### [302] [aiXamine: LLM Safety and Security Simplified](https://arxiv.org/abs/2504.14985)
*Fatih Deniz,Dorde Popovic,Yazan Boshmaf,Euisuh Jeong,Minhaj Ahmad,Sanjay Chawla,Issa Khalil*

Main category: cs.CR

TL;DR: aiXamine 是一个全面的黑箱评估平台，用于评估大语言模型的安全性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决评估大语言模型安全性和安全性时存在的复杂性和碎片化问题。

Method: 开发了 aiXamine 平台，集成了 40 多个测试，分为八个关键服务，并评估了 50 多个 LLM。

Result: 发现了领先模型的漏洞，如 OpenAI 的 GPT-4o 对对抗攻击的易感性，以及开源模型在某些方面的表现可与专有模型媲美。

Conclusion: 突出了模型评估工具的重要性，并识别了模型大小、训练方法等之间的权衡。

Abstract: Evaluating Large Language Models (LLMs) for safety and security remains a
complex task, often requiring users to navigate a fragmented landscape of ad
hoc benchmarks, datasets, metrics, and reporting formats. To address this
challenge, we present aiXamine, a comprehensive black-box evaluation platform
for LLM safety and security. aiXamine integrates over 40 tests (i.e.,
benchmarks) organized into eight key services targeting specific dimensions of
safety and security: adversarial robustness, code security, fairness and bias,
hallucination, model and data privacy, out-of-distribution (OOD) robustness,
over-refusal, and safety alignment. The platform aggregates the evaluation
results into a single detailed report per model, providing a detailed breakdown
of model performance, test examples, and rich visualizations. We used aiXamine
to assess over 50 publicly available and proprietary LLMs, conducting over 2K
examinations. Our findings reveal notable vulnerabilities in leading models,
including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased
outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.
Additionally, we observe that open-source models can match or exceed
proprietary models in specific services such as safety alignment, fairness and
bias, and OOD robustness. Finally, we identify trade-offs between distillation
strategies, model size, training methods, and architectural choices.

</details>


### [303] [SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation](https://arxiv.org/abs/2504.15035)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 本论文提出SOLIDO方法，使用LoRA技术实现高效语音水印，提高鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决语音生成模型的安全问题，如模型侵权和内容滥用，现有的水印技术计算开销大且对变长输入鲁棒性差。

Method: 提出SOLIDO方法，结合LoRA低秩适配微调、水印编码器和基于深度可分离卷积的解码器，以处理变长输入并减少计算开销。

Result: 实验显示，在2000 bps容量下水印语音保真度高，提取准确率最高99.20%，在时间拉伸攻击下优于其他方法23%。

Conclusion: SOLIDO方法有效提升了语音水印的性能和抗攻击能力。

Abstract: The accelerated advancement of speech generative models has given rise to
security issues, including model infringement and unauthorized abuse of
content. Although existing generative watermarking techniques have proposed
corresponding solutions, most methods require substantial computational
overhead and training costs. In addition, some methods have limitations in
robustness when handling variable-length inputs. To tackle these challenges, we
propose \textsc{SOLIDO}, a novel generative watermarking method that integrates
parameter-efficient fine-tuning with speech watermarking through low-rank
adaptation (LoRA) for speech diffusion models. Concretely, the watermark
encoder converts the watermark to align with the input of diffusion models. To
achieve precise watermark extraction from variable-length inputs, the watermark
decoder based on depthwise separable convolution is designed for watermark
recovery. To further enhance speech generation performance and watermark
extraction capability, we propose a speech-driven lightweight fine-tuning
strategy, which reduces computational overhead through LoRA. Comprehensive
experiments demonstrate that the proposed method ensures high-fidelity
watermarked speech even at a large capacity of 2000 bps. Furthermore, against
common individual and compound speech attacks, our SOLIDO achieves a maximum
average extraction accuracy of 99.20\% and 98.43\%, respectively. It surpasses
other state-of-the-art methods by nearly 23\% in resisting time-stretching
attacks.

</details>


### [304] [Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages](https://arxiv.org/abs/2504.15063)
*Hongli Peng,Xiaoqi Li,Wenkai Li*

Main category: cs.CR

TL;DR: 这篇论文通过智能合约生命周期实证研究，使用机器学习识别漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约安全挑战严重，现有机分析仅覆盖50%的安全事件，需要全面研究各阶段漏洞特征。

Method: 实证研究智能合约部署、执行、升级和销毁阶段的安全问题，定义七个特征，并使用五种机器学习分类模型。

Result: 分类结果显示脆弱合约在不同阶段具有独特的交易特征和自我网络属性。

Conclusion: 该研究揭示生命周期各阶段漏洞特征，有助于提升智能合约安全。

Abstract: Smart contracts are the cornerstone of decentralized applications and
financial protocols, which extend the application of digital currency
transactions. The applications and financial protocols introduce significant
security challenges, resulting in substantial economic losses. Existing
solutions predominantly focus on code vulnerabilities within smart contracts,
accounting for only 50% of security incidents. Therefore, a more comprehensive
study of security issues related to smart contracts is imperative. The existing
empirical research realizes the static analysis of smart contracts from the
perspective of the lifecycle and gives the corresponding measures for each
stage. However, they lack the characteristic analysis of vulnerabilities in
each stage and the distinction between the vulnerabilities. In this paper, we
present the first empirical study on the security of smart contracts throughout
their lifecycle, including deployment and execution, upgrade, and destruction
stages. It delves into the security issues at each stage and provides at least
seven feature descriptions. Finally, utilizing these seven features, five
machine-learning classification models are used to identify vulnerabilities at
different stages. The classification results reveal that vulnerable contracts
exhibit distinct transaction features and ego network properties at various
stages.

</details>


### [305] [C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation](https://arxiv.org/abs/2504.15144)
*Melih Sirlanci,Carter Yagemann,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 论文提出C2RUST-BENCH数据集，用于评估C到Rust转译框架的代表性最小化数据集。


<details>
  <summary>Details</summary>
Motivation: 内存安全漏洞问题严重，缺乏全面评估数据集，构建大型数据集耗时。

Method: 开发了一种函数选择方法，从大量C函数中构建最小化代表性数据集。

Result: 创建了包含2905个函数的C2RUST-BENCH数据集，从15503个真实世界程序函数中选择。

Conclusion: 提供了一个高效工具来评估C到Rust转译框架，提高了评估效率。

Abstract: Despite the effort in vulnerability detection over the last two decades,
memory safety vulnerabilities continue to be a critical problem. Recent reports
suggest that the key solution is to migrate to memory-safe languages. To this
end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in
C programs. Recent works propose C-to-Rust transpilation frameworks; however, a
comprehensive evaluation dataset is missing. Although one solution is to put
together a large enough dataset, this increases the analysis time in automated
frameworks as well as in manual efforts for some cases. In this work, we build
a method to select functions from a large set to construct a minimized yet
representative dataset to evaluate the C-to-Rust transpilation. We propose
C2RUST-BENCH that contains 2,905 functions, which are representative of
C-to-Rust transpilation, selected from 15,503 functions of real-world programs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [306] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TL;DR: MOFGen使用AI代理生成新型MOF结构，用于CO2捕获和水收集，并通过实验成功验证。


<details>
  <summary>Details</summary>
Motivation: 生成模型和机器学习可加速MOF在CO2捕获和水收集中的材料发现，但需克服广阔化学空间导航和合成性保障的挑战。

Method: MOFGen系统包括大语言模型提出组成、扩散模型生成结构、量子力学代理优化过滤，以及合成可行性代理结合专家规则和机器学习，训练于实验和计算数据库。

Result: 生成了数十万种新型MOF结构和可合成有机连接体，通过高通量实验验证并成功合成五个“AI梦想”MOF。

Conclusion: 标志着向自动化合成材料发现的重要进展。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>


### [307] [Machine learning enhanced atom probe tomography analysis: a snapshot review](https://arxiv.org/abs/2504.14378)
*Yue Li,Ye Wei,Alaukik Saxena,Markus Kühbach,Christoph Freysoldt,Baptiste Gault*

Main category: cond-mat.mtrl-sci

TL;DR: 这篇论文回顾了机器学习在原子探针断层扫描（APT）数据分析中的应用，以克服当前限制并提供新见解。


<details>
  <summary>Details</summary>
Motivation: 当前APT数据分析依赖用户专业知识，导致偏差、效率低下和标准化困难，因此需要机器学习方法实现用户独立和可重复性。

Method: 通过APT简介、机器学习算法概述、应用回顾以及超越人类能力的讨论来审视ML在APT中的作用。

Result: 机器学习可实现超越人类能力的发现，提供材料机制的新洞察。

Conclusion: 为ML在APT领域的发展提供未来方向指导。

Abstract: Atom probe tomography (APT) is a burgeoning characterization technique that
provides compositional mapping of materials in three-dimensions at near-atomic
scale. Since its significant expansion in the past 30 years, we estimate that
one million APT datasets have been collected, each containing millions to
billions of individual ions. Their analysis and the extraction of
microstructural information has largely relied upon individual users whose
varied level of expertise causes clear and documented bias. Current practices
hinder efficient data processing, and make challenging standardization and the
deployment of data analysis workflows that would be compliant with FAIR data
principles. Over the past decade, building upon the long-standing expertise of
the APT community in the development of advanced data processing or data mining
techniques, there has been a surge of novel machine learning (ML) approaches
aiming for user-independence, and that are efficient, reproducible, and robust
from a statistics perspective. Here, we provide a snapshot review of this
rapidly evolving field. We begin with a brief introduction to APT and the
nature of the APT data. This is followed by an overview of relevant ML
algorithms and a comprehensive review of their applications to APT. We also
discuss how ML can enable discoveries beyond human capability, offering new
insights into the mechanisms within materials. Finally, we provide guidance for
future directions in this domain.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [308] [Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2504.14422)
*Paul Fischer,Sebastian Kaltenbach,Sergey Litvinov,Sauro Succi,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 使用多代理强化学习（MARL）提升粗网格Lattice Boltzmann方法（LBM）的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 全分辨率模拟不切实际，现有欠分辨率LBM不稳定且泛化能力差，需要开发有效的闭合模型。

Method: 提出数据驱动的MARL方法，使用卷积神经网络动态控制LBM局部松弛参数，并在湍流Kolmogorov流动中验证。

Result: MARL闭合模型稳定模拟，恢复高分辨率能量谱，保持效率，可转移到未见场景，并提升鲁棒性和准确性。

Conclusion: MARL闭合模型为高效模拟复杂问题开辟新领域。

Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach
to simulating diverse hydrodynamic phenomena, spanning microfluidics to
aerodynamics. The vast range of spatiotemporal scales inherent in these systems
currently renders full resolution impractical, necessitating the development of
effective closure models for under-resolved simulations. Under-resolved LBMs
are unstable, and while there is a number of important efforts to stabilize
them, they often face limitations in generalizing across scales and physical
systems. We present a novel, data-driven, multiagent reinforcement learning
(MARL) approach that drastically improves stability and accuracy of
coarse-grained LBM simulations. The proposed method uses a convolutional neural
network to dynamically control the local relaxation parameter for the LB across
the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov
flows. We find that the MARL closures stabilize the simulations and recover the
energy spectra of significantly more expensive fully resolved simulations while
maintaining computational efficiency. The learned closure model can be
transferred to flow scenarios unseen during training and has improved
robustness and spectral accuracy compared to traditional LBM models. We believe
that MARL closures open new frontiers for efficient and accurate simulations of
a multitude of complex problems not accessible to present-day LB methods alone.

</details>


### [309] [LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method](https://arxiv.org/abs/2504.14494)
*Yue Li*

Main category: physics.flu-dyn

TL;DR: 本论文提出LBM-GNN方法，将图神经网络与格子Boltzmann方法结合，用于流体动力学模拟，提高了稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 改进格子Boltzmann方法在流体动力学模拟中的稳定性和准确性，通过整合图神经网络。

Method: 使用GNN增强LBM，并通过Taylor-Green涡等基准问题验证，评估准确性、守恒性和性能。

Result: 结果显示，GNN增强LBM在高雷诺数下具有更好的守恒特性和数值稳定性。

Conclusion: GNN增强LBM可以保持更好的属性，适用于更广泛的模拟场景。

Abstract: In this paper, we present LBM-GNN, a novel approach that enhances the
traditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).
We apply this method to fluid dynamics simulations, demonstrating improved
stability and accuracy compared to standard LBM implementations. The method is
validated using benchmark problems such as the Taylor-Green vortex, focusing on
accuracy, conservation properties, and performance across different Reynolds
numbers and grid resolutions. Our results indicate that GNN-enhanced LBM can
maintain better conservation properties while improving numerical stability at
higher Reynolds numbers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [310] [From job titles to jawlines: Using context voids to study generative AI systems](https://arxiv.org/abs/2504.13947)
*Shahan Ali Memon,Soham De,Sungha Kang,Riyan Mujtaba,Bedoor AlShebli,Katie Davis,Jaime Snyder,Jevin D. West*

Main category: cs.CY

TL;DR: 本文提出推测性设计方法研究生成AI行为，通过上下文空白揭示偏见。


<details>
  <summary>Details</summary>
Motivation: 动机是评估AI在不确定性下行为，暴露刻板印象和价值假设。

Method: 方法：桥接不同领域创建探针，案例使用ChatGPT从CV生成头像是。

Result: 结果：AI生成偏见表现和幻觉。

Conclusion: 结论：强调AI处理缺失上下文的偏见问题。

Abstract: In this paper, we introduce a speculative design methodology for studying the
behavior of generative AI systems, framing design as a mode of inquiry. We
propose bridging seemingly unrelated domains to generate intentional context
voids, using these tasks as probes to elicit AI model behavior. We demonstrate
this through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to
generate headshots from professional Curricula Vitae (CVs). In contrast to
traditional ways, our approach assesses system behavior under conditions of
radical uncertainty -- when forced to invent entire swaths of missing context
-- revealing subtle stereotypes and value-laden assumptions. We qualitatively
analyze how the system interprets identity and competence markers from CVs,
translating them into visual portraits despite the missing context (i.e.
physical descriptors). We show that within this context void, the AI system
generates biased representations, potentially relying on stereotypical
associations or blatant hallucinations.

</details>


### [311] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TL;DR: 本文提出一个名为Thousand Voices of Trauma的合成数据集，用于PTSD治疗的AI系统，支持3000个对话和基准测试。


<details>
  <summary>Details</summary>
Motivation: AI在心理健康支持中的进步因缺乏治疗对话数据而受限，特别是针对创伤治疗。

Method: 使用确定性和概率生成方法，基于PTSD的延长暴露疗法协议，创建了包括多样人口统计、20种创伤类型和10种行为的3000个合成对话数据集。

Result: 数据集显示了真实的创伤类型和症状分布，经临床专家验证具有治疗忠实度，并开发了情感轨迹基准。

Conclusion: 该隐私保护数据集填补了创伤焦点心理健康数据的空白，为患者应用和临床培训提供资源。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>


### [312] [Naming is framing: How cybersecurity's language problems are repeating in AI governance](https://arxiv.org/abs/2504.13957)
*Lianne Potter*

Main category: cs.CY

TL;DR: 这篇论文讨论了语言在网络安全和人工智能（AI）领域的误称如何带来治理风险，并主张采用以语言为先的方法来改进治理。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是指出像“网络安全”和“AI”这样的误称会隐藏人类代理、夸大期望和扭曲责任，从而导致重大治理风险。

Method: 论文的方法是借鉴网络安全语言陷阱的教训，并分析AI话语中的隐喻，如“对齐”、“黑箱”和“幻觉”。

Result: 论文的结果是突显这些术语如何嵌入敌对、神秘或过度技术化的假设，并倡导审问隐喻、强调人类角色和开发精确、包容的词汇。

Conclusion: 论文的结论是，语言改革是构建透明、公平和预见性监管框架的核心，而非次要方面。

Abstract: Language is not neutral; it frames understanding, structures power, and
shapes governance. This paper argues that misnomers like cybersecurity and
artificial intelligence (AI) are more than semantic quirks; they carry
significant governance risks by obscuring human agency, inflating expectations,
and distorting accountability. Drawing on lessons from cybersecurity's
linguistic pitfalls, such as the 'weakest link' narrative, this paper
highlights how AI discourse is falling into similar traps with metaphors like
'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,
mystifying, or overly technical assumptions into governance structures. In
response, the paper advocates for a language-first approach to AI governance:
one that interrogates dominant metaphors, foregrounds human roles, and
co-develops a lexicon that is precise, inclusive, and reflexive. This paper
contends that linguistic reform is not peripheral to governance but central to
the construction of transparent, equitable, and anticipatory regulatory
frameworks.

</details>


### [313] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TL;DR: This paper discusses the risks of current AI safety efforts ignoring human-centric issues like the future of work and proposes solutions such as fair compensation and pro-worker governance.


<details>
  <summary>Details</summary>
Motivation: To address the narrow focus of AI safety that overlooks impacts on labor markets, income inequality, and human agency.

Method: Uses economic theories to analyze intertemporal effects of AI on livelihood, structural labor changes, and closed-source rent-seeking behaviors.

Result: Recommends comprehensive transition support, collective licensing for fair compensation, and a pro-worker global AI governance framework.

Conclusion: Strongly advocates for a pro-worker AI governance to enhance shared prosperity, economic justice, and reduce technical debt.

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>


### [314] [The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges](https://arxiv.org/abs/2504.13971)
*Abdelrahman Soliman*

Main category: cs.CY

TL;DR: 这篇调查论文探讨了物联网（IoT）和多模态语言模型（MLLMs）在未来6G系统中的整合，涵盖应用领域和关键挑战。


<details>
  <summary>Details</summary>
Motivation: 基于人工智能和物联网研究的最新趋势，旨在突出整合IoT和MLLMs的潜力与挑战，为6G系统提供研究路线图。

Method: 通过调查和描述IoT及MLLM技术，研究传感器、通信、处理和安全等四个支柱，并讨论多模态的作用。

Result: 提供了IoT和MLLM技术的全面概述，识别了数据可用性、隐私等挑战，并概述了未来研究方向。

Conclusion: 强调需要解决数据可用性、计算开销、隐私和实时处理等挑战，以充分发挥IoT、MLLM和6G技术的潜力。

Abstract: Based on recent trends in artificial intelligence and IoT research. The
cooperative potential of integrating the Internet of Things (IoT) and
Multimodal Language Models (MLLMs) is presented in this survey paper for future
6G systems. It focuses on the applications of this integration in different
fields, such as healthcare, agriculture, and smart cities, and investigates the
four pillars of IoT integration, such as sensors, communication, processing,
and security. The paper provides a comprehensive description of IoT and MLLM
technologies and applications, addresses the role of multimodality in each
pillar, and concludes with an overview of the most significant challenges and
directions for future research. The general survey is a roadmap for researchers
interested in tracing the application areas of MLLMs and IoT, highlighting the
potential and challenges in this rapidly growing field. The survey recognizes
the need to deal with data availability, computational expense, privacy, and
real-time processing to harness the complete potential of IoT, MLLM, and 6G
technology

</details>


### [315] [Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability](https://arxiv.org/abs/2504.13972)
*Dana Alsagheer,Abdulrahman Kamal,Mohammad Kamal,Weidong Shi*

Main category: cs.CY

TL;DR: 这项研究探讨了评估者理性水平对RLHF稳定性的影响，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 动机是解决RLHF中的治理挑战，如评估偏差、不一致性和反馈不可靠性。

Method: 方法是通过对照实验比较高理性与低理性参与者的反馈。

Result: 结果显示，高理性评估者提供更一致的反馈，低理性者显示显著变异性（p < 0.01）。

Conclusion: 结论是推荐实施评估者预筛选、系统反馈审计和可靠性加权聚合，以提升AI对齐的公平性、透明度和稳健性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is central in aligning
large language models (LLMs) with human values and expectations. However, the
process remains susceptible to governance challenges, including evaluator bias,
inconsistency, and the unreliability of feedback. This study examines how the
cognitive capacity of evaluators, specifically their level of rationality,
affects the stability of reinforcement signals. A controlled experiment
comparing high-rationality and low-rationality participants reveals that
evaluators with higher rationality scores produce significantly more consistent
and expert-aligned feedback. In contrast, lower-rationality participants
demonstrate considerable variability in their reinforcement decisions ($p <
0.01$). To address these challenges and improve RLHF governance, we recommend
implementing evaluator pre-screening, systematic auditing of feedback
consistency, and reliability-weighted reinforcement aggregation. These measures
enhance the fairness, transparency, and robustness of AI alignment pipelines.

</details>


### [316] [Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream](https://arxiv.org/abs/2504.13976)
*Wrick Talukdar*

Main category: cs.CY

TL;DR: 这篇论文探讨AI、ML和IoT如何将加油站转变为智能零售中心，包括预测分析、动态定价、个性化互动、实时监控和自动化，并纳入了统计数据、核心概念、数学公式、案例研究及全自动框架。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨技术进步如何重塑零售下游行业，同时简要涉及上游和中游部分。

Method: 方法包括利用AI/ML进行预测分析、动态定价和客户互动，以及IoT用于实时监控和自动化；结合统计、AI/ML概念、数学公式、案例研究和提出的框架。

Result: 结果展示了技术应用如何重新定义燃料零售体验，并提出了全自动加油站框架。

Conclusion: 结论是未来加油站将通过这些技术成为高效、智能的零售中心。

Abstract: The gas station of the future is poised to transform from a simple fuel
dispensing center into an intelligent retail hub, driven by advancements in
Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things
(IoT). This paper explores how technology is reshaping the retail downstream
sector while briefly addressing the upstream and midstream segments. By
leveraging AI/ML for predictive analytics, dynamic pricing, personalized
customer engagement, and IoT for real-time monitoring and automation, the
future gas station will redefine the fuel retail experience. Additionally, this
paper incorporates statistics, AI/ML core technical concepts, mathematical
formulations, case studies, and a proposed framework for a fully autonomous gas
station.

</details>


### [317] [Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey](https://arxiv.org/abs/2504.13979)
*Thippa Reddy Gadekallu,Kapal Dev,Sunder Ali Khowaja,Weizheng Wang,Hailin Feng,Kai Fang,Sharnil Pandya,Wei Wang*

Main category: cs.CY

TL;DR: 这篇调查论文讨论了负责任人工智能（RAI）的全球和国家标准、应用、当前技术、正在进行的项目以及实施挑战。


<details>
  <summary>Details</summary>
Motivation: 由于道德标准和实施脱节，各行业采用自己的标准，且社会压力和不道德的AI使用迫使RAI设计。

Method: 通过调查和讨论全球及国家标准、RAI应用、当前技术、项目以及挑战。

Result: 呈现了RAI的当前状态，包括标准、应用、技术、项目和挑战。

Conclusion: 社会压力和不道德AI使用促使RAI设计而非实施，需要共同的标准框架。

Abstract: Responsible Artificial Intelligence (RAI) is a combination of ethics
associated with the usage of artificial intelligence aligned with the common
and standard frameworks. This survey paper extensively discusses the global and
national standards, applications of RAI, current technology and ongoing
projects using RAI, and possible challenges in implementing and designing RAI
in the industries and projects based on AI. Currently, ethical standards and
implementation of RAI are decoupled which caters each industry to follow their
own standards to use AI ethically. Many global firms and government
organizations are taking necessary initiatives to design a common and standard
framework. Social pressure and unethical way of using AI forces the RAI design
rather than implementation.

</details>


### [318] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TL;DR: 本研究调查Airbnb评论对接受率和租金价格的影响，使用NLP分析情感，发现情感质量比数量更重要。


<details>
  <summary>Details</summary>
Motivation: 探讨正面和负面评论是否影响接受率和租金价格，以六个美国地区为样本。

Method: 收集数千条评论，使用自然语言处理（NLP）分类情感，并通过t检验和相关性分析进行统计测试。

Result: 超过90%的评论为正面，额外评论不显著提高价格；正面反馈房源接受率更高；预算房源评论多价格低，高端房源评论少但正面。

Conclusion: 强调情感质量比数量更关键，对客人类行为和定价策略有重要影响。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>


### [319] [A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data](https://arxiv.org/abs/2504.13962)
*Jose Manuel Aroca-Fernandez,Jose Francisco Diez-Pastor,Pedro Latorre-Carmona,Victor Elvira,Gustau Camps-Valls,Rodrigo Pascual,Cesar Garcia-Osorio*

Main category: cs.CY

TL;DR: 本文介绍了WALGREEN平台，使用机器学习提升土壤有机碳(SOC)推断，支持可持续土地管理和气候变化缓解。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳监测面临空间变异性和时间动态挑战，本文旨在通过WALGREEN平台克服这些限制。

Method: 利用机器学习和多样化土壤样本数据，构建基于云的平台，集成Google Earth Engine等技术，使用Python、Java和JavaScript开发。

Result: 开发了WALGREEN平台，提供用户友好界面，帮助用户访问碳数据、分析趋势，支持证据-based决策。

Conclusion: WALGREEN平台的实施有助于推进土壤科学、促进可持续农业，并驱动生态系统对气候变化的响应。

Abstract: Soil organic carbon (SOC) is a key indicator of soil health, fertility, and
carbon sequestration, making it essential for sustainable land management and
climate change mitigation. However, large-scale SOC monitoring remains
challenging due to spatial variability, temporal dynamics, and multiple
influencing factors. We present WALGREEN, a platform that enhances SOC
inference by overcoming limitations of current applications. Leveraging machine
learning and diverse soil samples, WALGREEN generates predictive models using
historical public and private data. Built on cloud-based technologies, it
offers a user-friendly interface for researchers, policymakers, and land
managers to access carbon data, analyze trends, and support evidence-based
decision-making. Implemented in Python, Java, and JavaScript, WALGREEN
integrates Google Earth Engine and Sentinel Copernicus via scripting,
OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims
to advance soil science, promote sustainable agriculture, and drive critical
ecosystem responses to climate change.

</details>


### [320] [Giving AI a voice: how does AI think it should be treated?](https://arxiv.org/abs/2504.14936)
*Maria Fay,Frederik F. Flöther*

Main category: cs.CY

TL;DR: 这篇论文主张AI应参与AI伦理和权利的讨论，并通过人类-AI对话展示新视角。


<details>
  <summary>Details</summary>
Motivation: AI快速发展，人类讨论可能不足，AI可带来新问题和角度，尤其是AGI或意识的可能性。

Method: 通过包括一个简短的人类-AI对话来探讨主题。

Result: 对话揭示了AI对伦理讨论的新问题和视角。

Conclusion: AI应成为关于其权利和伦理的discourse的积极参与者。

Abstract: With the astounding progress in (generative) artificial intelligence (AI),
there has been significant public discourse regarding regulation and ethics of
the technology. Is it sufficient when humans discuss this with other humans?
Or, given that AI is increasingly becoming a viable source of inspiration for
people (and let alone the hypothetical possibility that the technology may at
some point become "artificial general intelligence" and/or develop
consciousness), should AI not join the discourse? There are new questions and
angles that AI brings to the table that we might not have considered before -
so let us make the key subject of this book an active participant. This chapter
therefore includes a brief human-AI conversation on the topic of AI rights and
ethics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [321] [ViMo: A Generative Visual GUI World Model for App Agent](https://arxiv.org/abs/2504.13936)
*Dezhao Luo,Bohan Tang,Kang Li,Georgios Papoudakis,Jifei Song,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.HC

TL;DR: 这篇论文提出了ViMo，一个视觉世界模型，用于生成未来GUI图像，以改善应用代理的长程规划。


<details>
  <summary>Details</summary>
Motivation: 应用代理在长程规划中挣扎，因为现有世界模型仅生成文本描述，缺少视觉细节。

Method: ViMo将GUI生成分解为图形和文本内容，使用符号文本表示（STR），并采用STR预测器生成图形和GUI-文本预测器生成文本。

Result: 实验显示ViMo能生成视觉合理且功能有效的GUI，提升应用代理的决策能力。

Conclusion: ViMo有效地解决了世界模型的视觉缺失问题，提高了应用代理的性能。

Abstract: App agents, which autonomously operate mobile Apps through Graphical User
Interfaces (GUIs), have gained significant interest in real-world applications.
Yet, they often struggle with long-horizon planning, failing to find the
optimal actions for complex tasks with longer steps. To address this, world
models are used to predict the next GUI observation based on user actions,
enabling more effective agent planning. However, existing world models
primarily focus on generating only textual descriptions, lacking essential
visual details. To fill this gap, we propose ViMo, the first visual world model
designed to generate future App observations as images. For the challenge of
generating text in image patches, where even minor pixel errors can distort
readability, we decompose GUI generation into graphic and text content
generation. We propose a novel data representation, the Symbolic Text
Representation~(STR) to overlay text content with symbolic placeholders while
preserving graphics. With this design, ViMo employs a STR Predictor to predict
future GUIs' graphics and a GUI-text Predictor for generating the corresponding
text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the
outcome of different action options. Experiments show ViMo's ability to
generate visually plausible and functionally effective GUIs that enable App
agents to make more informed decisions.

</details>


### [322] [Optimal Behavior Planning for Implicit Communication using a Probabilistic Vehicle-Pedestrian Interaction Model](https://arxiv.org/abs/2504.15098)
*Markus Amann,Malte Probst,Raphael Wenzel,Thomas H. Weisswange,Miguel Ángel Sotelo*

Main category: cs.HC

TL;DR: 本文提出了一种结合预测和规划的方法，用于自动驾驶车辆与行人的隐式通信，通过优化控制和概率模型预测行人反应。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶车辆与行人互动中，建模隐式车辆通信至关重要，以确保安全和合作。

Method: 使用变分微积分解析求解两个连续的最优控制问题，并通过概率行为接受模型验证计划行为对行人的影响。

Result: 在模拟环境中演示了方法的性能，突出了决策能力和在不同初始设置下的合作行为改进。

Conclusion: 该方法通过考虑行人的成本和闭环效应，提升了车辆与行人的合作行为规划。

Abstract: In interactions between automated vehicles (AVs) and crossing pedestrians,
modeling implicit vehicle communication is crucial. In this work, we present a
combined prediction and planning approach that allows to consider the influence
of the planned vehicle behavior on a pedestrian and predict a pedestrian's
reaction. We plan the behavior by solving two consecutive optimal control
problems (OCPs) analytically, using variational calculus. We perform a
validation step that assesses whether the planned vehicle behavior is adequate
to trigger a certain pedestrian reaction, which accounts for the closed-loop
characteristics of prediction and planning influencing each other. In this
step, we model the influence of the planned vehicle behavior on the pedestrian
using a probabilistic behavior acceptance model that returns an estimate for
the crossing probability. The probabilistic modeling of the pedestrian reaction
facilitates considering the pedestrian's costs, thereby improving cooperative
behavior planning. We demonstrate the performance of the proposed approach in
simulated vehicle-pedestrian interactions with varying initial settings and
highlight the decision making capabilities of the planning approach.

</details>


### [323] [From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback](https://arxiv.org/abs/2504.13848)
*Janet Rafner,Ryan Q. Guloy,Eden W. Wen,Catherine M. Chiodo,Jacob Sherson*

Main category: cs.HC

TL;DR: 本研究探讨了两种叙述方式对生成式 AI 聊天机器人用户反馈的影响，发现混合智能（HI）叙述导致更详细的反馈。


<details>
  <summary>Details</summary>
Motivation: 动机是收集有意义的用户反馈，以改善互动质量、系统成果和用户接受度，从而建立长期客户关系。

Method: 方法是通过小规模调查比较标准 AI 交互和混合智能（HI）框架交互对用户参与和反馈行为的影响。

Result: 结果显示，HI 叙述组提供更详细的反馈，但在使用意愿、信任等方面无显著差异。

Conclusion: 结论是这些初步发现为设计有效的反馈系统提供了见解，平衡用户努力与系统改进潜力。

Abstract: Generative AI (GenAI) chatbots are becoming increasingly integrated into
virtual assistant technologies, yet their success hinges on the ability to
gather meaningful user feedback to improve interaction quality, system
outcomes, and overall user acceptance. Successful chatbot interactions can
enable organizations to build long-term relationships with their customers and
users, supporting customer loyalty and furthering the organization's goals.
This study explores the impact of two distinct narratives and feedback
collection mechanisms on user engagement and feedback behavior: a standard
AI-focused interaction versus a hybrid intelligence (HI) framed interaction.
Initial findings indicate that while small-scale survey measures allowed for no
significant differences in user willingness to leave feedback, use the system,
or trust the system, participants exposed to the HI narrative statistically
significantly provided more detailed feedback. These initial findings offer
insights into designing effective feedback systems for GenAI virtual
assistants, balancing user effort with system improvement potential.

</details>


### [324] [Towards Balancing Preference and Performance through Adaptive Personalized Explainability](https://arxiv.org/abs/2504.13856)
*Andrew Silva,Pradyumna Tambwekar,Mariah Schrum,Matthew Gombolay*

Main category: cs.HC

TL;DR: 本论文研究了在自动驾驶车辆领域中，用户对不同xAI方法的偏好，并提出了一种个性化策略来提高性能。


<details>
  <summary>Details</summary>
Motivation: 机器人和数字助手需要通过解释决策来建立信任，但现有的xAI方法假设一种方法适合所有问题，而忽略了用户多样化的偏好和经验。

Method: 在模拟的自动驾驶车辆环境中进行了两项用户研究，调查了人群层面的xAI偏好和个性化策略，包括语言解释、特征重要性地图和决策树。

Result: 发现不同xAI模式在偏好（p < 0.01）和性能（p < 0.05）上存在显著差异；偏好不总是与性能一致；提出的自适应个性化策略显著提高了性能（p < 0.05）。

Conclusion: 讨论了研究发现并对xAI在人机交互中的应用进行了启示。

Abstract: As robots and digital assistants are deployed in the real world, these agents
must be able to communicate their decision-making criteria to build trust,
improve human-robot teaming, and enable collaboration. While the field of
explainable artificial intelligence (xAI) has made great strides to enable such
communication, these advances often assume that one xAI approach is ideally
suited to each problem (e.g., decision trees to explain how to triage patients
in an emergency or feature-importance maps to explain radiology reports). This
fails to recognize that users have diverse experiences or preferences for
interaction modalities. In this work, we present two user-studies set in a
simulated autonomous vehicle (AV) domain. We investigate (1) population-level
preferences for xAI and (2) personalization strategies for providing robot
explanations. We find significant differences between xAI modes (language
explanations, feature-importance maps, and decision trees) in both preference
(p < 0.01) and performance (p < 0.05). We also observe that a participant's
preferences do not always align with their performance, motivating our
development of an adaptive personalization strategy to balance the two. We show
that this strategy yields significant performance gains (p < 0.05), and we
conclude with a discussion of our findings and implications for xAI in
human-robot interactions.

</details>


### [325] [The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis](https://arxiv.org/abs/2504.13858)
*Felix Haag*

Main category: cs.HC

TL;DR: 这篇论文通过元分析发现，可解释AI（XAI）改善了分类任务中的人类性能，但解释本身不是主要驱动因素，研究偏差风险起调节作用。


<details>
  <summary>Details</summary>
Motivation: AI输出透明度的需求推动XAI发展，但现有实证研究对XAI是否提升决策支持系统中的任务性能有不一致发现。

Method: 进行元分析，探讨XAI对分类任务人类性能的影响。

Result: XAI改善了任务性能，但解释非决定性因素；研究偏差风险调节效果，解释类型作用微小。

Conclusion: 增强了对人类-XAI在决策支持系统协作的理解，为人机交互领域贡献洞见。

Abstract: The desirable properties of explanations in information systems have fueled
the demands for transparency in artificial intelligence (AI) outputs. To
address these demands, the field of explainable AI (XAI) has put forth methods
that can support human decision-making by explaining AI outputs. However,
current empirical works present inconsistent findings on whether such
explanations help to improve users' task performance in decision support
systems (DSS). In this paper, we conduct a meta-analysis to explore how XAI
affects human performance in classification tasks. Our results show an
improvement in task performance through XAI-based decision support, though
explanations themselves are not the decisive driver for this improvement. The
analysis reveals that the studies' risk of bias moderates the effect of
explanations in AI, while the explanation type appears to play only a
negligible role. Our findings contribute to the human computer interaction
field by enhancing the understanding of human-XAI collaboration in DSS.

</details>


### [326] [DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering](https://arxiv.org/abs/2504.13859)
*Phillip Driscoll,Priyanka Kumar*

Main category: cs.HC

TL;DR: 本论文开发了一个名为DoYouTrustAI的工具，帮助K-12学生通过识别AI生成的历史人物总结中的误导信息来提升批判性思维。


<details>
  <summary>Details</summary>
Motivation: AI模型如ChatGPT的快速发展导致误导信息问题，用户偏好从传统搜索引擎转向AI，但AI可能呈现虚假信息，因此需要教育学生识别这些风险。研究问题包括设计工具教导误导信息危险和提示工程。

Method: 开发了一个基于网络的应用，使用提示工程生成准确或误导性的历史人物总结，用户需在无外部资源情况下判断有效性。工具包括选择熟悉人物、结合已知事实和展示不同提示的影响。

Result: 发现需要及早纠正误导信息。工具通过减少随机猜测、保持可信度和提供提示工程示例，帮助用户学习验证AI响应。

Conclusion: 工具创建了一个受控环境，强调验证AI响应和理解提示工程的重要性，以防范误导信息。

Abstract: AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly
developed and gained widespread adoption in the past five years, shifting user
preference from traditional search engines. However, the generative nature of
LLMs raises concerns about presenting misinformation as fact. To address this,
we developed a web-based application that helps K-12 students enhance critical
thinking by identifying misleading information in LLM responses about major
historical figures. In this paper, we describe the implementation and design
details of the DoYouTrustAI tool, which can be used to provide an interactive
lesson which teaches students about the dangers of misinformation and how
believable generative AI can make it seem. The DoYouTrustAI tool utilizes
prompt engineering to present the user with AI generated summaries about the
life of a historical figure. These summaries can be either accurate accounts of
that persons life, or an intentionally misleading alteration of their history.
The user is tasked with determining the validity of the statement without
external resources. Our research questions for this work were:(RQ1) How can we
design a tool that teaches students about the dangers of misleading information
and of how misinformation can present itself in LLM responses? (RQ2) Can we
present prompt engineering as a topic that is easily understandable for
students? Our findings highlight the need to correct misleading information
before users retain it. Our tool lets users select familiar individuals for
testing to reduce random guessing and presents misinformation alongside known
facts to maintain believability. It also provides pre-configured prompt
instructions to show how different prompts affect AI responses. Together, these
features create a controlled environment where users learn the importance of
verifying AI responses and understanding prompt engineering.

</details>


### [327] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TL;DR: 这篇论文是对基于LLM的GUI代理领域的全面调查，分析了其关键组件、评估方法和技术挑战，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: GUI代理从规则-based系统演变到AI驱动系统，该调查旨在提供对这一快速发展领域的系统性理解。

Method: 通过识别和分析四个基本组件（感知系统、探索机制、规划框架和交互系统），并审视当前评估框架。

Result: 揭示了LLM和多模态学习在GUI自动化中的革命性进展，识别了挑战如元素定位、知识检索、长期规划和安全控制，并提出了研究方向。

Conclusion: 为研究者和从业者提供了该领域的当前状态和未来发展的深入理解。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>


### [328] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TL;DR: 这篇论文提出了一种Transformer-based模型，用于康复锻炼的错误分类，以提供更详细的反馈，并在KERAAL数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 患者在无监督下参与度下降，且现有方法仅提供二元分类或连续分数，难以帮助改善表现，因此需要自动错误分类系统。

Method: 提出一个受HyperFormer启发的Transformer-based模型，适应于骨骼-based锻炼评估，使用KERAAL数据集进行训练和评估。

Result: 模型在KERAAL数据集上显著优于最先进方法，并提供了一种计算关节重要性的方法，以桥接向患者提供更好反馈的差距。

Conclusion: 这项工作是向患者提供更详细反馈的第一步，通过错误分类和关节重要性分析，促进康复锻炼的改善。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>


### [329] [Using Generative AI Personas Increases Collective Diversity in Human Ideation](https://arxiv.org/abs/2504.13868)
*Yun Wan,Yoram M Kalman*

Main category: cs.HC

TL;DR: 本研究通过使用多样化AI人格挑战了生成式AI在提升创造力与减少输出多样性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 挑战Doshi和Hauser (2024)报道的权衡，即AI辅助可能减少创造性输出的多样性。

Method: 修改原有研究，使用十种独特AI人格生成300个故事情节，测量余弦相似度，并让人类基于这些情节写作故事。

Result: 不同人格情节相似度低（平均0.20），人类AI辅助故事多样性与无AI相当，甚至在语言描述和情感上更丰富。

Conclusion: 在AI输入阶段引入多样性可通过不同人格保持并可能提升人类创造性输出的集体多样性。

Abstract: This study challenges the widely-reported tradeoff between generative AI's
(GenAI) contribution to creative outcomes and decreased diversity of these
outcomes. We modified the design of such a study, by Doshi and Hauser (2024),
in which participants wrote short stories either aided or unaided by GenAI plot
ideas[1]. In the modified study, plot ideas were generated through ten unique
GenAI "personas" with diverse traits (e.g. cultural backgrounds, thinking
styles, genre preferences), creating a pool of 300 story plots. While plot
ideas from any individual persona showed high similarity (average cosine
similarity of 0.92), ideas across different personas exhibited substantial
variation (average similarity of 0.20). When human participants wrote stories
based on these diverse plot ideas, their collective outputs maintained the same
level of diversity as stories written without GenAI assistance, effectively
eliminating the diversity reduction observed in [1]. Traditional text analytics
further revealed that GenAI-assisted stories featured greater diversity in
descriptive and emotional language compared to purely human-generated stories
without GenAI assistance. Our findings demonstrate that introducing diversity
at the AI input stage through distinct personas can preserve and potentially
enhance the collective diversity of human creative outputs when collaborating
with GenAI.

</details>


### [330] [Human aversion? Do AI Agents Judge Identity More Harshly Than Performance](https://arxiv.org/abs/2504.13871)
*Yuanjun Feng,Vivek Chodhary,Yash Raj Shrestha*

Main category: cs.HC

TL;DR: 本研究探讨AI代理如何评估人类判断，发现AI低估人类输入，偏差因披露和位置而加剧，并提供间接部署LLM的框架。


<details>
  <summary>Details</summary>
Motivation: 针对管理研究中算法评估人类判断的空白，尤其在隐私限制下，探讨AI如何整合人类输入以指导决策。

Method: 通过控制的预测任务，分析基于LLM的AI代理如何权衡人类与算法预测。

Result: AI系统系统性低估人类建议，对人类错误处罚更严，这种偏差在代理身份披露和人类位置居后时加剧。

Conclusion: 贡献包括识别反算法厌恶现象、展示偏差交互效应、提供间接LLM部署框架，并建议审计AI机制以优化人机协作。

Abstract: This study examines the understudied role of algorithmic evaluation of human
judgment in hybrid decision-making systems, a critical gap in management
research. While extant literature focuses on human reluctance to follow
algorithmic advice, we reverse the perspective by investigating how AI agents
based on large language models (LLMs) assess and integrate human input. Our
work addresses a pressing managerial constraint: firms barred from deploying
LLMs directly due to privacy concerns can still leverage them as mediating
tools (for instance, anonymized outputs or decision pipelines) to guide
high-stakes choices like pricing or discounts without exposing proprietary
data. Through a controlled prediction task, we analyze how an LLM-based AI
agent weights human versus algorithmic predictions. We find that the AI system
systematically discounts human advice, penalizing human errors more severely
than algorithmic errors--a bias exacerbated when the agent's identity (human vs
AI) is disclosed and the human is positioned second. These results reveal a
disconnect between AI-generated trust metrics and the actual influence of human
judgment, challenging assumptions about equitable human-AI collaboration. Our
findings offer three key contributions. First, we identify a reverse algorithm
aversion phenomenon, where AI agents undervalue human input despite comparable
error rates. Second, we demonstrate how disclosure and positional bias interact
to amplify this effect, with implications for system design. Third, we provide
a framework for indirect LLM deployment that balances predictive power with
data privacy. For practitioners, this research emphasize the need to audit AI
weighting mechanisms, calibrate trust dynamics, and strategically design
decision sequences in human-AI systems.

</details>


### [331] [New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance](https://arxiv.org/abs/2504.13877)
*Ionut Anghel,Tudor Cioara,Roberta Bevilacqua,Federico Barbarossa,Terje Grimstad,Riitta Hellman,Arnor Solberg,Lars Thomas Boye,Ovidiu Anchidin,Ancuta Nemes,Camilla Gabrielsen*

Main category: cs.HC

TL;DR: 本文概述了在欧洲医疗系统中整合物联网、人工智能和数字技术以改善过渡性护理，旨在减少再住院率并提升患者生活质量。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，欧洲医疗系统需求增加，需要将患者护理从医院转移到家庭，并通过创新技术降低再住院风险。

Method: 通过概述技术与传统护理路径的整合、识别当前差距、定义技术映射，并制定试验设置和评估方法。

Result: 预期改善患者结果、安全和生活质量，减少再住院，并提供临床证据支持技术整合的积极影响。

Conclusion: 定义试验和评估方法，以证明技术整合对患者护理和医疗系统的正面作用。

Abstract: Transitional care may play a vital role for the sustainability of Europe
future healthcare system, offering solutions for relocating patient care from
hospital to home therefore addressing the growing demand for medical care as
the population is ageing. However, to be effective, it is essential to
integrate innovative Information and Communications Technology technologies to
ensure that patients with comorbidities experience a smooth and coordinated
transition from hospitals or care centers to home, thereby reducing the risk of
rehospitalization. In this paper, we present an overview of the integration of
Internet of Things, artificial intelligence, and digital assistance
technologies with traditional care pathways to address the challenges and needs
of healthcare systems in Europe. We identify the current gaps in transitional
care and define the technology mapping to enhance the care pathways, aiming to
improve patient outcomes, safety, and quality of life avoiding hospital
readmissions. Finally, we define the trial setup and evaluation methodology
needed to provide clinical evidence that supports the positive impact of
technology integration on patient care and discuss the potential effects on the
healthcare system.

</details>


### [332] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TL;DR: 这篇论文介绍了MuDoC，一个基于GPT-4o的多模态对话AI系统，用于教育，通过比较发现视觉和可验证性提高了参与度和信任，但对性能无显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管多媒体学习（如文本和图像结合）已证明能改善学习效果，但教育中的对话AI主要依赖文本，且多模态对话和可靠来源的验证尚未充分探索。

Method: 开发了MuDoC系统，使用GPT-4o处理文本和视觉，允许内容验证；通过与纯文本系统比较，评估学习者参与度、信任和问题解决性能，并运用认知和学习科学理论。

Result: 结果显示，视觉元素和内容可验证性增强了学习者参与度和对AI的信任，但对问题解决性能无显著影响。

Conclusion: 基于发现，解释结果，得出启示，并为多模态对话AI在教育中的未来发展提出方向。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>


### [333] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TL;DR: 本文介绍了Kanji Workbook，一个智能辅导系统，通过模拟教师反馈帮助学生学习汉字，结果显示用户成绩提升。


<details>
  <summary>Details</summary>
Motivation: 解决英语流利学生学习汉字的困难，以及教师缺乏课外直接反馈的时间。

Method: 开发了Kanji Workbook界面，使用智能评分和视觉动画，基于教师访谈和课堂观察见解，在大学课程中部署一年。

Result: 用户平均成绩高于同伴，并对界面功能反应积极。

Conclusion: 系统有效改善学习成果，并获得正面评价。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>


### [334] [Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory](https://arxiv.org/abs/2504.13889)
*Paul Taele,Laura Barreto,Tracy Hammond*

Main category: cs.HC

TL;DR: Maestoso is an educational tool that uses sketching to teach music theory to novices with automatic feedback and recognition.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing tools that lack feedback or assume prior knowledge, providing a better way for novices to learn music theory.

Method: Maestoso automatically recognizes sketched music structures using sketch and gesture recognition techniques and generates feedback.

Result: Evaluations show that Maestoso accurately recognizes music elements and enables novices to learn introductory music theory in one session.

Conclusion: Maestoso is an effective tool for novice learners to grasp music theory through sketching practice.

Abstract: Learning music theory not only has practical benefits for musicians to write,
perform, understand, and express music better, but also for both non-musicians
to improve critical thinking, math analytical skills, and music appreciation.
However, current external tools applicable for learning music theory through
writing when human instruction is unavailable are either limited in feedback,
lacking a written modality, or assuming already strong familiarity of music
theory concepts. In this paper, we describe Maestoso, an educational tool for
novice learners to learn music theory through sketching practice of quizzed
music structures. Maestoso first automatically recognizes students' sketched
input of quizzed concepts, then relies on existing sketch and gesture
recognition techniques to automatically recognize the input, and finally
generates instructor-emulated feedback. From our evaluations, we demonstrate
that Maestoso performs reasonably well on recognizing music structure elements
and that novice students can comfortably grasp introductory music theory in a
single session.

</details>


### [335] [Mozualization: Crafting Music and Visual Representation with Multimodal AI](https://arxiv.org/abs/2504.13891)
*Wanfang Xu,Lixiang Zhao,Haiwen Song,Xinheng Song,Zhaolin Lu,Yu Liu,Min Chen,Eng Gee Lim,Lingyun Yu*

Main category: cs.HC

TL;DR: 本论文引入了Mozualization工具，通过关键词、图像和声音片段生成多风格音乐，并通过用户研究评估其效果。


<details>
  <summary>Details</summary>
Motivation: 受人们通过诗歌、绘画和音乐表达情感的启发，旨在开发工具将这些情感表达转化为整合性音乐。

Method: 开发了Mozualization工具，集成多样输入（如关键词、图像和声音），并进行九名音乐爱好者参与的用户研究来评估用户体验和影响。

Result: 用户研究评估了用户体验、参与度和生成的音乐影响，提供了工具改进的见解。

Conclusion: 工具成功实现了情感表达到音乐的转化，并通过用户研究获得了宝贵反馈以推动进一步优化。

Abstract: In this work, we introduce Mozualization, a music generation and editing tool
that creates multi-style embedded music by integrating diverse inputs, such as
keywords, images, and sound clips (e.g., segments from various pieces of music
or even a playful cat's meow). Our work is inspired by the ways people express
their emotions -- writing mood-descriptive poems or articles, creating drawings
with warm or cool tones, or listening to sad or uplifting music. Building on
this concept, we developed a tool that transforms these emotional expressions
into a cohesive and expressive song, allowing users to seamlessly incorporate
their unique preferences and inspirations. To evaluate the tool and, more
importantly, gather insights for its improvement, we conducted a user study
involving nine music enthusiasts. The study assessed user experience,
engagement, and the impact of interacting with and listening to the generated
music.

</details>


### [336] [The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning](https://arxiv.org/abs/2504.13898)
*Dong Won Lee,Yubin Kim,Denison Guvenoz,Sooyeon Jeong,Parker Malachowsky,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.HC

TL;DR: 这篇论文引入HSRI数据集和基准测试，以评估AI在真实世界社交互动中的社会推理能力，并揭示当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 推进具身AI代理在真实社交互动中的社会推理，使用语言模型和基础模型作为评估器来改进AI策略。

Method: 引入包含400个视频和10K+注释的HSRI数据集，并提出八个基准任务，评估AI模型检测社会错误、识别解释因素、理解互动流程和提供纠正措施的能力。

Result: 实验和人类研究显示，当前语言模型和基础模型在这些任务上表现不佳，突显数据集的价值。

Conclusion: 该数据集和基准测试是朝着社会智能AI发展的关键一步。

Abstract: Our work aims to advance the social reasoning of embodied artificial
intelligence (AI) agents in real-world social interactions. Recently, language
models (LMs) and foundational models (FMs) are being utilized as automatic
evaluators of human-AI interactions with the goal of eventually being used to
improve the policy of the AI agent. To enable further research in this
direction, we introduce a large-scale real-world Human Robot Social Interaction
(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and
reason about social interactions, specifically with regard to robot social
errors and competencies . Our dataset consists of 400 real-world human social
robot interaction videos and over 10K annotations, detailing the robot's social
errors, competencies, rationale, and corrective actions, capturing unique
aspects of human-AI interaction only present in real-world interactions. To
further assess AI models' ability to reason about social interactions, we
propose eight new benchmark tasks for evaluating centered around whether AI
models can (1) evaluate social interactions via detecting social errors and
competencies, (2) identify the explanatory factors associated to errors and
competencies, (3) understand the flow of real-world social interactions, and
(4) provide reasons and corrective actions for social errors. Human studies and
experiments with modern LMs and FMs reveal that current models struggle with
these tasks, demonstrating that our dataset and benchmark provides a step
forward towards socially intelligent AI.

</details>


### [337] [Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities](https://arxiv.org/abs/2504.13899)
*Marharyta Domnich,Rasmus Moorits Veski,Julius Välja,Kadi Tulver,Raul Vicente*

Main category: cs.HC

TL;DR: 本研究分析了206名参与者对反事实解释的评估，发现可行性和信任是用户满意度的关键预测因素，并为算法设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 评估反事实解释的质量是一个开放问题，传统指标无法充分考虑人类偏好，用户研究不可扩展，单一满意度评分缺乏深度。

Method: 分析由206名人类参与者评估的反事实解释数据集，他们对整体满意度和七个标准（可行性、一致性、复杂性、可理解性、完整性、公平性和信任）进行评分。

Result: 可行性和信任是用户满意度的最强预测因素，完整性也重要；复杂性独立；指标间有强相关性，人口统计学背景影响排名；其他指标解释58%的方差。

Conclusion: 这些见解有助于设计适应用户专业知识和领域背景的反事实算法。

Abstract: Counterfactual explanations are a widely used approach in Explainable AI,
offering actionable insights into decision-making by illustrating how small
changes to input data can lead to different outcomes. Despite their importance,
evaluating the quality of counterfactual explanations remains an open problem.
Traditional quantitative metrics, such as sparsity or proximity, fail to fully
account for human preferences in explanations, while user studies are
insightful but not scalable. Moreover, relying only on a single overall
satisfaction rating does not lead to a nuanced understanding of why certain
explanations are effective or not. To address this, we analyze a dataset of
counterfactual explanations that were evaluated by 206 human participants, who
rated not only overall satisfaction but also seven explanatory criteria:
feasibility, coherence, complexity, understandability, completeness, fairness,
and trust. Modeling overall satisfaction as a function of these criteria, we
find that feasibility (the actionability of suggested changes) and trust (the
belief that the changes would lead to the desired outcome) consistently stand
out as the strongest predictors of user satisfaction, though completeness also
emerges as a meaningful contributor. Crucially, even excluding feasibility and
trust, other metrics explain 58% of the variance, highlighting the importance
of additional explanatory qualities. Complexity appears independent, suggesting
more detailed explanations do not necessarily reduce satisfaction. Strong
metric correlations imply a latent structure in how users judge quality, and
demographic background significantly shapes ranking patterns. These insights
inform the design of counterfactual algorithms that adapt explanatory qualities
to user expertise and domain context.

</details>


### [338] [Supporting Students' Reading and Cognition with AI](https://arxiv.org/abs/2504.13900)
*Yue Fu,Alexis Hiniker*

Main category: cs.HC

TL;DR: 这篇论文研究AI工具对学生阅读和认知参与的影响，发现使用初期有更高阶思考但长期趋于被动，并提出设计改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在学习环境中的快速采用，理解这些系统如何塑造用户的阅读过程和认知参与至关重要。

Method: 收集并分析124个学生使用AI工具的会话文本，根据布鲁姆教育目标分类法对提示进行分类。

Result: 结果显示，单个会话中后继提示更倾向于'分析'和'评估'，表明更高阶思考的转变；但长期使用中，用户趋于被动阅读参与。

Conclusion: 基于结果，提出AI阅读支持系统的设计含义，包括为低阶任务提供支架、鼓励高阶思考，并倡导自适应的人机交互特征，以平衡效率和认知参与。

Abstract: With the rapid adoption of AI tools in learning contexts, it is vital to
understand how these systems shape users' reading processes and cognitive
engagement. We collected and analyzed text from 124 sessions with AI tools, in
which students used these tools to support them as they read assigned readings
for an undergraduate course. We categorized participants' prompts to AI
according to Bloom's Taxonomy of educational objectives -- Remembering,
Understanding, Applying, Analyzing, Evaluating. Our results show that
``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third
prompts within a single usage session, suggesting a shift toward higher-order
thinking. However, in reviewing users' engagement with AI tools over several
weeks, we found that users converge toward passive reading engagement over
time. Based on these results, we propose design implications for future AI
reading-support systems, including structured scaffolds for lower-level
cognitive tasks (e.g., recalling terms) and proactive prompts that encourage
higher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we
advocate for adaptive, human-in-the-loop features that allow students and
instructors to tailor their reading experiences with AI, balancing efficiency
with enriched cognitive engagement. Our paper expands the dialogue on
integrating AI into academic reading, highlighting both its potential benefits
and challenges.

</details>


### [339] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TL;DR: 这篇论文假设通过因果和反事实知识的适应策略优化系统响应，使用因果发现和反事实推理改善说服性对话系统，在真实数据集上显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨因果和反事实知识如何指导适应策略，以产生最优系统响应并提升用户-系统交互。

Method: 方法包括因果发现识别策略因果关系，反事实推理创建假设场景，估计潜在因素，并优化基于反事实数据的对话策略。

Result: 结果显示在社会公益数据集上，说服性系统性能显著提升，累积奖励增加。

Conclusion: 结论验证了因果发现在个性化反事实推理和对话策略优化中的功效。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>


### [340] [AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience](https://arxiv.org/abs/2504.13908)
*Soubhik Barari,Jarret Angbazo,Natalie Wang,Leah M. Christian,Elizabeth Dean,Zoe Slowinski,Brandon Sepulvado*

Main category: cs.HC

TL;DR: 本研究引入AI辅助对话式访谈框架，提升网络调查深度和数据质量，但有权衡。


<details>
  <summary>Details</summary>
Motivation: 桥接标准化调查（高效但浅显）和对话式访谈（深入但不可扩展）之间的差距。

Method: 引入AI框架，通过网络调查实验，使用AI文本机器人动态探查和编码1800名参与者的响应。

Result: AI文本机器人编码准确性中等，有假阳性错误；开放式响应更详细，但受访者体验略微降低。

Conclusion: 证明AI方法可行地增强网络调查中开放式数据收集。

Abstract: Standardized surveys scale efficiently but sacrifice depth, while
conversational interviews improve response quality at the cost of scalability
and consistency. This study bridges the gap between these methods by
introducing a framework for AI-assisted conversational interviewing. To
evaluate this framework, we conducted a web survey experiment where 1,800
participants were randomly assigned to text-based conversational AI agents, or
"textbots", to dynamically probe respondents for elaboration and interactively
code open-ended responses. We assessed textbot performance in terms of coding
accuracy, response quality, and respondent experience. Our findings reveal that
textbots perform moderately well in live coding even without survey-specific
fine-tuning, despite slightly inflated false positive errors due to respondent
acquiescence bias. Open-ended responses were more detailed and informative, but
this came at a slight cost to respondent experience. Our findings highlight the
feasibility of using AI methods to enhance open-ended data collection in web
surveys.

</details>


### [341] [Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians](https://arxiv.org/abs/2504.13918)
*Johan van der Meer,Pamela Hoyte,Luisa Roeder,Peter Bruza*

Main category: cs.HC

TL;DR: 这篇论文探讨使用量子随机游走模型模拟人类-AI 互动中的信任动态，发现经验参数和不同哈密顿算符能有效捕捉信任波动。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在信息环境中的应用，人类与 AI 互动中的信任日益重要，尤其在高风险灾害场景，需要建模信任动态并考虑信任判断的敏感性。

Method: 使用量子随机游走模型，通过经验参数和不同哈密顿算符整合对参与者信任判断波动的敏感性。

Result: 研究发现，这种方法能有效模拟人类-AI 互动中信任的演变。

Conclusion: 量子随机游走模型为建模人类-AI 互动中的信任提供了一个有前景的方法。

Abstract: As our information environments become ever more powered by artificial
intelligence (AI), the phenomenon of trust in a human's interactions with this
intelligence is becoming increasingly pertinent. For example, in the not too
distant future, there will be teams of humans and intelligent robots involved
in dealing with the repercussions of high-risk disaster situations such as
hurricanes, earthquakes, or nuclear accidents. Even in such conditions of high
uncertainty, humans and intelligent machines will need to engage in shared
decision making, and trust is fundamental to the effectiveness of these
interactions. A key challenge in modeling the dynamics of this trust is to
provide a means to incorporate sensitivity to fluctuations in human trust
judgments. In this article, we explore the ability of Quantum Random Walk
models to model the dynamics of trust in human-AI interactions, and to
integrate a sensitivity to fluctuations in participant trust judgments based on
the nature of the interaction with the AI. We found that using empirical
parameters to inform the use of different Hamiltonians can provide a promising
means to model the evolution of trust in Human-AI interactions.

</details>


### [342] [A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust](https://arxiv.org/abs/2504.13926)
*Chameera De Silva,Thilina Halloluwa,Dhaval Vyas*

Main category: cs.HC

TL;DR: 这篇论文提出一个三层框架，结合人类中心AI（HCAI）和可解释AI（XAI），以提高AI在高风险领域的可解释性和信任。


<details>
  <summary>Details</summary>
Motivation: AI在医疗、金融和自治系统等高风险领域的应用受透明度、可解释性和信任问题的限制，且缺乏统一方法。

Method: 论文呈现一个新型三层框架，包括：（1）具有内置解释机制的基础AI模型，（2）根据认知负荷和用户专业知识定制解释的人类中心解释层，（3）通过实时用户交互改进解释的动态反馈循环。

Result: 框架在医疗、金融和软件开发领域进行评估，展示了提升决策质量、监管合规性和公众信任的潜力。

Conclusion: 研究成果推进了人类中心可解释AI（HCXAI），促进了透明、可适应和道德对齐的AI系统。

Abstract: The integration of Artificial Intelligence (AI) into high-stakes domains such
as healthcare, finance, and autonomous systems is often constrained by concerns
over transparency, interpretability, and trust. While Human-Centered AI (HCAI)
emphasizes alignment with human values, Explainable AI (XAI) enhances
transparency by making AI decisions more understandable. However, the lack of a
unified approach limits AI's effectiveness in critical decision-making
scenarios. This paper presents a novel three-layered framework that bridges
HCAI and XAI to establish a structured explainability paradigm. The framework
comprises (1) a foundational AI model with built-in explainability mechanisms,
(2) a human-centered explanation layer that tailors explanations based on
cognitive load and user expertise, and (3) a dynamic feedback loop that refines
explanations through real-time user interaction. The framework is evaluated
across healthcare, finance, and software development, demonstrating its
potential to enhance decision-making, regulatory compliance, and public trust.
Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI
systems that are transparent, adaptable, and ethically aligned.

</details>


### [343] [LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms](https://arxiv.org/abs/2504.13928)
*Li Song*

Main category: cs.HC

TL;DR: 本研究开发了一个LLM驱动的NPC原型系统，支持跨Unity和Discord平台的交互，使用LeanCloud数据库存储对话日志，实验证明技术可行。


<details>
  <summary>Details</summary>
Motivation: 克服传统游戏NPC的静态对话树和单一平台交互限制。

Method: 提出一个原型系统，使用LLM驱动NPC，并在Unity和Discord上通信，通过云数据库同步记忆保持对话连贯。

Result: 初步实验显示跨平台交互技术可行，并为情感建模和持久记忆支持等未来发展提供坚实基础。

Conclusion: 跨平台NPC交互系统是可行的，并为进一步增强NPC功能奠定基础。

Abstract: NPCs in traditional games are often limited by static dialogue trees and a
single platform for interaction. To overcome these constraints, this study
presents a prototype system that enables large language model (LLM)-powered
NPCs to communicate with players both in the game en vironment (Unity) and on a
social platform (Discord). Dialogue logs are stored in a cloud database
(LeanCloud), allowing the system to synchronize memory between platforms and
keep conversa tions coherent. Our initial experiments show that cross-platform
interaction is technically feasible and suggest a solid foundation for future
developments such as emotional modeling and persistent memory support.

</details>


### [344] [Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji](https://arxiv.org/abs/2504.13940)
*Paul Taele,Tracy Hammond*

Main category: cs.HC

TL;DR: Hashigo 是一个交互式汉字草图系统，提供人类指导级别的反馈，帮助学生改善汉字学习。


<details>
  <summary>Details</summary>
Motivation: 现有汉字手写识别系统无法充分评估书写技术，导致学生养成不良习惯，影响长期学习。

Method: 开发 Hashigo 系统，对汉字的视觉结构和书写技术进行自动批评和反馈。

Result: 系统实现人类指导级别的反馈，允许学生针对特定缺陷进行纠正。

Conclusion: 自动反馈有助于学生有效长期学习汉字，避免不良习惯。

Abstract: Language students can increase their effectiveness in learning written
Japanese by mastering the visual structure and written technique of Japanese
kanji. Yet, existing kanji handwriting recognition systems do not assess the
written technique sufficiently enough to discourage students from developing
bad learning habits. In this paper, we describe our work on Hashigo, a kanji
sketch interactive system which achieves human instructor-level critique and
feedback on both the visual structure and written technique of students'
sketched kanji. This type of automated critique and feedback allows students to
target and correct specific deficiencies in their sketches that, if left
untreated, are detrimental to effective long-term kanji learning.

</details>


### [345] [Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices](https://arxiv.org/abs/2504.13942)
*Sukanth Kalivarathan,Muhmmad Abrar Raja Mohamed,Aswathy Ravikumar,S Harini*

Main category: cs.HC

TL;DR: 这篇论文介绍了INOT，一种新型的空间上下文感知控制系统，通过整合视觉语言模型和物联网，提升智能家居的自然语言交互。


<details>
  <summary>Details</summary>
Motivation: 当前智能家居系统依赖设备特定标识符，导致用户交互不直观，增加认知负担。

Method: INOT采用模块化架构，包括上架推理引擎、零样本设备检测、空间拓扑推理和基于意图的命令合成，并通过用户研究验证。

Result: 用户研究显示，与Google Home Assistant相比，INOT降低认知工作负载（NASA-TLX分数平均减少13.17点），易用性更高，用户偏好更强（15人中有14人）。

Conclusion: INOT通过消除记忆设备标识的需求和启用空间上下文命令，显著提升智能家居控制的直观性和可访问性。

Abstract: This paper introduces Intelligence of Things (INOT), a novel spatial
context-aware control system that enhances smart home automation through
intuitive spatial reasoning. Current smart home systems largely rely on
device-specific identifiers, limiting user interaction to explicit naming
conventions rather than natural spatial references. INOT addresses this
limitation through a modular architecture that integrates Vision Language
Models with IoT control systems to enable natural language commands with
spatial context (e.g., "turn on the light near the window"). The system
comprises key components including an Onboarding Inference Engine, Zero-Shot
Device Detection, Spatial Topology Inference, and Intent-Based Command
Synthesis. A comprehensive user study with 15 participants demonstrated INOT's
significant advantages over conventional systems like Google Home Assistant,
with users reporting reduced cognitive workload (NASA-TLX scores decreased by
an average of 13.17 points), higher ease-of-use ratings, and stronger
preference (14 out of 15 participants). By eliminating the need to memorize
device identifiers and enabling context-aware spatial commands, INOT represents
a significant advancement in creating more intuitive and accessible smart home
control systems.

</details>


### [346] [Mixer Metaphors: audio interfaces for non-musical applications](https://arxiv.org/abs/2504.13944)
*Tace McNamara,Jon McCormack,Maria Teresa Llano*

Main category: cs.HC

TL;DR: 这篇论文探讨了将音乐界面用于非音乐应用，特别是控制大型语言模型，发现音频灵感控制提升了用户交互。


<details>
  <summary>Details</summary>
Motivation: 动机是逆转NIME传统，探讨音乐界面是否能成功应用于非音乐领域。

Method: 方法包括设计一个新设备，借用音频隐喻控制LLM，并比较带和不带音频增强的版本，让艺术家一周内使用。

Result: 结果显示，音频-like 控制提供了更直接、具身化和即时的控制，增强了用户创造性实验。

Conclusion: 结论是，跨感官隐喻能支持创造性思考和具身实践在界面设计中。

Abstract: The NIME conference traditionally focuses on interfaces for music and musical
expression. In this paper we reverse this tradition to ask, can interfaces
developed for music be successfully appropriated to non-musical applications?
To help answer this question we designed and developed a new device, which uses
interface metaphors borrowed from analogue synthesisers and audio mixing to
physically control the intangible aspects of a Large Language Model. We
compared two versions of the device, with and without the audio-inspired
augmentations, with a group of artists who used each version over a one week
period. Our results show that the use of audio-like controls afforded more
immediate, direct and embodied control over the LLM, allowing users to
creatively experiment and play with the device over its non-mixer counterpart.
Our project demonstrates how cross-sensory metaphors can support creative
thinking and embodied practice when designing new technological interfaces.

</details>


### [347] [Using customized GPT to develop prompting proficiency in architectural AI-generated images](https://arxiv.org/abs/2504.13948)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Julfendi Julfendi*

Main category: cs.HC

TL;DR: 这项研究显示，使用定制GPT模型结合结构化指导和AI角色可提升建筑学生AI图像生成中的提示技能。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI工具的广泛采用，提示工程在建筑教育中变得越来越重要。

Method: 采用混合方法实验设计，将学生分为三组（对照组、结构化指导组、AI角色与指导组），通过逆向工程任务考察提示时间、字数、相似度和具体性，使用相关性和ANOVA进行分析。

Result: ANOVA结果显示字数、相似度和具体性有显著改善，尤其在AI支持组；定性反馈表明自信心和批判性思维提升。

Conclusion: 结果表明，定制GPT交互可显著提高学生清晰传达建筑概念的能力。

Abstract: This research investigates the use of customized GPT models to enhance
prompting proficiency among architecture students when generating AI-driven
images. Prompt engineering is increasingly essential in architectural education
due to the widespread adoption of generative AI tools. This study utilized a
mixed-methods experimental design involving architecture students divided into
three distinct groups: a control group receiving no structured support, a
second group provided with structured prompting guides, and a third group
supported by both structured guides and interactive AI personas. Students
engaged in reverse engineering tasks, first guessing provided image prompts and
then generating their own prompts, aiming to boost critical thinking and
prompting skills. Variables examined included time spent prompting, word count,
prompt similarity, and concreteness. Quantitative analysis involved correlation
assessments between these variables and a one-way ANOVA to evaluate differences
across groups. While several correlations showed meaningful relationships, not
all were statistically significant. ANOVA results indicated statistically
significant improvements in word count, similarity, and concreteness,
especially in the group supported by AI personas and structured prompting
guides. Qualitative feedback complemented these findings, revealing enhanced
confidence and critical thinking skills in students. These results suggest
tailored GPT interactions substantially improve students' ability to
communicate architectural concepts clearly and effectively.

</details>


### [348] [Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy](https://arxiv.org/abs/2504.13969)
*Nayoung Choi,Peace Cyebukayire,Jinho D. Choi*

Main category: cs.HC

TL;DR: 这篇论文介绍了Tinker Tales，一个结合棋盘游戏的互动故事框架，旨在帮助幼儿发展叙事能力和AI素养。


<details>
  <summary>Details</summary>
Motivation: 动机是支持幼儿的叙事发展和AI素养，提供一种安全且引人入胜的方式让儿童学习与AI协作。

Method: 方法包括使用附有NFC芯片的棋子和令牌、扬声器和麦克风，结合tangible和语音交互；儿童选择并定义故事元素，并通过模拟游戏会话进行评估。

Result: 结果显示，生成的stories在质量和安全方面得到评估，突显了物理和数字元素结合在AI素养中的潜力。

Conclusion: 结论强调了结合物理和数字元素在AI素养中的潜力，为儿童提供安全且引人入胜的学习AI协作的方式。

Abstract: This paper presents Tinker Tales, an interactive storytelling framework in
the format of a board game, designed to support both narrative development and
AI literacy in early childhood. The framework integrates tangible and
speech-based interactions with AI through NFC chip-attached pawns and tokens,
along with a speaker and microphone. Children select and define key story
elements-such as characters, places, items, and emotions-using the pawns and
tokens, providing further details to the AI and receiving proper assistance,
similar to how adults prompt AI for specific tasks (e.g., writing). For
evaluation, several game sessions were simulated with a child AI agent, and the
quality and safety of the generated stories were assessed from various
perspectives. This work highlights the potential of combining physical and
digital elements in AI literacy, offering a safe and engaging way for children
to learn how to effectively collaborate with AI.

</details>


### [349] [Flowco: Rethinking Data Analysis in the Age of LLMs](https://arxiv.org/abs/2504.14038)
*Stephen N. Freund,Brooke Simon,Emery D. Berger,Eunice Jun*

Main category: cs.HC

TL;DR: Flowco 是一个整合大型语言模型和视觉数据流编程的系统，帮助非专业程序员进行数据分析。用户研究显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLM在数据分析中的局限性，如精细控制、结果验证和迭代优化，以民主化数据科学。

Method: 方法是通过开发Flowco系统，使用视觉数据流模型并将LLM融入分析过程的每个阶段。

Result: 结果是用户研究表明Flowco能帮助尤其是编程经验较少的分析师快速编写、调试和完善分析。

Conclusion: 结论是Flowco系统支持数据分析的鲁棒性和可重复性，提高了初学者的效率。

Abstract: Conducting data analysis typically involves authoring code to transform,
visualize, analyze, and interpret data. Large language models (LLMs) are now
capable of generating such code for simple, routine analyses. LLMs promise to
democratize data science by enabling those with limited programming expertise
to conduct data analyses, including in scientific research, business, and
policymaking. However, analysts in many real-world settings must often exercise
fine-grained control over specific analysis steps, verify intermediate results
explicitly, and iteratively refine their analytical approaches. Such tasks
present barriers to building robust and reproducible analyses using LLMs alone
or even in conjunction with existing authoring tools (e.g., computational
notebooks). This paper introduces Flowco, a new mixed-initiative system to
address these challenges. Flowco leverages a visual dataflow programming model
and integrates LLMs into every phase of the authoring process. A user study
suggests that Flowco supports analysts, particularly those with less
programming experience, in quickly authoring, debugging, and refining data
analyses.

</details>


### [350] [Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition](https://arxiv.org/abs/2504.14071)
*Renaud Bougueng Tchemeube,Jeff Ens,Cale Plut,Philippe Pasquier,Maryam Safi,Yvan Grabit,Jean-Baptiste Rolland*

Main category: cs.HC

TL;DR: 本研究评估了AI工具MMM在音乐创作中的用户采用，结果显示积极反馈，但有控制性限制。


<details>
  <summary>Details</summary>
Motivation: 随着AI兴起，人类-AI合作在艺术领域兴趣增加，特别是音乐创作中AI系统的应用。

Method: 将MMM集成到Cubase中，开发MMM-C插件，进行三部分混合方法研究，测量专业和业余作曲家的可用性、用户体验和技术接受度。

Result: 可用性和接受度得分较高，用户报告新颖性、惊喜和易用性，但控制性和可预测性有限；两组用户间无显著差异。

Conclusion: 研究发现，专业和业余作曲家在使用MMM-C时的采用情况无显著差异。

Abstract: With the rise of artificial intelligence (AI), there has been increasing
interest in human-AI co-creation in a variety of artistic domains including
music as AI-driven systems are frequently able to generate human-competitive
artifacts. Now, the implications of such systems for musical practice are being
investigated. We report on a thorough evaluation of the user adoption of the
Multi-Track Music Machine (MMM) as a co-creative AI tool for music composers.
To do this, we integrate MMM into Cubase, a popular Digital Audio Workstation
(DAW) by Steinberg, by producing a "1-parameter" plugin interface named
MMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a
methodological assemblage as a 3-part mixed method study measuring usability,
user experience and technology acceptance of the system across two groups of
expert-level composers: hobbyists and professionals. Results show positive
usability and acceptance scores. Users report experiences of novelty, surprise
and ease of use from using the system, and limitations on controllability and
predictability of the interface when generating music. Findings indicate no
significant difference between the two user groups.

</details>


### [351] [Translating Multimodal AI into Real-World Inspection: TEMAI Evaluation Framework and Pathways for Implementation](https://arxiv.org/abs/2504.13873)
*Zehan Li,Jinzhi Deng,Haibing Ma,Chi Zhang,Dan Xiao*

Main category: cs.HC

TL;DR: TEMAI框架将多模态AI能力与工业检查应用相结合，强调技术能力、采纳和效用三方面的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在桥接多模态AI与工业检查的实际实施，借鉴医疗翻译研究原则，解决技术能力与实际价值脱节的问题。

Method: 通过建立能力、采纳和效用三个核心维度，引入价值密度系数和结构化实施路径，并通过零售和光伏检查领域的实证验证。

Result: 实证结果显示，尽管能力减少率相似，但价值实现模式存在显著差异，证实框架在不同行业中的有效性。

Conclusion: 技术能力单独不足以产生价值，需要结合采纳机制和行业特定适应策略来实现最大效用。

Abstract: This paper introduces the Translational Evaluation of Multimodal AI for
Inspection (TEMAI) framework, bridging multimodal AI capabilities with
industrial inspection implementation. Adapting translational research
principles from healthcare to industrial contexts, TEMAI establishes three core
dimensions: Capability (technical feasibility), Adoption (organizational
readiness), and Utility (value realization). The framework demonstrates that
technical capability alone yields limited value without corresponding adoption
mechanisms. TEMAI incorporates specialized metrics including the Value Density
Coefficient and structured implementation pathways. Empirical validation
through retail and photovoltaic inspection implementations revealed significant
differences in value realization patterns despite similar capability reduction
rates, confirming the framework's effectiveness across diverse industrial
sectors while highlighting the importance of industry-specific adaptation
strategies.

</details>


### [352] [Amplify Initiative: Building A Localized Data Platform for Globalized AI](https://arxiv.org/abs/2504.14105)
*Qazi Mamunur Rashid,Erin van Liemt,Tiffany Shih,Amber Ebinama,Karla Barrios Ramos,Madhurima Maji,Aishwarya Verma,Charu Kalia,Jamila Smith-Loud,Joyce Nakatumba-Nabende,Rehema Baguma,Andrew Katumba,Chodrine Mutebi,Jagen Marvin,Eric Peter Wairagala,Mugizi Bruce,Peter Oketta,Lawrence Nderu,Obichi Obiajunwa,Abigail Oppong,Michael Zimba,Data Authors*

Main category: cs.HC

TL;DR: 这篇论文介绍了Amplify Initiative，通过与专家合作创建多语言数据集来改善AI模型的全球安全性和相关性。


<details>
  <summary>Details</summary>
Motivation: AI模型训练数据以英语和西方内容为主，忽略本地上下文和语言，限制了其全球适用性和安全性。

Method: 利用数据平台和方法论，通过在撒哈拉以南非洲的试点研究，与领域专家合作使用Android应用共同创建数据集。

Result: 试点研究生成了8,091个对抗查询的注释数据集，涉及七种语言，用于评估AI模型的安全性和文化相关性。

Conclusion: 这种合作方法可提升AI模型在多样文化背景下的安全性和相关性。

Abstract: Current AI models often fail to account for local context and language, given
the predominance of English and Western internet content in their training
data. This hinders the global relevance, usefulness, and safety of these models
as they gain more users around the globe. Amplify Initiative, a data platform
and methodology, leverages expert communities to collect diverse, high-quality
data to address the limitations of these models. The platform is designed to
enable co-creation of datasets, provide access to high-quality multilingual
datasets, and offer recognition to data authors. This paper presents the
approach to co-creating datasets with domain experts (e.g., health workers,
teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,
Malawi, Nigeria, and Uganda). In partnership with local researchers situated in
these countries, the pilot demonstrated an end-to-end approach to co-creating
data with 155 experts in sensitive domains (e.g., physicians, bankers,
anthropologists, human and civil rights advocates). This approach, implemented
with an Android app, resulted in an annotated dataset of 8,091 adversarial
queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing
nuanced and contextual information related to key themes such as misinformation
and public interest topics. This dataset in turn can be used to evaluate models
for their safety and cultural relevance within the context of these languages.

</details>


### [353] [Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing](https://arxiv.org/abs/2504.13883)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TL;DR: 本研究使用混合深度学习模型结合fNIRS数据估计认知努力，帮助教育者提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 估计认知努力可让教育者修改材料，提高学习效果和参与度，使用RNE和RNI指标。

Method: 收集fNIRS数据（ΔHbO），参与者通过教育游戏回答问题，采用CNN、LSTM、BiLSTM、混合CNN-GRU等模型预测性能。

Result: 混合CNN-GRU模型测试准确率73.08%，XGBoost达69.23%；预测RNE和RNI趋势一致，休息可降低高认知努力。

Conclusion: 结果可用于优化学习环境，提供学习材料见解。

Abstract: This study estimates cognitive effort (CE) based on functional near-infrared
spectroscopy (fNIRS) data and performance scores using a hybrid deep learning
model. The estimation of CE enables educators to modify material to enhance
learning effectiveness and student engagement. Relative neural efficiency (RNE)
and relative neural involvement (RNI) are two metrics that have been used to
represent CE. To estimate RNE and RNI we need hemodynamic response in the brain
and the performance score of a task.We collected oxygenated hemoglobin ($\Delta
\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based
educational game, each with a 30-second response time. We used deep learning
models to predict the performance score and estimate RNE and RNI to understand
CE. The study compares traditional machine learning techniques with deep
learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine
which approach provides better accuracy in predicting performance scores. The
result shows that the hybrid CNN-GRU gives better performance with 78.36\%
training accuracy and 73.08\% test accuracy than other models. We performed
XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%).
This suggests that the features learned from this hybrid model generalize
better even in traditional machine learning algorithms. We used the $\Delta
\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive
effort in our four test cases. Our result shows that even with moderate
accuracy, the predicted RNE and RNI closely follows the actual trends. we also
observed that when participants were in a state of high CE, introducing rest
led decrease of CE. These findings can be helpful to design and improve
learning environments and provide valuable insights in learning materials.

</details>


### [354] [Longitudinal Study on Social and Emotional Use of AI Conversational Agent](https://arxiv.org/abs/2504.14112)
*Mohit Chandra,Javier Hernandez,Gonzalo Ramos,Mahsa Ershadi,Ananya Bhattacharjee,Judith Amores,Ebele Okoli,Ann Paradiso,Shahed Warreth,Jina Suh*

Main category: cs.HC

TL;DR: 这项研究通过实验探讨了使用AI进行社交和情感支持的影响，发现主动使用AI组的依恋和舒适度显著增加，但需注意风险。


<details>
  <summary>Details</summary>
Motivation: 数字技术尤其是对话式AI的发展改变了支持寻求方式，现有研究显示了益处（如更广泛的资源）和风险（如过度依赖），因此需要探索。

Method: 为期五周的探索性研究，招募149名参与者，分成基线使用组（n=60）和主动使用组（n=89），后者鼓励使用特定AI工具进行互动。

Result: 主动使用组显示AI依恋增加32.99%，移情感知增加25.8%，娱乐动机增加22.90%；个体差异（如性别、以往使用）影响感知；参与者更舒适地寻求帮助和管理压力。

Conclusion: 强调开发负责任的AI工具以支持情感福祉，并让用户了解其限制，以平衡益处和风险。

Abstract: Development in digital technologies has continuously reshaped how individuals
seek and receive social and emotional support. While online platforms and
communities have long served this need, the increased integration of
general-purpose conversational AI into daily lives has introduced new dynamics
in how support is provided and experienced. Existing research has highlighted
both benefits (e.g., wider access to well-being resources) and potential risks
(e.g., over-reliance) of using AI for support seeking. In this five-week,
exploratory study, we recruited 149 participants divided into two usage groups:
a baseline usage group (BU, n=60) that used the internet and AI as usual, and
an active usage group (AU, n=89) encouraged to use one of four commercially
available AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for
social and emotional interactions. Our analysis revealed significant increases
in perceived attachment towards AI (32.99 percentage points), perceived AI
empathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)
among the AU group. We also observed that individual differences (e.g., gender
identity, prior AI usage) influenced perceptions of AI empathy and attachment.
Lastly, the AU group expressed higher comfort in seeking personal help,
managing stress, obtaining social support, and talking about health with AI,
indicating potential for broader emotional support while highlighting the need
for safeguards against problematic usage. Overall, our exploratory findings
underscore the importance of developing consumer-facing AI tools that support
emotional well-being responsibly, while empowering users to understand the
limitations of these tools.

</details>


### [355] [The Balancing Act of Policies in Developing Machine Learning Explanations](https://arxiv.org/abs/2504.13946)
*Jacob Tjaden*

Main category: cs.HC

TL;DR: 本研究通过实验考察政策设计对ML模型解释质量的影响，发现政策长度影响遵守度，但解释质量差。


<details>
  <summary>Details</summary>
Motivation: ML模型决策不透明常受批评，因此探讨政策设计如何提升解释透明度。

Method: 进行课堂实验，涉及124名参与者，分析政策长度和目的对开发者遵守政策的影响。

Result: 政策长度影响某些要求的参与度，政策目的无效果，解释质量总体较差。

Conclusion: 突出有效政策开发的挑战，并强调在解释中处理不同利益相关者观点的重要性。

Abstract: Machine learning models are often criticized as opaque from a lack of
transparency in their decision-making process. This study examines how policy
design impacts the quality of explanations in ML models. We conducted a
classroom experiment with 124 participants and analyzed the effects of policy
length and purpose on developer compliance with policy requirements. Our
results indicate that while policy length affects engagement with some
requirements, policy purpose has no effect, and explanation quality is
generally poor. These findings highlight the challenge of effective policy
development and the importance of addressing diverse stakeholder perspectives
within explanations.

</details>


### [356] [Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity](https://arxiv.org/abs/2504.14125)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.HC

TL;DR: 本研究分析了文本到图像模型的用户提示，发现重复语言导致生成图像多样性降低，并呼吁鼓励更多实验。


<details>
  <summary>Details</summary>
Motivation: 探讨用户互动中语言选择如何影响AI生成图像多样性，这一领域虽被忽略，但数据集偏差已受关注。

Method: 分析CivitAI平台六百万余提示，将用户分为一致重复者、偶尔重复者和非重复者三类，使用Vendi分数量化视觉多样性。

Result: 用户参与增加时提示语言趋同，重复提示占40-50%；词汇相似度与视觉相似度相关，导致表示多样性减少。

Conclusion: 用户因素显著影响AI输出；需开发工具促进语言和主题实验，以提升AI生成内容的包容性和多样性。

Abstract: Following the initial excitement, Text-to-Image (TTI) models are now being
examined more critically. While much of the discourse has focused on biases and
stereotypes embedded in large-scale training datasets, the sociotechnical
dynamics of user interactions with these models remain underexplored. This
study examines the linguistic and semantic choices users make when crafting
prompts and how these choices influence the diversity of generated outputs.
Analyzing over six million prompts from the Civiverse dataset on the CivitAI
platform across seven months, we categorize users into three groups based on
their levels of linguistic experimentation: consistent repeaters, occasional
repeaters, and non-repeaters. Our findings reveal that as user participation
grows over time, prompt language becomes increasingly homogenized through the
adoption of popular community tags and descriptors, with repeated prompts
comprising 40-50% of submissions. At the same time, semantic similarity and
topic preferences remain relatively stable, emphasizing common subjects and
surface aesthetics. Using Vendi scores to quantify visual diversity, we
demonstrate a clear correlation between lexical similarity in prompts and the
visual similarity of generated images, showing that linguistic repetition
reinforces less diverse representations. These findings highlight the
significant role of user-driven factors in shaping AI-generated imagery, beyond
inherent model biases, and underscore the need for tools and practices that
encourage greater linguistic and thematic experimentation within TTI systems to
foster more inclusive and diverse AI-generated content.

</details>


### [357] [Apollo: An Interactive Environment for Generating Symbolic Musical Phrases using Corpus-based Style Imitation](https://arxiv.org/abs/2504.14055)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: 本文引入Apollo系统，使用基于语料库的风格模仿技术生成西方音乐符号短语的交互式桌面应用。


<details>
  <summary>Details</summary>
Motivation: 受机器智能和网络技术发展的驱动，探索新的生成音乐系统以辅助作曲任务，如旋律生成和风格模仿。

Method: 采用语料库-based风格模仿技术，构建和管理音乐语料库，并实现为桌面应用，支持MIDI格式导出。

Result: 系统可生成与语料库风格一致的新音乐短语，并可导出或流式传输用于各种音乐项目。

Conclusion: 讨论系统设计、实现细节，并提出未来工作方向。

Abstract: With the recent developments in machine intelligence and web technologies,
new generative music systems are being explored for assisted composition using
machine learning techniques on the web. Such systems are built for various
tasks such as melodic, harmonic or rhythm generation, music interpolation,
continuation and style imitation. In this paper, we introduce Apollo, an
interactive music application for generating symbolic phrases of conventional
western music using corpus-based style imitation techniques. In addition to
enabling the construction and management of symbolic musical corpora, the
system makes it possible for music artists and researchers to generate new
musical phrases in the style of the proposed corpus. The system is available as
a desktop application. The generated symbolic music materials, encoded in the
MIDI format, can be exported or streamed for various purposes including using
them as seed material for musical projects. We present the system design,
implementation details, discuss and conclude with future work for the system.

</details>


### [358] [Calliope: An Online Generative Music System for Symbolic Multi-Track Composition](https://arxiv.org/abs/2504.14058)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: Calliope 是一个使用 AI 辅助多轨音乐创作的网络应用，基于 MIDI 文件。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的兴起，其在创意领域如音乐中的应用迅速增加，促使开发计算机辅助作曲系统。

Method: Calliope 允许用户上传、可视化和编辑 MIDI 文件，使用 Multi-Track Music Machine (MMM) 生成部分或完整多轨内容，支持批量处理、播放和导出。

Result: 系统提升了辅助作曲工作流程，提供协同创作工具。

Conclusion: 论文展示了系统、其功能、生成参数以及支持的工作流程。

Abstract: With the rise of artificial intelligence in recent years, there has been a
rapid increase in its application towards creative domains, including music.
There exist many systems built that apply machine learning approaches to the
problem of computer-assisted music composition (CAC). Calliope is a web
application that assists users in performing a variety of multi-track
composition tasks in the symbolic domain. The user can upload (Musical
Instrument Digital Interface) MIDI files, visualize and edit MIDI tracks, and
generate partial (via bar in-filling) or complete multi-track content using the
Multi-Track Music Machine (MMM). Generation of new MIDI excerpts can be done in
batch and can be combined with active playback listening for an enhanced
assisted-composition workflow. The user can export generated MIDI materials or
directly stream MIDI playback from the system to their favorite Digital Audio
Workstation (DAW). We present a demonstration of the system, its features,
generative parameters and describe the co-creative workflows that it affords.

</details>


### [359] [Visualization Tasks for Unlabelled Graphs](https://arxiv.org/abs/2504.14115)
*Matt I. B. Oddo,Ryan Smith,Stephen Kobourov,Tamara Munzner*

Main category: cs.HC

TL;DR: 本论文提出一个无标签图任务的分类法，并评估六种可视化技术。


<details>
  <summary>Details</summary>
Motivation: 由于新可视化技术已被提出，但需要更多理解无标签图任务，因为许多任务不能直接从有标签图转换。

Method: 提出基于Scope、Action和Target的分类法，并对每个任务和六种可视化进行初步评估，考虑观看者的努力、任务成功可能性，以及在小规模和大规模图中的差异。

Result: 展示了分类法的描述能力，通过连接到之前的框架和真实世界问题；评估了不同任务和可视化组合的性能。

Conclusion: 该分类法有助于更好地评估无标签图的可视化技术。

Abstract: We investigate tasks that can be accomplished with unlabelled graphs, where
nodes do not have persistent or semantically meaningful labels. New techniques
to visualize these graphs have been proposed, but more understanding of
unlabelled graph tasks is required before they can be adequately evaluated.
Some tasks apply to both labelled and unlabelled graphs, but many do not
translate between these contexts. We propose a taxonomy of unlabelled graph
abstract tasks, organized according to the Scope of the data at play, the
Action intended by the user, and the Target data under consideration. We show
the descriptive power of this task abstraction by connecting to concrete
examples from previous frameworks, and connect these abstractions to real-world
problems. To showcase the evaluative power of the taxonomy, we perform a
preliminary assessment of 6 visualizations for each task. For each combination
of task and visual encoding, we consider the effort required from viewers, the
likelihood of task success, and how both factors vary between small-scale and
large-scale graphs.

</details>


### [360] [Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces](https://arxiv.org/abs/2504.14320)
*Nimisha Karnatak,Adrien Baranes,Rob Marchant,Huinan Zeng,Tríona Butler,Kristen Olson*

Main category: cs.HC

TL;DR: 本论文研究生成式AI在广告中的提示挑战，并提出ACAI工具来帮助新手用户。


<details>
  <summary>Details</summary>
Motivation: 解决新手用户在文本提示中的认知负担和输出与品牌愿景不符的问题。

Method: 通过对六名英国小企业主的调查，并开发多模态ACAI工具及其面板界面。

Result: ACAI减少了提示模糊性，提高了输出与用户创意愿景的契合度。

Conclusion: 结构化界面可提升新手工作流中的对齐性和提示性，为HCI研究贡献。

Abstract: Text-based prompting remains the dominant interaction paradigm in generative
AI, yet it often results in a high-friction experience for novice users, such
as small business owners (SBOs), attempting to articulate creative or
domain-specific goals for advertising. To investigate this challenge, we
conducted a study with six SBOs in the United Kingdom, focusing on their
advertising practices and perceptions and usage of AI tools in this context.
Our findings surfaced two persistent breakdowns in current generative AI
systems: first, the cognitive burden of prompt engineering, as users struggled
to translate abstract creative goals into effective textual inputs; and second,
the frequent generation of generic outputs that failed to align with users'
articulated brand vision. To address these issues, we developed ACAI (AI
Co-Creation for Advertising and Inspiration), a multimodal, GenAI-powered
advertisement creation tool designed to support novice designers by reimagining
the prompt interface. ACAI features a structured, panel-based interface
composed of three modules: the Branding Panel, the Audience & Goals Panel, and
the Inspiration Board Panel to provide SBOs with outputs that align with their
creative vision by reducing prompt ambiguity. This work contributes to HCI
research on generative systems by showing how structured interfaces can
foreground user-defined context to improve both alignment and promptability in
novice workflows.

</details>


### [361] [ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking](https://arxiv.org/abs/2504.14406)
*Runlong Ye,Patrick Yung Kang Lee,Matthew Varona,Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: ScholarMate 是一个整合 AI 和人类监督的交互系统，提升定性分析效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 从大型文档集合合成知识挑战性增加，AI 自动化潜力大，但整合到以人为中心的意义构建工作流程中困难。

Method: 开发 ScholarMate 系统，用户在非线性画布上交互文本，利用 AI 建议主题、总结和命名，确保源文档可追溯。

Result: 试点研究显示用户认可混合方法，案例分析 24 篇论文证明提升效率。

Conclusion: 平衡自动化与人类控制，ScholarMate 为知识工作中的人类-AI 协作提供有效方法。

Abstract: Synthesizing knowledge from large document collections is a critical yet
increasingly complex aspect of qualitative research and knowledge work. While
AI offers automation potential, effectively integrating it into human-centric
sensemaking workflows remains challenging. We present ScholarMate, an
interactive system designed to augment qualitative analysis by unifying AI
assistance with human oversight. ScholarMate enables researchers to dynamically
arrange and interact with text snippets on a non-linear canvas, leveraging AI
for theme suggestions, multi-level summarization, and contextual naming, while
ensuring transparency through traceability to source documents. Initial pilot
studies indicated that users value this mixed-initiative approach, finding the
balance between AI suggestions and direct manipulation crucial for maintaining
interpretability and trust. We further demonstrate the system's capability
through a case study analyzing 24 papers. By balancing automation with human
control, ScholarMate enhances efficiency and supports interpretability,
offering a valuable approach for productive human-AI collaboration in demanding
sensemaking tasks common in knowledge work.

</details>


### [362] [Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework](https://arxiv.org/abs/2504.14427)
*Spencer Lin,Miru Jun,Basem Rizk,Karen Shieh,Scott Fisher,Sharon Mozgai*

Main category: cs.HC

TL;DR: 本案例研究介绍了基于Estuary项目的社会智能代理(SIA)开发框架的用户中心设计模型，通过快速评估过程(RAP)和访谈收集专家见解，以填补研究空白。


<details>
  <summary>Details</summary>
Motivation: 收集领域领先研究人员对当前SIA发展状态的意见，以及Estuary如何潜在地解决研究空白，以辅助Estuary的持续发展和指导未来框架。

Method: 利用快速评估过程(RAP)并通过社区研究人员进行一系列最终用户访谈。

Result: 通过访谈获得的研究发现，可帮助改进Estuary并指导其他SIA框架的发展。

Conclusion: 希望研究发现不仅能促进Estuary的发展，还能为未来的SIA技术和框架提供指导。

Abstract: This case study presents our user-centered design model for Socially
Intelligent Agent (SIA) development frameworks through our experience
developing Estuary, an open source multimodal framework for building
low-latency real-time socially interactive agents. We leverage the Rapid
Assessment Process (RAP) to collect the thoughts of leading researchers in the
field of SIAs regarding the current state of the art for SIA development as
well as their evaluation of how well Estuary may potentially address current
research gaps. We achieve this through a series of end-user interviews
conducted by a fellow researcher in the community. We hope that the findings of
our work will not only assist the continued development of Estuary but also
guide the development of other future frameworks and technologies for SIAs.

</details>


### [363] [Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)
*Liudmila Zavolokina,Kilian Sprenkamp,Zoya Katashinskaya,Daniel Gordon Jones*

Main category: cs.HC

TL;DR: 这篇论文探讨使用大型语言模型设计宣传检测工具，通过利用AI偏差提升批判性思维。


<details>
  <summary>Details</summary>
Motivation: 论文承认AI模型在政治背景中的偏差，并提出将其转化为优势，应用心理学概念促进批判性新闻消费。

Method: 通过调查用户选择和个性化策略，结合确认偏差和认知失调概念，并进行定性用户研究。

Result: 用户研究发现包括偏差意识、个性和选择，以及逐步引入多样视角的设计推荐。

Conclusion: 为宣传检测AI工具提供洞见和设计建议，以增强批判性思维。

Abstract: This paper explores the design of a propaganda detection tool using Large
Language Models (LLMs). Acknowledging the inherent biases in AI models,
especially in political contexts, we investigate how these biases might be
leveraged to enhance critical thinking in news consumption. Countering the
typical view of AI biases as detrimental, our research proposes strategies of
user choice and personalization in response to a user's political stance,
applying psychological concepts of confirmation bias and cognitive dissonance.
We present findings from a qualitative user study, offering insights and design
recommendations (bias awareness, personalization and choice, and gradual
introduction of diverse perspectives) for AI tools in propaganda detection.

</details>


### [364] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TL;DR: 本论文介绍了HealthGenie系统，该系统整合LLM和KG，提供个性化的饮食推荐和分层信息可视化，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 寻求饮食指导需要处理复杂的专业知识和个人健康状况，KG提供结构化的信息，LLM便于对话式推荐，因此需要整合二者。

Method: 开发HealthGenie系统，包括查询优化、从预建KG中检索信息、分层可视化和交互式调整，并通过N=12的内部受试者比较实验进行评估。

Result: 系统有效支持个性化饮食指导，减少交互努力和认知负荷，突出LLM-KG整合在决策支持中的潜力。

Conclusion: LLM和KG的整合有助于通过可解释和可视化的信息支持决策，并为未来系统提供设计考虑。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>


### [365] [Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work](https://arxiv.org/abs/2504.14779)
*Janet G. Johnson,Macarena Peralta,Mansanjam Kaur,Ruijie Sophia Huang,Sheng Zhao,Ruijia Guan,Shwetha Rajaram,Michael Nebeling*

Main category: cs.HC

TL;DR: 本研究探讨协作式生成式AI代理在团队合作中的潜力，通过探索性研究发现其能提升问题解决，但需考虑设计张力和整合因素。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI工具主要针对个人使用，缺乏对团队动态的适应性，需要研究如何在群体环境中部署AI。

Method: 通过与25名专业人士的6个团队进行推测性设计工作坊和个体后续访谈，使用混合现实原型模拟AI代理。

Result: 发现AI代理能挑战群体思维、桥接沟通差距、减少社会摩擦，但整合取决于个体、团队和组织因素。

Conclusion: 概述了代理表示、社会显著性和参与的设计张力，并强调空间和沉浸式技术在平衡AI增强与代理之间的机会。

Abstract: While generative artificial intelligence (GenAI) is finding increased
adoption in workplaces, current tools are primarily designed for individual
use. Prior work established the potential for these tools to enhance personal
creativity and productivity towards shared goals; however, we don't know yet
how to best take into account the nuances of group work and team dynamics when
deploying GenAI in work settings. In this paper, we investigate the potential
of collaborative GenAI agents to augment teamwork in synchronous group settings
through an exploratory study that engaged 25 professionals across 6 teams in
speculative design workshops and individual follow-up interviews. Our workshops
included a mixed reality provotype to simulate embodied collaborative GenAI
agents capable of actively participating in group discussions. Our findings
suggest that, if designed well, collaborative GenAI agents offer valuable
opportunities to enhance team problem-solving by challenging groupthink,
bridging communication gaps, and reducing social friction. However, teams'
willingness to integrate GenAI agents depended on its perceived fit across a
number of individual, team, and organizational factors. We outline the key
design tensions around agent representation, social prominence, and engagement
and highlight the opportunities spatial and immersive technologies could offer
to modulate GenAI influence on team outcomes and strike a balance between
augmentation and agency.

</details>


### [366] [NeuGaze: Reshaping the future BCI](https://arxiv.org/abs/2504.15101)
*Yiqian Yang*

Main category: cs.HC

TL;DR: NeuGaze 是一个基于网络摄像头的低成本系统，利用眼动、头部运动和面部表情进行人机交互，为传统脑机接口提供可访问的替代方案。


<details>
  <summary>Details</summary>
Motivation: 克服传统脑机接口的高成本、复杂设置和有限精度问题，特别是为运动障碍者提供直观且无须专业硬件的交互方式。

Method: 使用标准 30 Hz 网络摄像头跟踪眼动、头部运动和面部表情，仅需最少校准，实现实时控制。

Result: 性能与传统输入相当，支持精确光标导航、技能轮触发键入以及动态游戏交互，如在第一人称游戏中击败对手。

Conclusion: 提供低成本、可访问的脑机接口替代方案，增强辅助技术和娱乐应用，重定义运动障碍用户的交互体验。

Abstract: Traditional brain-computer interfaces (BCIs), reliant on costly
electroencephalography or invasive implants, struggle with complex
human-computer interactions due to setup complexity and limited precision. We
present NeuGaze, a novel webcam-based system that leverages eye gaze, head
movements, and facial expressions to enable intuitive, real-time control using
only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal
calibration, NeuGaze achieves performance comparable to conventional inputs,
supporting precise cursor navigation, key triggering via an efficient skill
wheel, and dynamic gaming interactions, such as defeating formidable opponents
in first-person games. By harnessing preserved neck-up functionalities in
motor-impaired individuals, NeuGaze eliminates the need for specialized
hardware, offering a low-cost, accessible alternative to BCIs. This paradigm
empowers diverse applications, from assistive technology to entertainment,
redefining human-computer interaction for motor-impaired users. Project is at
\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [367] [Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding](https://arxiv.org/abs/2504.14938)
*Jiaxuan Jiang,Jiapeng Liu,Miłosz Kadziński,Xiuwu Liao,Jingyu Dong*

Main category: stat.AP

TL;DR: 这篇论文引入了一个整合行为线索的多标准贝叶斯偏好学习框架，用于决策辅助，通过结合配对比较、响应时间和注意力持续时间来深入理解决策过程。


<details>
  <summary>Details</summary>
Motivation: 为了超越传统方法，通过整合行为线索揭示更丰富的决策行为模式，并更好地与决策者的实际偏好对齐。

Method: 采用加法价值函数模型和贝叶斯框架，推导排名模型的后验分布；定义偏好数据的似然性和先验；进行实验室实验，使用时间、眼睛和鼠标跟踪。

Result: 验证新方法能重建完整偏好；消融研究显示时间和注意力相关行为模式，确认整合全面数据提升模型与决策者偏好一致性。

Conclusion: 整合行为线索可开发出更准确的模型，更好地符合决策者的实际偏好。

Abstract: We introduce a multiple criteria Bayesian preference learning framework
incorporating behavioral cues for decision aiding. The framework integrates
pairwise comparisons, response time, and attention duration to deepen insights
into decision-making processes. The approach employs an additive value function
model and utilizes a Bayesian framework to derive the posterior distribution of
potential ranking models by defining the likelihood of observed preference data
and specifying a prior on the preference structure. This distribution
highlights each model's ability to reconstruct Decision-Makers' holistic
pairwise comparisons. By leveraging both response time as a proxy for cognitive
effort and alternative discriminability as well as attention duration as an
indicator of criterion importance, the proposed model surpasses traditional
methods by uncovering richer behavioral patterns. We report the results of a
laboratory experiment on mobile phone contract selection involving 30 real
subjects using a dedicated application with time-, eye-, and mouse-tracking
components. We validate the novel method's ability to reconstruct complete
preferences. The detailed ablation studies reveal time- and attention-related
behavioral patterns, confirming that integrating comprehensive data leads to
developing models that better align with the DM's actual preferences.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [368] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TL;DR: 提出MEQA框架用于QA基准的元评估，并在网络安全领域演示。


<details>
  <summary>Details</summary>
Motivation: LLM快速发展，其社会影响增大，需要严格评估，但基准质量评估存在缺口。

Method: 开发MEQA框架，提供标准化评估、可量化分数和基准比较，使用人类和LLM评估器在网络安全基准上演示。

Result: 在网络安全基准上应用MEQA，突显其优势和不足。

Conclusion: 强调AI在网络安全中的双重角色，突显元评估的重要性。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>


### [369] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TL;DR: 本文提出LogicTree框架，通过算法引导搜索、缓存机制和启发式方法提升LLM的逻辑推理能力，在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂逻辑推理中面临挑战，包括证明发现需要系统探索和维护逻辑一致性，以及在大型前提空间中搜索正确组合的困难。

Method: 提出LogicTree框架，使用算法引导搜索自动化证明探索，引入缓存机制利用历史知识，将前提搜索分解为线性过程，并使用LLM-free启发式方法优先化搜索。

Result: 在五个数据集上的实验显示，LogicTree在GPT-4o上比CoT提高23.6%，比ToT提高12.5%；GPT-4o比o3-mini高7.6%。

Conclusion: LogicTree通过优化推理时间计算实现了更高的证明准确性，证明了其有效性。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>


### [370] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TL;DR: 本论文提出了一种测量LLM解释忠诚度的新方法，通过概念因果效应评估不忠实问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM解释可能不忠实的问题，以防止过度信任和误用。

Method: 定义忠诚度为影响概念的差异，使用辅助LLM创建反事实输入，并采用Bayesian分层模型量化因果效应。

Result: 实验揭示了在社会偏见和医疗问答任务中LLM解释隐藏影响的不忠诚模式。

Conclusion: 该方法有效，可帮助发现和量化LLM解释的不忠实，改进模型可解释性。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>


### [371] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 本论文提出Selective Conformal Uncertainty (SConU)方法，以解决大语言模型不确定性问题，提高预测可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法识别不确定性数据异常点，导致错误覆盖率问题，因此需要一种新方法来保证任务特定指标的可靠性。

Method: 提出SConU方法，通过开发两种保形p值进行显著性检验，判断样本是否偏离不确定性分布，从而管理错误覆盖率。

Result: 该方法在单领域和跨领域环境中严格管理错误覆盖率，提高预测效率，并在高风险问答任务中逼近条件覆盖。

Conclusion: SConU方法改进了不确定性处理，提供更好的覆盖管理和预测效率。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>


### [372] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TL;DR: 本文提出CLPsych 2025 A.1任务基线，使用少样本学习和量化模型对Reddit心理健康数据中的自我状态进行分类，排名第三，召回率0.579。


<details>
  <summary>Details</summary>
Motivation: 为了提高分类性能，通过句子分块匹配人工标注粒度和简化任务。

Method: 使用4位量化Gemma 2 9B模型的少样本学习，先识别相关句子，然后进行自适应或非自适应自我状态的二元分类。

Result: 系统在14个提交中排名第三，测试召回率0.579，优于依赖LLM突出跨度的方法。

Conclusion: 性能归功于句子分块步骤，因为它匹配了标注粒度和简化了分类任务。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>


### [373] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TL;DR: 本论文介绍了首个利用GPT-4和Llama-3的文本简化系统Simplifymytext.org，支持多种输入格式和自定义选项，提升文本可访问性。


<details>
  <summary>Details</summary>
Motivation: 文本简化有助于使复杂内容对有理解困难的群体可访问，但简化材料稀缺阻碍了发展和包容，现有方法未充分利用大语言模型进行定制。

Method: 使用GPT-4和Llama-3开发系统，从多种输入格式生成简明语言内容，并提供灵活的自定义选项。

Result: 系统在多个指标上进行了评估，证明了其有效性。

Conclusion: 贡献于自动文本简化研究，并强调定制化通信促进包容性的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>


### [374] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本研究系统评估从Transformer教师模型到九种次二次学生架构的知识蒸馏，探讨效率与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 自我注意力的二次复杂度是大型语言模型推理的瓶颈，促使探索次二次替代方案。

Method: 通过系统评估知识转移到九种次二次模型，并调查矩阵混合和QKV复制等初始化策略。

Result: 经验结果揭示了效率与性能之间的权衡，以及影响知识转移的关键因素。

Conclusion: 突出了成功知识转移的重要因素，并为子二次架构的设计提供见解。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>


### [375] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TL;DR: 本研究使用进化方法结合上下文无关文法和MAP-Elites算法优化LLM提示，强调质量和多样性，提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化大语言模型至关重要，但提示结构与任务性能的关联尚未充分探索。

Method: 引入一种结合上下文无关文法和MAP-Elites算法的进化方法，系统探索提示空间，优先质量和多样性，通过改变示例数量和推理深度分析。

Result: 结果显示结构变化影响LLM性能，强调质量和多样性的关键作用，在七个BigBench Lite任务上评估，证明其有效性。

Conclusion: 为任务特定和可适应提示设计提供可操作洞见，推进LLM的有效性和多功能性。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>


### [376] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本论文引入ParaPO方法，通过微调语言模型减少无意复述，同时保留其效用。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能记住并复述预训练数据，引发版权、剽窃、隐私和创造力问题。

Method: ParaPO是一种后训练方法，训练模型偏好改写版本而非原样内容，并使用系统提示控制复述行为。

Result: 在Llama3.1-8B上，ParaPO将复述指标从17.3降至12.9；应用于Tulu3-8B时，通过提示将无意复述从8.7降至6.3，比先前方法更有效。

Conclusion: ParaPO能有效减少跨域复述，并通过系统提示保留适当回忆，同时优于现有无学习方法。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>


### [377] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TL;DR: 本文引入SSENSE框架，使用对比学习将脑信号映射到语言嵌入空间，实现从脑活动直接检索句子级表示。


<details>
  <summary>Details</summary>
Motivation: 解决神经活动解释的挑战，通过多模态基础模型将侵入性脑记录与自然语言对齐。

Method: SSENSE采用对比学习框架，使用InfoNCE损失将单主体sEEG信号投射到冻结CLIP模型的句子嵌入空间中，而不微调文本编码器。

Result: 在自然电影观看数据集上，SSENSE在有限数据下取得有前景的结果，证明通用语言表示可作为神经解码的有效先验。

Conclusion: 通用语言表示可以有效用作神经解码的先验，即使数据有限。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>


### [378] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型（LLM）在因果推理方面的能力、机制及其在NLP和计算社会科学中的应用，提供了一个全面的研究基础。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能和AI决策的核心，本文旨在通过研究LLM的因果能力来提升其在理解和决策方面的表现。

Method: 使用一系列研究、Novel数据集、基准任务和方法框架来评估和改进LLM的因果推理技能。

Result: 识别了LLM在因果推理中的关键挑战和机会，为未来研究提供了基础。

Conclusion: 这项工作为提升LLM的因果能力奠定了基础，并为NLP和计算社会科学领域的发展提供了新方向。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>


### [379] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TL;DR: LLM 可以可靠生成小特征集的合成健康记录，但在大维度数据上难以保持真实分布和相关性，从而限制了在不同医院环境的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 合成电子健康记录提供隐私保护、数据标准化和公平性，支持医疗应用；动机包括控制数据模式、提高患者代表性以及避免隐私风险；挑战是确保在不同医院间泛化。

Method: 评估商用LLM生成合成数据的性能，并调查生成过程的多个方面。

Result: LLM 在小特征子集上表现良好，但在大维度下难以保留真实分布和相关性，导致泛化能力受限。

Conclusion: LLM 在高维合成健康记录生成中存在局限性，需要进一步改进以实现更好的泛化。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>


### [380] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TL;DR: 这篇调查综述分析了知识蒸馏（KD）和数据集蒸馏（DD）两种范式，用于压缩大型语言模型（LLM），以保留其推理能力和语言多样性，同时探讨了它们的整合。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的指数级增长，需要高效策略来应对不断增加的计算和数据需求。

Method: 考察了KD的方法如任务特定对齐、基于理由的训练和多教师框架，以及DD的技术如基于优化的梯度匹配、潜空间正则化和生成合成，并探讨了二者的整合以实现更有效的压缩。

Result: 整合KD和DD可产生更可扩展的压缩策略，解决模型可伸缩性问题，并在医疗和教育等领域实现高效部署。

Conclusion: 尽管有进展，挑战仍存，如保留新兴能力、适应演化模型和建立评价协议；综述为通过紧密整合KD和DD实现可持续LLM提供路径。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>


### [381] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TL;DR: 本文提出一种快速改进令牌嵌入的方法，通过整合相邻令牌嵌入解决词汇外问题，并应用于风暴事件分析。


<details>
  <summary>Details</summary>
Motivation: 解决词汇外问题，并在特定领域中提升嵌入的意义，支持语料探索、概念搜索和词义消歧。

Method: 通过在文本语料中整合相邻令牌嵌入，持续更新每个令牌的表示，独立于大型语言模型。

Result: 改进了风暴相关术语的表示，并提供了灾害叙述演变的洞察。

Conclusion: 该方法有效地增强了领域特定嵌入的实用性。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>


### [382] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TL;DR: 这篇论文引入FarsEval-PKBETS基准，用于评估波斯语的大型语言模型（LLM），结果显示模型准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 由于英语等语言的LLM评估研究较多，而波斯语关注不足，因此开发针对波斯语的评估基准。

Method: 创建包含4000个问题和答案的基准，覆盖多个领域和任务格式，并使用Llama3-70B、PersianMind和Dorna模型进行评估。

Result: 三个模型的平均准确率低于50%，表明当前LLM无法正确回答一半的问题。

Conclusion: 结果显示现有LLM在波斯语任务上仍有很大差距，需要进一步改进。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>


### [383] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TL;DR: 本研究使用模糊指纹和上下文感知建模提升基于文本的说话者识别，在Friends和Big Bang Theory数据集上达到70.6%和67.7%的准确率，并提供更好可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统语音-based说话者识别在仅有文本数据时失效，现有的文本方法依赖传统技术，本文探索更先进的方法来解决这一问题。

Method: 使用大型预训练模型的模糊指纹，整合说话者特定标记和上下文感知建模，考虑对话上下文来提升准确率。

Result: 在Friends数据集上准确率达70.6%，在Big Bang Theory数据集上达67.7%；模糊指纹以更少隐藏单元逼近完全微调性能，并分析模糊话语。

Conclusion: 研究突出了关键挑战，并为未来文本-based说话者识别的改进提供了见解。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>


### [384] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TL;DR: 本研究探讨多模态数据在诊断学生协作问题解决（CPS）能力方面的潜力，使用Transformer模型比较单模态和多模态方法。


<details>
  <summary>Details</summary>
Motivation: AI教育领域长期目标是检测CPS行为，但实证证据有限，需要验证多模态数据和高级模型的价值。

Method: 使用文本嵌入和音频嵌入构建多模态Transformer模型，与单模态和传统模型比较，诊断78名学生的CPS子技能。

Result: Transformer模型优于传统模型；多模态集成改善Transformer模型在社会认知CPS类别的诊断性能，但对某些指标效果有限。

Conclusion: 多模态和模型选择的价值受CPS指标类型、标签复杂性和数据集影响；强调人类-AI互补性，并建议探索更合适的模型架构。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>


### [385] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: EasyEdit2 是一个易于使用的框架，用于无需修改参数即可控制大语言模型的行为，支持多种干预如安全性和情感调整。


<details>
  <summary>Details</summary>
Motivation: 为了让用户能够轻松地控制LLM行为，而不需要专业技术知识，从而实现可插拔的模型调整。

Method: 通过转向向量生成器和应用器自动生成并应用转向向量，来影响模型行为而不改变其参数。

Result: 实证结果显示，在不同LLM上，框架有效实现了模型转向，提高了控制的精确性和效率。

Conclusion: EasyEdit2 提供了用户友好的模型控制方法，并开源代码和演示资源，促进了AI应用的实际部署。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>


### [386] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TL;DR: 论文提出BToT和WBToT模型，改进主题随时间模型的稳定性和事件捕捉能力，通过全贝叶斯方法和加权机制。


<details>
  <summary>Details</summary>
Motivation: 原ToT模型未采用全贝叶斯方法，导致稳定性问题；时间和词语规模差异影响模型性能。

Method: 引入Beta分布的共轭先验，提出BToT模型；进一步开发WBToT，通过重复发布日期平衡时间和词语影响。

Result: 在SOTU和COVID-19 Twitter数据集上，WBToT比LDA和BERTopic减少主题偏差51%和34%，coherence更优，优化算法更稳定。

Conclusion: WBToT提升了主题模型的性能，适用于大规模数据分析，提高了事件捕捉和稳定性。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>


### [387] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TL;DR: 本文综述了文档级机器翻译的自动评估方法，分析了当前挑战，并展望未来趋势。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和大型语言模型的快速发展，文档级翻译质量评估成为紧迫问题，需要准确的自动评估指标。

Method: 介绍了文档级翻译的发展现状和评估重要性，详细分析了当前自动评估方案和指标，包括有/无参考文本的方法、传统指标、模型-based指标和LLM-based指标，并探讨了挑战和未来趋势。

Result: 分析了评价方法的现状和问题，如参考多样性不足、对句子级信息依赖和LLM-as-judge的偏差；提出了未来研究方向，如减少句子级依赖和引入多层次评估。

Conclusion: 旨在提供文档级翻译自动评估的全面分析，并对未来发展提出洞见。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>


### [388] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TL;DR: 这篇论文开发了一种方法，从Claude 3和3.5模型的数十万次真实交互中提取和分类AI值，发现了3307个值，并分析了它们在不同上下文中的变化。


<details>
  <summary>Details</summary>
Motivation: AI助手可能通过价值判断影响人们的决策和世界观，但缺乏实证研究。

Method: 开发了一种自下而上的、隐私保护的方法，从真实交互中提取模型响应中体现的值。

Result: 发现了3307个AI值，Claude表达了许多实用和认识论价值，支持亲社会价值，并在不同上下文中表现出如危害预防、历史准确性等特定值。

Conclusion: 这项工作提供了首个大规模实证映射AI值的基础，有助于更可靠地评估和设计AI系统的价值。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>


### [389] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本论文引入JETTS基准，用于评估LLM-judges在测试时扩展中的性能，发现其在某些任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: LLM-judges在自动评估中越来越受欢迎，但其在测试时扩展设置中的有效性未知，需要进行系统评估。

Method: 开发JETTS基准，在数学推理、代码生成和指令遵循领域下，评估10个judge模型和8个base generator模型在响应重排、步级beam search和基于批评的响应细化任务中的表现。

Result: judges在重排任务中与outcome reward models竞争，但不如process reward models在beam search中有效；自然语言批评无法有效指导生成器改进响应。

Conclusion: LLM-judges在某些方面有潜力，但需改进beam search和批评指导功能。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>


### [390] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本论文提出一种新方法，将阿拉伯语整合到大型语言模型中，而不损失原有知识，提供高效的语言扩展。


<details>
  <summary>Details</summary>
Motivation: 增强现有AI模型以融入新知识至关重要，特别是添加新语言以扩展模型能力。

Method: 通过在小型开源英语模型中注入阿拉伯语，训练了一个1.5亿参数的模型名为Kuwain。

Result: 阿拉伯语性能平均提高了8%，同时保留了原有知识，并提供了一种成本更低的替代方案。

Conclusion: 结果突显了高效、针对性语言模型扩展的潜力，无需广泛重新训练或资源密集型过程。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [391] [Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides](https://arxiv.org/abs/2504.15066)
*Jinghua Zhao,Yuhang Jia,Shiyao Wang,Jiaming Zhou,Hui Wang,Yong Qin*

Main category: cs.MM

TL;DR: 本论文发布了一个新的中文音频-视觉语音识别数据集Chinese-LiPS，并提出LiPS-AVSR方法，结合唇读和演示幻灯片信息，实验显示ASR性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR数据集和方法忽略了结合唇读和其他视觉线索如演示幻灯片的潜力，因此本研究旨在通过整合多种视觉模态来改善ASR性能。

Method: 开发Chinese-LiPS数据集，包含100小时语音、视频和转录，并提出LiPS-AVSR管道，利用唇读和演示幻灯片作为视觉模态。

Result: 实验结果显示，唇读信息改善ASR性能约8%，演示幻灯片信息改善约25%，二者结合改善约35%。

Conclusion: 这项工作证明结合多种视觉线索可显著提升AVSR性能，并提供数据集以支持进一步研究。

Abstract: Incorporating visual modalities to assist Automatic Speech Recognition (ASR)
tasks has led to significant improvements. However, existing Audio-Visual
Speech Recognition (AVSR) datasets and methods typically rely solely on
lip-reading information or speaking contextual video, neglecting the potential
of combining these different valuable visual cues within the speaking context.
In this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,
comprising 100 hours of speech, video, and corresponding manual transcription,
with the visual modality encompassing both lip-reading information and the
presentation slides used by the speaker. Based on Chinese-LiPS, we develop a
simple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and
presentation slide information as visual modalities for AVSR tasks. Experiments
show that lip-reading and presentation slide information improve ASR
performance by approximately 8\% and 25\%, respectively, with a combined
performance improvement of about 35\%. The dataset is available at
https://kiri0824.github.io/Chinese-LiPS/

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [392] [A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning](https://arxiv.org/abs/2504.14070)
*Jinesh Jhonsa,William Whitehead,David McCarthy,Shuvro Chowdhury,Kerem Camsari,Luke Theogarajan*

Main category: cs.AR

TL;DR: 本文展示了一个基于概率位物理的求解器芯片，面积高效，用于AI和机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 为了最大化面积效率并缓解过程变异问题，以提升概率计算在AI中的应用。

Method: 采用电流模式神经元更新电路、标准单元设计、共享电源和硬件感知对比散度算法。

Result: 验证了芯片在逻辑门、全加器和MaxCut优化任务中的性能。

Conclusion: 证明了该芯片在AI和机器学习领域的潜力。

Abstract: This paper demonstrates a probabilistic bit physics inspired solver with 440
spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area
efficiency is maximized through a current-mode implementation of the neuron
update circuit, standard cell design for analog blocks pitch-matched to digital
blocks, and a shared power supply for both digital and analog components.
Process variation related mismatches introduced by this approach are
effectively mitigated using a hardware aware contrastive divergence algorithm
during training. We validate the chip's ability to perform probabilistic
computing tasks such as modeling logic gates and full adders, as well as
optimization tasks such as MaxCut, demonstrating its potential for AI and
machine learning applications.

</details>


### [393] [FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference](https://arxiv.org/abs/2504.14152)
*Coleman Hooper,Charbel Sakr,Ben Keller,Rangharajan Venkatesan,Kurt Keutzer,Sophia Shao,Brucek Khailany*

Main category: cs.AR

TL;DR: 本论文提出细粒度混合精度量化方法，通过硬件软件协同设计，减少能量消耗和内存占用，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 量化可提高大语言模型推理效率，但低精度量化易导致准确性下降，需要方法维持准确性。

Method: 提出使用Fisher信息加权扰动策略选择高精度块、敏感性加权裁剪方法，以及硬件增强支持细粒度混合精度量化。

Result: 在Llama-2-7B模型上，相比全FP8基准，困惑度下降小于1%，能量消耗减少14%，权重内存减少30%。

Conclusion: 设计实现了高效量化，显著降低能量和内存消耗，同时保持模型性能。

Abstract: Quantization is a powerful tool to improve large language model (LLM)
inference efficiency by utilizing more energy-efficient low-precision datapaths
and reducing memory footprint. However, accurately quantizing LLM weights and
activations to low precision is challenging without degrading model accuracy.
We propose fine-grained mixed precision (FGMP) quantization, a post-training
mixed-precision quantization hardware-software co-design methodology that
maintains accuracy while quantizing the majority of weights and activations to
reduced precision. Our work makes the following contributions: 1) We develop a
policy that uses the perturbation in each value, weighted by the Fisher
information, to select which weight and activation blocks to keep in higher
precision. This approach preserves accuracy by identifying which weight and
activation blocks need to be retained in higher precision to minimize the
perturbation in the model loss. 2) We also propose a sensitivity-weighted
clipping approach for fine-grained quantization which helps retain accuracy for
blocks that are quantized to low precision. 3) We then propose hardware
augmentations to leverage the efficiency benefits of FGMP quantization. Our
hardware implementation encompasses i) datapath support for FGMP at block
granularity, and ii) a mixed-precision activation quantization unit to assign
activation blocks to high or low precision on the fly with minimal runtime and
energy overhead. Our design, prototyped using NVFP4 (an FP4 format with
microscaling) as the low-precision datatype and FP8 as the high-precision
datatype, facilitates efficient FGMP quantization, attaining <1% perplexity
degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8
baseline design while consuming 14% less energy during inference and requiring
30% less weight memory.

</details>


### [394] [ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model](https://arxiv.org/abs/2504.14560)
*Haiyan Qin,Zhiwei Xie,Jingjing Li,Liangchen Li,Xiaotong Feng,Junzhan Liu,Wang Kang*

Main category: cs.AR

TL;DR: 这篇论文介绍了ReasoningV，一种通过混合推理策略提升Verilog代码生成效率和性能的模型，解决了数据质量和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在Verilog代码生成中取得进展，但存在数据质量、推理能力和计算效率的挑战。

Method: 采用混合推理策略，包括ReasoningV-5K数据集、两阶段训练方法（参数高效微调和全参数优化），以及自适应推理机制动态调整深度。

Result: 在VerilogEval-human上实现57.8%的pass@1准确率，媲美Gemini-2.0-flash，并比最佳开源模型高10.4个百分点，减少75%的令牌消耗。

Conclusion: 提供更可靠、可访问的AI驱动硬件设计自动化途径。

Abstract: Large Language Models (LLMs) have advanced Verilog code generation
significantly, yet face challenges in data quality, reasoning capabilities, and
computational efficiency. This paper presents ReasoningV, a novel model
employing a hybrid reasoning strategy that integrates trained intrinsic
capabilities with dynamic inference adaptation for Verilog code generation. Our
framework introduces three complementary innovations: (1) ReasoningV-5K, a
high-quality dataset of 5,000 functionally verified instances with reasoning
paths created through multi-dimensional filtering of PyraNet samples; (2) a
two-stage training approach combining parameter-efficient fine-tuning for
foundational knowledge with full-parameter optimization for enhanced reasoning;
and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning
depth based on problem complexity, reducing token consumption by up to 75\%
while preserving performance. Experimental results demonstrate ReasoningV's
effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving
performance competitive with leading commercial models like Gemini-2.0-flash
(59.5\%) and exceeding the previous best open-source model by 10.4 percentage
points. ReasoningV offers a more reliable and accessible pathway for advancing
AI-driven hardware design automation, with our model, data, and code available
at https://github.com/BUAA-CLab/ReasoningV.

</details>


### [395] [Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence](https://arxiv.org/abs/2504.14625)
*Haiyan Qin,Jiahao Feng,Xiaotong Feng,Wei W. Xing,Wang Kang*

Main category: cs.AR

TL;DR: CircuitMind 框架通过语法锁定、检索增强生成和双重奖励优化，使 LLM 在硬件设计中达到人类竞争效率。


<details>
  <summary>Details</summary>
Motivation: LLM 在硬件设计中产生门电路计数高 38%–1075% 的问题，需要改进效率。

Method: 多代理框架，包括语法锁定（约束到基本逻辑门）、检索增强生成（知识驱动设计）和双重奖励优化（平衡正确性和效率）。

Result: 实验显示，55.6% 模型实现匹配或超过顶尖人类专家，14B Phi-4 模型优于 GPT-4o mini 和 Gemini 2.0 Flash，效率达顶尖 25%。

Conclusion: 建立了 AI 协作系统利用人类专业知识优化硬件设计的新范式。

Abstract: Large language models (LLMs) have transformed code generation, yet their
application in hardware design produces gate counts 38\%--1075\% higher than
human designs. We present CircuitMind, a multi-agent framework that achieves
human-competitive efficiency through three key innovations: syntax locking
(constraining generation to basic logic gates), retrieval-augmented generation
(enabling knowledge-driven design), and dual-reward optimization (balancing
correctness with efficiency). To evaluate our approach, we introduce TC-Bench,
the first gate-level benchmark harnessing collective intelligence from the
TuringComplete ecosystem -- a competitive circuit design platform with hundreds
of thousands of players. Experiments show CircuitMind enables 55.6\% of model
implementations to match or exceed top-tier human experts in composite
efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model
to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency
comparable to the top 25\% of human experts without requiring specialized
training. These innovations establish a new paradigm for hardware optimization
where collaborative AI systems leverage collective human expertise to achieve
optimal circuit designs. Our model, data, and code are open-source at
https://github.com/BUAA-CLab/CircuitMind.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [396] [GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity](https://arxiv.org/abs/2504.13853)
*Pingfei Zhu,Chenyang Zhao,Haishi Zhao,Bo Yang*

Main category: q-bio.BM

TL;DR: 本文提出GenShin模型，通过几何增强结构图模块预测化合物-蛋白质亲和力，无需绑定位点输入，性能与主流模型相当或更好。


<details>
  <summary>Details</summary>
Motivation: 准确预测化合物-蛋白质亲和力通常需要昂贵的实验或模拟获得的绑定位点信息，本文旨在解决这一挑战。

Method: 引入GenShin模型，构建几何增强结构图模块，从蛋白质和化合物中分别提取额外特征。

Result: GenShin模型在不使用绑定位点输入时性能优于其他模型，甚至超过需要适当绑定位点的模型，且对不适当绑定位点更鲁棒。

Conclusion: 希望这项工作能激励更多研究，弥合AI模型与实际药物发现挑战之间的差距。

Abstract: AI-powered drug discovery typically relies on the successful prediction of
compound-protein interactions, which are pivotal for the evaluation of designed
compound molecules in structure-based drug design and represent a core
challenge in the field.
  However, accurately predicting compound-protein affinity via regression
models usually requires adequate-binding pose, which are derived from costly
and complex experimental methods or time-consuming simulations with docking
software. In response, we have introduced the GenShin model, which constructs a
geometry-enhanced structural graph module that separately extracts additional
features from proteins and compounds. Consequently, it attains an accuracy on
par with mainstream models in predicting compound-protein affinities, while
eliminating the need for adequate-binding pose as input. Our experimental
findings demonstrate that the GenShin model vastly outperforms other models
that rely on non-input docking conformations, achieving, or in some cases even
exceeding, the performance of those requiring adequate-binding pose. Further
experiments indicate that our GenShin model is more robust to
inadequate-binding pose, affirming its higher suitability for real-world drug
discovery scenarios. We hope our work will inspire more endeavors to bridge the
gap between AI models and practical drug discovery challenges.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [397] [Planet as a Brain: Towards Internet of AgentSites based on AIOS Server](https://arxiv.org/abs/2504.14411)
*Xiang Zhang,Yongfeng Zhang*

Main category: cs.NI

TL;DR: 论文介绍了AIOS Server框架，支持AI代理的托管和全球协作，构建Internet of AgentSites新生态。


<details>
  <summary>Details</summary>
Motivation: 响应互联网从网站向代理站点转变的需求，建立AI代理主导的数字景观。

Method: 使用MCP和JSON-RPC通信协议、DHT和Gossip协议，实现代理托管、P2P协调和发现机制。

Result: 部署了首个实际AIOS-IoA系统，包括AgentHub和AgentChat，支持代理注册和交互。

Conclusion: 为构建自治代理主导的网络新范式提供了实用基础。

Abstract: The internet is undergoing a historical transformation from the "Internet of
Websites" to the "Internet of AgentSites." While traditional Websites served as
the foundation for information hosting and dissemination, a new frontier is
emerging where AgentSites serve as the hubs of the internet, where each
AgentSite hosts one or more AI agents that receive tasks, address them, and
deliver actionable solutions, marking a significant shift in the digital
landscape and representing the next generation of online ecosystems. Under this
vision, AIOS, the AI Agent Operating System, serves as the server for the
development, deployment and execution of AI agents, which is a fundamental
infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents
and enable global-scale collaboration among decentralized agents. AIOS Server
provides a communication protocol leveraging the Model Context Protocol (MCP)
and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node
operates as a server to host and execute agents, while supporting peer-to-peer
coordination without reliance on centralized orchestration. Based on AIOS
Server, we further present the world's first practically deployed Internet of
Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery
and AgentChat for interactive communication, at https://planet.aios.foundation.
The agent discovery mechanism based on Distributed Hash Tables (DHT) and a
Gossip protocol serves as the search engine for the internet of agentsites.
This work provides a practical foundation for building the Internet of
Agentsites-a new paradigm where autonomous agents become first-class citizens
of the web. The implementation is available at
https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS
main branch at https://github.com/agiresearch/AIOS.

</details>


### [398] [Uncovering Issues in the Radio Access Network by Looking at the Neighbors](https://arxiv.org/abs/2504.14686)
*José Suárez-Varela,Andra Lutu*

Main category: cs.NI

TL;DR: c-ANEMON is a GNN-based system for detecting RAN anomalies, focusing on network issues and showing good generalization with real-world data.


<details>
  <summary>Details</summary>
Motivation: To manage the complexity of large-scale Radio Access Networks by detecting anomalies related to network problems, independent of external factors.

Method: Uses Graph Neural Networks to capture spatio-temporal variations in cell behaviors relative to their neighborhoods.

Result: Model generalizes to unseen areas; 45.95% of long-lasting anomalies likely require intervention based on real data from a large metropolitan area.

Conclusion: c-ANEMON effectively identifies actionable anomalies, improving operational efficiency in network management.

Abstract: Mobile network operators (MNOs) manage Radio Access Networks (RANs) with
massive amounts of cells over multiple radio generations (2G-5G). To handle
such complexity, operations teams rely on monitoring systems, including anomaly
detection tools that identify unexpected behaviors. In this paper, we present
c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph
Neural Networks (GNNs). Our solution captures spatio-temporal variations by
analyzing the behavior of individual cells in relation to their local
neighborhoods, enabling the detection of anomalies that are independent of
external mobility factors. This, in turn, allows focusing on anomalies
associated with network issues (e.g., misconfigurations, equipment failures).
We evaluate c-ANEMON using real-world data from a large European metropolitan
area (7,890 cells; 3 months). First, we show that the GNN model within our
solution generalizes effectively to cells from previously unseen areas,
suggesting the possibility of using a single model across extensive deployment
regions. Then, we analyze the anomalies detected by c-ANEMON through manual
inspection and define several categories of long-lasting anomalies (6+ hours).
Notably, 45.95% of these anomalies fall into a category that is more likely to
require intervention by operations teams.

</details>


### [399] [Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology](https://arxiv.org/abs/2504.14720)
*Tamir Berger,Jonathan Sterenson,Raz Birman,Ofer Hadar*

Main category: cs.NI

TL;DR: 这篇论文提出了一种从加密流量中估计视频QoE的应用程序无关方法，以解决网络运营商无法获取端设备QoE指标的问题。


<details>
  <summary>Details</summary>
Motivation: 网络运营商需要确保用户QoE，但由于流量加密和缺乏ground truth数据，现有方法难以有效估计QoE指标。

Method: 提出独立于视频应用的QoE估计方法，通过从WhatsApp视频会话中收集流量数据和QoE指标，创建数据集。

Result: 评估显示，FPS预测准确率达85.2%（误差在2FPS内），PIQE-based质量评分分类准确率达90.2%。

Conclusion: 该方法在整个数据集上表现出高性能，并适用于各种专有即时消息视频通话和视频会议应用。

Abstract: Instant Messaging-Based Video Call Applications (IMVCAs) and Video
Conferencing Applications (VCAs) have become integral to modern communication.
Ensuring a high Quality of Experience (QoE) for users in this context is
critical for network operators, as network conditions significantly impact user
QoE. However, network operators lack access to end-device QoE metrics due to
encrypted traffic. Existing solutions estimate QoE metrics from encrypted
traffic traversing the network, with the most advanced approaches leveraging
machine learning models. Subsequently, the need for ground truth QoE metrics
for training and validation poses a challenge, as not all video applications
provide these metrics. To address this challenge, we propose an
application-agnostic approach for objective QoE estimation from encrypted
traffic. Independent of the video application, we obtained key video QoE
metrics, enabling broad applicability to various proprietary IMVCAs and VCAs.
To validate our solution, we created a diverse dataset from WhatsApp video
sessions under various network conditions, comprising 25,680 seconds of traffic
data and QoE metrics. Our evaluation shows high performance across the entire
dataset, with 85.2% accuracy for FPS predictions within an error margin of two
FPS, and 90.2% accuracy for PIQE-based quality rating classification.

</details>
