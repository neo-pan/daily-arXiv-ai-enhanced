{"id": "2504.12417", "pdf": "https://arxiv.org/pdf/2504.12417", "abs": "https://arxiv.org/abs/2504.12417", "authors": ["Dewang Kumar Agarwal", "Dimitris J. Bertsimas"], "title": "Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data", "categories": ["cs.AI"], "comment": null, "summary": "Objective: Create precise, structured, data-backed guidelines for type 2\ndiabetes treatment progression, suitable for clinical adoption.\n  Research Design and Methods: Our training cohort was composed of patient\n(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to\n2014. We divide visits into 4 groups based on the patient's treatment regimen\nbefore the visit, and further divide them into subgroups based on the\nrecommended treatment during the visit. Since each subgroup has observational\ndata, which has confounding bias (sicker patients are prescribed more\naggressive treatments), we used machine learning and optimization to remove\nsome datapoints so that the remaining data resembles a randomized trial. On\neach subgroup, we train AI-backed tree-based models to prescribe treatment\nchanges. Once we train these tree models, we manually combine the models for\nevery group to create an end-to-end prescription pipeline for all patients in\nthat group. In this process, we prioritize stepping up to a more aggressive\ntreatment before considering less aggressive options. We tested this pipeline\non unseen data from BMC, and an external dataset from Hartford healthcare (type\n2 diabetes patient visits from January 2020 to May 2024).\n  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more\nthan what the doctors achieved on the unseen BMC patients. For the Hartford\ncohort, our pipelines were better by 0.13%.\n  Conclusions: This precise, interpretable, and efficient AI-backed approach to\ntreatment progression in type 2 diabetes is predicted to outperform the current\npractice and can be deployed to improve patient outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528AI\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf92\u578b\u7cd6\u5c3f\u75c5\u7684\u6cbb\u7597\u8fdb\u5c55\u6307\u5357\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4f18\u5316\u6570\u636e\uff0c\u5e76\u5728\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u6bd4\u533b\u751f\u66f4\u597d\u7684HbA1c\u964d\u4f4e\u6548\u679c\u3002", "motivation": "\u521b\u5efa\u7cbe\u786e\u3001\u7ed3\u6784\u5316\u3001\u6570\u636e\u652f\u6301\u76842\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u8fdb\u5c55\u6307\u5357\uff0c\u9002\u5408\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u4f18\u5316\u53bb\u9664\u6df7\u6742\u504f\u5dee\uff0c\u8bad\u7ec3\u6811\u6a21\u578b\u5f00\u5177\u6cbb\u7597\u65b9\u6848\uff0c\u5e76\u5728BMC\u548cHartford\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u3002", "result": "\u5728BMC\u672a\u89c1\u6570\u636e\u4e0a\uff0cHbA1c\u964d\u4f4e\u4e2d\u4f4d\u6570\u6bd4\u533b\u751f\u591a0.26%\uff1b\u5728Hartford\u961f\u5217\u4e0a\u591a0.13%\u3002", "conclusion": "\u8fd9\u79cdAI\u652f\u6301\u7684\u65b9\u6cd5\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\uff0c\u9884\u8ba1\u80fd\u4f18\u4e8e\u5f53\u524d\u5b9e\u8df5\uff0c\u5e76\u53ef\u90e8\u7f72\u4ee5\u6539\u5584\u60a3\u8005\u7ed3\u679c\u3002"}}
{"id": "2504.12444", "pdf": "https://arxiv.org/pdf/2504.12444", "abs": "https://arxiv.org/abs/2504.12444", "authors": ["Jiawei Zhang", "Yu Zhang", "Wei Xu", "Yifei Zhang", "Weiran Jiang", "Qi Jiao", "Yao Ren", "Ziyou Song"], "title": "Enhanced Battery Capacity Estimation in Data-Limited Scenarios through Swarm Learning", "categories": ["eess.SY", "cs.LG", "cs.SY", "physics.chem-ph"], "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  Transportation Electrification Conference & Expo (ITEC)", "summary": "Data-driven methods have shown potential in electric-vehicle battery\nmanagement tasks such as capacity estimation, but their deployment is\nbottlenecked by poor performance in data-limited scenarios. Sharing battery\ndata among algorithm developers can enable accurate and generalizable\ndata-driven models. However, an effective battery management framework that\nsimultaneously ensures data privacy and fault tolerance is still lacking. This\npaper proposes a swarm battery management system that unites a decentralized\nswarm learning (SL) framework and credibility weight-based model merging\nmechanism to enhance battery capacity estimation in data-limited scenarios\nwhile ensuring data privacy and security. The effectiveness of the SL framework\nis validated on a dataset comprising 66 commercial LiNiCoAlO2 cells cycled\nunder various operating conditions. Specifically, the capacity estimation\nperformance is validated in four cases, including data-balanced, volume-biased,\nfeature-biased, and quality-biased scenarios. Our results show that SL can\nenhance the estimation accuracy in all data-limited cases and achieve a similar\nlevel of accuracy with central learning where large amounts of data are\navailable.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7fa4\u96c6\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u5206\u6563\u5f0f\u7fa4\u96c6\u5b66\u4e60\u548c\u53ef\u4fe1\u5ea6\u6743\u91cd\u673a\u5236\uff0c\u63d0\u5347\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7535\u52a8\u8f66\u7535\u6c60\u5bb9\u91cf\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u6027\u80fd\u5dee\uff0c\u7f3a\u4e4f\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u548c\u5bb9\u9519\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u5206\u6563\u5f0f\u7fa4\u96c6\u5b66\u4e60\uff08SL\uff09\u548c\u53ef\u4fe1\u5ea6\u6743\u91cd\u6a21\u578b\u5408\u5e76\u673a\u5236\u7684\u7fa4\u96c6\u7535\u6c60\u7ba1\u7406\u7cfb\u7edf\u3002", "result": "\u572866\u4e2a\u5546\u7528LiNiCoAlO2\u7535\u6c60\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cSL\u5728\u6570\u636e\u5e73\u8861\u3001\u5bb9\u91cf\u504f\u5dee\u3001\u7279\u5f81\u504f\u5dee\u548c\u8d28\u91cf\u504f\u5dee\u573a\u666f\u4e0b\u63d0\u9ad8\u4e86\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u4e0e\u4e2d\u5fc3\u5b66\u4e60\u76f8\u5f53\u3002", "conclusion": "\u7fa4\u96c6\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.12477", "pdf": "https://arxiv.org/pdf/2504.12477", "abs": "https://arxiv.org/abs/2504.12477", "authors": ["George Fatouros", "Georgios Makridis", "George Kousiouris", "John Soldatos", "Anargyros Tsadimas", "Dimosthenis Kyriazis"], "title": "Towards Conversational AI for Human-Machine Collaborative MLOps", "categories": ["cs.AI", "cs.CL", "cs.HC", "68T50, 68T99, 68U35, 68N19", "I.2.1; H.5.2; D.2.11; I.2.7"], "comment": "8 pages, 5 figures", "summary": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u7cfb\u7edfSwarm Agent\uff0c\u7528\u4e8e\u63d0\u5347MLOps\u4e2d\u4eba\u673a\u534f\u4f5c\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7b80\u5316ML\u5de5\u5177\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3\u590d\u6742MLOps\u5e73\u53f0\u5982Kubeflow\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u4f7f\u4e0d\u540c\u6280\u672f\u6c34\u5e73\u7684\u7528\u6237\u80fd\u66f4\u6613\u4f7f\u7528\u9ad8\u7ea7ML\u5de5\u5177\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u67b6\u6784\uff0c\u96c6\u6210KFP Agent\u3001MinIO Agent\u548cRAG Agent\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u5faa\u73af\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5904\u7406\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "result": "\u7528\u6237\u53ef\u901a\u8fc7\u5bf9\u8bdd\u63a5\u53e3\u53d1\u73b0\u3001\u6267\u884c\u548c\u76d1\u63a7ML\u7ba1\u9053\uff0c\u7ba1\u7406\u6570\u636e\u548c\u5de5\u4ef6\uff0c\u8bbf\u95ee\u6587\u6863\uff0c\u4ece\u800c\u964d\u4f4e\u64cd\u4f5c\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86ML\u5de5\u5177\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.12506", "pdf": "https://arxiv.org/pdf/2504.12506", "abs": "https://arxiv.org/abs/2504.12506", "authors": ["Victor Nan Fernandez-Ayala", "Jorge Silva", "Meng Guo", "Dimos V. Dimarogonas"], "title": "Robust Visual Servoing under Human Supervision for Assembly Tasks", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We propose a framework enabling mobile manipulators to reliably complete\npick-and-place tasks for assembling structures from construction blocks. The\npicking uses an eye-in-hand visual servoing controller for object tracking with\nControl Barrier Functions (CBFs) to ensure fiducial markers in the blocks\nremain visible. An additional robot with an eye-to-hand setup ensures precise\nplacement, critical for structural stability. We integrate human-in-the-loop\ncapabilities for flexibility and fault correction and analyze robustness to\ncamera pose errors, proposing adapted barrier functions to handle them. Lastly,\nexperiments validate the framework on 6-DoF mobile arms.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9\u4f3a\u670d\u548c\u63a7\u5236\u969c\u788d\u51fd\u6570\u4f7f\u79fb\u52a8\u673a\u68b0\u81c2\u53ef\u9760\u5b8c\u6210\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u52a8\u673a\u662f\u5b9e\u73b0\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u7ec4\u88c5\u7ed3\u6784\u5757\u65f6\u7684\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5904\u7406\u76f8\u673a\u4f4d\u59ff\u9519\u8bef\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u624b\u773c\u89c6\u89c9\u4f3a\u670d\u7ed3\u5408\u63a7\u5236\u969c\u788d\u51fd\u6570\u7528\u4e8e\u62fe\u53d6\uff0c\u773c\u624b\u89c6\u89c9\u7528\u4e8e\u7cbe\u786e\u653e\u7f6e\uff0c\u4eba\u673a\u4ea4\u4e92\u548c\u9002\u5e94\u6027\u969c\u788d\u51fd\u6570\u5904\u7406\u9519\u8bef\u3002", "result": "\u7ed3\u679c\u901a\u8fc7\u57286\u81ea\u7531\u5ea6\u79fb\u52a8\u81c2\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u6846\u67b6\u80fd\u53ef\u9760\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u5bf9\u76f8\u673a\u9519\u8bef\u6709\u8f83\u597d\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.12482", "pdf": "https://arxiv.org/pdf/2504.12482", "abs": "https://arxiv.org/abs/2504.12482", "authors": ["Luciano Floridi", "Carlotta Buttaboni", "Emmie Hine", "Jessica Morley", "Claudio Novelli", "Tyler Schroder"], "title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86Agentic AI Optimisation (AAIO)\uff0c\u65e8\u5728\u4f18\u5316\u7f51\u7ad9\u4e0e\u81ea\u4e3bAI\u4ee3\u7406\u7684\u4ea4\u4e92\uff0c\u5f3a\u8c03\u5176\u91cd\u8981\u6027\u5e76\u8ba8\u8bba\u6cbb\u7406\u3001\u4f26\u7406\u548c\u76d1\u7ba1\u95ee\u9898\u3002", "motivation": "Agentic AI\u7cfb\u7edf\u7684\u5174\u8d77\u9700\u8981\u65b0\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u4ee5\u786e\u4fdd\u4ee3\u7406\u4e0e\u5e73\u53f0\u7684\u65e0\u7f1d\u4ea4\u4e92\uff0c\u7c7b\u4f3c\u4e8eSEO\u5bf9\u5185\u5bb9\u53d1\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5f15\u5165AAIO\uff0c\u5206\u6790\u7f51\u7ad9\u4f18\u5316\u4e0eAI\u6210\u529f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u5e76\u63a2\u8ba8\u6cbb\u7406\u3001\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u793e\u4f1a\u5f71\u54cd\uff08GELSI\uff09\u3002", "result": "\u7a81\u51fa\u4e86AAIO\u521b\u5efa\u7684\u826f\u6027\u5faa\u73af\uff0c\u5e76\u5f3a\u8c03\u4e3b\u52a8\u76d1\u7ba1\u6846\u67b6\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u51cf\u8f7b\u6f5c\u5728\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "AAIO\u662f\u81ea\u4e3b\u6570\u5b57\u4ee3\u7406\u65f6\u4ee3\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4e3b\u5f20\u516c\u5e73\u548c\u5305\u5bb9\u6027\u7684\u8bbf\u95ee\u3002"}}
{"id": "2504.12508", "pdf": "https://arxiv.org/pdf/2504.12508", "abs": "https://arxiv.org/abs/2504.12508", "authors": ["Papa Yaw Owusu-Obeng", "Steven R. Miller", "Sarah Banas Mills", "Michael T. Craig"], "title": "Optimizing Utility-Scale Solar Siting for Local Economic Benefits and Regional Decarbonization", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "The Midwest, with its vast agricultural lands, is rapidly emerging as a key\nregion for utility-scale solar expansion. However, traditional power planning\nhas yet to integrate local economic impact directly into capacity expansion to\nguide optimal siting decisions. Moreover, existing economic assessments tend to\nemphasize local benefits while overlooking the opportunity costs of converting\nproductive farmland for solar development. This study addresses these gaps by\nendogenously incorporating local economic metrics into a power system planning\nmodel to evaluate how economic impacts influence solar siting, accounting for\nthe cost of lost agricultural output. We analyze all counties within the Great\nLakes region, constructing localized supply and marginal benefit curves that\nare embedded within a multi-objective optimization framework aimed at\nminimizing system costs and maximizing community economic benefits. Our\nfindings show that counties with larger economies and lower farmland\nproductivity deliver the highest local economic benefit per megawatt (MW) of\ninstalled solar capacity. In Ohio, for example, large counties generate up to\n$34,500 per MW, driven in part by high property tax revenues, while smaller\ncounties yield 31% less. Accounting for the opportunity cost of displaced\nagricultural output reduces local benefits by up to 16%, depending on farmland\nquality. A scenario prioritizing solar investment in counties with higher\neconomic returns increases total economic benefits by $1 billion (or 11%) by\n2040, with solar investment shifting away from Michigan and Wisconsin (down by\n39%) toward Ohio and Indiana (up by 75%), with only a marginal increase of 0.5%\nin system-wide costs. These findings underscore the importance of integrating\neconomic considerations into utility-scale solar planning to better align\ndecarbonization goals with regional and local economic development.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u672c\u5730\u7ecf\u6d4e\u5f71\u54cd\u6574\u5408\u5230\u4e2d\u897f\u90e8\u592a\u9633\u80fd\u9009\u5740\u51b3\u7b56\u4e2d\uff0c\u53d1\u73b0\u4f18\u5148\u9009\u62e9\u9ad8\u56de\u62a5\u53bf\u53ef\u63d0\u9ad8\u7ecf\u6d4e\u6548\u76ca\uff0c\u540c\u65f6\u7cfb\u7edf\u6210\u672c\u4ec5\u5fae\u589e\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7535\u529b\u89c4\u5212\u672a\u76f4\u63a5\u6574\u5408\u672c\u5730\u7ecf\u6d4e\u5f71\u54cd\u548c\u5ffd\u7565\u519c\u7530\u8f6c\u6362\u673a\u4f1a\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5185\u751f\u5730\u5c06\u672c\u5730\u7ecf\u6d4e\u6307\u6807\u7eb3\u5165\u7535\u529b\u7cfb\u7edf\u89c4\u5212\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u6784\u5efa\u672c\u5730\u4f9b\u5e94\u548c\u8fb9\u9645\u6536\u76ca\u66f2\u7ebf\u3002", "result": "\u7ecf\u6d4e\u89c4\u6a21\u8f83\u5927\u3001\u519c\u7530\u751f\u4ea7\u529b\u8f83\u4f4e\u7684\u53bf\u6bcf\u5146\u74e6\u592a\u9633\u80fd\u5bb9\u91cf\u4ea7\u751f\u6700\u9ad8\u672c\u5730\u7ecf\u6d4e\u6536\u76ca\uff1b\u8003\u8651\u673a\u4f1a\u6210\u672c\u53ef\u51cf\u5c11\u6536\u76ca\u6700\u591a16%\uff1b\u4f18\u5148\u9ad8\u56de\u62a5\u573a\u666f\u53ef\u4f7f\u603b\u7ecf\u6d4e\u6536\u76ca\u52302040\u5e74\u589e\u52a011%\uff0c\u6295\u8d44\u5411\u4fc4\u4ea5\u4fc4\u548c\u5370\u7b2c\u5b89\u7eb3\u5dde\u8f6c\u79fb\u3002", "conclusion": "\u5f3a\u8c03\u5728\u516c\u7528\u4e8b\u4e1a\u89c4\u6a21\u592a\u9633\u80fd\u89c4\u5212\u4e2d\u6574\u5408\u7ecf\u6d4e\u8003\u8651\uff0c\u4ee5\u66f4\u597d\u5730\u534f\u8c03\u8131\u78b3\u76ee\u6807\u4e0e\u533a\u57df\u7ecf\u6d4e\u5f00\u53d1\u3002"}}
{"id": "2504.12497", "pdf": "https://arxiv.org/pdf/2504.12497", "abs": "https://arxiv.org/abs/2504.12497", "authors": ["Robert E. Wray", "Steven J. Jones", "John E. Laird"], "title": "Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope", "categories": ["cs.AI", "I.2.8"], "comment": "12 pages, 3 figures. Submitted to AGI25 conference", "summary": "Regardless of past learning, an agent in an open world will face unfamiliar\nsituations and events outside of prior experience, existing models, or\npolicies. Further, the agent will sometimes lack relevant knowledge and/or\nsufficient time to assess the situation, generate and evaluate options, and\npursue a robustly considered course of action. How can an agent respond\nreasonably to situations that are outside of its original design scope? How can\nit recognize such situations sufficiently quickly and reliably to determine\nreasonable, adaptive courses of action? We identify key characteristics needed\nfor solutions, evaluate the state-of-the-art by these requirements, and outline\na proposed, novel approach that combines domain-general meta-knowledge (in the\nform of appraisals inspired by human cognition) and metareasoning. It has the\npotential to provide fast, adaptive responses to unfamiliar situations, more\nfully meeting the performance characteristics required for open-world, general\nagents.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8AI\u4ee3\u7406\u5982\u4f55\u5e94\u5bf9\u5f00\u653e\u4e16\u754c\u4e2d\u8d85\u51fa\u8bbe\u8ba1\u8303\u56f4\u7684\u672a\u77e5\u60c5\u51b5\uff0c\u63d0\u51fa\u7ed3\u5408\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u548c\u5143\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u4ee3\u7406\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u4f1a\u9047\u5230\u5148\u524d\u7ecf\u9a8c\u4e4b\u5916\u7684\u60c5\u51b5\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u6216\u65f6\u95f4\u505a\u51fa\u5408\u7406\u54cd\u5e94\u3002", "method": "\u65b9\u6cd5\u662f\u7ed3\u5408\u9886\u57df\u901a\u7528\u5143\u77e5\u8bc6\uff08\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8bc4\u4f30\u542f\u53d1\uff09\u548c\u5143\u63a8\u7406\uff0c\u4ee5\u5feb\u901f\u8bc6\u522b\u548c\u9002\u5e94\u672a\u77e5\u60c5\u51b5\u3002", "result": "\u7ed3\u679c\u662f\u8fd9\u79cd\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u4f9b\u5feb\u901f\u3001\u9002\u5e94\u7684\u54cd\u5e94\uff0c\u63d0\u5347\u5f00\u653e\u4e16\u754c\u4ee3\u7406\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u79cd\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u5730\u6ee1\u8db3\u5f00\u653e\u4e16\u754c\u901a\u7528\u4ee3\u7406\u7684\u9700\u6c42\u3002"}}
{"id": "2504.12703", "pdf": "https://arxiv.org/pdf/2504.12703", "abs": "https://arxiv.org/abs/2504.12703", "authors": ["Xun Xiao", "Junbo Tie", "Jinyue Zhao", "Ziqi Wang", "Yuan Li", "Qiang Dou", "Lei Wang"], "title": "Spike-Kal: A Spiking Neuron Network Assisted Kalman Filter", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "Kalman filtering can provide an optimal estimation of the system state from\nnoisy observation data. This algorithm's performance depends on the accuracy of\nsystem modeling and noise statistical characteristics, which are usually\nchallenging to obtain in practical applications. The powerful nonlinear\nmodeling capabilities of deep learning, combined with its ability to extract\nfeatures from large amounts of data automatically, offer new opportunities for\nimproving the Kalman filter. This paper proposes a novel method that leverages\nthe Spiking Neural Network to optimize the Kalman filter. Our approach aims to\nreduce the reliance on prior knowledge of system and observation noises,\nallowing for adaptation to varying statistical characteristics of time-varying\nnoise. Furthermore, we investigate the potential of SNNs in improving the\ncomputational efficiency of the Kalman filter. In our method, we design an\nintegration strategy between the SNN and the Kalman filter. The SNN is trained\nto directly approximate the optimal gain matrix from observation data, thereby\nalleviating the computational burden of complex matrix operations inherent in\ntraditional Kalman filtering while maintaining the accuracy and robustness of\nstate estimation. Its average error has been reduced by 18\\%-65\\% compared with\nother methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u51cf\u5c11\u5bf9\u566a\u58f0\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u5e73\u5747\u9519\u8bef\u964d\u4f4e18%-65%\u3002", "motivation": "\u5361\u5c14\u66fc\u6ee4\u6ce2\u4f9d\u8d56\u51c6\u786e\u7684\u7cfb\u7edf\u5efa\u6a21\u548c\u566a\u58f0\u7edf\u8ba1\u7279\u6027\uff0c\u4f46\u8fd9\u4e9b\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u83b7\u53d6\uff0c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u53ef\u81ea\u52a8\u63d0\u53d6\u6570\u636e\u7279\u5f81\u5e76\u9002\u5e94\u566a\u58f0\u53d8\u5316\u3002", "method": "\u8bbe\u8ba1SNN\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u6574\u5408\u7b56\u7565\uff0c\u8bad\u7ec3SNN\u76f4\u63a5\u4ece\u89c2\u6d4b\u6570\u636e\u903c\u8fd1\u6700\u4f18\u589e\u76ca\u77e9\u9635\uff0c\u51cf\u8f7b\u77e9\u9635\u8fd0\u7b97\u8d1f\u62c5\u3002", "result": "\u4e0e\u5176\u5b83\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e73\u5747\u9519\u8bef\u51cf\u5c1118%-65%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.12359", "pdf": "https://arxiv.org/pdf/2504.12359", "abs": "https://arxiv.org/abs/2504.12359", "authors": ["Yuanbo Tang", "Yan Tang", "Naifan Zhang", "Meixuan Chen", "Yang Li"], "title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts based large language models (MoE LLMs) have shown\nsignificant promise in multitask adaptability by dynamically routing inputs to\nspecialized experts. Despite their success, the collaborative mechanisms among\nexperts are still not well understood, limiting both the interpretability and\noptimization of these models. In this paper, we focus on two critical issues:\n(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs\nthrough expert pruning. To address the first issue, we propose a hierarchical\nsparse dictionary learning (HSDL) method that uncovers the collaboration\npatterns among experts. For the second issue, we introduce the\nContribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes\nlow-contribution experts. Our extensive experiments demonstrate that expert\ncollaboration patterns are closely linked to specific input types and exhibit\nsemantic significance across various tasks. Moreover, pruning experiments show\nthat our approach improves overall performance by 2.5\\% on average,\noutperforming existing methods. These findings offer valuable insights into\nenhancing the efficiency and interpretability of MoE LLMs, offering a clearer\nunderstanding of expert interactions and improving model optimization.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u63d0\u51fa\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u4f18\u5316 MoE LLMs \u4e2d\u7684\u4e13\u5bb6\u534f\u4f5c\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3 MoE LLMs \u4e2d\u4e13\u5bb6\u534f\u4f5c\u673a\u5236\u4e0d\u88ab\u5145\u5206\u7406\u89e3\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u7a00\u758f\u5b57\u5178\u5b66\u4e60 (HSDL) \u65b9\u6cd5\u8bc6\u522b\u4e13\u5bb6\u534f\u4f5c\u6a21\u5f0f\uff0c\u4ee5\u53ca\u8d21\u732e\u611f\u77e5\u4e13\u5bb6\u526a\u679d (CAEP) \u7b97\u6cd5\u526a\u679d\u4f4e\u8d21\u732e\u4e13\u5bb6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e13\u5bb6\u534f\u4f5c\u6a21\u5f0f\u4e0e\u8f93\u5165\u7c7b\u578b\u5bc6\u5207\u76f8\u5173\uff0c\u5177\u6709\u8bed\u4e49\u610f\u4e49\uff1b\u526a\u679d\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8 2.5% \u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u63d0\u5347 MoE LLMs \u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u6d1e\u89c1\uff0c\u6539\u5584\u4e13\u5bb6\u4e92\u52a8\u7406\u89e3\u548c\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2504.12529", "pdf": "https://arxiv.org/pdf/2504.12529", "abs": "https://arxiv.org/abs/2504.12529", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis", "categories": ["cs.AI", "cs.CY"], "comment": "9 Page, 1 Figure", "summary": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5143\u5206\u6790\u65b9\u6cd5\u5ba1\u89c6\u4e86AI\u53ef\u89e3\u91ca\u6027\u662f\u5426\u5fc5\u7136\u63d0\u5347\u7528\u6237\u4fe1\u4efb\uff0c\u53d1\u73b0\u4e2d\u7b49\u6b63\u76f8\u5173\uff0c\u4f46\u5f3a\u8c03\u53ef\u89e3\u91ca\u6027\u5e76\u975e\u552f\u4e00\u56e0\u7d20\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u533b\u7597\u548c\u53f8\u6cd5\u9886\u57df\u7684\u793e\u4f1a\u6280\u672f\u5f71\u54cd\u3002", "motivation": "\u6311\u6218AI\u7cfb\u7edf\u4e2d\u53ef\u89e3\u91ca\u6027\u5fc5\u7136\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u7684\u666e\u904d\u5047\u8bbe\u3002", "method": "\u4f7f\u7528\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u5bf990\u9879\u73b0\u6709\u6587\u732e\u8fdb\u884c\u5168\u9762\u5ba1\u67e5\u3002", "result": "\u53d1\u73b0AI\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u4fe1\u4efb\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u4f46\u4e2d\u5ea6\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "\u5f3a\u8c03\u57f9\u517bAI\u7cfb\u7edf\u7684\u771f\u5b9e\u548c\u6301\u4e45\u53ef\u4fe1\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5904\u7406\u7b97\u6cd5\u504f\u5dee\u548c\u4f26\u7406\u900f\u660e\u5ea6\u7b49\u95ee\u9898\u3002"}}
{"id": "2504.12736", "pdf": "https://arxiv.org/pdf/2504.12736", "abs": "https://arxiv.org/abs/2504.12736", "authors": ["Alexander Winkler", "Pranav Shah", "Katrin Baumg\u00e4rtner", "Vasu Sharma", "David Gordon", "Jakob Andert"], "title": "Incorporating a Deep Neural Network into Moving Horizon Estimation for Embedded Thermal Torque Derating of an Electric Machine", "categories": ["eess.SY", "cs.SY"], "comment": "17 pages, 13 figures, data publication incl. all scripts and data\n  available, submitted to Energies Journal", "summary": "This study introduces a novel state estimation framework that incorporates\nDeep Neural Networks (DNNs) into Moving Horizon Estimation (MHE), shifting from\ntraditional physics-based models to rapidly developed data-driven techniques. A\nDNN model with Long Short-Term Memory (LSTM) nodes is trained on synthetic data\ngenerated by a high-fidelity thermal model of a Permanent Magnet Synchronous\nMachine (PMSM), which undergoes thermal derating as part of the torque control\nstrategy in a battery electric vehicle. The MHE is constructed by integrating\nthe trained DNN with a simplified driving dynamics model in a discrete-time\nformulation, incorporating the LSTM hidden and cell states in the state vector\nto retain system dynamics. The resulting optimal control problem (OCP) is\nformulated as a nonlinear program (NLP) and implemented using the acados\nframework. Model-in-the-loop (MiL) simulations demonstrate accurate temperature\nestimation, even under noisy sensor conditions or failures. Achieving threefold\nreal-time capability on embedded hardware confirms the feasibility of the\napproach for practical deployment. The primary focus of this study is to assess\nthe feasibility of the MHE framework using a DNN-based plant model instead of\nfocusing on quantitative comparisons of vehicle performance. Overall, this\nresearch highlights the potential of DNN-based MHE for real-time,\nsafety-critical applications by combining the strengths of model-based and\ndata-driven methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6574\u5408\u5230\u79fb\u52a8\u5730\u5e73\u7ebf\u4f30\u8ba1\uff08MHE\uff09\u4e2d\u7684\u65b0\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u6c38\u78c1\u540c\u6b65\u7535\u673a\uff08PMSM\uff09\u7684\u6e29\u5ea6\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u65f6\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u8f6c\u5411\u6570\u636e\u9a71\u52a8\u6280\u672f\u4ee5\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u5c24\u5176\u5728\u566a\u58f0\u6216\u4f20\u611f\u5668\u6545\u969c\u6761\u4ef6\u4e0b\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528LSTM\u8282\u70b9\u7684DNN\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5c06\u5176\u4e0e\u7b80\u5316\u7684\u9a7e\u9a76\u52a8\u6001\u6a21\u578b\u6574\u5408\u5230MHE\u7684\u79bb\u6563\u65f6\u95f4\u516c\u5f0f\u4e2d\uff0c\u4f7f\u7528acados\u6846\u67b6\u8fdb\u884c\u6a21\u578b-\u5728-\u73af\u4eff\u771f\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u5728\u566a\u58f0\u4f20\u611f\u5668\u6761\u4ef6\u4e0b\u4e5f\u80fd\u51c6\u786e\u4f30\u8ba1\u6e29\u5ea6\uff0c\u5e76\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e09\u500d\u5b9e\u65f6\u80fd\u529b\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03DNN-based MHE\u5728\u5b9e\u65f6\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u878d\u5408\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2504.12397", "pdf": "https://arxiv.org/pdf/2504.12397", "abs": "https://arxiv.org/abs/2504.12397", "authors": ["Kristjan Greenewald", "Luis Lastras", "Thomas Parnell", "Vraj Shah", "Lucian Popa", "Giulio Zizzo", "Chulaka Gunasekara", "Ambrish Rawat", "David Cox"], "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.11704", "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.", "AI": {"tldr": "aLoRA \u901a\u8fc7\u4fee\u6539 LoRA \u6846\u67b6\uff0c\u5b9e\u73b0\u5373\u65f6\u6fc0\u6d3b\u7279\u5b9a\u6a21\u578b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97 KV \u7f13\u5b58\uff0c\u63d0\u9ad8\u591a\u8f6e\u5bf9\u8bdd\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6 LoRA \u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u5207\u6362\u65f6\u9700\u91cd\u65b0\u8ba1\u7b97\u6574\u4e2a\u5386\u53f2 KV \u7f13\u5b58\u5bfc\u81f4\u7684\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa aLoRA\uff0c\u4ec5\u4e3a\u6fc0\u6d3b\u540e\u7684\u5e8f\u5217\u4ee4\u724c\u8c03\u6574\u6743\u91cd\uff0c\u5e76\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u7684 KV \u7f13\u5b58\u3002", "result": "aLoRA \u5728\u51c6\u786e\u6027\u4e0a\u4e0e\u6807\u51c6 LoRA \u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "aLoRA \u542f\u7528\u6784\u5efa\u4e13\u4e1a\u5316 intrinsics \u6a21\u578b\uff0c\u7528\u4e8e\u5728\u8f93\u5165\u94fe\u6216\u5bf9\u8bdd\u7279\u5b9a\u90e8\u5206\u6267\u884c\u9ad8\u6548\u64cd\u4f5c\u3002"}}
{"id": "2504.12562", "pdf": "https://arxiv.org/pdf/2504.12562", "abs": "https://arxiv.org/abs/2504.12562", "authors": ["Haidar Khan", "Hisham A. Alyahya", "Yazeed Alnumay", "M Saiful Bari", "B\u00fclent Yener"], "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.", "AI": {"tldr": "ZeroSumEval \u662f\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u96f6\u548c\u6e38\u620f\u7684\u7ade\u4e89\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u52a8\u6001\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf LLM \u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u3001\u9ad8\u6210\u672c\u548c\u504f\u5dee\u95ee\u9898\uff0c\u56e0\u6b64\u5f00\u53d1 ZeroSumEval \u6765\u63d0\u4f9b\u66f4\u52a8\u6001\u3001\u6297\u9971\u548c\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86 ZeroSumEval \u6846\u67b6\uff0c\u5305\u62ec\u591a\u79cd\u6e38\u620f\uff08\u5982 PyJail\u3001Chess \u7b49\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u8fc7 7000 \u6b21\u6a21\u62df\u5b9e\u9a8c\uff0c\u6d89\u53ca 13 \u4e2a\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cGPT \u548c Claude \u7cfb\u5217\u6a21\u578b\u5728\u5e38\u89c1\u6e38\u620f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u521b\u5efa\u65b0\u9896\u95ee\u9898\uff0c\u4e14\u5728\u8d8a\u72f1\u548c\u521b\u9020\u529b\u4efb\u52a1\u4e0a\u5931\u8d25\u3002", "conclusion": "ZeroSumEval \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u7684 LLM \u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.12877", "pdf": "https://arxiv.org/pdf/2504.12877", "abs": "https://arxiv.org/abs/2504.12877", "authors": ["Shijie Pan", "Gerrit Rolofs", "Luca Pontecorvi", "Charalambos Konstantinou"], "title": "Market-Driven Flexibility Provision: A Tri-Level Optimization Approach for Carbon Reduction", "categories": ["eess.SY", "cs.SY"], "comment": "2025 IEEE Kiel PowerTech", "summary": "The integration of renewable energy resources (RES) in the power grid can\nreduce carbon intensity, but also presents certain challenges. The uncertainty\nand intermittent nature of RES emphasize the need for flexibility in power\nsystems. Moreover, there are noticeable mismatches between real-time\nelectricity prices and carbon intensity patterns throughout the day. These\ndiscrepancies may lead customers to schedule energy-intensive tasks during the\nearly hours of the day, a period characterized by lower electricity prices but\nhigher carbon intensity. This paper introduces a novel and comprehensive\nframework aimed at encouraging customer participation in electricity markets\nand aligning their flexibility with carbon intensity trends. The proposed\napproach integrates an incentive-based tariff with a tri-level optimization\nmodel, where customers are motivated to submit flexibility bids and, in return,\nreceive financial rewards based on their contributions. The tri-level model\nensures a dynamic interaction between the market operation platform (MOP) and\nend-users. Simulations are performed on a modified IEEE-33 bus system,\nsupported by two scenarios with different RES generations and customer\nbehaviors. Results demonstrate the effectiveness of the proposed framework in\nguiding the customers' consumption behaviors towards low carbon intensity.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6fc0\u52b1\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u5c42\u4f18\u5316\u6a21\u578b\u5f15\u5bfc\u7528\u6237\u5728\u4f4e\u78b3\u5f3a\u5ea6\u65f6\u671f\u4f7f\u7528\u80fd\u6e90\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u95f4\u6b47\u6027\uff0c\u4ee5\u53ca\u7535\u529b\u4ef7\u683c\u4e0e\u78b3\u5f3a\u5ea6\u6a21\u5f0f\u4e0d\u5339\u914d\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u5728\u9ad8\u78b3\u5f3a\u5ea6\u65f6\u6bb5\u4f7f\u7528\u80fd\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u9f13\u52b1\u7528\u6237\u53c2\u4e0e\u5e76\u5bf9\u9f50\u78b3\u5f3a\u5ea6\u8d8b\u52bf\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u6fc0\u52b1-based\u5173\u7a0e\u548c\u4e09\u5c42\u4f18\u5316\u6a21\u578b\u7684\u6846\u67b6\uff0c\u6d89\u53ca\u5e02\u573a\u8fd0\u8425\u5e73\u53f0\u4e0e\u7528\u6237\u7684\u52a8\u6001\u4ea4\u4e92\uff0c\u5e76\u5728\u4fee\u6539\u540e\u7684IEEE-33\u6bcd\u7ebf\u7cfb\u7edf\u4e2d\u901a\u8fc7\u4e0d\u540c\u573a\u666f\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u6709\u6548\u5730\u5f15\u5bfc\u7528\u6237\u5c06\u80fd\u6e90\u6d88\u8d39\u8c03\u6574\u5230\u4f4e\u78b3\u5f3a\u5ea6\u65f6\u671f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5bf9\u9f50\u7528\u6237\u7684\u80fd\u6e90\u7075\u6d3b\u6027\u4e0e\u78b3\u5f3a\u5ea6\u8d8b\u52bf\uff0c\u4fc3\u8fdb\u53ef\u6301\u7eed\u80fd\u6e90\u4f7f\u7528\u3002"}}
{"id": "2504.12419", "pdf": "https://arxiv.org/pdf/2504.12419", "abs": "https://arxiv.org/abs/2504.12419", "authors": ["Loong Kuan Lee", "Thore Thassilo Gerlach", "Nico Piatkowski"], "title": "Standardization of Multi-Objective QUBOs", "categories": ["cs.LG", "math.OC", "quant-ph"], "comment": "7 pages, 3 figures", "summary": "Multi-objective optimization involving Quadratic Unconstrained Binary\nOptimization (QUBO) problems arises in various domains. A fundamental challenge\nin this context is the effective balancing of multiple objectives, each\npotentially operating on very different scales. This imbalance introduces\ncomplications such as the selection of appropriate weights when scalarizing\nmultiple objectives into a single objective function. In this paper, we propose\na novel technique for scaling QUBO objectives that uses an exact computation of\nthe variance of each individual QUBO objective. By scaling each objective to\nhave unit variance, we align all objectives onto a common scale, thereby\nallowing for more balanced solutions to be found when scalarizing the\nobjectives with equal weights, as well as potentially assisting in the search\nor choice of weights during scalarization. Finally, we demonstrate its\nadvantages through empirical evaluations on various multi-objective\noptimization problems. Our results are noteworthy since manually selecting\nscalarization weights is cumbersome, and reliable, efficient solutions are\nscarce.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2aQUBO\u76ee\u6807\u7684\u65b9\u5dee\u5e76\u7f29\u653e\u5230\u5355\u4f4d\u65b9\u5dee\u6765\u5e73\u8861\u591a\u76ee\u6807\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6807\u91cf\u5316\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u591a\u76ee\u6807\u4f18\u5316\u4e2d\uff0c\u76ee\u6807\u89c4\u6a21\u4e0d\u540c\u5bfc\u81f4\u6743\u91cd\u9009\u62e9\u56f0\u96be\uff0c\u624b\u52a8\u8c03\u6574\u7e41\u7410\uff0c\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u7a00\u7f3a\u3002", "method": "\u63d0\u51fa\u7cbe\u786e\u8ba1\u7b97\u6bcf\u4e2aQUBO\u76ee\u6807\u65b9\u5dee\uff0c\u5e76\u5c06\u6bcf\u4e2a\u76ee\u6807\u7f29\u653e\u5230\u5355\u4f4d\u65b9\u5dee\u7684\u6280\u672f\u3002", "result": "\u901a\u8fc7\u5bf9\u5404\u79cd\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6743\u91cd\u9009\u62e9\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u627e\u5230\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12612", "pdf": "https://arxiv.org/pdf/2504.12612", "abs": "https://arxiv.org/abs/2504.12612", "authors": ["Ching-Chun Chang", "Isao Echizen"], "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "categories": ["cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7b26\u53f7\u7f16\u5e74\u53f2\u4ece\u5185\u5bb9\u4e2d\u4e8b\u540e\u8ffd\u8e2a\u591a\u4ee3\u7406\u751f\u6210\u5386\u53f2\u7684\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0AI\u534f\u4f5c\u7684\u53ef\u95ee\u8d23\u6027\u3002", "motivation": "\u5728\u591a\u4ee3\u7406AI\u7cfb\u7edf\u4e2d\uff0c\u5185\u5bb9\u4e0d\u65ad\u88ab\u4fee\u6539\uff0c\u96be\u4ee5\u8ffd\u8e2a\u8d21\u732e\u6765\u6e90\uff0c\u4e9f\u9700\u65b9\u6cd5\u6765\u63ed\u793a\u751f\u6210\u5386\u53f2\u4ee5\u652f\u6301\u900f\u660e\u6027\u3002", "method": "\u901a\u8fc7\u53cd\u9988\u5faa\u73af\uff0c\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u66f4\u65b0\u5e26\u65f6\u95f4\u6233\u7684\u7b26\u53f7\u7f16\u5e74\u53f2\uff0c\u5e76\u4e0e\u5408\u6210\u5185\u5bb9\u540c\u6b65\uff0c\u65e0\u9700\u4f9d\u8d56\u5185\u90e8\u8bb0\u5fc6\u6216\u5916\u90e8\u5143\u4fe1\u606f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7c7b\u4f3c\u6cd5\u533b\u94fe custody \u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u5185\u5bb9\u672c\u8eab\u91cd\u5efa\u751f\u6210\u5386\u53f2\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63a8\u52a8\u4e86\u5728\u6f14\u8fdb\u7f51\u7edc\u751f\u6001\u4e2d\u5b9e\u73b0\u53ef\u95ee\u8d23\u534f\u4f5cAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.12952", "pdf": "https://arxiv.org/pdf/2504.12952", "abs": "https://arxiv.org/abs/2504.12952", "authors": ["Jan Drgona", "Truong X. Nghiem", "Thomas Beckers", "Mahyar Fazlyab", "Enrique Mallada", "Colin Jones", "Draguna Vrabie", "Steven L. Brunton", "Rolf Findeisen"], "title": "Safe Physics-Informed Machine Learning for Dynamics and Control", "categories": ["eess.SY", "cs.SY"], "comment": null, "summary": "This tutorial paper focuses on safe physics-informed machine learning in the\ncontext of dynamics and control, providing a comprehensive overview of how to\nintegrate physical models and safety guarantees. As machine learning techniques\nenhance the modeling and control of complex dynamical systems, ensuring safety\nand stability remains a critical challenge, especially in safety-critical\napplications like autonomous vehicles, robotics, medical decision-making, and\nenergy systems. We explore various approaches for embedding and ensuring safety\nconstraints, such as structural priors, Lyapunov functions, Control Barrier\nFunctions, predictive control, projections, and robust optimization techniques,\nensuring that the learned models respect stability and safety criteria.\nAdditionally, we delve into methods for uncertainty quantification and safety\nverification, including reachability analysis and neural network verification\ntools, which help validate that control policies remain within safe operating\nbounds even in uncertain environments. The paper includes illustrative examples\ndemonstrating the implementation aspects of safe learning frameworks that\ncombine the strengths of data-driven approaches with the rigor of physical\nprinciples, offering a path toward the safe control of complex dynamical\nsystems.", "AI": {"tldr": "\u8fd9\u7bc7\u6559\u7a0b\u8bba\u6587\u6982\u8ff0\u4e86\u5728\u52a8\u529b\u5b66\u548c\u63a7\u5236\u4e2d\u5b89\u5168\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6574\u5408\u7269\u7406\u6a21\u578b\u4e0e\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u63d0\u5347\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\uff0c\u4f46\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u662f\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u548c\u80fd\u6e90\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5148\u9a8c\u3001Lyapunov\u51fd\u6570\u3001\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u3001\u9884\u6d4b\u63a7\u5236\u3001\u6295\u5f71\u3001\u9c81\u68d2\u4f18\u5316\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u53ef\u8fbe\u6027\u5206\u6790\u548c\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u7b49\u65b9\u6cd5\uff0c\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u6f14\u793a\u5b89\u5168\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u539f\u5219\uff0c\u63d0\u4f9b\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5b89\u5168\u63a7\u5236\u8def\u5f84\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u5168\u9762\u6982\u8ff0\uff0c\u5e2e\u52a9\u9a8c\u8bc1\u63a7\u5236\u7b56\u7565\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fb9\u754c\u3002"}}
{"id": "2504.12446", "pdf": "https://arxiv.org/pdf/2504.12446", "abs": "https://arxiv.org/abs/2504.12446", "authors": ["Sebastian Seidel", "Uwe M. Borghoff"], "title": "Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 19 figures", "summary": "Artificial intelligence (AI) has emerged as a transformative force across\nindustries, driven by advances in deep learning and natural language\nprocessing, and fueled by large-scale data and computing resources. Despite its\nrapid adoption, the opacity of AI systems poses significant challenges to trust\nand acceptance.\n  This work explores the intersection of connectionist and symbolic approaches\nto artificial intelligence, focusing on the derivation of interpretable\nsymbolic models, such as decision trees, from feedforward neural networks\n(FNNs). Decision trees provide a transparent framework for elucidating the\noperations of neural networks while preserving their functionality. The\nderivation is presented in a step-by-step approach and illustrated with several\nexamples. A systematic methodology is proposed to bridge neural and symbolic\nparadigms by exploiting distributed representations in FNNs to identify\nsymbolic components, including fillers, roles, and their interrelationships.\nThe process traces neuron activation values and input configurations across\nnetwork layers, mapping activations and their underlying inputs to decision\ntree edges. The resulting symbolic structures effectively capture FNN decision\nprocesses and enable scalability to deeper networks through iterative\nrefinement of subpaths for each hidden layer.\n  To validate the theoretical framework, a prototype was developed using Keras\n.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This\nprototype demonstrates the feasibility of extracting symbolic representations\nfrom neural networks, enhancing trust in AI systems, and promoting\naccountability.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4ece\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4e2d\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u7684\u900f\u660e\u5ea6\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684 opacity \u5bfc\u81f4\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u8fde\u63a5\u4e3b\u4e49\u548c\u7b26\u53f7\u4e3b\u4e49\u65b9\u6cd5\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u795e\u7ecf\u5143\u6fc0\u6d3b\u503c\u548c\u8f93\u5165\u914d\u7f6e\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u6620\u5c04\u5230\u51b3\u7b56\u6811\u7ed3\u6784\u3002", "result": "\u5f00\u53d1\u4e86\u539f\u578b\uff0c\u4f7f\u7528 Keras \u548c Java \u73af\u5883\uff0c\u8bc1\u660e\u4e86\u63d0\u53d6\u7b26\u53f7\u8868\u793a\u7684\u53ef\u884c\u6027\uff0c\u5e76\u589e\u5f3a\u4e86 AI \u7cfb\u7edf\u7684\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6865\u63a5\u4e86\u795e\u7ecf\u548c\u7b26\u53f7\u8303\u5f0f\uff0c\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u7684\u8d23\u4efb\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.12680", "pdf": "https://arxiv.org/pdf/2504.12680", "abs": "https://arxiv.org/abs/2504.12680", "authors": ["Baining Zhao", "Ziyou Wang", "Jianjie Fang", "Chen Gao", "Fanhang Man", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li", "Wenwu Zhu"], "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning", "categories": ["cs.AI", "cs.CV"], "comment": "12 pages, 5 figures", "summary": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.", "AI": {"tldr": "This paper introduces Embodied-R, a framework combining VLMs and LMs with RL for efficient spatial reasoning, achieving SOTA performance with limited data.", "motivation": "To explore how pretrained models acquire spatial reasoning abilities and develop a resource-efficient method for high-level reasoning from visual observations.", "method": "Embodied-R uses large-scale VLMs for perception and small-scale LMs for reasoning, trained with RL and a reward system focused on logical consistency.", "result": "After training on 5k samples, it matches SOTA models like OpenAI-o1 and Gemini-2.5-pro on spatial reasoning tasks and exhibits emergent thinking patterns.", "conclusion": "The approach demonstrates effective slow-thinking capabilities with limited resources and provides insights into training strategies and model generalization."}}
{"id": "2504.13056", "pdf": "https://arxiv.org/pdf/2504.13056", "abs": "https://arxiv.org/abs/2504.13056", "authors": ["L. Wan", "S. Smith", "Y. -J. Pan", "E. Witrant"], "title": "Adaptive Task Space Non-Singular Terminal Super-Twisting Sliding Mode Control of a 7-DOF Robotic Manipulator", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "10 pages, 8 figures", "summary": "This paper presents a new task-space Non-singular Terminal Super-Twisting\nSliding Mode (NT-STSM) controller with adaptive gains for robust trajectory\ntracking of a 7-DOF robotic manipulator. The proposed approach addresses the\nchallenges of chattering, unknown disturbances, and rotational motion tracking,\nmaking it suited for high-DOF manipulators in dexterous manipulation tasks. A\nrigorous boundedness proof is provided, offering gain selection guidelines for\npractical implementation. Simulations and hardware experiments with external\ndisturbances demonstrate the proposed controller's robust, accurate tracking\nwith reduced control effort under unknown disturbances compared to other\nNT-STSM and conventional controllers. The results demonstrated that the\nproposed NT-STSM controller mitigates chattering and instability in complex\nmotions, making it a viable solution for dexterous robotic manipulations and\nvarious industrial applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u81ea\u9002\u5e94\u589e\u76ca\u7684\u4efb\u52a1\u7a7a\u95f4\u975e\u5947\u5f02\u7ec8\u7aef\u8d85\u626d\u8f6c\u6ed1\u52a8\u6a21\u5f0f\u63a7\u5236\u5668\uff0c\u7528\u4e8e7-DOF\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u9c81\u68d2\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u6296\u52a8\u3001\u672a\u77e5\u5e72\u6270\u548c\u65cb\u8f6c\u8fd0\u52a8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u64cd\u7eb5\u5668\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u6296\u52a8\u3001\u672a\u77e5\u5e72\u6270\u548c\u65cb\u8f6c\u8fd0\u52a8\u8ddf\u8e2a\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u65b0\u7684NT-STSM\u63a7\u5236\u5668\u3001\u63d0\u4f9b\u4e25\u683c\u8fb9\u754c\u6027\u8bc1\u660e\u548c\u589e\u76ca\u9009\u62e9\u6307\u5357\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u63a7\u5236\u5668\u5728\u5916\u90e8\u5e72\u6270\u4e0b\u6bd4\u5176\u4ed6\u63a7\u5236\u5668\u66f4\u9c81\u68d2\u3001\u51c6\u786e\u8ddf\u8e2a\uff0c\u5e76\u51cf\u5c11\u63a7\u5236\u52aa\u529b\u548c\u6296\u52a8\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8be5\u63a7\u5236\u5668\u662f\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5de5\u4e1a\u5e94\u7528\u7684\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.12450", "pdf": "https://arxiv.org/pdf/2504.12450", "abs": "https://arxiv.org/abs/2504.12450", "authors": ["Ziqi Li", "Zhan Peng"], "title": "Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation", "categories": ["cs.LG", "econ.EM", "stat.ML"], "comment": null, "summary": "Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in\naccounting for spatial effects in statistical models. Can this extend to\nmachine learning? This paper examines the effectiveness of using Moran\nEigenvectors as additional spatial features in machine learning models. We\ngenerate synthetic datasets with known processes involving spatially varying\nand nonlinear effects across two different geometries. Moran Eigenvectors\ncalculated from different spatial weights matrices, with and without a priori\neigenvector selection, are tested. We assess the performance of popular machine\nlearning models, including Random Forests, LightGBM, XGBoost, and TabNet, and\nbenchmark their accuracies in terms of cross-validated R2 values against models\nthat use only coordinates as features. We also extract coefficients and\nfunctions from the models using GeoShapley and compare them with the true\nprocesses. Results show that machine learning models using only location\ncoordinates achieve better accuracies than eigenvector-based approaches across\nvarious experiments and datasets. Furthermore, we discuss that while these\nfindings are relevant for spatial processes that exhibit positive spatial\nautocorrelation, they do not necessarily apply when modeling network\nautocorrelation and cases with negative spatial autocorrelation, where Moran\nEigenvectors would still be useful.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u4f4d\u7f6e\u5750\u6807\u6bd4\u4f7f\u7528Moran\u7279\u5f81\u5411\u91cf\u66f4\u51c6\u786e\uff0c\u5c24\u5176\u5728\u6b63\u7a7a\u95f4\u81ea\u76f8\u5173\u60c5\u51b5\u4e0b\u3002", "motivation": "\u63a2\u8ba8Moran\u7279\u5f81\u5411\u91cf\u662f\u5426\u80fd\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u7a7a\u95f4\u7279\u5f81\u6765\u5904\u7406\u7a7a\u95f4\u6548\u5e94\u3002", "method": "\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e0d\u540c\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\u7684Moran\u7279\u5f81\u5411\u91cf\uff0c\u6d4b\u8bd5\u968f\u673a\u68ee\u6797\u3001LightGBM\u7b49\u6a21\u578b\uff0c\u5e76\u4e0e\u4ec5\u7528\u5750\u6807\u7684\u6a21\u578b\u6bd4\u8f83\uff0c\u4f7f\u7528GeoShapley\u89e3\u91ca\u3002", "result": "\u4f7f\u7528\u5750\u6807\u7684\u6a21\u578b\u51c6\u786e\u6027\u66f4\u9ad8\uff1b\u53d1\u73b0\u9002\u7528\u4e8e\u6b63\u7a7a\u95f4\u81ea\u76f8\u5173\uff0c\u4f46\u4e0d\u4e00\u5b9a\u9002\u7528\u4e8e\u8d1f\u81ea\u76f8\u5173\u6216\u7f51\u7edc\u81ea\u76f8\u5173\u3002", "conclusion": "Moran\u7279\u5f81\u5411\u91cf\u5728\u67d0\u4e9b\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u53ef\u80fd\u4e0d\u5982\u9884\u671f\u6709\u6548\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u5982\u8d1f\u81ea\u76f8\u5173\u65f6\u4ecd\u6709\u7528\u5904\u3002"}}
{"id": "2504.12682", "pdf": "https://arxiv.org/pdf/2504.12682", "abs": "https://arxiv.org/abs/2504.12682", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faWebLists\u57fa\u51c6\u6d4b\u8bd5\u548cBardeenAgent\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7f51\u9875\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u4ee3\u7406\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bfc\u822a\u548c\u4ea4\u6613\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faBardeenAgent\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u6267\u884c\u8f6c\u6362\u4e3a\u53ef\u91cd\u590d\u7a0b\u5e8f\uff0c\u5229\u7528HTML\u7ed3\u6784\u6784\u5efaCSS\u9009\u62e9\u5668\u6765\u63d0\u53d6\u6570\u636e\u3002", "result": "\u5728WebLists\u57fa\u51c6\u4e0a\uff0cBardeenAgent\u8fbe\u523066%\u7684\u53ec\u56de\u7387\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u7ffb\u500d\uff0c\u5e76\u964d\u4f4e\u6210\u672c3\u500d\u3002", "conclusion": "BardeenAgent\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2504.12428", "pdf": "https://arxiv.org/pdf/2504.12428", "abs": "https://arxiv.org/abs/2504.12428", "authors": ["Adri\u00e0 Momp\u00f3 Alepuz", "Dimitrios Papageorgiou", "Silvia Tolu"], "title": "Learning-based Delay Compensation for Enhanced Control of Assistive Soft Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Soft robots are increasingly used in healthcare, especially for assistive\ncare, due to their inherent safety and adaptability. Controlling soft robots is\nchallenging due to their nonlinear dynamics and the presence of time delays,\nespecially in applications like a soft robotic arm for patient care. This paper\npresents a learning-based approach to approximate the nonlinear state predictor\n(Smith Predictor), aiming to improve tracking performance in a two-module soft\nrobot arm with a short inherent input delay. The method uses Kernel Recursive\nLeast Squares Tracker (KRLST) for online learning of the system dynamics and a\nLegendre Delay Network (LDN) to compress past input history for efficient delay\ncompensation. Experimental results demonstrate significant improvement in\ntracking performance compared to a baseline model-based non-linear controller.\nStatistical analysis confirms the significance of the improvements. The method\nis computationally efficient and adaptable online, making it suitable for\nreal-world scenarios and highlighting its potential for enabling safer and more\naccurate control of soft robots in assistive care applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684Smith Predictor\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u8f6f\u673a\u5668\u4eba\u81c2\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u548c\u65f6\u95f4\u5ef6\u8fdf\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0c\u7531\u4e8e\u975e\u7ebf\u6027\u52a8\u6001\u548c\u65f6\u95f4\u5ef6\u8fdf\uff0c\u5c24\u5176\u662f\u8f85\u52a9\u62a4\u7406\u5e94\u7528\u3002", "method": "\u4f7f\u7528Kernel Recursive Least Squares Tracker (KRLST)\u5728\u7ebf\u5b66\u4e60\u7cfb\u7edf\u52a8\u6001\uff0c\u5e76\u91c7\u7528Legendre Delay Network (LDN)\u538b\u7f29\u8f93\u5165\u5386\u53f2\u4ee5\u8865\u507f\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8ddf\u8e2a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e0e\u57fa\u7ebf\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u7edf\u8ba1\u5206\u6790\u8bc1\u5b9e\u6539\u8fdb\u7684\u663e\u8457\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u3001\u53ef\u5728\u7ebf\u9002\u5e94\uff0c\u9002\u5408\u771f\u5b9e\u573a\u666f\uff0c\u63d0\u5347\u8f6f\u673a\u5668\u4eba\u8f85\u52a9\u62a4\u7406\u4e2d\u7684\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.12458", "pdf": "https://arxiv.org/pdf/2504.12458", "abs": "https://arxiv.org/abs/2504.12458", "authors": ["Jansen S. B. Pereira", "Giovani Valdrighi", "Marcos Medeiros Raimundo"], "title": "M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness", "categories": ["cs.LG"], "comment": "17 pages, 7 figures", "summary": "In recent years, fairness in machine learning has emerged as a critical\nconcern to ensure that developed and deployed predictive models do not have\ndisadvantageous predictions for marginalized groups. It is essential to\nmitigate discrimination against individuals based on protected attributes such\nas gender and race. In this work, we consider applying subgroup justice\nconcepts to gradient-boosting machines designed for supervised learning\nproblems. Our approach expanded gradient-boosting methodologies to explore a\nbroader range of objective functions, which combines conventional losses such\nas the ones from classification and regression and a min-max fairness term. We\nstudy relevant theoretical properties of the solution of the min-max\noptimization problem. The optimization process explored the primal-dual\nproblems at each boosting round. This generic framework can be adapted to\ndiverse fairness concepts. The proposed min-max primal-dual gradient boosting\nalgorithm was theoretically shown to converge under mild conditions and\nempirically shown to be a powerful and flexible approach to address binary and\nsubgroup fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdmin-max primal-dual\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u76ee\u6807\u51fd\u6570\u4e2d\u52a0\u5165\u516c\u5e73\u6027\u9879\u6765\u63d0\u5347\u673a\u5668\u5b66\u4e60\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u516c\u5e73\u6027\u5df2\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u51cf\u8f7b\u57fa\u4e8e\u6027\u522b\u3001\u79cd\u65cf\u7b49\u4fdd\u62a4\u5c5e\u6027\u7684\u6b67\u89c6\uff0c\u4ee5\u4fdd\u62a4\u8fb9\u7f18\u5316\u7fa4\u4f53\u3002", "method": "\u6269\u5c55\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\uff0c\u5c06\u5e38\u89c4\u635f\u5931\u4e0emin-max\u516c\u5e73\u6027\u9879\u7ed3\u5408\uff0c\u5728\u6bcf\u4e2a\u63d0\u5347\u8f6e\u6b21\u4f7f\u7528\u539f\u59cb-\u5bf9\u5076\u4f18\u5316\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u6536\u655b\uff0c\u5e76\u5b9e\u8bc1\u4e0a\u663e\u793a\u51fa\u5728\u4e8c\u5143\u548c\u5b50\u7fa4\u516c\u5e73\u6027\u65b9\u9762\u7684\u5f3a\u5927\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u9002\u5e94\u5404\u79cd\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u662f\u5904\u7406\u76d1\u7763\u5b66\u4e60\u4e2d\u516c\u5e73\u6027\u7684\u6709\u529b\u65b9\u6cd5\u3002"}}
{"id": "2504.13032", "pdf": "https://arxiv.org/pdf/2504.13032", "abs": "https://arxiv.org/abs/2504.13032", "authors": ["Zheng Wang", "Shu Xian Teo", "Jun Jie Chew", "Wei Shi"], "title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning", "categories": ["cs.AI", "cs.IR"], "comment": "This paper has been accepted by SIGIR 2025", "summary": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInstructRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u591a\u4ee3\u7406\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u89c4\u5212\u6027\u80fd\uff0c\u5728\u5b9e\u9a8c\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u6539\u5584\u9ad8\u8fbe19.2%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56TAO\u8fc7\u7a0b\uff0c\u4f46LLM\u5bf9\u590d\u6742\u4efb\u52a1\u77e5\u8bc6\u6709\u9650\uff1b\u672c\u6587\u8bc6\u522b\u51fa\u5e94\u7528RAG\u5230\u4efb\u52a1\u89c4\u5212\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1aenlargability\u548ctransferability\u3002", "method": "\u63d0\u51faInstructRAG\uff0c\u5305\u62ec\u4e00\u4e2a\u56fe\u7ec4\u7ec7\u8fc7\u53bb\u7684\u6307\u4ee4\u8def\u5f84\u3001RL-Agent\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u56fe\u8986\u76d6\u4ee5\u63d0\u9ad8enlargability\u3001ML-Agent\u4f7f\u7528\u5143\u5b66\u4e60\u63d0\u9ad8\u4efb\u52a1\u6cdb\u5316\u4ee5\u6539\u5584transferability\uff0c\u4e8c\u8005\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u5e38\u7528\u4efb\u52a1\u89c4\u5212\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0cInstructRAG\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u9ad8\u6548\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u6700\u9ad8\u6539\u558419.2%\u3002", "conclusion": "InstructRAG\u6709\u6548\u89e3\u51b3\u4e86\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u89c4\u5212\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.12441", "pdf": "https://arxiv.org/pdf/2504.12441", "abs": "https://arxiv.org/abs/2504.12441", "authors": ["Asutay Ozmen", "Jo\u00e3o P. Hespanha", "Katie Byl"], "title": "Learning Transferable Friction Models and LuGre Identification via Physics Informed Neural Networks", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "7 pages, 8 figures, Submitted to 2025 64th IEEE Conference on\n  Decision and Control (CDC)", "summary": "Accurately modeling friction in robotics remains a core challenge, as\nrobotics simulators like Mujoco and PyBullet use simplified friction models or\nheuristics to balance computational efficiency with accuracy, where these\nsimplifications and approximations can lead to substantial differences between\nsimulated and physical performance. In this paper, we present a\nphysics-informed friction estimation framework that enables the integration of\nwell-established friction models with learnable components-requiring only\nminimal, generic measurement data. Our approach enforces physical consistency\nyet retains the flexibility to adapt to real-world complexities. We\ndemonstrate, on an underactuated and nonlinear system, that the learned\nfriction models, trained solely on small and noisy datasets, accurately\nsimulate dynamic friction properties and reduce the sim-to-real gap. Crucially,\nwe show that our approach enables the learned models to be transferable to\nsystems they are not trained on. This ability to generalize across multiple\nsystems streamlines friction modeling for complex, underactuated tasks,\noffering a scalable and interpretable path toward bridging the sim-to-real gap\nin robotics and control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u6469\u64e6\u4f30\u8ba1\u6846\u67b6\uff0c\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u5b66\u4e60\u6a21\u578b\uff0c\u51cf\u5c11\u673a\u5668\u4eba\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u5e76\u5b9e\u73b0\u6a21\u578b\u8f6c\u79fb\u3002", "motivation": "\u673a\u5668\u4eba\u6a21\u62df\u4e2d\u6469\u64e6\u5efa\u6a21\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u7b80\u5316\u6a21\u578b\u5bfc\u81f4\u6a21\u62df\u4e0e\u5b9e\u9645\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u6574\u5408\u5df2\u5efa\u7acb\u6469\u64e6\u6a21\u578b\u4e0e\u53ef\u5b66\u4e60\u7ec4\u4ef6\u7684\u6846\u67b6\uff0c\u53ea\u9700\u6700\u5c0f\u901a\u7528\u6d4b\u91cf\u6570\u636e\uff0c\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u5e76\u9002\u5e94\u73b0\u5b9e\u590d\u6742\u6027\u3002", "result": "\u5728\u6b20\u9a71\u52a8\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u4f7f\u7528\u5c0f\u566a\u58f0\u6570\u636e\u96c6\u51c6\u786e\u6a21\u62df\u52a8\u6001\u6469\u64e6\uff0c\u7f29\u5c0f\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\uff0c\u5e76\u53ef\u8f6c\u79fb\u81f3\u672a\u8bad\u7ec3\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u6865\u63a5\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\u3002"}}
{"id": "2504.12463", "pdf": "https://arxiv.org/pdf/2504.12463", "abs": "https://arxiv.org/abs/2504.12463", "authors": ["Ashwinee Panda", "Vatsal Baherwani", "Zain Sarwar", "Benjamin Therien", "Supriyo Chakraborty", "Tom Goldstein"], "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faDefault MoE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u4e13\u5bb6\u8f93\u51fa\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u4f5c\u4e3a\u9ed8\u8ba4\u8f93\u51fa\uff0c\u63d0\u4f9b\u5bc6\u96c6\u68af\u5ea6\u66f4\u65b0\uff0c\u63d0\u9ad8Mixture of Experts\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u6807\u51c6TopK\u8def\u7531\u3002", "motivation": "Mixture of Experts\u9884\u8bad\u7ec3\u6bd4\u5bc6\u96c6Transformer\u66f4\u53ef\u6269\u5c55\uff0c\u4f46\u7531\u4e8e\u7a00\u758f\u53cd\u5411\u66f4\u65b0\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "Default MoE\u901a\u8fc7\u7528\u4e13\u5bb6\u8f93\u51fa\u5386\u53f2\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u9ed8\u8ba4\u8f93\u51fa\u66ff\u6362\u7f3a\u5931\u7684\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u4f7f\u8def\u7531\u5668\u4e3a\u6bcf\u4e2a\u6807\u8bb0\u83b7\u5f97\u6bcf\u4e2a\u4e13\u5bb6\u7684\u4fe1\u53f7\u3002", "result": "Default MoE\u5728\u5404\u79cd\u8bbe\u7f6e\u4e2d\u663e\u8457\u6539\u5584\u8bad\u7ec3\u6027\u80fd\uff0c\u4f18\u4e8e\u6807\u51c6TopK\u8def\u7531\uff0c\u4e14\u4e0d\u589e\u52a0\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Default MoE\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86MoE\u8bad\u7ec3\u4e2d\u7684\u7a00\u758f\u66f4\u65b0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.13145", "pdf": "https://arxiv.org/pdf/2504.13145", "abs": "https://arxiv.org/abs/2504.13145", "authors": ["Li-Cheng Lan", "Andrew Bai", "Minhao Cheng", "Ruochen Wang", "Cho-Jui Hsieh", "Tianyi Zhou"], "title": "Exploring Expert Failures Improves LLM Agent Tuning", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEEF\u65b9\u6cd5\uff0c\u5229\u7528\u4e13\u5bb6\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684\u6709\u76ca\u884c\u52a8\u6539\u8fdbLLM\u4ee3\u7406\u5fae\u8c03\uff0c\u5728WebShop\u4e2d\u8fbe\u523062%\u7684\u80dc\u7387\uff0c\u8d85\u8d8aRFT\u548cGPT-4\u3002", "motivation": "RFT\u66f4\u9002\u5408\u7b80\u5355\u573a\u666f\uff0c\u590d\u6742\u5b50\u4efb\u52a1\u672a\u89e3\u51b3\uff0c\u4e13\u5bb6\u5931\u8d25\u8f68\u8ff9\u53ef\u63d0\u4f9b\u5b9d\u8d35\u6307\u5bfc\u3002", "method": "EEF\u4ece\u4e13\u5bb6\u5931\u8d25\u8f68\u8ff9\u4e2d\u8bc6\u522b\u6709\u76ca\u884c\u52a8\uff0c\u878d\u5165\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u6392\u9664\u6709\u5bb3\u884c\u52a8\u3002", "result": "\u5728WebShop\u4e2d\u80dc\u738762%\uff0c\u4f18\u4e8eRFT\uff0853.6%\uff09\u548cGPT-4\uff0835.6%\uff09\uff0c\u5728WebShop\u548cSciWorld\u4e2d\u8bbe\u5b9a\u65b0\u72b6\u6001\u3002", "conclusion": "EEF\u901a\u8fc7\u5229\u7528\u4e13\u5bb6\u5931\u8d25\uff0c\u6210\u529f\u89e3\u51b3\u4e4b\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u63d0\u5347\u4ee3\u7406\u8c03\u4f18\u6027\u80fd\u3002"}}
{"id": "2504.12512", "pdf": "https://arxiv.org/pdf/2504.12512", "abs": "https://arxiv.org/abs/2504.12512", "authors": ["Isabella Huang", "Richard Cheng", "Sangwoon Kim", "Dan Kruse", "Carolyn Matl", "Lukas Kaul", "JC Hancock", "Shanmuga Harikumar", "Mark Tjersland", "James Borders", "Dan Helmick"], "title": "Practical Insights on Grasp Strategies for Mobile Manipulation in the Wild", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 8 figures, submitted to IROS 2025", "summary": "Mobile manipulation robots are continuously advancing, with their grasping\ncapabilities rapidly progressing. However, there are still significant gaps\npreventing state-of-the-art mobile manipulators from widespread real-world\ndeployments, including their ability to reliably grasp items in unstructured\nenvironments. To help bridge this gap, we developed SHOPPER, a mobile\nmanipulation robot platform designed to push the boundaries of reliable and\ngeneralizable grasp strategies. We develop these grasp strategies and deploy\nthem in a real-world grocery store -- an exceptionally challenging setting\nchosen for its vast diversity of manipulable items, fixtures, and layouts. In\nthis work, we present our detailed approach to designing general grasp\nstrategies towards picking any item in a real grocery store. Additionally, we\nprovide an in-depth analysis of our latest real-world field test, discussing\nkey findings related to fundamental failure modes over hundreds of distinct\npick attempts. Through our detailed analysis, we aim to offer valuable\npractical insights and identify key grasping challenges, which can guide the\nrobotics community towards pressing open problems in the field.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f00\u53d1\u4e86SHOPPER\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7528\u4e8e\u5728\u6742\u8d27\u5e97\u7b49\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u6293\u53d6\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u5730\u6d4b\u8bd5\u5206\u6790\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u79fb\u52a8\u64cd\u7eb5\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6293\u53d6\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6865\u63a5\u8fd9\u4e00\u5dee\u8ddd\uff0c\u901a\u8fc7\u5728\u771f\u5b9e\u6742\u8d27\u5e97\u90e8\u7f72\u6765\u63a8\u52a8\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u901a\u7528\u7684\u6293\u53d6\u7b56\u7565\uff0c\u5e76\u5728\u771f\u5b9e\u6742\u8d27\u5e97\u73af\u5883\u4e2d\u90e8\u7f72\u548c\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u6570\u767e\u6b21\u6293\u53d6\u5c1d\u8bd5\u7684\u5b9e\u5730\u6d4b\u8bd5\uff0c\u8bc6\u522b\u5e76\u5206\u6790\u4e86\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u4f9b\u5b9e\u9645\u6d1e\u89c1\uff0c\u8bc6\u522b\u6293\u53d6\u6311\u6218\uff0c\u4ee5\u6307\u5bfc\u673a\u5668\u4eba\u793e\u533a\u89e3\u51b3\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2504.12465", "pdf": "https://arxiv.org/pdf/2504.12465", "abs": "https://arxiv.org/abs/2504.12465", "authors": ["Yuta Kambe", "Yota Maeda", "Tristan Vaccon"], "title": "Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation", "categories": ["cs.LG", "cs.SC", "math.AG", "stat.ML"], "comment": "19 pages", "summary": "The intersection of deep learning and symbolic mathematics has seen rapid\nprogress in recent years, exemplified by the work of Lample and Charton. They\ndemonstrated that effective training of machine learning models for solving\nmathematical problems critically depends on high-quality, domain-specific\ndatasets. In this paper, we address the computation of Gr\\\"obner basis using\nTransformers. While a dataset generation method tailored to Transformer-based\nGr\\\"obner basis computation has previously been proposed, it lacked theoretical\nguarantees regarding the generality or quality of the generated datasets. In\nthis work, we prove that datasets generated by the previously proposed\nalgorithm are sufficiently general, enabling one to ensure that Transformers\ncan learn a sufficiently diverse range of Gr\\\"obner bases. Moreover, we propose\nan extended and generalized algorithm to systematically construct datasets of\nideal generators, further enhancing the training effectiveness of Transformer.\nOur results provide a rigorous geometric foundation for Transformers to address\na mathematical problem, which is an answer to Lample and Charton's idea of\ntraining on diverse or representative inputs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u6539\u8fdb\u4e86Transformer\u7528\u4e8eGr\u00f6bner\u57fa\u8ba1\u7b97\u7684\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u4e4b\u524d\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u57fa\u4e8eLample\u548cCharton\u7684\u5de5\u4f5c\uff0c\u65e8\u5728\u4e3aTransformer\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u8bc1\u660e\u4e86\u4e4b\u524d\u7b97\u6cd5\u7684\u666e\u9002\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6269\u5c55\u7684\u7b97\u6cd5\u6765\u7cfb\u7edf\u6784\u5efa\u7406\u60f3\u751f\u6210\u5143\u7684\u591a\u6837\u6570\u636e\u96c6\u3002", "result": "\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u786e\u4fddTransformer\u80fd\u591f\u5b66\u4e60\u591a\u79cdGr\u00f6bner\u57fa\u3002", "conclusion": "\u4e3aTransformer\u5e94\u7528\u4e8e\u7b26\u53f7\u6570\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u56de\u5e94\u4e86Lample\u548cCharton\u5173\u4e8e\u591a\u6837\u5316\u8f93\u5165\u8bad\u7ec3\u7684\u60f3\u6cd5\u3002"}}
{"id": "2504.13146", "pdf": "https://arxiv.org/pdf/2504.13146", "abs": "https://arxiv.org/abs/2504.13146", "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"], "title": "Antidistillation Sampling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u53cd\u84b8\u998f\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6a21\u578b\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6982\u7387\u5206\u5e03\u6765\u6bd2\u5316\u63a8\u7406\u75d5\u8ff9\uff0c\u51cf\u5c11\u84b8\u998f\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6a21\u578b\u6240\u6709\u8005\u5e0c\u671b\u9632\u6b62\u4ed6\u4eba\u901a\u8fc7\u63a8\u7406\u75d5\u8ff9\u8fdb\u884c\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u84b8\u998f\u3002", "method": "\u6218\u7565\u6027\u5730\u4fee\u6539\u6a21\u578b\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6982\u7387\u5206\u5e03\uff0c\u4ee5\u6bd2\u5316\u63a8\u7406\u75d5\u8ff9\u3002", "result": "\u4f7f\u63a8\u7406\u75d5\u8ff9\u5728\u84b8\u998f\u4e2d\u53d8\u5f97\u663e\u8457\u4e0d\u6709\u6548\uff0c\u4f46\u4e0d\u5f71\u54cd\u6a21\u578b\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u53cd\u84b8\u998f\u91c7\u6837\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u4fdd\u62a4\u673a\u5236\uff0c\u9650\u5236\u4e86\u84b8\u998f\u7684\u53ef\u80fd\u3002"}}
{"id": "2504.12616", "pdf": "https://arxiv.org/pdf/2504.12616", "abs": "https://arxiv.org/abs/2504.12616", "authors": ["Farhad Nawaz", "Minjun Sung", "Darshan Gadginmath", "Jovin D'sa", "Sangjae Bae", "David Isele", "Nadia Figueroa", "Nikolai Matni", "Faizan M. Tariq"], "title": "Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Safe and efficient path planning in parking scenarios presents a significant\nchallenge due to the presence of cluttered environments filled with static and\ndynamic obstacles. To address this, we propose a novel and computationally\nefficient planning strategy that seamlessly integrates the predictions of\ndynamic obstacles into the planning process, ensuring the generation of\ncollision-free paths. Our approach builds upon the conventional Hybrid A star\nalgorithm by introducing a time-indexed variant that explicitly accounts for\nthe predictions of dynamic obstacles during node exploration in the graph, thus\nenabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A\nstar algorithm within an online planning framework to compute local paths at\neach planning step, guided by an adaptively chosen intermediate goal. The\nproposed method is validated in diverse parking scenarios, including\nperpendicular, angled, and parallel parking. Through simulations, we showcase\nour approach's potential in greatly improving the efficiency and safety when\ncompared to the state of the art spline-based planning method for parking\nsituations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u505c\u8f66\u573a\u666f\u4e2d\u5b89\u5168\u9ad8\u6548\u5730\u907f\u5f00\u52a8\u6001\u969c\u788d\u3002", "motivation": "\u505c\u8f66\u573a\u666f\u4e2d\u8def\u5f84\u89c4\u5212\u9762\u4e34\u6311\u6218\uff0c\u7531\u4e8e\u73af\u5883\u6742\u4e71\uff0c\u5b58\u5728\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\u3002", "method": "\u57fa\u4e8eHybrid A*\u7b97\u6cd5\u7684\u6539\u8fdb\uff0c\u5f15\u5165\u65f6\u95f4\u7d22\u5f15\u53d8\u4f53\uff0c\u5e76\u6574\u5408\u52a8\u6001\u969c\u788d\u9884\u6d4b\u5230\u5728\u7ebf\u89c4\u5212\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u505c\u8f66\u573a\u666f\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6837\u6761-based\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u505c\u8f66\u8def\u5f84\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2504.12471", "pdf": "https://arxiv.org/pdf/2504.12471", "abs": "https://arxiv.org/abs/2504.12471", "authors": ["Shiwei Ding", "Lan Zhang", "Zhenlin Wang", "Giuseppe Ateniese", "Xiaoyong Yuan"], "title": "You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models", "categories": ["cs.LG", "cs.DC", "cs.PF"], "comment": null, "summary": "Fine-tuning plays a crucial role in adapting models to downstream tasks with\nminimal training efforts. However, the rapidly increasing size of foundation\nmodels poses a daunting challenge for accommodating foundation model\nfine-tuning in most commercial devices, which often have limited memory\nbandwidth. Techniques like model sharding and tensor parallelism address this\nissue by distributing computation across multiple devices to meet memory\nrequirements. Nevertheless, these methods do not fully leverage their\nfoundation nature in facilitating the fine-tuning process, resulting in high\ncomputational costs and imbalanced workloads. We introduce a novel Distributed\nDynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations\nacross attention modules based on our observation that not all attention\nmodules are necessary for forward and backward propagation in fine-tuning\nfoundation models. Through three innovative selection strategies, D2FT\nsignificantly reduces the computational workload required for fine-tuning\nfoundation models. Furthermore, D2FT addresses workload imbalances in\ndistributed computing environments by optimizing these selection strategies via\nmultiple knapsack optimization. Our experimental results demonstrate that the\nproposed D2FT framework reduces the training computational costs by 40% and\ntraining communication costs by 50% with only 1% to 2% accuracy drops on the\nCIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show\nthat D2FT can be effectively extended to recent LoRA, a state-of-the-art\nparameter-efficient fine-tuning technique. By reducing 40% computational cost\nor 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on\nStanford Cars dataset.", "AI": {"tldr": "D2FT\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u64cd\u4f5c\u6ce8\u610f\u529b\u6a21\u5757\u51cf\u5c11\u5fae\u8c03\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5c3a\u5bf8\u589e\u5927\u5bfc\u81f4\u8bbe\u5907\u5185\u5b58\u4e0d\u8db3\uff0c\u73b0\u6709\u5206\u5e03\u5f0f\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002", "method": "\u5f15\u5165D2FT\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u9009\u62e9\u7b56\u7565\u4f18\u5316\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u901a\u8fc7\u591a\u80cc\u5305\u4f18\u5316\u89e3\u51b3\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8ba1\u7b97\u6210\u672c\u51cf\u5c1140%\u3001\u901a\u4fe1\u6210\u672c\u51cf\u5c1150%\uff0c\u51c6\u786e\u7387\u4e0b\u964d1%\u81f32%\uff1b\u5728LoRA\u4e0a\u6269\u5c55\uff0c\u51c6\u786e\u7387\u4e0b\u964d4%\u81f36%\u3002", "conclusion": "D2FT\u6846\u67b6\u6709\u6548\u964d\u4f4e\u5fae\u8c03\u6210\u672c\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u53c2\u6570\u9ad8\u6548\u6280\u672f\u5982LoRA\u3002"}}
{"id": "2504.13150", "pdf": "https://arxiv.org/pdf/2504.13150", "abs": "https://arxiv.org/abs/2504.13150", "authors": ["Krzysztof Pancerz", "Piotr Kulicki", "Micha\u0142 Kalisz", "Andrzej Burda", "Maciej Stanis\u0142awski", "Jaromir Sarzy\u0144ski"], "title": "Readable Twins of Unreadable Models", "categories": ["cs.AI", "cs.CV"], "comment": "Based on the abstract accepted for ISFS 2025", "summary": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u521b\u5efa\u53ef\u8bfb\u53cc\u80de\u80ce\uff08\u4e0d\u7cbe\u786e\u4fe1\u606f\u6d41\u6a21\u578b\uff09\u6765\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u4ee5MNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b\u4e3a\u4f8b\u3002", "motivation": "\u521b\u5efa\u8d1f\u8d23\u4efb\u7684AI\u7cfb\u7edf\u662f\u5f53\u524d\u7814\u7a76\u91cd\u70b9\uff0c\u53ef\u89e3\u91ca\u6027\u662f\u5176\u5173\u952e\u7279\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0d\u53ef\u8bfb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u6982\u5ff5\uff0c\u4ecb\u7ecd\u4ece\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5230\u4e0d\u7cbe\u786e\u4fe1\u606f\u6d41\u6a21\u578b\u7684\u5b8c\u6574\u8f6c\u6362\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7MNIST\u6570\u636e\u96c6\u793a\u4f8b\u8bf4\u660e\u3002", "result": "\u65b9\u6cd5\u88ab\u5e94\u7528\u4e8eMNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4fc3\u8fdb\u8d1f\u8d23\u4efbAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.12744", "pdf": "https://arxiv.org/pdf/2504.12744", "abs": "https://arxiv.org/abs/2504.12744", "authors": ["Sebastiano Taddei", "Mattia Piccinini", "Francesco Biral"], "title": "Biasing the Driving Style of an Artificial Race Driver for Online Time-Optimal Maneuver Planning", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "In this work, we present a novel approach to bias the driving style of an\nartificial race driver (ARD) for online time-optimal trajectory planning. Our\nmethod leverages a nonlinear model predictive control (MPC) framework that\ncombines time minimization with exit speed maximization at the end of the\nplanning horizon. We introduce a new MPC terminal cost formulation based on the\ntrajectory planned in the previous MPC step, enabling ARD to adapt its driving\nstyle from early to late apex maneuvers in real-time. Our approach is\ncomputationally efficient, allowing for low replan times and long planning\nhorizons. We validate our method through simulations, comparing the results\nagainst offline minimum-lap-time (MLT) optimal control and online minimum-time\nMPC solutions. The results demonstrate that our new terminal cost enables ARD\nto bias its driving style, and achieve online lap times close to the MLT\nsolution and faster than the minimum-time MPC solution. Our approach paves the\nway for a better understanding of the reasons behind human drivers' choice of\nearly or late apex maneuvers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bMPC\u65b9\u6cd5\uff0c\u7528\u4e8e\u504f\u7f6e\u4eba\u5de5\u8d5b\u8f66\u624b\u7684\u9a7e\u9a76\u98ce\u683c\uff0c\u5b9e\u73b0\u5728\u7ebf\u65f6\u95f4\u6700\u4f18\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u63a5\u8fd1\u6700\u4f18\u5708\u901f\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u9a7e\u9a76\u5458\u5728\u65e9\u9876\u70b9\u548c\u665a\u9876\u70b9 maneuvers \u4e2d\u7684\u9009\u62e9\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u5728\u7ebf\u8f68\u8ff9\u89c4\u5212\u3002", "method": "\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u6700\u5c0f\u5316\u548c\u51fa\u53e3\u901f\u5ea6\u6700\u5927\u5316\uff0c\u5f15\u5165\u57fa\u4e8e\u524d\u4e00MPC\u6b65\u9aa4\u7684\u65b0\u7ec8\u7aef\u6210\u672c\u516c\u5f0f\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u504f\u7f6e\u9a7e\u9a76\u98ce\u683c\uff0c\u5b9e\u73b0\u5728\u7ebf\u5708\u901f\u63a5\u8fd1\u6700\u4f18\u63a7\u5236\u65b9\u6848\uff0c\u5e76\u4f18\u4e8e\u6700\u5c0f\u65f6\u95f4MPC\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u4eba\u7c7b\u9a7e\u9a76\u5458\u65e9\u9876\u70b9\u6216\u665a\u9876\u70b9 maneuvers \u7684\u539f\u56e0\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.12501", "pdf": "https://arxiv.org/pdf/2504.12501", "abs": "https://arxiv.org/abs/2504.12501", "authors": ["Nathan Lambert"], "title": "Reinforcement Learning from Human Feedback", "categories": ["cs.LG"], "comment": "123 pages. Web-native version at https://rlhfbook.com/", "summary": "Reinforcement learning from human feedback (RLHF) has become an important\ntechnical and storytelling tool to deploy the latest machine learning systems.\nIn this book, we hope to give a gentle introduction to the core methods for\npeople with some level of quantitative background. The book starts with the\norigins of RLHF -- both in recent literature and in a convergence of disparate\nfields of science in economics, philosophy, and optimal control. We then set\nthe stage with definitions, problem formulation, data collection, and other\ncommon math used in the literature. The core of the book details every\noptimization stage in using RLHF, from starting with instruction tuning to\ntraining a reward model and finally all of rejection sampling, reinforcement\nlearning, and direct alignment algorithms. The book concludes with advanced\ntopics -- understudied research questions in synthetic data and evaluation --\nand open questions for the field.", "AI": {"tldr": "\u8fd9\u672c\u4e66\u6e29\u548c\u5730\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u8d77\u6e90\u3001\u5b9a\u4e49\u3001\u4f18\u5316\u9636\u6bb5\u548c\u9ad8\u7ea7\u4e3b\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u6709\u91cf\u5316\u80cc\u666f\u7684\u4eba\u63d0\u4f9bRLHF\u6838\u5fc3\u65b9\u6cd5\u7684\u6e29\u548c\u4ecb\u7ecd\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u7ecf\u6d4e\u5b66\u3001\u54f2\u5b66\u548c\u6700\u4f18\u63a7\u5236\u7b49\u9886\u57df\u7684\u8d77\u6e90\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4eceRLHF\u8d77\u6e90\u5f00\u59cb\uff0c\u8bbe\u7f6e\u5b9a\u4e49\u548c\u95ee\u9898\u8868\u8ff0\uff0c\u7136\u540e\u8be6\u7ec6\u8bf4\u660e\u4f18\u5316\u9636\u6bb5\uff0c\u5982\u6307\u4ee4\u8c03\u6574\u3001\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u3001\u62d2\u7edd\u91c7\u6837\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u76f4\u63a5\u5bf9\u9f50\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u662f\u63d0\u4f9b\u4e86\u5168\u9762\u7684RLHF\u6307\u5357\uff0c\u5305\u62ec\u9ad8\u7ea7\u4e3b\u9898\u548c\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u90e8\u5206\u8ba8\u8bba\u4e86\u5408\u6210\u6570\u636e\u548c\u8bc4\u4f30\u4e2d\u7684\u672a\u7814\u7a76\u95ee\u9898\u4ee5\u53ca\u9886\u57df\u7684\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2504.13171", "pdf": "https://arxiv.org/pdf/2504.13171", "abs": "https://arxiv.org/abs/2504.13171", "authors": ["Kevin Lin", "Charlie Snell", "Yu Wang", "Charles Packer", "Sarah Wooders", "Ion Stoica", "Joseph E. Gonzalez"], "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time", "categories": ["cs.AI", "cs.CL"], "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute", "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165\u7761\u7720\u65f6\u95f4\u8ba1\u7b97\uff08sleep-time compute\uff09\uff0c\u901a\u8fc7\u79bb\u7ebf\u9884\u8ba1\u7b97\u7528\u6237\u53ef\u80fd\u67e5\u8be2\u7684\u5185\u5bb9\uff0c\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u9700\u6c42\uff0c\u4ece\u800c\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3LLMs\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u5927\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u548c\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u56f0\u96be\u4efb\u52a1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4fee\u6539\u63a8\u7406\u4efb\u52a1\uff08\u5982Stateful GSM-Symbolic\u548cStateful AIME\uff09\uff0c\u4f7f\u7528\u7761\u7720\u65f6\u95f4\u8ba1\u7b97\u9884\u5148\u8ba1\u7b97\u6709\u7528\u91cf\uff0c\u5e76\u5f15\u5165Multi-Query GSM-Symbolic\u6765\u8de8\u76f8\u5173\u67e5\u8be2\u5206\u644a\u8ba1\u7b97\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u7761\u7720\u65f6\u95f4\u8ba1\u7b97\u53ef\u5c06\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\u51cf\u5c11\u7ea65\u500d\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u6700\u591a13%\uff08Stateful GSM-Symbolic\uff09\u548c18%\uff08Stateful AIME\uff09\uff0c\u591a\u67e5\u8be2\u8bbe\u7f6e\u4e0b\u5e73\u5747\u6210\u672c\u6bcf\u67e5\u8be2\u51cf\u5c112.5\u500d\u3002", "conclusion": "\u7ed3\u8bba\u662f\u7761\u7720\u65f6\u95f4\u8ba1\u7b97\u5728\u67e5\u8be2\u53ef\u9884\u6d4b\u6027\u9ad8\u65f6\u6548\u679c\u6700\u4f73\uff0c\u5e76\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.12814", "pdf": "https://arxiv.org/pdf/2504.12814", "abs": "https://arxiv.org/abs/2504.12814", "authors": ["V. Cerone", "S. M. Fosson", "A. Re", "D. Regruto"], "title": "Integral control of the proximal gradient method for unbiased sparse optimization", "categories": ["math.OC", "cs.SY", "eess.SY"], "comment": null, "summary": "Proximal gradient methods are popular in sparse optimization as they are\nstraightforward to implement. Nevertheless, they achieve biased solutions,\nrequiring many iterations to converge. This work addresses these issues through\na suitable feedback control of the algorithm's hyperparameter. Specifically, by\ndesigning an integral control that does not substantially impact the\ncomputational complexity, we can reach an unbiased solution in a reasonable\nnumber of iterations. In the paper, we develop and analyze the convergence of\nthe proposed approach for strongly-convex problems. Moreover, numerical\nsimulations validate and extend the theoretical results to the non-strongly\nconvex framework.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u53cd\u9988\u63a7\u5236\u6539\u8fdb\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\uff0c\u51cf\u5c11\u504f\u5dee\u5e76\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\u5728\u7a00\u758f\u4f18\u5316\u4e2d\u6613\u5b9e\u73b0\uff0c\u4f46\u5b58\u5728\u504f\u5dee\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u79ef\u5206\u63a7\u5236\u8c03\u6574\u7b97\u6cd5\u8d85\u53c2\u6570\uff0c\u4ee5\u51cf\u5c11\u504f\u5dee\u800c\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4e3a\u5f3a\u51f8\u95ee\u9898\u8bc1\u660e\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u975e\u5f3a\u51f8\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u5728\u5408\u7406\u8fed\u4ee3\u6b21\u6570\u5185\u8fbe\u5230\u65e0\u504f\u5dee\u89e3\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u5e7f\u6846\u67b6\u3002"}}
{"id": "2504.12503", "pdf": "https://arxiv.org/pdf/2504.12503", "abs": "https://arxiv.org/abs/2504.12503", "authors": ["Kaira M. Samuel", "Faez Ahmed"], "title": "Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Engineering problems that apply machine learning often involve\ncomputationally intensive methods but rely on limited datasets. As engineering\ndata evolves with new designs and constraints, models must incorporate new\nknowledge over time. However, high computational costs make retraining models\nfrom scratch infeasible. Continual learning (CL) offers a promising solution by\nenabling models to learn from sequential data while mitigating catastrophic\nforgetting, where a model forgets previously learned mappings. This work\nintroduces CL to engineering design by benchmarking several CL methods on\nrepresentative regression tasks. We apply these strategies to five engineering\ndatasets and construct nine new engineering CL benchmarks to evaluate their\nability to address forgetting and improve generalization. Preliminary results\nshow that applying existing CL methods to these tasks improves performance over\nnaive baselines. In particular, the Replay strategy achieved performance\ncomparable to retraining in several benchmarks while reducing training time by\nnearly half, demonstrating its potential for real-world engineering workflows.\nThe code and datasets used in this work will be available at:\nhttps://github.com/kmsamuel/cl-for-engineering-release.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u6301\u7eed\u5b66\u4e60\u5230\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5176\u5728\u5904\u7406\u6570\u636e\u6f14\u53d8\u65f6\u7684\u4f18\u52bf\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5de5\u7a0b\u673a\u5668\u5b66\u4e60\u95ee\u9898\u6d89\u53ca\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u6570\u636e\u96c6\uff0c\u6570\u636e\u6f14\u53d8\u9700\u8981\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4ee5\u907f\u514d\u9057\u5fd8\u5148\u524d\u77e5\u8bc6\u3002", "method": "\u57fa\u51c6\u6d4b\u8bd5\u51e0\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e8e\u5de5\u7a0b\u56de\u5f52\u4efb\u52a1\uff0c\u5e94\u7528\u4e8e\u4e94\u4e2a\u6570\u636e\u96c6\u5e76\u6784\u5efa\u4e5d\u4e2a\u65b0\u57fa\u51c6\u3002", "result": "\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u6539\u5584\u4e86\u6027\u80fd\uff0cReplay\u7b56\u7565\u5728\u591a\u4e2a\u57fa\u51c6\u4e2d\u4e0e\u91cd\u65b0\u8bad\u7ec3\u76f8\u5f53\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u8fd1\u4e00\u534a\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u5728\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2504.12309", "pdf": "https://arxiv.org/pdf/2504.12309", "abs": "https://arxiv.org/abs/2504.12309", "authors": ["Yi-De Lin", "Guan-Ze Liao"], "title": "Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "From 2000 to 2015, the UN's Millennium Development Goals guided global\npriorities. The subsequent Sustainable Development Goals (SDGs) adopted a more\ndynamic approach, with annual indicator updates. As 2030 nears and progress\nlags, innovative acceleration strategies are critical. This study develops an\nAI-powered knowledge graph system to analyze SDG interconnections, discover\npotential new goals, and visualize them online. Using official SDG texts,\nElsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot\non 269 talks from 2023 applies AI-speculative design, large language models,\nand retrieval-augmented generation. Key findings include: (1) Heatmap analysis\nreveals strong associations between Goal 10 and Goal 16, and minimal coverage\nof Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new\ncentral nodes, showing how richer data supports divergent thinking and goal\nclarity. (3) Six potential new goals are proposed, centered on equity,\nresilience, and technology-driven inclusion. This speculative-AI framework\noffers fresh insights for policymakers and lays groundwork for future\nmultimodal and cross-system SDG applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528AI\u77e5\u8bc6\u56fe\u8c31\u5206\u6790\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08SDG\uff09\uff0c\u57fa\u4e8eTED Talk\u6570\u636e\u53d1\u73b0\u65b0\u5173\u8054\u5e76\u63d0\u51fa\u6f5c\u5728\u65b0\u76ee\u6807\u3002", "motivation": "\u7531\u4e8e2030\u5e74\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u8fdb\u5c55\u6ede\u540e\uff0c\u9700\u8981\u521b\u65b0\u7b56\u7565\u6765\u5206\u6790\u76ee\u6807\u95f4\u8fde\u63a5\u548c\u53d1\u73b0\u65b0\u76ee\u6807\u3002", "method": "\u5f00\u53d1AI\u77e5\u8bc6\u56fe\u8c31\u7cfb\u7edf\uff0c\u4f7f\u7528SDG\u6587\u672c\u3001\u5173\u952e\u8bcd\u6570\u636e\u96c6\u548cTED Talk transcripts\uff0c\u5e94\u7528AI\u63a8\u6d4b\u8bbe\u8ba1\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "result": "\u53d1\u73b0Goal 10\u548c16\u5f3a\u5173\u8054\u3001Goal 6\u8986\u76d6\u5c11\uff1b\u6a21\u62df\u5bf9\u8bdd\u63ed\u793a\u65b0\u4e2d\u5fc3\u8282\u70b9\uff1b\u63d0\u51fa\u516d\u4e2a\u65b0\u76ee\u6807\uff0c\u805a\u7126\u516c\u5e73\u3001\u97e7\u6027\u548c\u6280\u672f\u5305\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u6a21\u6001SDG\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.12889", "pdf": "https://arxiv.org/pdf/2504.12889", "abs": "https://arxiv.org/abs/2504.12889", "authors": ["Quan Zhou", "Jingjing Zhao", "Kaiquan Cai", "Yanbo Zhu"], "title": "RIS-Assisted Beamfocusing in Near-Field IoT Communication Systems: A Transformer-Based Approach", "categories": ["eess.SP", "cs.SY", "eess.SY"], "comment": null, "summary": "The massive number of antennas in extremely large aperture array (ELAA)\nsystems shifts the propagation regime of signals in internet of things (IoT)\ncommunication systems towards near-field spherical wave propagation. We propose\na reconfigurable intelligent surfaces (RIS)-assisted beamfocusing mechanism,\nwhere the design of the two-dimensional beam codebook that contains both the\nangular and distance domains is challenging. To address this issue, we\nintroduce a novel Transformer-based two-stage beam training algorithm, which\nincludes the coarse and fine search phases. The proposed mechanism provides a\nfine-grained codebook with enhanced spatial resolution, enabling precise\nbeamfocusing. Specifically, in the first stage, the beam training is performed\nto estimate the approximate location of the device by using a simple codebook,\ndetermining whether it is within the beamfocusing range (BFR) or the\nnone-beamfocusing range (NBFR). In the second stage, by using a more precise\ncodebook, a fine-grained beam search strategy is conducted. Experimental\nresults unveil that the precision of the RIS-assisted beamfocusing is greatly\nimproved. The proposed method achieves beam selection accuracy up to 97% at\nsignal-to-noise ratio (SNR) of 20 dB, and improves 10% to 50% over the baseline\nmethod at different SNRs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u7684\u6ce2\u675f\u805a\u7126\u673a\u5236\uff0c\u4f7f\u7528Transformer-based\u7684\u4e24\u9636\u6bb5\u6ce2\u675f\u8bad\u7ec3\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6781\u5927\u89c4\u6a21\u5b54\u5f84\u9635\u5217\uff08ELAA\uff09\u7cfb\u7edf\u4e2d\u7684\u7cbe\u5ea6\u3002", "motivation": "ELAA\u7cfb\u7edf\u4e2d\u5929\u7ebf\u6570\u91cf\u5de8\u5927\u5bfc\u81f4\u4fe1\u53f7\u4f20\u64ad\u8f6c\u5411\u8fd1\u573a\u7403\u6ce2\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u89d2\u5ea6\u548c\u8ddd\u79bb\u57df\u7684\u6ce2\u675f\u8bbe\u8ba1\u3002", "method": "\u5f15\u5165Transformer-based\u7684\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u7b80\u5355\u7801\u672c\u4f30\u7b97\u8bbe\u5907\u4f4d\u7f6e\uff0c\u5224\u65ad\u6ce2\u675f\u805a\u7126\u8303\u56f4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u7cbe\u786e\u7801\u672c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u641c\u7d22\u3002", "result": "\u5728SNR 20dB\u65f6\u51c6\u786e\u7387\u8fbe97%\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad810%\u81f350%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RIS\u8f85\u52a9\u6ce2\u675f\u805a\u7126\u7684\u7cbe\u5ea6\uff0c\u6539\u5584\u4e86\u7269\u8054\u7f51\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2504.12526", "pdf": "https://arxiv.org/pdf/2504.12526", "abs": "https://arxiv.org/abs/2504.12526", "authors": ["Junyang Zhang", "Tianyi Zhu", "Cheng Luo", "Anima Anandkumar"], "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Submitted to COLM", "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faMOM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u5206\u533a\u548cKV\u7f13\u5b58\u5378\u8f7d\uff0c\u663e\u8457\u51cf\u5c11\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u9ad8GPU\u5185\u5b58\u9700\u6c42\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\u3002", "method": "\u63d0\u51faMOM\u65b9\u6cd5\uff0c\u5c06\u5173\u952e\u5c42\u5206\u533a\u4e3a\u8f83\u5c0f\u7684'\u8ff7\u4f60\u5e8f\u5217'\uff0c\u5e76\u4e0eKV\u7f13\u5b58\u5378\u8f7d\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMOM\u5e73\u5747\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u8d85\u8fc750%\uff0c\u5c06\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u81f3\u539f\u6765\u7684\u8fd1\u4e09\u500d\uff0c\u4fdd\u6301\u51c6\u786e\u6027\u548c\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664prefill\u5185\u5b58\u74f6\u9888\uff0c\u6539\u53d8\u7814\u7a76\u4f18\u5148\u7ea7\uff0c\u8f6c\u5411\u4f18\u5316decode\u9636\u6bb5KV\u7f13\u5b58\u6548\u7387\u3002"}}
{"id": "2504.12314", "pdf": "https://arxiv.org/pdf/2504.12314", "abs": "https://arxiv.org/abs/2504.12314", "authors": ["Hao Li", "Liuzhenghao Lv", "He Cao", "Zijing Liu", "Zhiyuan Yan", "Yu Wang", "Yonghong Tian", "Yu Li", "Li Yuan"], "title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large language models are increasingly used in scientific domains, especially\nfor molecular understanding and analysis. However, existing models are affected\nby hallucination issues, resulting in errors in drug design and utilization. In\nthis paper, we first analyze the sources of hallucination in LLMs for molecular\ncomprehension tasks, specifically the knowledge shortcut phenomenon observed in\nthe PubChem dataset. To evaluate hallucination in molecular comprehension tasks\nwith computational efficiency, we introduce \\textbf{Mol-Hallu}, a novel\nfree-form evaluation metric that quantifies the degree of hallucination based\non the scientific entailment relationship between generated text and actual\nmolecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze\nthe extent of hallucination in various LLMs performing molecular comprehension\ntasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is\nproposed to alleviate molecular hallucinations, Experiments show the\neffectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our\nfindings provide critical insights into mitigating hallucination and improving\nthe reliability of LLMs in scientific applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u5206\u6790LLM\u5728\u5206\u5b50\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5f15\u5165Mol-Hallu\u6307\u6807\u548cHRPP\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5206\u5b50\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5bfc\u81f4\u836f\u7269\u8bbe\u8ba1\u9519\u8bef\uff0c\u9700\u8981\u5206\u6790\u6765\u6e90\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165Mol-Hallu\u6307\u6807\u91cf\u5316\u5e7b\u89c9\uff0c\u5e76\u63d0\u51faHRPP\u540e\u5904\u7406\u9636\u6bb5\u6765\u7f13\u89e3\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHRPP\u5728\u5404\u79cd\u5206\u5b50LLM\u4e0a\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8LLM\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u89c1\u3002"}}
{"id": "2504.13088", "pdf": "https://arxiv.org/pdf/2504.13088", "abs": "https://arxiv.org/abs/2504.13088", "authors": ["Haonan He", "Yuheng Qiu", "Junyi Geng"], "title": "Imperative MPC: An End-to-End Self-Supervised Learning with Differentiable MPC for UAV Attitude Control", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "14 pages, 3 figures, accepted by L4DC 2025", "summary": "Modeling and control of nonlinear dynamics are critical in robotics,\nespecially in scenarios with unpredictable external influences and complex\ndynamics. Traditional cascaded modular control pipelines often yield suboptimal\nperformance due to conservative assumptions and tedious parameter tuning. Pure\ndata-driven approaches promise robust performance but suffer from low sample\nefficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid\nmethods combining learning-based and traditional model-based control in an\nend-to-end manner offer a promising alternative. This work presents a\nself-supervised learning framework combining learning-based inertial odometry\n(IO) module and differentiable model predictive control (d-MPC) for Unmanned\nAerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and\npredicts UAV attitudes, which are then optimized by MPC for control actions in\na bi-level optimization (BLO) setup, where the inner MPC optimizes control\nactions and the upper level minimizes discrepancy between real-world and\npredicted performance. The framework is thus end-to-end and can be trained in a\nself-supervised manner. This approach combines the strength of learning-based\nperception with the interpretable model-based control. Results show the\neffectiveness even under strong wind. It can simultaneously enhance both the\nMPC parameter learning and IMU prediction performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u53ef\u5fae\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u59ff\u6001\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u5728\u5f3a\u98ce\u7b49\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u6027\u80fd\u6b21\u4f18\u4e14\u8c03\u53c2\u7e41\u7410\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6837\u672c\u6548\u7387\u4f4e\u4e14\u4f9d\u8d56\u5927\u91cf\u6570\u636e\uff1b\u9700\u8981\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u5b66\u4e60\u548c\u6a21\u578b-based\u63a7\u5236\u3002", "method": "\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u4e60-based\u60ef\u6027\u91cc\u7a0b\u8ba1\u6a21\u5757\u548c\u53ef\u5fae\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4f18\u5316\u63a7\u5236\u52a8\u4f5c\u548c\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u5f3a\u98ce\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u53c2\u6570\u5b66\u4e60\u548cIMU\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u4e86\u5b66\u4e60-based\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u6a21\u578b-based\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.12532", "pdf": "https://arxiv.org/pdf/2504.12532", "abs": "https://arxiv.org/abs/2504.12532", "authors": ["John J. Vastola"], "title": "Generalization through variance: how noise shapes inductive biases in diffusion models", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": "Accepted to ICLR 2025", "summary": "How diffusion models generalize beyond their training set is not known, and\nis somewhat mysterious given two facts: the optimum of the denoising score\nmatching (DSM) objective usually used to train diffusion models is the score\nfunction of the training distribution; and the networks usually used to learn\nthe score function are expressive enough to learn this score to high accuracy.\nWe claim that a certain feature of the DSM objective -- the fact that its\ntarget is not the training distribution's score, but a noisy quantity only\nequal to it in expectation -- strongly impacts whether and to what extent\ndiffusion models generalize. In this paper, we develop a mathematical theory\nthat partly explains this 'generalization through variance' phenomenon. Our\ntheoretical analysis exploits a physics-inspired path integral approach to\ncompute the distributions typically learned by a few paradigmatic under- and\noverparameterized diffusion models. We find that the distributions diffusion\nmodels effectively learn to sample from resemble their training distributions,\nbut with 'gaps' filled in, and that this inductive bias is due to the\ncovariance structure of the noisy target used during training. We also\ncharacterize how this inductive bias interacts with feature-related inductive\nbiases.", "AI": {"tldr": "\u672c\u8bba\u6587\u89e3\u91ca\u4e86\u6269\u6563\u6a21\u578b\u5982\u4f55\u5728\u8bad\u7ec3\u96c6\u5916\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u7406\u8bba\u63ed\u793a'\u901a\u8fc7\u65b9\u5dee\u7684\u6cdb\u5316'\u73b0\u8c61\u3002", "motivation": "\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u673a\u5236\uff0c\u56e0\u4e3a\u5176\u8bad\u7ec3\u76ee\u6807\u662f\u8bad\u7ec3\u5206\u5e03\u7684\u5206\u6570\u51fd\u6570\uff0c\u4f46\u7f51\u7edc\u8db3\u591f expressive \u5374\u80fd\u6cdb\u5316\u3002", "method": "\u5f00\u53d1\u6570\u5b66\u7406\u8bba\uff0c\u4f7f\u7528\u7269\u7406\u542f\u53d1\u7684\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\uff0c\u5206\u6790\u6b20\u53c2\u6570\u5316\u548c\u8fc7\u53c2\u6570\u5316\u6269\u6563\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5230\u7684\u5206\u5e03\u7c7b\u4f3c\u4e8e\u8bad\u7ec3\u5206\u5e03\u4f46\u586b\u8865\u4e86'\u95f4\u9699'\uff0c\u5f52\u56e0\u4e8e\u8bad\u7ec3\u4e2d\u566a\u58f0\u76ee\u6807\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u3002", "conclusion": "\u8868\u5f81\u4e86\u8fd9\u79cd\u5f52\u7eb3\u504f\u5dee\u5982\u4f55\u4e0e\u7279\u5f81\u76f8\u5173\u7684\u5f52\u7eb3\u504f\u5dee\u4e92\u52a8\u3002"}}
{"id": "2504.12315", "pdf": "https://arxiv.org/pdf/2504.12315", "abs": "https://arxiv.org/abs/2504.12315", "authors": ["Xingguang Ji", "Jiakang Wang", "Hongzhi Zhang", "Jingyuan Zhang", "Haonan Zhou", "Chenxi Sun", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "With the development of Multimodal Large Language Models (MLLMs), numerous\noutstanding accomplishments have emerged within the open-source community. Due\nto the complexity of creating and training multimodal data pairs, it is still a\ncomputational and time-consuming process to build powerful MLLMs. In this work,\nwe introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient\nmanner and supports understanding text, image, video, and audio modalities. We\npresent in detail the framework design, the data construction, and the training\nrecipe, to develop an MLLM step-by-step to obtain competitive performance. We\nalso provide exclusive benchmarks utilized in our experiments to show how to\nproperly verify understanding capabilities across different modalities. Results\nshow that by following our guidance, we can efficiently build an MLLM that\nachieves competitive performance among models of the same scale on various\nmultimodal benchmarks. Additionally, to enhance the multimodal instruction\nfollowing and conversational capabilities of the model, we further discuss how\nto train the chat version upon an MLLM understanding model, which is more in\nline with user habits for tasks like real-time interaction with humans. We\npublicly disclose the Capybara-OMNI model, along with its chat-based version.\nThe disclosure includes both the model weights, a portion of the training data,\nand the inference codes, which are made available on GitHub.", "AI": {"tldr": "Capybara-OMNI \u662f\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u6a21\u6001\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u3001\u90e8\u5206\u6570\u636e\u548c\u4ee3\u7801\u3002", "motivation": "\u7531\u4e8e\u521b\u5efa\u548c\u8bad\u7ec3\u591a\u6a21\u6001\u6570\u636e\u5bf9\u7684\u590d\u6742\u6027\uff0c\u6784\u5efa\u5f3a\u5927 MLLM \u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u6846\u67b6\u8bbe\u8ba1\u3001\u6570\u636e\u6784\u5efa\u3001\u8bad\u7ec3\u914d\u65b9\uff0c\u4ee5\u53ca\u9010\u6b65\u5f00\u53d1 MLLM \u7684\u8fc7\u7a0b\uff1b\u8fd8\u5305\u62ec\u8bad\u7ec3\u804a\u5929\u7248\u672c\u4ee5\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u548c\u5bf9\u8bdd\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u540c\u89c4\u6a21\u6a21\u578b\u7ade\u4e89\u6027\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u4ee3\u7801\u3002", "conclusion": "\u9075\u5faa\u6307\u5bfc\u53ef\u4ee5\u9ad8\u6548\u6784\u5efa\u6027\u80fd-competitive \u7684 MLLM\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u7528\u6237\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2504.13170", "pdf": "https://arxiv.org/pdf/2504.13170", "abs": "https://arxiv.org/abs/2504.13170", "authors": ["Lujie Yang", "Tobia Marcucci", "Pablo A. Parrilo", "Russ Tedrake"], "title": "A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal Control with Time Scaling", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce a semidefinite relaxation for optimal control of linear systems\nwith time scaling. These problems are inherently nonconvex, since the system\ndynamics involves bilinear products between the discretization time step and\nthe system state and controls. The proposed relaxation is closely related to\nthe standard second-order semidefinite relaxation for quadratic constraints,\nbut we carefully select a subset of the possible bilinear terms and apply a\nchange of variables to achieve empirically tight relaxations while keeping the\ncomputational load light. We further extend our method to handle\npiecewise-affine (PWA) systems by formulating the PWA optimal-control problem\nas a shortest-path problem in a graph of convex sets (GCS). In this GCS,\ndifferent paths represent different mode sequences for the PWA system, and the\nconvex sets model the relaxed dynamics within each mode. By combining a tight\nconvex relaxation of the GCS problem with our semidefinite relaxation with time\nscaling, we can solve PWA optimal-control problems through a single\nsemidefinite program.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u534a\u5b9a\u677e\u5f1b\u65b9\u6cd5\u5904\u7406\u5e26\u65f6\u95f4\u7f29\u653e\u7684\u7ebf\u6027\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\uff0c\u5e76\u6269\u5c55\u5230\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u6b21\u534a\u5b9a\u89c4\u5212\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u975e\u51f8\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u65f6\u95f4\u6b65\u957f\u4e0e\u72b6\u6001\u3001\u63a7\u5236\u91cf\u7684\u53cc\u7ebf\u6027\u4e58\u79ef\uff0c\u5e76\u5904\u7406\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u534a\u5b9a\u677e\u5f1b\u7ed3\u5408\u53d8\u91cf\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5206\u6bb5\u4eff\u5c04\u7cfb\u7edf\u8868\u8ff0\u4e3a\u51f8\u96c6\u56fe\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u4e0e\u677e\u5f1b\u7ed3\u5408\u3002", "result": "\u5b9e\u73b0\u7ecf\u9a8c\u4e0a\u7d27\u5bc6\u7684\u677e\u5f1b\uff0c\u8ba1\u7b97\u8d1f\u62c5\u8f7b\uff0c\u5e76\u80fd\u901a\u8fc7\u5355\u4e2a\u534a\u5b9a\u89c4\u5212\u6c42\u89e3\u5206\u6bb5\u4eff\u5c04\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u975e\u51f8\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u9014\u5f84\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2504.12557", "pdf": "https://arxiv.org/pdf/2504.12557", "abs": "https://arxiv.org/abs/2504.12557", "authors": ["Siow Meng Low", "Akshat Kumar"], "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7a00\u758f\u6807\u8bb0\u6570\u636e\u4e2d\u5b66\u4e60\u672a\u77e5\u5b89\u5168\u7ea6\u675f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5b89\u5168\u7ea6\u675f\u672a\u77e5\u6216\u96be\u4ee5\u6307\u5b9a\uff0c\u56e0\u4e3a\u9700\u8981\u9884\u89c1\u6240\u6709\u53ef\u80fd\u7684unsafe\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5b89\u5168\u5b9a\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b89\u5168\u6a21\u578b\u8fdb\u884ccredit assignment\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u51b3\u7b56\u6b65\u9aa4\u7684\u5b89\u5168\u5f71\u54cd\uff1b\u5c55\u793a\u4e86\u6a21\u578b\u67b6\u6784\uff1b\u6539\u8fdb\u4e86safe RL\u95ee\u9898\u5e76\u5bfc\u51fa\u4e86\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6ee1\u8db3\u672a\u77e5\u5b89\u5168\u5b9a\u4e49\u548c\u6269\u5c55\u5230\u5404\u79cd\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u672a\u77e5\u5b89\u5168\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.12316", "pdf": "https://arxiv.org/pdf/2504.12316", "abs": "https://arxiv.org/abs/2504.12316", "authors": ["Jingyuan Zhang", "Hongzhi Zhang", "Zhou Haonan", "Chenxi Sun", "Xingguang ji", "Jiakang Wang", "Fanheng Kong", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Data Metabolism: An Efficient Data Design Schema For Vision Language Model", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To be presented at ICLR 2025, First Workshop on Open Science for\n  Foundation Models", "summary": "Data curation plays a crucial role in training powerful Visual Language\nModels (VLMs). In this work, we introduce the concept of Data Metabolism and\npresent our data-centric framework to build VLMs throughout the development\nlifecycle. Starting from a standard model architecture, we discuss and provide\ninsights into two crucial development steps: data curation and iteration,\nforming a closed-loop system that continuously improves model performance. We\nshow a detailed codebook on how to process existing massive datasets and build\nuser-specific data flywheel. As a demonstration, we release a VLM, named\nCapybara-VL, which excels in typical multimodal tasks (e.g. , visual question\nanswering, scientific reasoning, and text-rich tasks). Despite its relatively\ncompact size, Capybara-VL surpasses several open-source models that are up to\n10 times larger in size. Moreover, it achieves results that are on par with\nthose of several leading proprietary models, demonstrating its remarkable\ncompetitiveness. These results highlight the power of our data-centric\nframework and the potential of training smaller and more efficient VLMs.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165\u6570\u636e\u4ee3\u8c22\u6982\u5ff5\uff0c\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u6846\u67b6\u7528\u4e8e\u6784\u5efa\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7Capybara-VL\u6a21\u578b\u5c55\u793a\u4e86\u5176\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u6574\u7406\u5728\u8bad\u7ec3\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u901a\u8fc7\u95ed\u73af\u7cfb\u7edf\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165\u6570\u636e\u4ee3\u8c22\u6846\u67b6\uff0c\u8ba8\u8bba\u6570\u636e\u6574\u7406\u548c\u8fed\u4ee3\uff0c\u63d0\u4f9b\u6570\u636e\u96c6\u5904\u7406\u4ee3\u7801\u4e66\uff0c\u5e76\u6784\u5efa\u7528\u6237\u7279\u5b9a\u6570\u636e\u98de\u8f6e\u3002", "result": "\u53d1\u5e03\u4e86Capybara-VL\u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u9886\u5148\u4e13\u6709\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u7a81\u663e\u4e86\u6570\u636e\u4e2d\u5fc3\u6846\u67b6\u7684\u5f3a\u5927\u548c\u8bad\u7ec3\u66f4\u5c0f\u66f4\u9ad8\u6548\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12559", "pdf": "https://arxiv.org/pdf/2504.12559", "abs": "https://arxiv.org/abs/2504.12559", "authors": ["Emil Ryd", "Grey Nearing"], "title": "Fine Flood Forecasts: Incorporating local data into global models through fine-tuning", "categories": ["cs.LG", "physics.geo-ph"], "comment": null, "summary": "Floods are the most common form of natural disaster and accurate flood\nforecasting is essential for early warning systems. Previous work has shown\nthat machine learning (ML) models are a promising way to improve flood\npredictions when trained on large, geographically-diverse datasets. This\nrequirement of global training can result in a loss of ownership for national\nforecasters who cannot easily adapt the models to improve performance in their\nregion, preventing ML models from being operationally deployed. Furthermore,\ntraditional hydrology research with physics-based models suggests that local\ndata -- which in many cases is only accessible to local agencies -- is valuable\nfor improving model performance. To address these concerns, we demonstrate a\nmethodology of pre-training a model on a large, global dataset and then\nfine-tuning that model on data from individual basins. This results in\nperformance increases, validating our hypothesis that there is extra\ninformation to be captured in local data. In particular, we show that\nperformance increases are most significant in watersheds that underperform\nduring global training. We provide a roadmap for national forecasters who wish\nto take ownership of global models using their own data, aiming to lower the\nbarrier to operational deployment of ML-based hydrological forecast systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u9884\u8bad\u7ec3\u5168\u7403\u6d2a\u6c34\u9884\u6d4b\u6a21\u578b\u540e\u5728\u672c\u5730\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u64cd\u4f5c\u90e8\u7f72\u969c\u788d\u3002", "motivation": "\u6d2a\u6c34\u9884\u6d4b\u9700\u8981\u51c6\u786e\u6027\uff0cML\u6a21\u578b\u4f9d\u8d56\u5168\u7403\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u672c\u5730\u9884\u6d4b\u8005\u5931\u53bb\u6240\u6709\u6743\uff0c\u800c\u672c\u5730\u6570\u636e\u5bf9\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5927\u578b\u5168\u7403\u6570\u636e\u96c6\u4e0a\uff0c\u7136\u540e\u5728\u5355\u4e2a\u6d41\u57df\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u5168\u7403\u8bad\u7ec3\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u6d41\u57df\u3002", "conclusion": "\u63d0\u4f9b\u8def\u7ebf\u56fe\uff0c\u6307\u5bfc\u672c\u5730\u9884\u6d4b\u8005\u4f7f\u7528\u81ea\u8eab\u6570\u636e\u5fae\u8c03\u5168\u7403\u6a21\u578b\uff0c\u4fc3\u8fdbML-based\u6d2a\u6c34\u9884\u6d4b\u7cfb\u7edf\u7684\u64cd\u4f5c\u90e8\u7f72\u3002"}}
{"id": "2504.12318", "pdf": "https://arxiv.org/pdf/2504.12318", "abs": "https://arxiv.org/abs/2504.12318", "authors": ["Mir Md Sajid Sarwar", "Sudip Samanta", "Rajarshi Ray"], "title": "AUTONAV: A Toolfor Autonomous Navigation of Robots", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.4"], "comment": "5 pages, 5 figures", "summary": "We present a tool AUTONAV that automates the mapping, localization, and\npath-planning tasks for autonomous navigation of robots. The modular\narchitecture allows easy integration of various algorithms for these tasks for\ncomparison. We present the generated maps and path-plans by AUTONAV in indoor\nsimulation scenarios.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecdAUTONAV\u5de5\u5177\uff0c\u81ea\u52a8\u5316\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\uff0c\u5305\u62ec\u6620\u5c04\u3001\u5b9a\u4f4d\u548c\u8def\u5f84\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u4fbf\u4e8e\u7b97\u6cd5\u6bd4\u8f83\uff0c\u5728\u5ba4\u5185\u6a21\u62df\u4e2d\u5c55\u793a\u7ed3\u679c\u3002", "motivation": "\u52a8\u673a\u662f\u7b80\u5316\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u7684\u81ea\u52a8\u5316\uff0c\u5e76\u6613\u4e8e\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u8bbe\u8ba1AUTONAV\u5de5\u5177\uff0c\u5141\u8bb8\u96c6\u6210\u548c\u6bd4\u8f83\u5404\u79cd\u5bfc\u822a\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u662f\u5728\u5ba4\u5185\u6a21\u62df\u573a\u666f\u4e2d\u751f\u6210\u4e86\u5730\u56fe\u548c\u8def\u5f84\u89c4\u5212\u3002", "conclusion": "\u7ed3\u8bba\u662fAUTONAV\u5de5\u5177\u6709\u6548\u5730\u652f\u6301\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u548c\u7b97\u6cd5\u8bc4\u4f30\u3002"}}
{"id": "2504.12561", "pdf": "https://arxiv.org/pdf/2504.12561", "abs": "https://arxiv.org/abs/2504.12561", "authors": ["Akira Tamamori"], "title": "Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks", "categories": ["cs.LG", "cs.NE"], "comment": "4 pages, 4 figures", "summary": "Hebbian learning limits Hopfield network capacity. While kernel methods like\nKernel Logistic Regression (KLR) improve performance via iterative learning, we\npropose Kernel Ridge Regression (KRR) as an alternative. KRR learns dual\nvariables non-iteratively via a closed-form solution, offering significant\nlearning speed advantages. We show KRR achieves comparably high storage\ncapacity (reaching ratio 1.5 shown) and noise robustness (recalling from around\n80% corrupted patterns) as KLR, while drastically reducing training time,\nestablishing KRR as an efficient method for building high-performance\nassociative memories.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faKernel Ridge Regression (KRR)\u4f5c\u4e3aKernel Logistic Regression (KLR)\u7684\u66f4\u5feb\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u6548\u7684\u5173\u8054\u8bb0\u5fc6\uff0c\u5177\u6709\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "Hebbian\u5b66\u4e60\u9650\u5236\u4e86Hopfield\u7f51\u7edc\u7684\u5bb9\u91cf\uff0c\u800cKLR\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u5b66\u4e60\u901f\u5ea6\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u975e\u8fed\u4ee3\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528KRR\uff0c\u901a\u8fc7\u95ed\u5f0f\u89e3\u975e\u8fed\u4ee3\u5b66\u4e60\u53cc\u53d8\u91cf\uff0c\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\u3002", "result": "KRR\u5b9e\u73b0\u4e86\u9ad8\u5b58\u50a8\u5bb9\u91cf\uff08\u6bd4\u73871.5\uff09\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff08\u4ece80%\u635f\u574f\u6a21\u5f0f\u4e2d\u56de\u5fc6\uff09\uff0c\u4e0eKLR\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "KRR\u88ab\u786e\u7acb\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u5173\u8054\u8bb0\u5fc6\u7684\u6548\u7387\u65b9\u6cd5\u3002"}}
{"id": "2504.12319", "pdf": "https://arxiv.org/pdf/2504.12319", "abs": "https://arxiv.org/abs/2504.12319", "authors": ["Duc Tuyen TA", "Wajdi Ben Saad", "Ji Young Oh"], "title": "Specialized text classification: an approach to classifying Open Banking transactions", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-fin.CP"], "comment": null, "summary": "With the introduction of the PSD2 regulation in the EU which established the\nOpen Banking framework, a new window of opportunities has opened for banks and\nfintechs to explore and enrich Bank transaction descriptions with the aim of\nbuilding a better understanding of customer behavior, while using this\nunderstanding to prevent fraud, reduce risks and offer more competitive and\ntailored services.\n  And although the usage of natural language processing models and techniques\nhas seen an incredible progress in various applications and domains over the\npast few years, custom applications based on domain-specific text corpus remain\nunaddressed especially in the banking sector.\n  In this paper, we introduce a language-based Open Banking transaction\nclassification system with a focus on the french market and french language\ntext. The system encompasses data collection, labeling, preprocessing,\nmodeling, and evaluation stages. Unlike previous studies that focus on general\nclassification approaches, this system is specifically tailored to address the\nchallenges posed by training a language model with a specialized text corpus\n(Banking data in the French context). By incorporating language-specific\ntechniques and domain knowledge, the proposed system demonstrates enhanced\nperformance and efficiency compared to generic approaches.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6cd5\u56fd\u5e02\u573a\u548c\u6cd5\u8bed\u7684\u94f6\u884c\u4ea4\u6613\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f7f\u7528NLP\u6280\u672f\u4f18\u5316\u7279\u5b9a\u9886\u57df\u6570\u636e\u3002", "motivation": "PSD2\u6cd5\u89c4\u63a8\u52a8\u5f00\u653e\u94f6\u884c\u6846\u67b6\uff0c\u5141\u8bb8\u94f6\u884c\u548c\u91d1\u878d\u79d1\u6280\u516c\u53f8\u66f4\u597d\u5730\u7406\u89e3\u5ba2\u6237\u884c\u4e3a\u3001\u9632\u6b62\u6b3a\u8bc8\uff0c\u4f46\u94f6\u884c\u9886\u57df\u7279\u5b9a\u6587\u672c\u7684NLP\u5e94\u7528\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u3001\u9884\u5904\u7406\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u878d\u5165\u8bed\u8a00\u7279\u5b9a\u6280\u672f\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u9488\u5bf9\u6cd5\u56fd\u8bed\u5883\u3002", "result": "\u4e0e\u901a\u7528\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cfb\u7edf\u663e\u793a\u51fa\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "\u5b9a\u5236\u7684\u8bed\u8a00\u6a21\u578b\u5728\u94f6\u884c\u4ea4\u6613\u5206\u7c7b\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.12568", "pdf": "https://arxiv.org/pdf/2504.12568", "abs": "https://arxiv.org/abs/2504.12568", "authors": ["Zelal Su \"Lain\" Mustafaoglu", "Keshav Pingali", "Risto Miikkulainen"], "title": "Evolutionary Policy Optimization", "categories": ["cs.LG", "cs.NE"], "comment": "Builds upon previous GECCO 2025 work", "summary": "A key challenge in reinforcement learning (RL) is managing the\nexploration-exploitation trade-off without sacrificing sample efficiency.\nPolicy gradient (PG) methods excel in exploitation through fine-grained,\ngradient-based optimization but often struggle with exploration due to their\nfocus on local search. In contrast, evolutionary computation (EC) methods excel\nin global exploration, but lack mechanisms for exploitation. To address these\nlimitations, this paper proposes Evolutionary Policy Optimization (EPO), a\nhybrid algorithm that integrates neuroevolution with policy gradient methods\nfor policy optimization. EPO leverages the exploration capabilities of EC and\nthe exploitation strengths of PG, offering an efficient solution to the\nexploration-exploitation dilemma in RL. EPO is evaluated on the Atari Pong and\nBreakout benchmarks. Experimental results show that EPO improves both policy\nquality and sample efficiency compared to standard PG and EC methods, making it\neffective for tasks that require both exploration and local optimization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faEPO\u7b97\u6cd5\uff0c\u7ed3\u5408\u8fdb\u5316\u8ba1\u7b97\u548c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u64c5\u957f\u5229\u7528\u4f46\u63a2\u7d22\u4e0d\u8db3\uff0c\u8fdb\u5316\u8ba1\u7b97\u65b9\u6cd5\u64c5\u957f\u63a2\u7d22\u4f46\u5229\u7528\u4e0d\u8db3\uff0c\u9700\u8981\u6574\u5408\u4e8c\u8005\u4ee5\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51faEvolutionary Policy Optimization (EPO)\uff0c\u5c06\u795e\u7ecf\u8fdb\u5316\u4e0e\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u6df7\u5408\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728Atari Pong\u548cBreakout\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEPO\u63d0\u5347\u4e86\u7b56\u7565\u8d28\u91cf\u548c\u6837\u672c\u6548\u7387\uff0c\u4f18\u4e8e\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u548c\u8fdb\u5316\u8ba1\u7b97\u65b9\u6cd5\u3002", "conclusion": "EPO\u9002\u7528\u4e8e\u9700\u8981\u540c\u65f6\u8fdb\u884c\u63a2\u7d22\u548c\u5c40\u90e8\u4f18\u5316\u7684\u4efb\u52a1\u3002"}}
{"id": "2504.12320", "pdf": "https://arxiv.org/pdf/2504.12320", "abs": "https://arxiv.org/abs/2504.12320", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "19 pages + Appendix, 13 figure", "summary": "Following the widespread adoption of ChatGPT in early 2023, numerous studies\nreported that large language models (LLMs) can match or even surpass human\nperformance in creative tasks. However, it remains unclear whether LLMs have\nbecome more creative over time, and how consistent their creative output is. In\nthis study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,\nGrok, Mistral, and DeepSeek -- across two validated creativity assessments: the\nDivergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary\nto expectations, we found no evidence of increased creative performance over\nthe past 18-24 months, with GPT-4 performing worse than in previous studies.\nFor the more widely used AUT, all models performed on average better than the\naverage human, with GPT-4o and o3-mini performing best. However, only 0.28% of\nLLM-generated responses reached the top 10% of human creativity benchmarks.\nBeyond inter-model differences, we document substantial intra-model\nvariability: the same LLM, given the same prompt, can produce outputs ranging\nfrom below-average to original. This variability has important implications for\nboth creativity research and practical applications. Ignoring such variability\nrisks misjudging the creative potential of LLMs, either inflating or\nunderestimating their capabilities. The choice of prompts affected LLMs\ndifferently. Our findings underscore the need for more nuanced evaluation\nframeworks and highlight the importance of model selection, prompt design, and\nrepeated assessment when using Generative AI (GenAI) tools in creative\ncontexts.", "AI": {"tldr": "This study assesses creativity in 14 LLMs using DAT and AUT tasks, finding no improvement over time, high output variability, and the need for better evaluation methods.", "motivation": "To investigate whether LLMs have become more creative and consistent in their outputs since ChatGPT's adoption in 2023.", "method": "Evaluated 14 LLMs (e.g., GPT-4, Claude) on the Divergent Association Task (DAT) and Alternative Uses Task (AUT).", "result": "No evidence of increased creativity over 18-24 months; GPT-4 performed worse than before; most models outperformed average humans in AUT, but only 0.28% of responses reached top human benchmarks; high intra-model variability observed.", "conclusion": "Highlights the need for nuanced evaluation frameworks, emphasizing model selection, prompt design, and repeated assessments for using GenAI in creative contexts."}}
{"id": "2504.12569", "pdf": "https://arxiv.org/pdf/2504.12569", "abs": "https://arxiv.org/abs/2504.12569", "authors": ["You Rim Choi", "Subeom Park", "Seojun Heo", "Eunchung Noh", "Hyung-Sin Kim"], "title": "The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of\nlearning from unlabeled data that may include both in-distribution (ID) and\nunknown out-of-distribution (OOD) classes. However, existing OSSL methods form\nsuboptimal feature spaces by either excluding OOD samples, interfering with\nthem, or overtrusting their information during training. In this work, we\nintroduce MagMatch, a novel framework that naturally isolates OOD samples\nthrough a prototype-based contrastive learning paradigm. Unlike conventional\nmethods, MagMatch does not assign any prototypes to OOD samples; instead, it\nselectively aligns ID samples with class prototypes using an ID-Selective\nMagnetic (ISM) module, while allowing OOD samples - the \"others\" - to remain\nunaligned in the feature space. To support this process, we propose Selective\nMagnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts\nalignment based on sample confidence. Extensive experiments on diverse datasets\ndemonstrate that MagMatch significantly outperforms existing methods in both\nclosed-set classification accuracy and OOD detection AUROC, especially in\ngeneralizing to unseen OOD data.", "AI": {"tldr": "MagMatch\u662f\u4e00\u79cd\u65b0\u7684OSSL\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b-based\u5bf9\u6bd4\u5b66\u4e60\u9694\u79bbOOD\u6837\u672c\uff0c\u5728\u5206\u7c7b\u548cOOD\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709OSSL\u65b9\u6cd5\u5728\u5904\u7406\u53ef\u80fd\u5305\u542bID\u548cOOD\u7684\u672a\u6807\u6ce8\u6570\u636e\u65f6 suboptimal\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u6311\u6218\u3002", "method": "\u63d0\u51faMagMatch\u6846\u67b6\uff0c\u4f7f\u7528ID-Selective Magnetic\u6a21\u5757\u548cSelective Magnetic Alignment\u635f\u5931\uff0c\u9009\u62e9\u6027\u5730\u5bf9\u9f50ID\u6837\u672c\u539f\u578b\uff0c\u800c\u8ba9OOD\u6837\u672c\u4e0d\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\uff0cMagMatch\u5728\u95ed\u96c6\u5206\u7c7b\u51c6\u786e\u7387\u548cOOD\u68c0\u6d4bAUROC\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6cdb\u5316\u5230\u672a\u89c1OOD\u6570\u636e\u65f6\u3002", "conclusion": "MagMatch\u662f\u4e00\u79cd\u6709\u6548\u7684OSSL\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406OOD\u6837\u672c\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2504.12321", "pdf": "https://arxiv.org/pdf/2504.12321", "abs": "https://arxiv.org/abs/2504.12321", "authors": ["Charlotte Siska", "Anush Sankaran"], "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAttentionDefense\uff0c\u4f7f\u7528\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u68c0\u6d4b\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u8be5\u65b9\u6cd5\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u73b0\u6709\u7684\u9632\u5fa1\u7b56\u7565\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u578b\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u63d0\u793a\u6ce8\u610f\u529b\u6765\u8868\u5f81\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6\u3001\u6d88\u878d\u7814\u7a76\u548c\u751f\u6210\u65b0\u8d8a\u72f1\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u3002", "result": "AttentionDefense\u7684\u68c0\u6d4b\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5bf9\u65b0\u8d8a\u72f1\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u8ba1\u7b97\u9700\u6c42\u4f4e\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u662f\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5bf9\u6076\u610f\u8f93\u5165\u54cd\u5e94\u7684\u5173\u952e\uff0cAttentionDefense\u662f\u4e00\u79cd\u7406\u60f3\u7684\u5b9e\u7528\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2504.12577", "pdf": "https://arxiv.org/pdf/2504.12577", "abs": "https://arxiv.org/abs/2504.12577", "authors": ["Leming Wu", "Yaochu Jin", "Kuangrong Hao", "Han Yu"], "title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "The paper has been accepted by ICME 2025", "summary": "Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faFedDua\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u5904\u7406\u5ba2\u6237\u7aef\u6570\u636e\u91cf\u62a5\u544a\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u9884\u6d4b\u6570\u636e\u91cf\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u52a0\u6743\u5e73\u5747\u6613\u53d7\u4e0d\u8bda\u5b9e\u5ba2\u6237\u7aef\u865a\u5047\u62a5\u544a\u6570\u636e\u91cf\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5dee\u3002", "method": "\u63d0\u51faFedDua\uff0c\u901a\u8fc7\u5206\u6790\u5ba2\u6237\u7aef\u4e0a\u4f20\u7684\u6a21\u578b\u68af\u5ea6\u9884\u6d4b\u6570\u636e\u91cf\uff0c\u5e76\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u8054\u90a6\u5b66\u4e60\u805a\u5408\u7b97\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u793a\uff0cFedDua\u6bd4\u56db\u79cd\u6d41\u884c\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad83.17%\u7684\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FedDua\u6709\u6548\u89e3\u51b3\u6570\u636e\u91cf\u62a5\u544a\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2504.12322", "pdf": "https://arxiv.org/pdf/2504.12322", "abs": "https://arxiv.org/abs/2504.12322", "authors": ["Xin Gao", "Qizhi Pei", "Zinan Tang", "Yu Li", "Honglin Lin", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.", "AI": {"tldr": "GRA\u6846\u67b6\u901a\u8fc7\u591a\u4e2a\u5c0f\u578bLLM\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5927\u578bLLM\u76f8\u5f53\u7684\u6570\u636e\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d56\u5927\u578bLLM\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u73af\u5883\u4f4e\u6548\u548c\u504f\u5dee\u95ee\u9898\uff0c\u501f\u9274\u4eba\u7c7b\u534f\u4f5c\uff08\u5982\u540c\u884c\u8bc4\u5ba1\uff09\u6a21\u5f0f\u3002", "method": "\u63d0\u51faGRA\u6846\u67b6\uff0c\u6d89\u53caGenerator\u751f\u6210\u6570\u636e\u3001Reviewer\u5ba1\u9605\u8d28\u91cf\u548c\u591a\u6837\u6027\u3001Adjudicator\u89e3\u51b3\u51b2\u7a81\u7684\u534f\u4f5c\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGRA\u6570\u636e\u8d28\u91cf\u5339\u914d\u6216\u8d85\u8fc7\u5927\u578bLLM\u8f93\u51fa\uff0c\u5982Qwen-2.5-72B-Instruct\u3002", "conclusion": "\u6311\u6218\u5927\u578b\u6a21\u578b\u5fc5\u8981\u6027\uff0c\u4e3b\u5f20\u5c0f\u578bLLM\u534f\u8c03\u7b56\u7565\uff1b\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.12580", "pdf": "https://arxiv.org/pdf/2504.12580", "abs": "https://arxiv.org/abs/2504.12580", "authors": ["Benjamin C. Koenig", "Suyong Kim", "Sili Deng"], "title": "ChemKANs for Combustion Chemistry Modeling and Acceleration", "categories": ["cs.LG", "physics.chem-ph"], "comment": "B.C.K. and S.K. contributed equally to this work. 23 pages, 8\n  figures, and 1 table", "summary": "Efficient chemical kinetic model inference and application for combustion\nproblems is challenging due to large ODE systems and wideley separated time\nscales. Machine learning techniques have been proposed to streamline these\nmodels, though strong nonlinearity and numerical stiffness combined with noisy\ndata sources makes their application challenging. The recently developed\nKolmogorov-Arnold Networks (KANs) and KAN ordinary differential equations\n(KAN-ODEs) have been demonstrated as powerful tools for scientific applications\nthanks to their rapid neural scaling, improved interpretability, and smooth\nactivation functions. Here, we develop ChemKANs by augmenting the KAN-ODE\nframework with physical knowledge of the flow of information through the\nrelevant kinetic and thermodynamic laws, as well as an elemental conservation\nloss term. This novel framework encodes strong inductive bias that enables\nstreamlined training and higher accuracy predictions, while facilitating\nparameter sparsity through full sharing of information across all inputs and\noutputs. In a model inference investigation, we find that ChemKANs exhibit no\noverfitting or model degradation when tasked with extracting predictive models\nfrom data that is both sparse and noisy, a task that a standard DeepONet\nstruggles to accomplish. Next, we find that a remarkably parameter-lean ChemKAN\n(only 344 parameters) can accurately represent hydrogen combustion chemistry,\nproviding a 2x acceleration over the detailed chemistry in a solver that is\ngeneralizable to larger-scale turbulent flow simulations. These demonstrations\nindicate potential for ChemKANs in combustion physics and chemical kinetics,\nand demonstrate the scalability of generic KAN-ODEs in significantly larger and\nmore numerically challenging problems than previously studied.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165ChemKANs\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7269\u7406\u77e5\u8bc6\u548c\u5143\u7d20\u5b88\u6052\u635f\u5931\uff0c\u63d0\u9ad8\u5316\u5b66\u52a8\u529b\u5b66\u6a21\u578b\u5728\u71c3\u70e7\u95ee\u9898\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5316\u5b66\u52a8\u529b\u5b66\u6a21\u578b\u63a8\u65ad\u9762\u4e34\u5927\u578bODE\u7cfb\u7edf\u3001\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u3001\u975e\u7ebf\u6027\u548c\u566a\u58f0\u6570\u636e\u7684\u6311\u6218\uff0c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u56f0\u96be\u3002", "method": "\u5f00\u53d1ChemKANs\uff0c\u901a\u8fc7\u5728KAN-ODE\u57fa\u7840\u4e0a\u6dfb\u52a0\u7269\u7406\u77e5\u8bc6\u3001\u52a8\u529b\u5b66\u5b9a\u5f8b\u3001\u70ed\u529b\u5b66\u5b9a\u5f8b\u548c\u5143\u7d20\u5b88\u6052\u635f\u5931\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u53c2\u6570\u7a00\u758f\u3002", "result": "ChemKANs\u5728\u7a00\u758f\u566a\u58f0\u6570\u636e\u4e0a\u65e0\u8fc7\u62df\u5408\uff0c\u4f18\u4e8eDeepONet\uff1b\u4e00\u4e2a\u4ec5344\u53c2\u6570\u7684\u6a21\u578b\u53ef\u51c6\u786e\u6a21\u62df\u6c22\u71c3\u70e7\uff0c\u63d0\u4f9b2\u500d\u52a0\u901f\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u6e4d\u6d41\u6a21\u62df\u3002", "conclusion": "ChemKANs\u5728\u71c3\u70e7\u7269\u7406\u548c\u5316\u5b66\u52a8\u529b\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c55\u793a\u4e86KAN-ODEs\u5728\u66f4\u5927\u6311\u6218\u95ee\u9898\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.12323", "pdf": "https://arxiv.org/pdf/2504.12323", "abs": "https://arxiv.org/abs/2504.12323", "authors": ["Zheng Zhang", "Ning Li", "Qi Liu", "Rui Li", "Weibo Gao", "Qingyang Mao", "Zhenya Huang", "Baosheng Yu", "Dacheng Tao"], "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86RAG\u5bf9LLM\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u65b9\u6cd5\u7f13\u89e3\u5c0f\u89c4\u6a21LLM\u7684\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u8c03\u67e5RAG\u5f15\u5165\u540e\u5bf9LLM\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u793e\u4f1a\u5f71\u54cd\u5927\u7684\u9886\u57df\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u901a\u8fc7\u53d8\u5316LLM\u3001\u68c0\u7d22\u5668\u548c\u6765\u6e90\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51faFairFT\u548cFairFilter\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u89c4\u6a21\u5c0f\u4e8e8B\u7684LLM\u5728RAG\u4e0b\u516c\u5e73\u6027\u66f4\u5dee\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6539\u5584\u516c\u5e73\u6027\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12587", "pdf": "https://arxiv.org/pdf/2504.12587", "abs": "https://arxiv.org/abs/2504.12587", "authors": ["Kewen Peng", "Hao Zhuo", "Yicheng Yang", "Tim Menzies"], "title": "Software Engineering Principles for Fairer Systems: Experiments with GroupCART", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "Discrimination-aware classification aims to make accurate predictions while\nsatisfying fairness constraints. Traditional decision tree learners typically\noptimize for information gain in the target attribute alone, which can result\nin models that unfairly discriminate against protected social groups (e.g.,\ngender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a\ntree-based ensemble optimizer that avoids bias during model construction by\noptimizing not only for decreased entropy in the target attribute but also for\nincreased entropy in protected attributes. Our experiments show that GroupCART\nachieves fairer models without data transformation and with minimal performance\ndegradation. Furthermore, the method supports customizable weighting, offering\na smooth and flexible trade-off between predictive performance and fairness\nbased on user requirements. These results demonstrate that algorithmic bias in\ndecision tree models can be mitigated through multi-task, fairness-aware\nlearning. All code and datasets used in this study are available at:\nhttps://github.com/anonymous12138/groupCART.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faGroupCART\uff0c\u4e00\u79cd\u516c\u5e73aware\u7684\u51b3\u7b56\u6811\u65b9\u6cd5\uff0c\u5e73\u8861\u9884\u6d4b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u6811\u4ec5\u4f18\u5316\u76ee\u6807\u5c5e\u6027\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u53d7\u4fdd\u62a4\u7fa4\u4f53\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\uff09\u7684\u6b67\u89c6\uff0c\u56e0\u6b64\u9700\u8981\u516c\u5e73aware\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGroupCART\uff0c\u4e00\u79cd\u6811-based\u96c6\u6210\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u51cf\u5c11\u76ee\u6807\u5c5e\u6027\u7684\u71b5\u548c\u589e\u52a0\u4fdd\u62a4\u5c5e\u6027\u7684\u71b5\u6765\u907f\u514d\u504f\u5dee\uff0c\u652f\u6301\u53ef\u81ea\u5b9a\u4e49\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGroupCART\u5728\u4e0d\u8fdb\u884c\u6570\u636e\u8f6c\u6362\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u7684\u6a21\u578b\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u516c\u5e73aware\u5b66\u4e60\u53ef\u4ee5\u7f13\u89e3\u51b3\u7b56\u6811\u6a21\u578b\u4e2d\u7684\u7b97\u6cd5\u504f\u5dee\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324", "abs": "https://arxiv.org/abs/2504.12324", "authors": ["Mengying Yuan", "Wangzi Xuan", "Fei Li"], "title": "Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in both natural\nlanguage processing and information retrieval. While NLI has developed many\nsub-directions such as sentence-level NLI, document-level NLI and cross-lingual\nNLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In\nthis paper, we propose a novel paradigm for CDCL-NLI that extends traditional\nNLI capabilities to multi-document, multilingual scenarios. To support this\ntask, we construct a high-quality CDCL-NLI dataset including 1,110 instances\nand spanning 26 languages. To build a baseline for this task, we also propose\nan innovative method that integrates RST-enhanced graph fusion and\ninterpretability prediction. Our method employs RST (Rhetorical Structure\nTheory) on RGAT (Relation-aware Graph Attention Network) for cross-document\ncontext modeling, coupled with a structure-aware semantic alignment mechanism\nbased on lexical chains for cross-lingual understanding. For NLI\ninterpretability, we develop an EDU-level attribution framework that generates\nextractive explanations. Extensive experiments demonstrate our approach's\nsuperior performance, achieving significant improvements over both traditional\nNLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our\nwork sheds light on the study of NLI and will bring research interest on\ncross-document cross-lingual context understanding, semantic retrieval and\ninterpretability inference. Our dataset and code are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer\nreview}.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faCDCL-NLI\u65b0\u8303\u5f0f\uff0c\u6784\u5efa\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u521b\u65b0\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u548cLLM\u3002", "motivation": "\u5c3d\u7ba1NLI\u4efb\u52a1\u6709\u591a\u79cd\u5b50\u65b9\u5411\uff0c\u4f46\u8de8\u6587\u6863\u8de8\u8bed\u8a00NLI\uff08CDCL-NLI\uff09\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u6574\u5408RST\u589e\u5f3a\u56fe\u878d\u5408\u7684RGAT\u65b9\u6cd5\u548c\u57fa\u4e8e\u8bcd\u6c47\u94fe\u7684\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u4ee5\u53caEDU-level\u5f52\u56e0\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eDocNLI\u3001R2F\u3001Llama3\u548cGPT-4o\u7b49\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u542f\u53d1NLI\u7814\u7a76\uff0c\u4fc3\u8fdb\u884c\u4e1a\u5bf9\u8de8\u6587\u6863\u8de8\u8bed\u8a00\u7406\u89e3\u3001\u8bed\u4e49\u68c0\u7d22\u548c\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u7684\u5173\u6ce8\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.12588", "pdf": "https://arxiv.org/pdf/2504.12588", "abs": "https://arxiv.org/abs/2504.12588", "authors": ["Liheng Ma", "Soumyasundar Pal", "Yingxue Zhang", "Philip H. S. Torr", "Mark Coates"], "title": "Simplifying Graph Transformers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e09\u4e2a\u7b80\u5355\u7684\u4fee\u6539\uff0c\u4f7fTransformer\u9002\u7528\u4e8e\u56fe\u5b66\u4e60\uff0c\u800c\u4e0d\u5f15\u5165\u91cd\u5927\u67b6\u6784\u6539\u53d8\u3002", "motivation": "Transformer\u5728\u5404\u79cd\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fc1\u79fb\u5230\u56fe\u5b66\u4e60\u65f6\u5f80\u5f80\u9700\u8981\u590d\u6742\u7684\u4fee\u6539\uff0c\u672c\u6587\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u4ee5\u4fbf\u66f4\u5bb9\u6613\u91c7\u7528Transformer\u7684\u8bad\u7ec3\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u4fee\u6539\uff1a(1) \u7b80\u5316\u7684L2\u6ce8\u610f\u529b\u6765\u6d4b\u91cf\u6807\u8bb0\u7684\u5927\u5c0f\u63a5\u8fd1\u5ea6\uff1b(2) \u81ea\u9002\u5e94\u7684\u5747\u65b9\u6839\u5f52\u4e00\u5316\u6765\u4fdd\u7559\u6807\u8bb0\u5927\u5c0f\u4fe1\u606f\uff1b(3) \u5e26\u6709\u5171\u4eab\u7f16\u7801\u5668\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u504f\u5dee\u3002", "result": "\u5728\u5404\u79cd\u56fe\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u56fe\u540c\u6784\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u8868\u8fbe\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u4fee\u6539\u7684\u6709\u6548\u6027\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u636e\u5f97\u5230\u8bc1\u5b9e\uff0c\u8bc1\u660e\u4e86\u5728\u56fe\u5b66\u4e60\u4e2d\u91c7\u7528\u7b80\u5355Transformer\u4fee\u6539\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12325", "pdf": "https://arxiv.org/pdf/2504.12325", "abs": "https://arxiv.org/abs/2504.12325", "authors": ["Haiqi Zhang", "Zhengyuan Zhu", "Zeyu Zhang", "Chengkai Li"], "title": "LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "With the vast expansion of content on social media platforms, analyzing and\ncomprehending online discourse has become increasingly complex. This paper\nintroduces LLMTaxo, a novel framework leveraging large language models for the\nautomated construction of taxonomy of factual claims from social media by\ngenerating topics from multi-level granularities. This approach aids\nstakeholders in more effectively navigating the social media landscapes. We\nimplement this framework with different models across three distinct datasets\nand introduce specially designed taxonomy evaluation metrics for a\ncomprehensive assessment. With the evaluations from both human evaluators and\nGPT-4, the results indicate that LLMTaxo effectively categorizes factual claims\nfrom social media, and reveals that certain models perform better on specific\ndatasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faLLMTaxo\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u793e\u4ea4\u5a92\u4f53\u4e8b\u5b9e\u58f0\u660e\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8bc4\u4f30\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u6025\u5267\u589e\u52a0\uff0c\u5206\u6790\u5728\u7ebf\u8bdd\u8bed\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u81ea\u52a8\u5206\u7c7b\u6cd5\u5e2e\u52a9\u5229\u76ca\u76f8\u5173\u8005\u5bfc\u822a\u3002", "method": "\u4f7f\u7528\u4e0d\u540cLLM\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6846\u67b6\uff0c\u751f\u6210\u591a\u5c42\u6b21\u7c92\u5ea6\u4e3b\u9898\uff0c\u5e76\u5f15\u5165\u4e13\u7528\u8bc4\u4f30\u6307\u6807\u3002", "result": "LLMTaxo\u6709\u6548\u5206\u7c7b\u4e8b\u5b9e\u58f0\u660e\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u7ecf\u4eba\u7c7b\u548cGPT-4\u8bc4\u4f30\u786e\u8ba4\u3002", "conclusion": "LLMTaxo\u6846\u67b6\u662f\u793e\u4ea4\u5a92\u4f53\u4e8b\u5b9e\u58f0\u660e\u5206\u7c7b\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.12589", "pdf": "https://arxiv.org/pdf/2504.12589", "abs": "https://arxiv.org/abs/2504.12589", "authors": ["Huaizhi Qu", "Inyoung Choi", "Zhen Tan", "Song Wang", "Sukwon Yun", "Qi Long", "Faizan Siddiqui", "Kwonjoon Lee", "Tianlong Chen"], "title": "Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer", "categories": ["cs.LG"], "comment": null, "summary": "LLM ensembles are widely used for LLM judges. However, how to estimate their\naccuracy, especially in an efficient way, is unknown. In this paper, we present\na principled maximum a posteriori (MAP) framework for an economical and precise\nestimation of the performance of LLM ensemble judgment. We first propose a\nmixture of Beta-Binomial distributions to model the judgment distribution,\nrevising from the vanilla Binomial distribution. Next, we introduce a conformal\nprediction-driven approach that enables adaptive stopping during iterative\nsampling to balance accuracy with efficiency. Furthermore, we design a prior\ntransfer mechanism that utilizes learned distributions on open-source datasets\nto improve estimation on a target dataset when only scarce annotations are\navailable. Finally, we present BetaConform, a framework that integrates our\ndistribution assumption, adaptive stopping, and the prior transfer mechanism to\ndeliver a theoretically guaranteed distribution estimation of LLM ensemble\njudgment with minimum labeled samples. BetaConform is also validated\nempirically. For instance, with only 10 samples from the TruthfulQA dataset,\nfor a Llama ensembled judge, BetaConform gauges its performance with error\nmargin as small as 3.37%.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faBetaConform\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7cbe\u786e\u4f30\u8ba1LLM\u96c6\u6210\u5224\u65ad\u7684\u6027\u80fd\u3002", "motivation": "LLM\u96c6\u6210\u5e7f\u6cdb\u7528\u4e8e\u5224\u65ad\u4efb\u52a1\uff0c\u4f46\u9ad8\u6548\u51c6\u786e\u6027\u4f30\u8ba1\u65b9\u6cd5\u672a\u77e5\u3002", "method": "\u63d0\u51faBeta-Binomial\u5206\u5e03\u6df7\u5408\u6a21\u578b\u3001\u81ea\u9002\u5e94\u505c\u6b62\u673a\u5236\u548c\u5148\u9a8c\u8f6c\u79fb\u673a\u5236\uff0c\u6574\u5408\u6210BetaConform\u6846\u67b6\u3002", "result": "\u4ee5\u5c11\u91cf\u6837\u672c\uff08\u5982TruthfulQA\u768410\u4e2a\uff09\u5b9e\u73b0\u8bef\u5dee\u5c0f\u81f33.37%\u7684\u6027\u80fd\u4f30\u8ba1\uff0c\u5e76\u6709\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "BetaConform\u6846\u67b6\u5728\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u9a8c\u8bc1\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2504.12326", "pdf": "https://arxiv.org/pdf/2504.12326", "abs": "https://arxiv.org/abs/2504.12326", "authors": ["Shahriar Noroozizadeh", "Jeremy C. Weiss"], "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7ba1\u9053\uff0c\u4ece\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u4e2d\u63d0\u53d6\u548c\u6807\u6ce8\u65f6\u95f4\u5b9a\u4f4d\u7684\u4e34\u5e8a\u53d1\u73b0\uff0c\u521b\u5efa\u8113\u6bd2\u75c7\u6570\u636e\u96c6\uff0c\u5e76\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u9ad8\u3002", "motivation": "\u4e34\u5e8a\u75c5\u4f8b\u62a5\u544a\u5b8c\u6574\u4f46\u5ef6\u8fdf\uff0c\u7ed3\u6784\u5316\u6570\u636e\u867d\u65e9\u4f46\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u66f4\u5b8c\u6574\u548c\u65f6\u95f4\u7cbe\u7ec6\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528LLM\u7684\u7ba1\u9053\u6765\u8868\u578b\u5316\u3001\u63d0\u53d6\u548c\u6807\u6ce8\u75c5\u4f8b\u62a5\u544a\u4e2d\u7684\u65f6\u95f4\u5b9a\u4f4d\u53d1\u73b0\uff0c\u5e76\u5e94\u7528\u4e8ePubMed Open Access\u5b50\u96c6\u751f\u6210\u8113\u6bd2\u75c7-3\u8bed\u6599\u5e93\uff0c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u6bd4\u8f83\u3002", "result": "LLM\u663e\u793a\u9ad8\u4e34\u5e8a\u53d1\u73b0\u6062\u590d\u7387\uff08\u4e8b\u4ef6\u5339\u914d\u7387\u7ea60.755\uff09\u548c\u5f3a\u65f6\u95f4\u987a\u5e8f\u4e00\u81f4\u6027\uff08\u534f\u8c03\u6027\u7ea60.932\uff09\u3002", "conclusion": "\u8bc1\u660e\u4e86LLM\u5728\u6587\u672c\u4e2d\u65f6\u95f4\u5b9a\u4f4d\u4e34\u5e8a\u53d1\u73b0\u7684\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u5176\u5c40\u9650\u6027\u548c\u901a\u8fc7\u591a\u6a21\u6001\u6574\u5408\u7684\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2504.12594", "pdf": "https://arxiv.org/pdf/2504.12594", "abs": "https://arxiv.org/abs/2504.12594", "authors": ["Bijan Mazaheri", "Jiaqi Zhang", "Caroline Uhler"], "title": "Meta-Dependence in Conditional Independence Testing", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u56e0\u679c\u53d1\u73b0\u4e2d\u6761\u4ef6\u72ec\u7acb\u6027\u7684\u5143\u4f9d\u8d56\uff0c\u4f7f\u7528\u51e0\u4f55\u76f4\u89c9\u548c\u4fe1\u606f\u6295\u5f71\u6765\u6d4b\u91cf\u3002", "motivation": "\u6709\u9650\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u56e0\u679c\u7ed3\u6784\u4e0e\u6761\u4ef6\u72ec\u7acb\u6027\u5bf9\u5e94\u5173\u7cfb\u7684\u7834\u574f\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u8fd9\u4e9b\u72ec\u7acb\u6027\u4e4b\u95f4\u7684\u5143\u4f9d\u8d56\u3002", "method": "\u901a\u8fc7\u5c06\u6761\u4ef6\u72ec\u7acb\u6027\u89c6\u4e3a\u7ea6\u675f\u8054\u5408\u5206\u5e03\u7684\u6d41\u5f62\uff0c\u4f7f\u7528\u4fe1\u606f\u6295\u5f71\u8ba1\u7b97\u5143\u4f9d\u8d56\u5ea6\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u8ba1\u7b97\u7684\u5143\u4f9d\u8d56\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u5de9\u56fa\u4e86\u53d1\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6539\u8fdb\u4e86\u5bf9\u56e0\u679c\u53d1\u73b0\u4e2d\u6761\u4ef6\u72ec\u7acb\u6027\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u63d0\u5347\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.12328", "pdf": "https://arxiv.org/pdf/2504.12328", "abs": "https://arxiv.org/abs/2504.12328", "authors": ["Jialun Zhong", "Wei Shen", "Yanzeng Li", "Songyang Gao", "Hua Lu", "Yicheng Chen", "Yang Zhang", "Wei Zhou", "Jinjie Gu", "Lei Zou"], "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9Reward Model (RM) \u5728Large Language Model (LLM) \u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u504f\u597d\u6536\u96c6\u3001\u5956\u52b1\u5efa\u6a21\u3001\u4f7f\u7528\u3001\u5e94\u7528\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u516c\u5f00\u8d44\u6e90\u3002", "motivation": "\u4e3a\u521d\u5b66\u8005\u63d0\u4f9bRM\u7684\u5168\u9762\u4ecb\u7ecd\uff0c\u5e76\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4ece\u504f\u597d\u6536\u96c6\u3001\u5956\u52b1\u5efa\u6a21\u548c\u4f7f\u7528\u89d2\u5ea6\u63a2\u7d22RM\uff0c\u4ecb\u7ecd\u5e94\u7528\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5206\u6790\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86RM\u7684\u6982\u8ff0\u3001\u6311\u6218\u5206\u6790\u3001\u672a\u6765\u7814\u7a76\u65b9\u5411\u548cGitHub\u8d44\u6e90\u3002", "conclusion": "\u81f4\u529b\u4e8e\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u4ecb\u7ecd\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u8d44\u6e90\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.12601", "pdf": "https://arxiv.org/pdf/2504.12601", "abs": "https://arxiv.org/abs/2504.12601", "authors": ["Ruinan Jin", "Difei Cheng", "Hong Qiao", "Xin Shi", "Shaodong Liu", "Bo Zhang"], "title": "Stochastic Gradient Descent in Non-Convex Problems: Asymptotic Convergence with Relaxed Step-Size via Stopping Time Methods", "categories": ["cs.LG", "math.OC", "math.PR", "40G15", "G.1.0"], "comment": "42 pages", "summary": "Stochastic Gradient Descent (SGD) is widely used in machine learning\nresearch. Previous convergence analyses of SGD under the vanishing step-size\nsetting typically require Robbins-Monro conditions. However, in practice, a\nwider variety of step-size schemes are frequently employed, yet existing\nconvergence results remain limited and often rely on strong assumptions. This\npaper bridges this gap by introducing a novel analytical framework based on a\nstopping-time method, enabling asymptotic convergence analysis of SGD under\nmore relaxed step-size conditions and weaker assumptions. In the non-convex\nsetting, we prove the almost sure convergence of SGD iterates for step-sizes $\n\\{ \\epsilon_t \\}_{t \\geq 1} $ satisfying $\\sum_{t=1}^{+\\infty} \\epsilon_t =\n+\\infty$ and $\\sum_{t=1}^{+\\infty} \\epsilon_t^p < +\\infty$ for some $p > 2$.\nCompared with previous studies, our analysis eliminates the global Lipschitz\ncontinuity assumption on the loss function and relaxes the boundedness\nrequirements for higher-order moments of stochastic gradients. Building upon\nthe almost sure convergence results, we further establish $L_2$ convergence.\nThese significantly relaxed assumptions make our theoretical results more\ngeneral, thereby enhancing their applicability in practical scenarios.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u505c\u6b62\u65f6\u95f4\u65b9\u6cd5\u5206\u6790SGD\u5728\u677e\u5f1b\u6b65\u957f\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u6027\uff0c\u6d88\u9664\u4e86\u5f3a\u5047\u8bbe\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709SGD\u6536\u655b\u5206\u6790\u4f9d\u8d56Robbins-Monro\u6761\u4ef6\uff0c\u800c\u5b9e\u9645\u4e2d\u5e38\u7528\u66f4\u5e7f\u6cdb\u7684\u6b65\u957f\u65b9\u6848\uff0c\u672c\u6587\u65e8\u5728\u6865\u63a5\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u505c\u6b62\u65f6\u95f4\u65b9\u6cd5\u7684novel\u5206\u6790\u6846\u67b6\uff0c\u5bf9SGD\u8fdb\u884c\u6e10\u8fdb\u6536\u655b\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6b65\u957f\u6ee1\u8db3sum \u03b5_t = \u221e\u548csum \u03b5_t^p < \u221e (p>2)\u6761\u4ef6\u4e0bSGD\u7684\u51e0\u4e4e\u5904\u5904\u6536\u655b\u548cL2\u6536\u655b\uff0c\u65e0\u9700\u5168\u5c40Lipschitz\u8fde\u7eed\u6027\u548c\u677e\u5f1b\u7684\u9ad8\u9636\u77e9\u6709\u754c\u6027\u3002", "conclusion": "\u677e\u5f1b\u7684\u5047\u8bbe\u4f7f\u7406\u8bba\u7ed3\u679c\u66f4\u901a\u7528\uff0c\u63d0\u5347\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2504.12329", "pdf": "https://arxiv.org/pdf/2504.12329", "abs": "https://arxiv.org/abs/2504.12329", "authors": ["Wang Yang", "Xiang Yue", "Vipin Chaudhary", "Xiaotian Han"], "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances leverage post-training to enhance model reasoning\nperformance, which typically requires costly training pipelines and still\nsuffers from inefficient, overly lengthy outputs. We introduce Speculative\nThinking, a training-free framework that enables large reasoning models to\nguide smaller ones during inference at the reasoning level, distinct from\nspeculative decoding, which operates at the token level. Our approach is based\non two observations: (1) reasoning-supportive tokens such as \"wait\" frequently\nappear after structural delimiters like \"\\n\\n\", serving as signals for\nreflection or continuation; and (2) larger models exhibit stronger control over\nreflective behavior, reducing unnecessary backtracking while improving\nreasoning quality. By strategically delegating reflective steps to a more\ncapable model, our method significantly boosts the reasoning accuracy of\nreasoning models while shortening their output. With the assistance of the 32B\nreasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to\n89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average\noutput length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%\ndecrease. Moreover, when applied to a non-reasoning model\n(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%\non the same benchmark, achieving a relative improvement of 7.8%.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSpeculative Thinking\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u6307\u5bfc\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u8f93\u51fa\u5197\u957f\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u8bad\u7ec3-free\u6846\u67b6\u6765\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u89c2\u5bdf\u63a8\u7406\u652f\u6301tokens\uff08\u5982'wait'\uff09\u548c\u6a21\u578b\u63a7\u5236\uff0c\u6218\u7565\u6027\u5c06\u53cd\u601d\u6b65\u9aa4\u59d4\u6258\u7ed9\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u63a8\u7406\u6c34\u5e73\u7684\u6307\u5bfc\u3002", "result": "1.5B\u6a21\u578b\u5728MATH500\u57fa\u51c6\u4e0a\u51c6\u786e\u7387\u4ece83.2%\u63d0\u5347\u81f389.4%\uff0c\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1115.7%\uff1bQwen-2.5-7B-Instruct\u51c6\u786e\u7387\u4ece74.0%\u63d0\u5347\u81f381.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12610", "pdf": "https://arxiv.org/pdf/2504.12610", "abs": "https://arxiv.org/abs/2504.12610", "authors": ["Akshata Hegde", "Tom Nguyen", "Jianlin Cheng"], "title": "Machine Learning Methods for Gene Regulatory Network Inference", "categories": ["cs.LG", "q-bio.MN"], "comment": "40 pages, 3 figures, 2 tables", "summary": "Gene Regulatory Networks (GRNs) are intricate biological systems that control\ngene expression and regulation in response to environmental and developmental\ncues. Advances in computational biology, coupled with high throughput\nsequencing technologies, have significantly improved the accuracy of GRN\ninference and modeling. Modern approaches increasingly leverage artificial\nintelligence (AI), particularly machine learning techniques including\nsupervised, unsupervised, semi-supervised, and contrastive learning to analyze\nlarge scale omics data and uncover regulatory gene interactions. To support\nboth the application of GRN inference in studying gene regulation and the\ndevelopment of novel machine learning methods, we present a comprehensive\nreview of machine learning based GRN inference methodologies, along with the\ndatasets and evaluation metrics commonly used. Special emphasis is placed on\nthe emerging role of cutting edge deep learning techniques in enhancing\ninference performance. The potential future directions for improving GRN\ninference are also discussed.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u52a8\u673a\u662f\u652f\u6301\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u7684\u5e94\u7528\u548c\u65b0\u578b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u7684\u7efc\u8ff0\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u56de\u987e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5982\u76d1\u7763\u3001\u975e\u76d1\u7763\u7b49\uff09\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u7279\u522b\u5f3a\u8c03\u6df1\u5ea6\u5b66\u4e60\u7684\u4f5c\u7528\u3002", "result": "\u7ed3\u679c\u662f\u63d0\u5347\u4e86\u5bf9\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u7684\u7406\u89e3\uff0c\u7a81\u51fa\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u7ed3\u8bba\u8ba8\u8bba\u4e86\u6539\u8fdb\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u7684\u6f5c\u5728\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2504.12330", "pdf": "https://arxiv.org/pdf/2504.12330", "abs": "https://arxiv.org/abs/2504.12330", "authors": ["Pei Liu", "Xin Liu", "Ruoyu Yao", "Junming Liu", "Siyuan Meng", "Ding Wang", "Jun Ma"], "title": "HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) augments Large Language Models\n(LLMs) with external knowledge, conventional single-agent RAG remains\nfundamentally limited in resolving complex queries demanding coordinated\nreasoning across heterogeneous data ecosystems. We present HM-RAG, a novel\nHierarchical Multi-agent Multimodal RAG framework that pioneers collaborative\nintelligence for dynamic knowledge synthesis across structured, unstructured,\nand graph-based data. The framework is composed of three-tiered architecture\nwith specialized agents: a Decomposition Agent that dissects complex queries\ninto contextually coherent sub-tasks via semantic-aware query rewriting and\nschema-guided context augmentation; Multi-source Retrieval Agents that carry\nout parallel, modality-specific retrieval using plug-and-play modules designed\nfor vector, graph, and web-based databases; and a Decision Agent that uses\nconsistency voting to integrate multi-source answers and resolve discrepancies\nin retrieval results through Expert Model Refinement. This architecture attains\ncomprehensive query understanding by combining textual, graph-relational, and\nweb-derived evidence, resulting in a remarkable 12.95% improvement in answer\naccuracy and a 3.56% boost in question classification accuracy over baseline\nRAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG\nestablishes state-of-the-art results in zero-shot settings on both datasets.\nIts modular architecture ensures seamless integration of new data modalities\nwhile maintaining strict data governance, marking a significant advancement in\naddressing the critical challenges of multimodal reasoning and knowledge\nsynthesis in RAG systems. Code is available at\nhttps://github.com/ocean-luna/HMRAG.", "AI": {"tldr": "\u7b80\u800c\u8a00\u4e4b\uff0cHM-RAG \u662f\u4e00\u4e2a\u5206\u5c42\u591a\u4ee3\u7406\u591a\u6a21\u6001 RAG \u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u667a\u80fd\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u63d0\u9ad8\u77e5\u8bc6\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u5355\u4ee3\u7406 RAG \u5728\u5904\u7406\u9700\u8981\u8de8\u5f02\u6784\u6570\u636e\u751f\u6001\u534f\u8c03\u63a8\u7406\u7684\u590d\u6742\u67e5\u8be2\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u6846\u67b6\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u5206\u89e3\u4ee3\u7406\uff08\u5904\u7406\u67e5\u8be2\u5206\u89e3\uff09\u3001\u591a\u6e90\u68c0\u7d22\u4ee3\u7406\uff08\u5e76\u884c\u6a21\u6001\u7279\u5b9a\u68c0\u7d22\uff09\u548c\u51b3\u7b56\u4ee3\u7406\uff08\u6574\u5408\u7b54\u6848\u5e76\u89e3\u51b3\u4e0d\u4e00\u81f4\uff09\u3002", "result": "\u5728 ScienceQA \u548c CrisisMMD \u57fa\u51c6\u4e0a\uff0c\u7b54\u6848\u51c6\u786e\u7387\u63d0\u5347 12.95%\uff0c\u95ee\u9898\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347 3.56%\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "HM-RAG \u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u77e5\u8bc6\u5408\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4fbf\u4e8e\u96c6\u6210\u65b0\u6a21\u6001\u5e76\u786e\u4fdd\u6570\u636e\u6cbb\u7406\u3002"}}
{"id": "2504.12627", "pdf": "https://arxiv.org/pdf/2504.12627", "abs": "https://arxiv.org/abs/2504.12627", "authors": ["Tirtha Vinchurkar", "Kareem Abdelmaqsoud", "John R. Kitchin"], "title": "Uncertainty Quantification in Graph Neural Networks with Shallow Ensembles", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Machine-learned potentials (MLPs) have revolutionized materials discovery by\nproviding accurate and efficient predictions of molecular and material\nproperties. Graph Neural Networks (GNNs) have emerged as a state-of-the-art\napproach due to their ability to capture complex atomic interactions. However,\nGNNs often produce unreliable predictions when encountering out-of-domain data\nand it is difficult to identify when that happens. To address this challenge,\nwe explore Uncertainty Quantification (UQ) techniques, focusing on Direct\nPropagation of Shallow Ensembles (DPOSE) as a computationally efficient\nalternative to deep ensembles. By integrating DPOSE into the SchNet model, we\nassess its ability to provide reliable uncertainty estimates across diverse\nDensity Functional Theory datasets, including QM9, OC20, and Gold Molecular\nDynamics. Our findings often demonstrate that DPOSE successfully distinguishes\nbetween in-domain and out-of-domain samples, exhibiting higher uncertainty for\nunobserved molecule and material classes. This work highlights the potential of\nlightweight UQ methods in improving the robustness of GNN-based materials\nmodeling and lays the foundation for future integration with active learning\nstrategies.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u4f7f\u7528DPOSE\u6574\u5408\u5230SchNet\u6a21\u578b\u4e2d\uff0c\u63d0\u9ad8GNN\u5728\u6750\u6599\u9884\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "GNNs\u5728\u5904\u7406\u57df\u5916\u6570\u636e\u65f6\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u4e14\u96be\u4ee5\u8bc6\u522b\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u53ef\u9760\u6027\u3002", "method": "\u5c06DPOSE\uff08\u6d45\u5c42\u96c6\u6210\u76f4\u63a5\u4f20\u64ad\uff09\u6574\u5408\u5230SchNet\u6a21\u578b\u4e2d\uff0c\u5e76\u5728QM9\u3001OC20\u548cGold Molecular Dynamics\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "DPOSE\u6210\u529f\u533a\u5206\u57df\u5185\u548c\u57df\u5916\u6837\u672c\uff0c\u5bf9\u672a\u89c2\u5bdf\u5230\u7684\u5206\u5b50\u548c\u6750\u6599\u7c7b\u522b\u663e\u793a\u66f4\u9ad8\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8f7b\u91cf\u7ea7UQ\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u9ad8GNN-based\u6750\u6599\u5efa\u6a21\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u7684\u6574\u5408\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.12331", "pdf": "https://arxiv.org/pdf/2504.12331", "abs": "https://arxiv.org/abs/2504.12331", "authors": ["Xiangju Li", "Dong Yang", "Xiaogang Zhu", "Faliang Huang", "Peng Zhang", "Zhongying Zhao"], "title": "Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Span-level emotion-cause-category triplet extraction represents a novel and\ncomplex challenge within emotion cause analysis. This task involves identifying\nemotion spans, cause spans, and their associated emotion categories within the\ntext to form structured triplets. While prior research has predominantly\nconcentrated on clause-level emotion-cause pair extraction and span-level\nemotion-cause detection, these methods often confront challenges originating\nfrom redundant information retrieval and difficulty in accurately determining\nemotion categories, particularly when emotions are expressed implicitly or\nambiguously. To overcome these challenges, this study explores a fine-grained\napproach to span-level emotion-cause-category triplet extraction and introduces\nan innovative framework that leverages instruction tuning and data augmentation\ntechniques based on large language models. The proposed method employs\ntask-specific triplet extraction instructions and utilizes low-rank adaptation\nto fine-tune large language models, eliminating the necessity for intricate\ntask-specific architectures. Furthermore, a prompt-based data augmentation\nstrategy is developed to address data scarcity by guiding large language models\nin generating high-quality synthetic training data. Extensive experimental\nevaluations demonstrate that the proposed approach significantly outperforms\nexisting baseline methods, achieving at least a 12.8% improvement in span-level\nemotion-cause-category triplet extraction metrics. The results demonstrate the\nmethod's effectiveness and robustness, offering a promising avenue for\nadvancing research in emotion cause analysis. The source code is available at\nhttps://github.com/zxgnlp/InstruDa-LLM.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u8c03\u4f18\u548c\u6570\u636e\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u5ea6\u7ea7\u60c5\u611f\u539f\u56e0\u7c7b\u522b\u4e09\u5143\u7ec4\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5197\u4f59\u4fe1\u606f\u548c\u9690\u5f0f\u60c5\u611f\u7c7b\u522b\u65f6\u5b58\u5728\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ec6\u7c92\u5ea6\u65b9\u6cd5\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u3001\u4f4e\u79e9\u9002\u914d\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u63d0\u793a-based\u6570\u636e\u589e\u5f3a\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u81f3\u5c11\u63d0\u9ad8\u4e8612.8%\u7684\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u9c81\u68d2\uff0c\u4e3a\u60c5\u611f\u539f\u56e0\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4ee3\u7801\u5f00\u6e90\u3002"}}
{"id": "2504.12644", "pdf": "https://arxiv.org/pdf/2504.12644", "abs": "https://arxiv.org/abs/2504.12644", "authors": ["Reek Majumder", "Mashrur Chowdhury", "Sakib Mahmud Khan", "Zadid Khan", "Fahim Ahmad", "Frank Ngeni", "Gurcan Comert", "Judith Mwakalonge", "Dimitra Michalaka"], "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.ET"], "comment": null, "summary": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u901a\u6807\u5fd7\u5206\u7c7b\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6a21\u5757\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5bf9\u6297\u653b\u51fb\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528AlexNet\u548cVGG-16\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u8f6c\u79fb\u5b66\u4e60\u6784\u5efa\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u6d4b\u8bd5\u8d85\u8fc71000\u4e2a\u91cf\u5b50\u7535\u8def\uff0c\u5bf9\u6297PGD\u3001FGSA\u548cGA\u653b\u51fb\u3002", "result": "HCQ-DL\u6a21\u578b\u5728\u65e0\u653b\u51fb\u573a\u666f\u4e0b\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u5728GA\u548cFGSA\u653b\u51fb\u4e0b\u8d85\u8fc791%\uff0c\u5728PGD\u653b\u51fb\u4e0bAlexNet-based\u6a21\u578b\u51c6\u786e\u7387\u8fbe85%\uff0c\u5747\u4f18\u4e8eC-DL\u6a21\u578b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cHCQ-DL\u6a21\u578b\u5728\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4ea4\u901a\u6807\u5fd7\u5206\u7c7b\u51c6\u786e\u7387\u3002"}}
{"id": "2504.12333", "pdf": "https://arxiv.org/pdf/2504.12333", "abs": "https://arxiv.org/abs/2504.12333", "authors": ["Andr\u00e9s Isaza-Giraldo", "Paulo Bala", "Lucas Pereira"], "title": "Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "2nd HEAL Workshop at CHI Conference on Human Factors in Computing\n  Systems. April 26, 2025. Yokohama, Japan", "summary": "The evaluation of open-ended responses in serious games presents a unique\nchallenge, as correctness is often subjective. Large Language Models (LLMs) are\nincreasingly being explored as evaluators in such contexts, yet their accuracy\nand consistency remain uncertain, particularly for smaller models intended for\nlocal execution. This study investigates the reliability of five small-scale\nLLMs when assessing player responses in \\textit{En-join}, a game that simulates\ndecision-making within energy communities. By leveraging traditional binary\nclassification metrics (including accuracy, true positive rate, and true\nnegative rate), we systematically compare these models across different\nevaluation scenarios. Our results highlight the strengths and limitations of\neach model, revealing trade-offs between sensitivity, specificity, and overall\nperformance. We demonstrate that while some models excel at identifying correct\nresponses, others struggle with false positives or inconsistent evaluations.\nThe findings highlight the need for context-aware evaluation frameworks and\ncareful model selection when deploying LLMs as evaluators. This work\ncontributes to the broader discourse on the trustworthiness of AI-driven\nassessment tools, offering insights into how different LLM architectures handle\nsubjective evaluation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5c0f\u578bLLM\u5728\u4e25\u8083\u6e38\u620f\u4e2d\u8bc4\u4f30\u5f00\u653e\u5f0f\u54cd\u5e94\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u4e86\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u7684\u6743\u8861\u3002", "motivation": "\u9488\u5bf9LLM\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5c0f\u578b\u672c\u5730\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u3001\u771f\u6b63\u6b63\u4f8b\u7387\u3001\u771f\u6b63\u8d1f\u4f8b\u7387\uff09\u6bd4\u8f83\u4e94\u79cdLLM\u5728En-join\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5b58\u5728\u654f\u611f\u6027\u3001\u7279\u5f02\u6027\u548c\u6027\u80fd\u6743\u8861\uff0c\u6709\u4e9b\u64c5\u957f\u6b63\u786e\u54cd\u5e94\u8bc6\u522b\uff0c\u4f46\u53ef\u80fd\u6709\u5047\u9633\u6027\u95ee\u9898\u3002", "conclusion": "\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\u548c\u8c28\u614e\u6a21\u578b\u9009\u62e9\uff0c\u4e3aAI\u8bc4\u4f30\u5de5\u5177\u53ef\u9760\u6027\u63d0\u4f9b\u6d1e\u89c1\u3002"}}
{"id": "2504.12651", "pdf": "https://arxiv.org/pdf/2504.12651", "abs": "https://arxiv.org/abs/2504.12651", "authors": ["Motonobu Uchikoshi", "Youhei Akimoto"], "title": "Feature selection based on cluster assumption in PU learning", "categories": ["cs.LG", "cs.NE"], "comment": "Accepted at GECCO 2025", "summary": "Feature selection is essential for efficient data mining and sometimes\nencounters the positive-unlabeled (PU) learning scenario, where only a few\npositive labels are available, while most data remains unlabeled. In certain\nreal-world PU learning tasks, data subjected to adequate feature selection\noften form clusters with concentrated positive labels. Conventional feature\nselection methods that treat unlabeled data as negative may fail to capture the\nstatistical characteristics of positive data in such scenarios, leading to\nsuboptimal performance. To address this, we propose a novel feature selection\nmethod based on the cluster assumption in PU learning, called FSCPU. FSCPU\nformulates the feature selection problem as a binary optimization task, with an\nobjective function explicitly designed to incorporate the cluster assumption in\nthe PU learning setting. Experiments on synthetic datasets demonstrate the\neffectiveness of FSCPU across various data conditions. Moreover, comparisons\nwith 10 conventional algorithms on three open datasets show that FSCPU achieves\ncompetitive performance in downstream classification tasks, even when the\ncluster assumption does not strictly hold.", "AI": {"tldr": "\u63d0\u51faFSCPU\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b63\u8d1f\u4e0d\u5e73\u8861\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u9009\u62e9\uff0c\u57fa\u4e8e\u805a\u7c7b\u5047\u8bbe\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5728\u6b63\u8d1f\u4e0d\u5e73\u8861\u5b66\u4e60\u4e2d\u53ef\u80fd\u56e0\u5c06\u65e0\u6807\u7b7e\u6570\u636e\u89c6\u4e3a\u8d1f\u6837\u672c\u800c\u65e0\u6cd5\u6355\u83b7\u6b63\u6837\u672c\u7edf\u8ba1\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFSCPU\u65b9\u6cd5\uff0c\u5c06\u7279\u5f81\u9009\u62e9\u95ee\u9898\u8868\u8ff0\u4e3a\u4e8c\u5143\u4f18\u5316\u4efb\u52a1\uff0c\u76ee\u6807\u51fd\u6570\u878d\u5165\u805a\u7c7b\u5047\u8bbe\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u663e\u793a\u6709\u6548\u6027\uff0c\u4e0e10\u79cd\u4f20\u7edf\u7b97\u6cd5\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u6bd4\u8f83\u4e2d\uff0cFSCPU\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u805a\u7c7b\u5047\u8bbe\u4e0d\u4e25\u683c\u6210\u7acb\u3002", "conclusion": "FSCPU\u662f\u4e00\u79cd\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6b63\u8d1f\u4e0d\u5e73\u8861\u5b66\u4e60\u573a\u666f\uff0c\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.12335", "pdf": "https://arxiv.org/pdf/2504.12335", "abs": "https://arxiv.org/abs/2504.12335", "authors": ["Alden Dima", "James Foulds", "Shimei Pan", "Philip Feldman"], "title": "You've Changed: Detecting Modification of Black-Box Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are often provided as a service via an API,\nmaking it challenging for developers to detect changes in their behavior. We\npresent an approach to monitor LLMs for changes by comparing the distributions\nof linguistic and psycholinguistic features of generated text. Our method uses\na statistical test to determine whether the distributions of features from two\nsamples of text are equivalent, allowing developers to identify when an LLM has\nchanged. We demonstrate the effectiveness of our approach using five OpenAI\ncompletion models and Meta's Llama 3 70B chat model. Our results show that\nsimple text features coupled with a statistical test can distinguish between\nlanguage models. We also explore the use of our approach to detect prompt\ninjection attacks. Our work enables frequent LLM change monitoring and avoids\ncomputationally expensive benchmark evaluations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u6587\u672c\u7684\u8bed\u8a00\u548c\u5fc3\u7406\u8bed\u8a00\u5b66\u7279\u5f81\u5206\u5e03\u6765\u76d1\u63a7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u884c\u4e3a\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bc4\u4f30\u3002", "motivation": "LLM \u901a\u8fc7 API \u63d0\u4f9b\u670d\u52a1\uff0c\u5f00\u53d1\u4eba\u5458\u96be\u4ee5\u68c0\u6d4b\u5176\u884c\u4e3a\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u76d1\u63a7\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u6d4b\u8bd5\u6bd4\u8f83\u4e24\u4e2a\u6587\u672c\u6837\u672c\u7684\u7279\u5f81\u5206\u5e03\uff0c\u4ee5\u786e\u5b9a LLM \u662f\u5426\u53d1\u751f\u53d8\u5316\u3002", "result": "\u5728 OpenAI \u7684\u4e94\u4e2a\u5b8c\u6210\u6a21\u578b\u548c Meta \u7684 Llama 3 70B \u804a\u5929\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u533a\u5206\u6a21\u578b\u5e76\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9891\u7e41\u7684 LLM \u53d8\u5316\u76d1\u63a7\uff0c\u5e76\u907f\u514d\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u57fa\u51c6\u8bc4\u4f30\u3002"}}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVLMGuard-R1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u9a71\u52a8\u63d0\u793a\u91cd\u5199\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u590d\u6742\u6027\u5bfc\u81f4\u4f20\u7edf\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u8986\u76d6\u6f5c\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u8de8\u6a21\u6001\u63a8\u7406\u6765\u9632\u8303\u8fd9\u4e9b\u9690\u60a3\u3002", "method": "\u5f15\u5165VLMGuard-R1\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u9636\u6bb5\u63a8\u7406\u7ba1\u9053\u8bad\u7ec3\u63d0\u793a\u91cd\u5199\u5668\uff0c\u52a8\u6001\u89e3\u91ca\u6587\u672c-\u56fe\u50cf\u4ea4\u4e92\uff0c\u751f\u6210\u66f4\u5b89\u5168\u7684\u63d0\u793a\uff0c\u800c\u4e0d\u6539\u53d8\u6a21\u578b\u6838\u5fc3\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7\u56db\u4e2a\u57fa\u7ebf\uff0c\u5728SIUO\u57fa\u51c6\u4e0a\u4e94\u79cd\u6a21\u578b\u7684\u5e73\u5747\u5b89\u5168\u6027\u63d0\u534743.59%\u3002", "conclusion": "VLMGuard-R1\u6846\u67b6\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u63a8\u7406\u9a71\u52a8\u63d0\u793a\u91cd\u5199\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347", "abs": "https://arxiv.org/abs/2504.12347", "authors": ["Mika Set\u00e4l\u00e4", "Pieta Sikstr\u00f6m", "Ville Heilala", "Tommi K\u00e4rkk\u00e4inen"], "title": "Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential to also support\neducational assessments at scale.", "AI": {"tldr": "This study evaluates LLMs' mathematical capabilities using a Finnish exam, showing rapid improvements to top performance.", "motivation": "To assess the evolving mathematical reasoning of LLMs in educational settings, given their increasing promise.", "method": "Testing various LLMs with the Finnish matriculation examination, a high-stakes digital test.", "result": "Initial moderate performance improved over time, with some models achieving perfect scores matching top students.", "conclusion": "Highlights rapid advances in LLMs' math proficiency and potential for supporting educational assessments at scale."}}
{"id": "2504.12665", "pdf": "https://arxiv.org/pdf/2504.12665", "abs": "https://arxiv.org/abs/2504.12665", "authors": ["Siwei Huang", "Chenhao Yang", "Chuan Hu"], "title": "Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy", "categories": ["cs.LG", "cs.HC"], "comment": "6pages, 8figures, 5tables. Accepted to be presented at the 2025 36th\n  IEEE Intelligent Vehicles Symposium (IV) (IV 2025)", "summary": "Drivers' perception of risk determines their acceptance, trust, and use of\nthe Automated Driving Systems (ADSs). However, perceived risk is subjective and\ndifficult to evaluate using existing methods. To address this issue, a driver's\nsubjective perceived risk (DSPR) model is proposed, regarding perceived risk as\na dynamically triggered mechanism with anisotropy and attenuation. 20\nparticipants are recruited for a driver-in-the-loop experiment to report their\nreal-time subjective risk ratings (SRRs) when experiencing various automatic\ndriving scenarios. A convolutional neural network and bidirectional long\nshort-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is\nembedded into a semi-supervised learning strategy to predict SRRs, aiming to\nreduce data noise caused by subjective randomness of participants. The results\nillustrate that DSPR achieves the highest prediction accuracy of 87.91% in\npredicting SRRs, compared to three state-of-the-art risk models. The\nsemi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA\nnetwork presents the highest accuracy among four different LSTM structures.\nThis study offers an effective method for assessing driver's perceived risk,\nproviding support for the safety enhancement of ADS and driver's trust\nimprovement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSPR\u6a21\u578b\uff0c\u4f7f\u7528CNN-Bi-LSTM-TPA\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u9884\u6d4b\u9a7e\u9a76\u5458\u4e3b\u89c2\u98ce\u9669\u8bc4\u5206\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe87.91%\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u4e3b\u89c2\u611f\u77e5\u98ce\u9669\u96be\u4ee5\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u5f71\u54cd\u7cfb\u7edf\u63a5\u53d7\u5ea6\u548c\u4fe1\u4efb\u3002", "method": "\u63d0\u51faDSPR\u6a21\u578b\u5e76\u8fdb\u884c\u9a7e\u9a76\u5458\u5728\u73af\u5b9e\u9a8c\uff0c\u62db\u52df20\u540d\u53c2\u4e0e\u8005\uff1b\u4f7f\u7528CNN-Bi-LSTM-TPA\u7f51\u7edc\u5d4c\u5165\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u9884\u6d4b\u98ce\u9669\u8bc4\u5206\u3002", "result": "DSPR\u6a21\u578b\u51c6\u786e\u738787.91%\uff0c\u534a\u76d1\u7763\u7b56\u7565\u63d0\u9ad8\u51c6\u786e\u738720.12%\uff0cCNN-Bi-LSTM-TPA\u5728LSTM\u7ed3\u6784\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u4f9b\u6709\u6548\u65b9\u6cd5\u8bc4\u4f30\u9a7e\u9a76\u5458\u611f\u77e5\u98ce\u9669\uff0c\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u548c\u4fe1\u4efb\u63d0\u5347\u3002"}}
{"id": "2504.12350", "pdf": "https://arxiv.org/pdf/2504.12350", "abs": "https://arxiv.org/abs/2504.12350", "authors": ["Jing Wang", "Jeremy C Weiss"], "title": "A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timing of clinical events is central to characterization of patient\ntrajectories, enabling analyses such as process tracing, forecasting, and\ncausal reasoning. However, structured electronic health records capture few\ndata elements critical to these tasks, while clinical reports lack temporal\nlocalization of events in structured form. We present a system that transforms\ncase reports into textual time series-structured pairs of textual events and\ntimestamps. We contrast manual and large language model (LLM) annotations\n(n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access\n(PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93).\nWe find that the LLM models have moderate event recall(O1-preview: 0.80) but\nhigh temporal concordance among identified events (O1-preview: 0.95). By\nestablishing the task, annotation, and assessment systems, and by demonstrating\nhigh concordance, this work may serve as a benchmark for leveraging the PMOA\ncorpus for temporal analytics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u7cfb\u7edf\u5c06\u75c5\u4f8b\u62a5\u544a\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u65f6\u95f4\u5e8f\u5217\uff0c\u4f7f\u7528LLM\u6807\u6ce8\uff0c\u663e\u793a\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4e34\u5e8a\u4e8b\u4ef6\u65f6\u95f4\u4fe1\u606f\u5bf9\u60a3\u8005\u8f68\u8ff9\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7f3a\u5c11\u5173\u952e\u6570\u636e\uff0c\u4e34\u5e8a\u62a5\u544a\u7f3a\u4e4f\u7ed3\u6784\u5316\u65f6\u95f4\u5b9a\u4f4d\u3002", "method": "\u5bf9\u6bd4\u624b\u52a8\u548cLLM\u6807\u6ce8\uff08\u6837\u672c\u91cf\u5206\u522b\u4e3a320\u548c390\uff09\uff0c\u5728\u968f\u673a\u62bd\u6837\u7684PubMed\u5f00\u653e\u83b7\u53d6\u75c5\u4f8b\u62a5\u544a\u4e0a\u8bc4\u4f30\u4e8b\u4ef6\u53ec\u56de\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "LLM\u6a21\u578b\u4e8b\u4ef6\u53ec\u56de\u4e2d\u7b49\uff080.80\uff09\uff0c\u65f6\u95f4\u4e00\u81f4\u6027\u9ad8\uff080.95\uff09\u3002", "conclusion": "\u6b64\u5de5\u4f5c\u53ef\u4f5c\u4e3a\u5229\u7528PMOA\u8bed\u6599\u5e93\u8fdb\u884c\u65f6\u95f4\u5206\u6790\u7684\u57fa\u51c6\u3002"}}
{"id": "2504.12675", "pdf": "https://arxiv.org/pdf/2504.12675", "abs": "https://arxiv.org/abs/2504.12675", "authors": ["Pengtao Dang", "Tingbo Guo", "Sha Cao", "Chi Zhang"], "title": "Physics Informed Constrained Learning of Dynamics from Static Data", "categories": ["cs.LG", "physics.bio-ph", "q-bio.MN"], "comment": "39 pages, 10 figures", "summary": "A physics-informed neural network (PINN) models the dynamics of a system by\nintegrating the governing physical laws into the architecture of a neural\nnetwork. By enforcing physical laws as constraints, PINN overcomes challenges\nwith data scarsity and potentially high dimensionality. Existing PINN\nframeworks rely on fully observed time-course data, the acquisition of which\ncould be prohibitive for many systems. In this study, we developed a new PINN\nlearning paradigm, namely Constrained Learning, that enables the approximation\nof first-order derivatives or motions using non-time course or partially\nobserved data. Computational principles and a general mathematical formulation\nof Constrained Learning were developed. We further introduced MPOCtrL (Message\nPassing Optimization-based Constrained Learning) an optimization approach\ntailored for the Constrained Learning framework that strives to balance the\nfitting of physical models and observed data. Its code is available at github\nlink: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and\nreal-world data demonstrated that MPOCtrL can effectively detect the nonlinear\ndependency between observed data and the underlying physical properties of the\nsystem. In particular, on the task of metabolic flux analysis, MPOCtrL\noutperforms all existing data-driven flux estimators.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8303\u5f0fConstrained Learning\u548c\u4f18\u5316\u65b9\u6cd5MPOCtrL\uff0c\u80fd\u591f\u4f7f\u7528\u90e8\u5206\u89c2\u5bdf\u6570\u636e\u903c\u8fd1\u7cfb\u7edf\u52a8\u6001\uff0c\u5e76\u5728\u4ee3\u8c22\u901a\u91cf\u5206\u6790\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709PINN\u6846\u67b6\u4f9d\u8d56\u5b8c\u5168\u89c2\u5bdf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u5904\u7406\u6570\u636e\u7a00\u7f3a\u6027\u548c\u90e8\u5206\u89c2\u5bdf\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86Constrained Learning\u6846\u67b6\u53ca\u5176\u6570\u5b66\u516c\u5f0f\uff0c\u5e76\u5f15\u5165MPOCtrL\u4f18\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u5e73\u8861\u7269\u7406\u6a21\u578b\u548c\u89c2\u5bdf\u6570\u636e\u7684\u62df\u5408\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMPOCtrL\u80fd\u68c0\u6d4b\u975e\u7ebf\u6027\u4f9d\u8d56\uff0c\u5e76\u5728\u4ee3\u8c22\u901a\u91cf\u5206\u6790\u4e2d\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u6570\u636e\u9a71\u52a8\u4f30\u8ba1\u5668\u3002", "conclusion": "MPOCtrL\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u90e8\u5206\u6570\u636e\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u516c\u5f00\u3002"}}
{"id": "2504.12351", "pdf": "https://arxiv.org/pdf/2504.12351", "abs": "https://arxiv.org/abs/2504.12351", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Vedrana Ivezic", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey Arnold"], "title": "Prototype-Guided Diffusion for Digital Pathology: Achieving Foundation Model Performance with Minimal Clinical Data", "categories": ["cs.GR", "cs.AI", "eess.IV", "q-bio.TO"], "comment": null, "summary": "Foundation models in digital pathology use massive datasets to learn useful\ncompact feature representations of complex histology images. However, there is\nlimited transparency into what drives the correlation between dataset size and\nperformance, raising the question of whether simply adding more data to\nincrease performance is always necessary. In this study, we propose a\nprototype-guided diffusion model to generate high-fidelity synthetic pathology\ndata at scale, enabling large-scale self-supervised learning and reducing\nreliance on real patient samples while preserving downstream performance. Using\nguidance from histological prototypes during sampling, our approach ensures\nbiologically and diagnostically meaningful variations in the generated data. We\ndemonstrate that self-supervised features trained on our synthetic dataset\nachieve competitive performance despite using ~60x-760x less data than models\ntrained on large real-world datasets. Notably, models trained using our\nsynthetic data showed statistically comparable or better performance across\nmultiple evaluation metrics and tasks, even when compared to models trained on\norders of magnitude larger datasets. Our hybrid approach, combining synthetic\nand real data, further enhanced performance, achieving top results in several\nevaluations. These findings underscore the potential of generative AI to create\ncompelling training data for digital pathology, significantly reducing the\nreliance on extensive clinical datasets and highlighting the efficiency of our\napproach.", "AI": {"tldr": "\u4f7f\u7528\u539f\u578b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u75c5\u7406\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u63a2\u8ba8\u6570\u636e\u96c6\u5927\u5c0f\u4e0e\u6027\u80fd\u7684\u76f8\u5173\u6027\uff0c\u8d28\u7591\u662f\u5426\u603b\u662f\u9700\u8981\u66f4\u591a\u6570\u636e\uff0c\u5e76\u51cf\u5c11\u5bf9\u771f\u5b9e\u60a3\u8005\u6837\u672c\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u539f\u578b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ec4\u7ec7\u5b66\u539f\u578b\u7684\u5f15\u5bfc\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u75c5\u7406\u6570\u636e\u3002", "result": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u7279\u5f81\u5728\u5c11\u5f97\u591a\u6570\u636e\u4e0b\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u6df7\u5408\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u663e\u8457\u51cf\u5c11\u5bf9\u4e34\u5e8a\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2504.12712", "pdf": "https://arxiv.org/pdf/2504.12712", "abs": "https://arxiv.org/abs/2504.12712", "authors": ["Hyunji Jung", "Hanseul Cho", "Chulhee Yun"], "title": "Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification", "categories": ["cs.LG", "math.OC"], "comment": "67 pages, 11 figures, accepted to ICLR 2025", "summary": "We study continual learning on multiple linear classification tasks by\nsequentially running gradient descent (GD) for a fixed budget of iterations per\ntask. When all tasks are jointly linearly separable and are presented in a\ncyclic/random order, we show the directional convergence of the trained linear\nclassifier to the joint (offline) max-margin solution. This is surprising\nbecause GD training on a single task is implicitly biased towards the\nindividual max-margin solution for the task, and the direction of the joint\nmax-margin solution can be largely different from these individual solutions.\nAdditionally, when tasks are given in a cyclic order, we present a\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\nalignment between tasks is indeed closely tied to catastrophic forgetting and\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\njointly separable and show that the model trained in a cyclic order converges\nto the unique minimum of the joint loss function.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u68af\u5ea6\u4e0b\u964d\u7684\u884c\u4e3a\uff0c\u5f53\u4efb\u52a1\u8054\u5408\u7ebf\u6027\u53ef\u5206\u65f6\uff0c\u6a21\u578b\u6536\u655b\u5230\u8054\u5408\u6700\u5927\u8fb9\u9645\u89e3\uff0c\u5e76\u5206\u6790\u4e86\u9057\u5fd8\u548c\u77e5\u8bc6\u8f6c\u79fb\u3002", "motivation": "\u63a2\u7d22\u68af\u5ea6\u4e0b\u964d\u5728\u591a\u4efb\u52a1\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6536\u655b\u7279\u6027\uff0c\u7279\u522b\u662f\u4efb\u52a1\u987a\u5e8f\u5bf9\u6a21\u578b\u504f\u7f6e\u548c\u9057\u5fd8\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u987a\u5e8f\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u6bcf\u4e2a\u4efb\u52a1\u56fa\u5b9a\u8fed\u4ee3\u9884\u7b97\uff0c\u5206\u6790\u5faa\u73af\u6216\u968f\u673a\u4efb\u52a1\u987a\u5e8f\u4e0b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5f53\u4efb\u52a1\u8054\u5408\u7ebf\u6027\u53ef\u5206\u65f6\uff0c\u6a21\u578b\u65b9\u5411\u6536\u655b\u5230\u8054\u5408\u6700\u5927\u8fb9\u9645\u89e3\uff1b\u5faa\u73af\u987a\u5e8f\u4e0b\uff0c\u9057\u5fd8\u91cf\u8d8b\u4e8e\u96f6\uff1b\u975e\u53ef\u5206\u60c5\u51b5\u4e0b\u6536\u655b\u5230\u8054\u5408\u635f\u5931\u6700\u5c0f\u503c\u3002", "conclusion": "\u4efb\u52a1\u5bf9\u9f50\u4e0e\u9057\u5fd8\u5bc6\u5207\u76f8\u5173\uff0c\u5faa\u73af\u8bad\u7ec3\u53ef\u51cf\u5c11\u9057\u5fd8\u5e76\u786e\u4fdd\u6536\u655b\uff0c\u63ed\u793a\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6f5c\u5728\u673a\u5236\u3002"}}
{"id": "2504.12352", "pdf": "https://arxiv.org/pdf/2504.12352", "abs": "https://arxiv.org/abs/2504.12352", "authors": ["Ruijie Wang", "Luca Rossetto", "Susan M\u00e9rillat", "Christina R\u00f6cke", "Mike Martin", "Abraham Bernstein"], "title": "Deep Generative Model-Based Generation of Synthetic Individual-Specific Brain MRI Segmentations", "categories": ["q-bio.NC", "cs.AI", "eess.IV"], "comment": null, "summary": "To the best of our knowledge, all existing methods that can generate\nsynthetic brain magnetic resonance imaging (MRI) scans for a specific\nindividual require detailed structural or volumetric information about the\nindividual's brain. However, such brain information is often scarce, expensive,\nand difficult to obtain. In this paper, we propose the first approach capable\nof generating synthetic brain MRI segmentations -- specifically, 3D white\nmatter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations --\nfor individuals using their easily obtainable and often readily available\ndemographic, interview, and cognitive test information. Our approach features a\nnovel deep generative model, CSegSynth, which outperforms existing prominent\ngenerative models, including conditional variational autoencoder (C-VAE),\nconditional generative adversarial network (C-GAN), and conditional latent\ndiffusion model (C-LDM). We demonstrate the high quality of our synthetic\nsegmentations through extensive evaluations. Also, in assessing the\neffectiveness of the individual-specific generation, we achieve superior volume\nprediction, with Pearson correlation coefficients reaching 0.80, 0.82, and 0.70\nbetween the ground-truth WM, GM, and CSF volumes of test individuals and those\nvolumes predicted based on generated individual-specific segmentations,\nrespectively.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e2a\u4eba\u4eba\u53e3\u7edf\u8ba1\u3001\u8bbf\u8c08\u548c\u8ba4\u77e5\u6d4b\u8bd5\u4fe1\u606f\u751f\u6210\u5408\u6210\u8111MRI\u5206\u5272\uff0c\u800c\u4e0d\u9700\u8981\u8be6\u7ec6\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8be6\u7ec6\u7684\u8111\u7ed3\u6784\u6216\u4f53\u79ef\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u7a00\u7f3a\u3001\u6602\u8d35\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f7f\u7528\u6613\u5f97\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u751f\u6210\u6a21\u578bCSegSynth\uff0c\u5e76\u4e0eC-VAE\u3001C-GAN\u3001C-LDM\u7b49\u6a21\u578b\u6bd4\u8f83\u3002", "result": "CSegSynth\u6a21\u578b\u6027\u80fd\u4f18\u8d8a\uff0c\u4f53\u79ef\u9884\u6d4b\u7684Pearson\u76f8\u5173\u7cfb\u6570\u5206\u522b\u4e3a0.80\u30010.82\u548c0.70\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5408\u6210\u5206\u5272\u7684\u9ad8\u8d28\u91cf\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u751f\u6210\u5206\u5272\u7684\u4f53\u79ef\u9884\u6d4b\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.12715", "pdf": "https://arxiv.org/pdf/2504.12715", "abs": "https://arxiv.org/abs/2504.12715", "authors": ["Long Zeng", "Jianxiang Yu", "Jiapeng Zhu", "Qingsong Zhong", "Xiang Li"], "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection", "categories": ["cs.LG"], "comment": null, "summary": "Graph self-supervised learning has gained significant attention recently.\nHowever, many existing approaches heavily depend on perturbations, and\ninappropriate perturbations may corrupt the graph's inherent information. The\nVector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder\nextensively used in fields such as computer vision; however, its application to\ngraph data remains underexplored. In this paper, we provide an empirical\nanalysis of vector quantization in the context of graph autoencoders,\ndemonstrating its significant enhancement of the model's capacity to capture\ngraph topology. Furthermore, we identify two key challenges associated with\nvector quantization when applying in graph data: codebook underutilization and\ncodebook space sparsity. For the first challenge, we propose an annealing-based\nencoding strategy that promotes broad code utilization in the early stages of\ntraining, gradually shifting focus toward the most effective codes as training\nprogresses. For the second challenge, we introduce a hierarchical two-layer\ncodebook that captures relationships between embeddings through clustering. The\nsecond layer codebook links similar codes, encouraging the model to learn\ncloser embeddings for nodes with similar features and structural topology in\nthe graph. Our proposed model outperforms 16 representative baseline methods in\nself-supervised link prediction and node classification tasks across multiple\ndatasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528VQ-VAE\u6539\u8fdb\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u89e3\u51b3\u6270\u52a8\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u65b0\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6270\u52a8\u53ef\u80fd\u7834\u574f\u56fe\u4fe1\u606f\uff0cVQ-VAE\u5728\u56fe\u6570\u636e\u4e0a\u7684\u5e94\u7528\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u9000\u706b\u7f16\u7801\u7b56\u7565\u548c\u5206\u5c42\u53cc\u5c42\u4ee3\u7801\u672c\uff0c\u89e3\u51b3\u4ee3\u7801\u672c\u5229\u7528\u4e0d\u8db3\u548c\u7a7a\u95f4\u7a00\u758f\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u81ea\u76d1\u7763\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e16\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5411\u91cf\u91cf\u5316\u5728\u56fe\u81ea\u7f16\u7801\u5668\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6539\u8fdb\u7b56\u7565\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.12354", "pdf": "https://arxiv.org/pdf/2504.12354", "abs": "https://arxiv.org/abs/2504.12354", "authors": ["Vinay Shukla", "Prachee Sharma", "Ryan Rossi", "Sungchul Kim", "Tong Yu", "Aditya Grover"], "title": "WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "The ability to embed watermarks in images is a fundamental problem of\ninterest for computer vision, and is exacerbated by the rapid rise of generated\nimagery in recent times. Current state-of-the-art techniques suffer from\ncomputational and statistical challenges such as the slow execution speed for\npractical deployments. In addition, other works trade off fast watermarking\nspeeds but suffer greatly in their robustness or perceptual quality. In this\nwork, we propose WaterFlow (WF), a fast and extremely robust approach for high\nfidelity visual watermarking based on a learned latent-dependent watermark. Our\napproach utilizes a pretrained latent diffusion model to encode an arbitrary\nimage into a latent space and produces a learned watermark that is then planted\ninto the Fourier Domain of the latent. The transformation is specified via\ninvertible flow layers that enhance the expressivity of the latent space of the\npre-trained model to better preserve image quality while permitting robust and\ntractable detection. Most notably, WaterFlow demonstrates state-of-the-art\nperformance on general robustness and is the first method capable of\neffectively defending against difficult combination attacks. We validate our\nfindings on three widely used real and generated datasets: MS-COCO,\nDiffusionDB, and WikiArt.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51faWaterFlow\uff0c\u4e00\u79cd\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u9ad8\u4fdd\u771f\u7684\u89c6\u89c9\u6c34\u5370\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u4f0f\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6c34\u5370\u7684\u57fa\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u751f\u6210\u56fe\u50cf\u7684\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u7684\u8ba1\u7b97\u548c\u7edf\u8ba1\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u7684\u6743\u8861\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u4f0f\u6269\u6563\u6a21\u578b\u5c06\u56fe\u50cf\u7f16\u7801\u5230\u6f5c\u4f0f\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5728\u5085\u91cc\u53f6\u57df\u901a\u8fc7\u53ef\u9006\u6d41\u5c42\u690d\u5165\u5b66\u4e60\u6c34\u5370\uff0c\u4ee5\u63d0\u5347\u8868\u8fbe\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u9c81\u68d2\u6027\u548c\u7ec4\u5408\u653b\u51fb\u9632\u5fa1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u662f\u9996\u4e2a\u6709\u6548\u5e94\u5bf9\u56f0\u96be\u7ec4\u5408\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u5728MS-COCO\u3001DiffusionDB\u548cWikiArt\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "conclusion": "WaterFlow\u662f\u89c6\u89c9\u6c34\u5370\u9886\u57df\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u3001\u5feb\u901f\u548c\u6781\u5f3a\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.12721", "pdf": "https://arxiv.org/pdf/2504.12721", "abs": "https://arxiv.org/abs/2504.12721", "authors": ["Yihang Lu", "Yangyang Xu", "Qitao Qing", "Xianwei Meng"], "title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165TimeCapsule\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u7ef4\u4fe1\u606f\u538b\u7f29\u7edf\u4e00\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08LTSF\uff09\u5173\u952e\u6280\u672f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u662f\u7b80\u5316LTSF\u6a21\u578b\u8bbe\u8ba1\uff0c\u56e0\u4e3a\u7b80\u5355\u67b6\u6784\u5982\u7ebf\u6027\u6a21\u578b\u6216MLP\u5f80\u5f80\u4f18\u4e8e\u590d\u6742\u6a21\u578b\u3002", "method": "\u65b9\u6cd5\u662f\u6784\u5efaTimeCapsule\u6a21\u578b\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e3a3D\u5f20\u91cf\uff0c\u4f7f\u7528\u6a21\u5f0f\u751f\u4ea7\u6355\u83b7\u591a\u6a21\u5f0f\u4f9d\u8d56\u5e76\u5b9e\u73b0\u7ef4\u5ea6\u538b\u7f29\uff0c\u5e76\u901a\u8fc7JEPA\u652f\u6301\u5185\u90e8\u9884\u6d4b\u3002", "result": "\u5728\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTimeCapsule \u53d6\u5f97\u4e86 state-of-the-art \u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u9ad8\u7ef4\u538b\u7f29\u6846\u67b6\u5728LTSF\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355", "abs": "https://arxiv.org/abs/2504.12355", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "ldar Batyrshin", "Grigori Sidorov"], "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faAI\u9a71\u52a8\u7684NLP\u6846\u67b6\uff0c\u4f7f\u7528\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u68c0\u6d4b\u836f\u7269\u548c\u8fc7\u91cf\u75c7\u72b6\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe98%\u3002", "motivation": "\u836f\u7269\u8fc7\u91cf\u662f\u5168\u7403\u5065\u5eb7\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6709\u9650\uff0c\u793e\u4ea4\u5a92\u4f53\u63d0\u4f9b\u5b9e\u65f6\u81ea\u62a5\u6570\u636e\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6807\u6ce8\u7b56\u7565\uff08LLM\u548c\u4eba\u5de5\uff09\uff0c\u5e94\u7528\u4f20\u7edfML\u3001\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u6a21\u578b\u8bad\u7ec3NLP\u6846\u67b6\u3002", "result": "\u591a\u7c7b\u5206\u7c7b\u51c6\u786e\u738798%\uff0c\u591a\u6807\u7b7e\u5206\u7c7b97%\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa8%\u3002", "conclusion": "\u7a81\u663eAI\u5728\u516c\u5171\u536b\u751f\u76d1\u6d4b\u548c\u4e2a\u6027\u5316\u5e72\u9884\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12740", "pdf": "https://arxiv.org/pdf/2504.12740", "abs": "https://arxiv.org/abs/2504.12740", "authors": ["Yifan Cao", "Zhilong Mi", "Ziqiao Yin", "Binghui Guo", "Jin Dong"], "title": "GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faGPMFS\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u4e2a\u6027\u5316\u7279\u5f81\u9009\u62e9\uff0c\u4ee5\u63d0\u5347\u9ad8\u7ef4\u591a\u6807\u7b7e\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6807\u7b7e\u4e2a\u6027\u5316\u7279\u6027\uff0c\u9650\u5236\u4e86\u6027\u80fd\uff1b\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u9ad8\u7ef4\u8bc5\u5492\u5e76\u6355\u83b7\u6807\u7b7e\u7279\u5b9a\u4fe1\u606f\u3002", "method": "GPMFS\u5148\u5229\u7528\u6807\u7b7e\u76f8\u5173\u6027\u8bc6\u522b\u5168\u5c40\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u9608\u503c\u7b56\u7565\u4e3a\u6bcf\u4e2a\u6807\u7b7e\u8865\u5145\u4e2a\u6027\u5316\u7279\u5f81\u5b50\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGPMFS\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u8d8a\uff0c\u5177\u6709\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u6807\u7b7e\u7279\u5b9a\u5f3a\u5ea6\u6d1e\u5bdf\u3002", "conclusion": "\u8bc1\u660e\u4e2a\u6027\u5316\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5fc5\u8981\u4e14\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.12358", "pdf": "https://arxiv.org/pdf/2504.12358", "abs": "https://arxiv.org/abs/2504.12358", "authors": ["Aditi Verma", "Elizabeth Williams"], "title": "Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance", "categories": ["cs.CY", "cs.AI", "physics.soc-ph"], "comment": "Presented at the Sociotechnical AI Governance Workshop at CHI 2025,\n  Yokohama", "summary": "AI models are rapidly becoming embedded in all aspects of nuclear energy\nresearch and work but the safety, security, and safeguards consequences of this\nembedding are not well understood. In this paper, we call for the creation of\nan anticipatory system of governance for AI in the nuclear sector as well as\nthe creation of a global AI observatory as a means for operationalizing\nanticipatory governance. The paper explores the contours of the nuclear AI\nobservatory and an anticipatory system of governance by drawing on work in\nscience and technology studies, public policy, and foresight studies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u5f20\u4e3a\u6838\u9886\u57df\u4eba\u5de5\u667a\u80fd\u5efa\u7acb\u9884\u89c1\u6027\u6cbb\u7406\u7cfb\u7edf\u548c\u5168\u7403\u4eba\u5de5\u667a\u80fd\u89c2\u5bdf\u7ad9\uff0c\u4ee5\u5e94\u5bf9\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u8fc5\u901f\u5d4c\u5165\u6838\u80fd\u7814\u7a76\u548c\u5de5\u4f5c\uff0c\u4f46\u5176\u5bf9\u5b89\u5168\u3001\u5b89\u5168\u4fdd\u969c\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u8ba8\u6cbb\u7406\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u501f\u9274\u79d1\u5b66\u548c\u6280\u672f\u7814\u7a76\u3001\u516c\u5171\u653f\u7b56\u548c\u524d\u77bb\u6027\u7814\u7a76\u9886\u57df\u7684\u6210\u679c\uff0c\u6765\u63a2\u8ba8\u6838\u4eba\u5de5\u667a\u80fd\u89c2\u5bdf\u7ad9\u548c\u9884\u89c1\u6027\u6cbb\u7406\u7cfb\u7edf\u7684\u8f6e\u5ed3\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u5efa\u7acb\u9884\u89c1\u6027\u6cbb\u7406\u7cfb\u7edf\u548c\u5168\u7403\u4eba\u5de5\u667a\u80fd\u89c2\u5bdf\u7ad9\uff0c\u4ee5\u64cd\u4f5c\u5316\u9884\u89c1\u6027\u6cbb\u7406\u3002", "conclusion": "\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u9488\u5bf9\u6838\u9886\u57df\u4eba\u5de5\u667a\u80fd\u7684\u9884\u89c1\u6027\u6cbb\u7406\u7cfb\u7edf\u548c\u5168\u7403\u89c2\u5bdf\u7ad9\uff0c\u4ee5\u7ba1\u7406\u5176\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2504.12742", "pdf": "https://arxiv.org/pdf/2504.12742", "abs": "https://arxiv.org/abs/2504.12742", "authors": ["Yuan Zhou", "Xinli Shi", "Xuelong Li", "Jiachen Zhong", "Guanghui Wen", "Jinde Cao"], "title": "Decentralized Nonconvex Composite Federated Learning with Gradient Tracking and Momentum", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "Decentralized Federated Learning (DFL) eliminates the reliance on the\nserver-client architecture inherent in traditional federated learning,\nattracting significant research interest in recent years. Simultaneously, the\nobjective functions in machine learning tasks are often nonconvex and\nfrequently incorporate additional, potentially nonsmooth regularization terms\nto satisfy practical requirements, thereby forming nonconvex composite\noptimization problems. Employing DFL methods to solve such general optimization\nproblems leads to the formulation of Decentralized Nonconvex Composite\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\npaper, we propose a novel DNCFL algorithm, termed \\bf{DEPOSITUM}. Built upon\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\nheterogeneity by enabling clients to approximate the global gradient. The\nintroduction of momentums in the proximal gradient descent step, replacing\ntracking variables, reduces the variance introduced by stochastic gradients.\nAdditionally, DEPOSITUM supports local updates of client variables,\nsignificantly reducing communication costs. Theoretical analysis demonstrates\nthat DEPOSITUM achieves an expected $\\epsilon$-stationary point with an\niteration complexity of $\\mathcal{O}(1/\\epsilon^2)$. The proximal gradient,\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\nof $\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\nachieves network-independent linear speedup without requiring mega-batch\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\nreal-world datasets, systematically examining the influence of various\nhyperparameters on its performance. Comparisons with other federated composite\noptimization algorithms validate the effectiveness of the proposed method.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faDEPOSITUM\u7b97\u6cd5\uff0c\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316\u975e\u51f8\u590d\u5408\u8054\u90a6\u5b66\u4e60\uff0c\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u5e76\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4f9d\u8d56\u670d\u52a1\u5668\u67b6\u6784\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u975e\u51f8\u4f18\u5316\u548c\u6b63\u5219\u5316\u6761\u4ef6\u4e0b\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u8fd1\u7aef\u968f\u673a\u68af\u5ea6\u8ddf\u8e2a\uff0c\u5f15\u5165\u52a8\u91cf\u548c\u672c\u5730\u66f4\u65b0\uff0c\u964d\u4f4e\u65b9\u5dee\u548c\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8fed\u4ee3\u590d\u6742\u5ea6\u4e3aO(1/\u03b5\u00b2)\uff0c\u5b50\u7ebf\u6027\u6536\u655b\u7387\uff0c\u5e76\u5b9e\u73b0\u7ebf\u6027\u52a0\u901f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u8bc1\u660eDEPOSITUM\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.12764", "pdf": "https://arxiv.org/pdf/2504.12764", "abs": "https://arxiv.org/abs/2504.12764", "authors": ["Hao Xu", "Xiangru Jian", "Xinjian Zhao", "Wei Pang", "Chao Zhang", "Suyuchen Wang", "Qixin Zhang", "Joao Monteiro", "Qiuzhuang Sun", "Tianshu Yu"], "title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks", "categories": ["cs.LG", "cs.DM"], "comment": "82 pages", "summary": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GraphOmni\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u7684\u56fe\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u5206\u6790\u56fe\u7c7b\u578b\u3001\u5e8f\u5217\u5316\u683c\u5f0f\u548c\u63d0\u793a\u65b9\u6848\uff0c\u53d1\u73b0\u65e0\u6700\u4f73\u7b56\u7565\uff0c\u5e76\u63d0\u51faRL-based\u52a8\u6001\u9009\u62e9\u65b9\u6cd5\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u52a8\u673a\u662f\u901a\u8fc7\u5206\u6790LLM\u5728\u56fe\u63a8\u7406\u4e2d\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u8bc6\u522b\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u9a71\u52a8\u6539\u8fdb\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u65b9\u6cd5\u662f\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u9009\u62e9\u5e8f\u5217\u5316-\u63d0\u793a\u914d\u5bf9\u7684\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6ca1\u6709\u5355\u4e00\u5e8f\u5217\u5316\u6216\u63d0\u793a\u7b56\u7565\u4e00\u81f4\u4f18\u4e8e\u5176\u4ed6\uff0c\u4e14\u52a8\u6001\u9009\u62e9\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662fGraphOmni\u7684\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u8bbe\u8ba1\u4e3a\u901a\u7528\u56fe\u63a8\u7406\u6a21\u578b\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2504.12360", "pdf": "https://arxiv.org/pdf/2504.12360", "abs": "https://arxiv.org/abs/2504.12360", "authors": ["Mieczys\u0142aw A. K\u0142opotek", "S\u0142awomir T. Wierzcho\u0144", "Bart\u0142omiej Starosta", "Dariusz Czerski", "Piotr Borkowski"], "title": "A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version", "categories": ["cs.CL", "cs.AI"], "comment": "1 figure, 17 pages, this is an extended version of a paper accepted\n  for the 25th International Conference on Computational Science (ICCS), 7-9\n  July 2025", "summary": "This paper investigates the problem of Graph Spectral Clustering with\nnegative similarities, resulting from document embeddings different from the\ntraditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for\ncombinatorial Laplacians and normalized Laplacians are discussed. An\nexperimental investigation shows the advantages and disadvantages of 6\ndifferent solutions proposed in the literature and in this research. The\nresearch demonstrates that GloVe embeddings frequently cause failures of\nnormalized Laplacian based GSC due to negative similarities. Furthermore,\napplication of methods curing similarity negativity leads to accuracy\nimprovement for both combinatorial and normalized Laplacian based GSC. It also\nleads to applicability for GloVe embeddings of explanation methods developed\noriginally bythe authors for Term Vector Space embeddings.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u8d1f\u76f8\u4f3c\u5ea6\u7684\u56fe\u8c31\u805a\u7c7b\u95ee\u9898\uff0c\u8ba8\u8bba\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u51c6\u786e\u6027\u6539\u8fdb\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u6587\u6863\u5d4c\u5165\uff08\u5982doc2vec\u3001GloVe\uff09\u5e26\u6765\u7684\u8d1f\u76f8\u4f3c\u5ea6\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u4f20\u7edf\u56fe\u8c31\u805a\u7c7b\u65b9\u6cd5\u5931\u8d25\u3002", "method": "\u8ba8\u8bba\u4e86\u7ec4\u5408\u548c\u5f52\u4e00\u5316\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u9a8c\u8c03\u67e5\u4e86\u6587\u732e\u548c\u672c\u7814\u7a76\u63d0\u51fa\u76846\u79cd\u4e0d\u540c\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGloVe\u5d4c\u5165\u7ecf\u5e38\u5bfc\u81f4\u5f52\u4e00\u5316\u62c9\u666e\u62c9\u65af\u57fa\u4e8e\u56fe\u8c31\u805a\u7c7b\u5931\u8d25\uff0c\u4f46\u5904\u7406\u8d1f\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\u53ef\u6539\u5584\u51c6\u786e\u6027\uff0c\u5e76\u6269\u5c55\u89e3\u91ca\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5904\u7406\u8d1f\u76f8\u4f3c\u5ea6\u7684\u6280\u672f\u63d0\u5347\u4e86\u56fe\u8c31\u805a\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u5176\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u6863\u5d4c\u5165\u3002"}}
{"id": "2504.12801", "pdf": "https://arxiv.org/pdf/2504.12801", "abs": "https://arxiv.org/abs/2504.12801", "authors": ["Advait Gadhikar", "Tom Jacobs", "Chao Zhou", "Rebekka Burkholz"], "title": "Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch", "categories": ["cs.LG", "cs.CV"], "comment": "21 pages, 9 figures", "summary": "The performance gap between training sparse neural networks from scratch\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\nfinding a problem specific parameter initialization. As we show, to this end,\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\nreparameterization that provably induces sign flips. Such sign flips are\ncomplementary to the ones that dense-to-sparse training can accomplish,\nrendering Sign-In as an orthogonal method. While our experiments and theory\nsuggest performance improvements of PaI, they also carve out the main open\nchallenge to close the gap between PaI and dense-to-sparse training.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faSign-In\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u53c2\u6570\u5316\u8bf1\u5bfc\u7b26\u53f7\u7ffb\u8f6c\uff0c\u4ee5\u6539\u5584\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u65e8\u5728\u7f29\u5c0f\u4e0e\u7a20\u5bc6\u5230\u7a00\u758f\u8bad\u7ec3\u7684\u5dee\u8ddd\u3002", "motivation": "PaI\u548c\u7a20\u5bc6\u5230\u7a00\u758f\u8bad\u7ec3\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u627e\u5230\u6b63\u786e\u7684\u53c2\u6570\u521d\u59cb\u5316\u7b26\u53f7\u65b9\u9762\u3002", "method": "\u63d0\u51faSign-In\u65b9\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u91cd\u53c2\u6570\u5316\u6765\u8bf1\u5bfc\u7b26\u53f7\u7ffb\u8f6c\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u8868\u660ePaI\u6027\u80fd\u6709\u6240\u6539\u5584\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u7f29\u5c0f\u5dee\u8ddd\u7684\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "Sign-In\u662f\u4e00\u79cd\u6b63\u4ea4\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8ePaI\uff0c\u4f46\u5b8c\u5168\u5173\u95ed\u6027\u80fd\u5dee\u8ddd\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2504.12365", "pdf": "https://arxiv.org/pdf/2504.12365", "abs": "https://arxiv.org/abs/2504.12365", "authors": ["Konstantin Grotov", "Sergey Titov"], "title": "Themisto: Jupyter-Based Runtime Benchmark", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "Accepted to the third Deep Learning for Code (DL4C) workshop @ ICLR\n  2025", "summary": "In this work, we present a benchmark that consists of Jupyter notebooks\ndevelopment trajectories and allows measuring how large language models (LLMs)\ncan leverage runtime information for predicting code output and code\ngeneration. We demonstrate that the current generation of LLMs performs poorly\non these tasks and argue that there exists a significantly understudied domain\nin the development of code-based models, which involves incorporating the\nruntime context.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528Jupyter\u7b14\u8bb0\u672c\u5f00\u53d1\u8f68\u8ff9\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5229\u7528\u8fd0\u884c\u65f6\u4fe1\u606f\u8fdb\u884c\u4ee3\u7801\u8f93\u51fa\u9884\u6d4b\u548c\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524dLLMs\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4ee3\u7801\u6a21\u578b\u4e2d\u6574\u5408\u8fd0\u884c\u65f6\u4e0a\u4e0b\u6587\u7684\u9886\u57df\u4e9f\u5f85\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u5f53\u524dLLMs\u5728\u5229\u7528\u8fd0\u884c\u65f6\u4fe1\u606f\u8fdb\u884c\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8bba\u6587\u65e8\u5728\u7a81\u51fa\u8fd9\u4e00\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u9886\u57df\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7531Jupyter\u7b14\u8bb0\u672c\u5f00\u53d1\u8f68\u8ff9\u7ec4\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6d4b\u91cfLLMs\u5982\u4f55\u5229\u7528\u8fd0\u884c\u65f6\u4fe1\u606f\u3002", "result": "\u5f53\u524d\u4e00\u4ee3LLMs\u5728\u9884\u6d4b\u4ee3\u7801\u8f93\u51fa\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u5728\u4ee3\u7801\u57fa\u7840\u6a21\u578b\u4e2d\u6574\u5408\u8fd0\u884c\u65f6\u4e0a\u4e0b\u6587\uff0c\u8fd9\u662f\u4e00\u4e2a\u88ab\u4f4e\u4f30\u7684\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2504.12803", "pdf": "https://arxiv.org/pdf/2504.12803", "abs": "https://arxiv.org/abs/2504.12803", "authors": ["Nitin Gupta", "Indu Bala", "Bapi Dutta", "Luis Mart\u00ednez", "Anupam Yadav"], "title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790PSO\u4e2d\u4e0d\u540c\u901a\u4fe1\u62d3\u6251\uff08Ring\u3001Star\u548cVon Neumann\uff09\u5bf9\u6536\u655b\u6027\u548c\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4f18\u5316\u4efb\u52a1\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u89e3\u51b3\u7fa4\u4f53\u667a\u80fd\u7b97\u6cd5\u56e0\u914d\u7f6e\u548c\u8d85\u53c2\u6570\u4e0d\u660e\u786e\u5bfc\u81f4\u53ef\u9760\u6027\u4f4e\u7684\u554f\u984c\u3002", "method": "\u4f7f\u7528\u6539\u7f16\u7684IOHxplainer\u5de5\u5177\uff0c\u7ed3\u5408PSO\u7684\u4e0d\u540c\u62d3\u6251\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u7edf\u8ba1\u5206\u6790\u7814\u7a76\u4fe1\u606f\u6d41\u3001\u591a\u6837\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "\u63d0\u5347PSO\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9610\u660e\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\uff0c\u5e76\u7ed9\u51fa\u9009\u62e9\u62d3\u6251\u7684\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u6709\u52a9\u4e8e\u4f7f\u57fa\u4e8e\u7fa4\u4f53\u7684\u4f18\u5316\u66f4\u900f\u660e\u3001\u7a33\u5065\u548c\u53ef\u4fe1\u3002"}}
{"id": "2504.12806", "pdf": "https://arxiv.org/pdf/2504.12806", "abs": "https://arxiv.org/abs/2504.12806", "authors": ["Georgios Papadopoulos", "Shaltiel Eloul", "Yash Satsangi", "Jamie Heredge", "Niraj Kumar", "Chun-Fu Chen", "Marco Pistoia"], "title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "9 pages, 17 figures", "summary": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u6570\u503c\u65b9\u6848\uff0c\u4ece\u53d8\u5206\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08VQNNs\uff09\u7684\u68af\u5ea6\u4e2d\u91cd\u5efa\u8f93\u5165\u6570\u636e\uff0c\u5e76\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "VQNNs\u7684\u635f\u5931\u666f\u89c2\u5b58\u5728\u6307\u6570\u7ea7\u589e\u957f\u7684\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u5bfc\u81f4\u4ece\u68af\u5ea6\u4e2d\u6062\u590d\u4fe1\u606f\u6bd4\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e\u68af\u5ea6\u53cd\u6f14\uff0c\u7ed3\u5408\u68af\u5ea6\u4f30\u8ba1\u3001\u6709\u9650\u5dee\u5206\u65b9\u6cd5\u3001\u81ea\u9002\u5e94\u4f4e\u901a\u6ee4\u6ce2\uff0c\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7b97\u6cd5\u80fd\u53cd\u6f14\u6279\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u524d\u63d0\u662fVQNN\u8fc7\u5ea6\u53c2\u6570\u5316\u3002", "conclusion": "\u8be5\u65b9\u6848\u6210\u529f\u91cd\u5efa\u5b9e\u9645\u8f93\u5165\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5728VQNNs\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.12422", "pdf": "https://arxiv.org/pdf/2504.12422", "abs": "https://arxiv.org/abs/2504.12422", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "title": "Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025", "summary": "High-stakes domains like cyber operations need responsible and trustworthy AI\nmethods. While large language models (LLMs) are becoming increasingly popular\nin these domains, they still suffer from hallucinations. This research paper\nprovides learning outcomes from a case study with LinkQ, an open-source natural\nlanguage interface that was developed to combat hallucinations by forcing an\nLLM to query a knowledge graph (KG) for ground-truth data during\nquestion-answering (QA). We conduct a quantitative evaluation of LinkQ using a\nwell-known KGQA dataset, showing that the system outperforms GPT-4 but still\nstruggles with certain question categories - suggesting that alternative query\nconstruction strategies will need to be investigated in future LLM querying\nsystems. We discuss a qualitative study of LinkQ with two domain experts using\na real-world cybersecurity KG, outlining these experts' feedback, suggestions,\nperceived limitations, and future opportunities for systems like LinkQ.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86 LinkQ \u7cfb\u7edf\uff0c\u901a\u8fc7\u67e5\u8be2\u77e5\u8bc6\u56fe\u8c31\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8ba8\u8bba\u4e86\u5176\u6027\u80fd\u548c\u672a\u6765\u6539\u8fdb\u3002", "motivation": "\u9488\u5bf9\u9ad8\u98ce\u9669\u9886\u57df\u5982\u7f51\u7edc\u64cd\u4f5c\u4e2d AI \u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\uff0c\u56e0\u6b64\u5f00\u53d1 LinkQ \u7cfb\u7edf\u5f3a\u5236 LLM \u67e5\u8be2\u77e5\u8bc6\u56fe\u8c31\u83b7\u53d6\u771f\u5b9e\u6570\u636e\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u81ea\u7136\u8bed\u8a00\u63a5\u53e3 LinkQ\uff0c\u5f3a\u5236 LLM \u5728\u95ee\u7b54\u4e2d\u67e5\u8be2\u77e5\u8bc6\u56fe\u8c31\uff1b\u4f7f\u7528 KGQA \u6570\u636e\u96c6\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\u3002", "result": "LinkQ \u5b9a\u91cf\u4e0a\u4f18\u4e8e GPT-4 \u4f46\u5728\u67d0\u4e9b\u95ee\u9898\u7c7b\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u4e13\u5bb6\u53cd\u9988\u63d0\u4f9b\u4e86\u5efa\u8bae\u3001\u9650\u5236\u548c\u672a\u6765\u673a\u4f1a\u3002", "conclusion": "\u9700\u8981\u8c03\u67e5\u66ff\u4ee3\u67e5\u8be2\u6784\u5efa\u7b56\u7565\uff0c\u4ee5\u6539\u8fdb\u672a\u6765 LLM \u67e5\u8be2\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2504.12828", "pdf": "https://arxiv.org/pdf/2504.12828", "abs": "https://arxiv.org/abs/2504.12828", "authors": ["Vishrut Ramraj", "Nithin Nagaraj", "Harikrishnan N B"], "title": "Predicting Stock Prices using Permutation Decision Trees and Strategic Trailing", "categories": ["cs.LG"], "comment": "17 pages, 7 figures", "summary": "In this paper, we explore the application of Permutation Decision Trees (PDT)\nand strategic trailing for predicting stock market movements and executing\nprofitable trades in the Indian stock market. We focus on high-frequency data\nusing 5-minute candlesticks for the top 50 stocks listed in the NIFTY 50 index.\nWe implement a trading strategy that aims to buy stocks at lower prices and\nsell them at higher prices, capitalizing on short-term market fluctuations. Due\nto regulatory constraints in India, short selling is not considered in our\nstrategy. The model incorporates various technical indicators and employs\nhyperparameters such as the trailing stop-loss value and support thresholds to\nmanage risk effectively. Our results indicate that the proposed trading bot has\nthe potential to outperform the market average and yield returns higher than\nthe risk-free rate offered by 10-year Indian government bonds. We trained and\ntested data on a 60 day dataset provided by Yahoo Finance. Specifically, 12\ndays for testing and 48 days for training. Our bot based on permutation\ndecision tree achieved a profit of 1.3468 % over a 12-day testing period, where\nas a bot based on LSTM gave a return of 0.1238 % over a 12-day testing period\nand a bot based on RNN gave a return of 0.3096 % over a 12-day testing period.\nAll of the bots outperform the buy-and-hold strategy, which resulted in a loss\nof 2.2508 %.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528Permutation Decision Trees\u548c\u6218\u7565\u8ffd\u8e2a\u9884\u6d4b\u5370\u5ea6\u80a1\u7968\u5e02\u573a\uff0c\u6bd4\u8f83LSTM\u548cRNN\uff0c\u663e\u793aPDT\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u5728\u5370\u5ea6\u5e02\u573a\u9884\u6d4b\u80a1\u7968\u8fd0\u52a8\u548c\u76c8\u5229\u4ea4\u6613\u7684\u7b56\u7565\uff0c\u8003\u8651\u76d1\u7ba1\u9650\u5236\u4e0d\u6d89\u53ca\u5356\u7a7a\u3002", "method": "\u4f7f\u75285\u5206\u949fK\u7ebf\u56fe\u3001NIFTY 50\u80a1\u7968\u7684\u6280\u672f\u6307\u6807\u548c\u8d85\u53c2\u6570\uff0c\u8bad\u7ec3PDT\u3001LSTM\u3001RNN\u6a21\u578b\uff0c48\u5929\u8bad\u7ec3\u300112\u5929\u6d4b\u8bd5\u6570\u636e\u3002", "result": "PDT\u673a\u5668\u4eba12\u5929\u76c8\u52291.3468%\uff0cLSTM\u4e3a0.1238%\uff0cRNN\u4e3a0.3096%\uff0c\u5747\u4f18\u4e8e\u6301\u6709\u7b56\u7565\u7684-2.2508%\u3002", "conclusion": "\u6240\u63d0\u673a\u5668\u4eba\u6709\u6f5c\u529b\u8d85\u8d8a\u5e02\u573a\u5e73\u5747\u56de\u62a5\uff0c\u5e76\u9ad8\u4e8e10\u5e74\u671f\u5370\u5ea6\u56fd\u503a\u65e0\u98ce\u9669\u5229\u7387\u3002"}}
{"id": "2504.12424", "pdf": "https://arxiv.org/pdf/2504.12424", "abs": "https://arxiv.org/abs/2504.12424", "authors": ["Ashley Suh", "Kenneth Alperin", "Harry Li", "Steven R Gomez"], "title": "Don't Just Translate, Agitate: Using Large Language Models as Devil's Advocates for AI Explanations", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the Human-centered Explainable AI Workshop (HCXAI) @ CHI\n  2025", "summary": "This position paper highlights a growing trend in Explainable AI (XAI)\nresearch where Large Language Models (LLMs) are used to translate outputs from\nexplainability techniques, like feature-attribution weights, into a natural\nlanguage explanation. While this approach may improve accessibility or\nreadability for users, recent findings suggest that translating into human-like\nexplanations does not necessarily enhance user understanding and may instead\nlead to overreliance on AI systems. When LLMs summarize XAI outputs without\nsurfacing model limitations, uncertainties, or inconsistencies, they risk\nreinforcing the illusion of interpretability rather than fostering meaningful\ntransparency. We argue that - instead of merely translating XAI outputs - LLMs\nshould serve as constructive agitators, or devil's advocates, whose role is to\nactively interrogate AI explanations by presenting alternative interpretations,\npotential biases, training data limitations, and cases where the model's\nreasoning may break down. In this role, LLMs can facilitate users in engaging\ncritically with AI systems and generated explanations, with the potential to\nreduce overreliance caused by misinterpreted or specious explanations.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20LLM\u5728XAI\u4e2d\u5e94\u5145\u5f53\u6279\u8bc4\u8005\u89d2\u8272\uff0c\u4ee5\u6539\u5584\u7528\u6237\u7406\u89e3\u5e76\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u800c\u4e0d\u662f\u4ec5\u8fdb\u884c\u89e3\u91ca\u7ffb\u8bd1\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u5728XAI\u4e2d\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u548c\u89e3\u91ca\u6027\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLLM\u4f5c\u4e3a\u5efa\u8bbe\u6027\u6405\u5c40\u8005\uff0c\u901a\u8fc7\u5ba1\u95ee\u89e3\u91ca\u3001\u7a81\u51fa\u504f\u89c1\u548c\u9650\u5236\u6765\u53d1\u6325\u4f5c\u7528\u3002", "result": "\u6709\u53ef\u80fd\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56\u5e76\u4fc3\u8fdb\u7528\u6237\u4e0eAI\u7cfb\u7edf\u7684\u6279\u5224\u6027\u4e92\u52a8\u3002", "conclusion": "LLM\u5e94\u5e2e\u52a9\u7528\u6237\u6279\u5224\u6027\u5730\u53c2\u4e0eAI\u89e3\u91ca\uff0c\u4ee5\u63d0\u5347\u900f\u660e\u5ea6\u3002"}}
{"id": "2504.12841", "pdf": "https://arxiv.org/pdf/2504.12841", "abs": "https://arxiv.org/abs/2504.12841", "authors": ["Bal\u00e1zs P. Halmos", "Bal\u00e1zs Haj\u00f3s", "Vince \u00c1. Moln\u00e1r", "Marcell T. Kurbucz", "Antal Jakov\u00e1c"], "title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MS", "stat.ML", "62M10, 62H30, 68T05, 68T10", "I.5.1; I.2.6; G.3; D.2.13"], "comment": "16 pages, 4 figures", "summary": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.", "AI": {"tldr": "ALT \u662f\u4e00\u4e2a\u5f00\u6e90 Python \u5305\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53d8\u6362\u6539\u8fdb LLT \u7b97\u6cd5\uff0c\u6355\u83b7\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u6a21\u5f0f\u3002", "motivation": "\u6539\u8fdb LLT \u7b97\u6cd5\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u6a21\u5f0f\uff0c\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u5b9e\u73b0\u81ea\u9002\u5e94\u6cd5\u5f8b-based \u53d8\u6362\uff08ALT\uff09\u7b97\u6cd5\uff0c\u4f7f\u7528\u53ef\u53d8\u957f\u5ea6\u79fb\u4f4d\u65f6\u95f4\u7a97\u53e3\uff0c\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u7ebf\u6027\u53ef\u5206\u7279\u5f81\u7a7a\u95f4\u3002", "result": "ALT \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u5176\u6548\u7528\u3002", "conclusion": "ALT \u5bf9\u4e8e\u7269\u7406\u548c\u76f8\u5173\u9886\u57df\u7684\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u5177\u6709\u9ad8\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.12427", "pdf": "https://arxiv.org/pdf/2504.12427", "abs": "https://arxiv.org/abs/2504.12427", "authors": ["Nikhil Kandpal", "Colin Raffel"], "title": "Position: The Most Expensive Part of an LLM should be its Training Data", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 pages, 3 figures", "summary": "Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u5f20LLM\u8bad\u7ec3\u6570\u636e\u7684\u4eba\u7c7b\u52b3\u52a8\u6210\u672c\u662f\u6700\u5927\u5f00\u9500\uff0c\u5e94\u4e88\u8865\u507f\uff1b\u7814\u7a76\u663e\u793a\u6570\u636e\u6210\u672c\u662f\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u768410-1000\u500d\u3002", "motivation": "\u5f3a\u8c03LLM\u8bad\u7ec3\u4e2d\u4eba\u7c7b\u52b3\u52a8\u6210\u672c\u88ab\u5ffd\u89c6\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u3001\u786c\u4ef6\u548c\u80fd\u6e90\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\u3002", "method": "\u7814\u7a762016-2024\u5e7464\u4e2aLLM\uff0c\u4f30\u8ba1\u4ece\u96f6\u5f00\u59cb\u652f\u4ed8\u4eba\u7c7b\u751f\u4ea7\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6210\u672c\u3002", "result": "\u4fdd\u5b88\u4f30\u8ba1\u4e0b\uff0c\u8bad\u7ec3\u6570\u636e\u96c6\u6210\u672c\u662f\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u768410-1000\u500d\uff0c\u6784\u6210LLM\u63d0\u4f9b\u8005\u7684\u91cd\u5927\u8d22\u52a1\u8d23\u4efb\u3002", "conclusion": "\u7a81\u51fa\u8bad\u7ec3\u6570\u636e\u4ef7\u503c\u4e0e\u8865\u507f\u7f3a\u5931\u7684\u5de8\u5927\u5dee\u8ddd\uff0c\u5e76\u8ba8\u8bba\u672a\u6765\u516c\u5e73\u5b9e\u8df5\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.12849", "pdf": "https://arxiv.org/pdf/2504.12849", "abs": "https://arxiv.org/abs/2504.12849", "authors": ["Phung Lai", "Xiaopeng Jiang", "Hai Phan", "Cristian Borcea", "Khang Tran", "An Chen", "Vijaya Datta Mayyuri", "Ruoming Jin"], "title": "FedX: Adaptive Model Decomposition and Quantization for IoT Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) allows collaborative training among multiple devices\nwithout data sharing, thus enabling privacy-sensitive applications on mobile or\nInternet of Things (IoT) devices, such as mobile health and asset tracking.\nHowever, designing an FL system with good model utility that works with low\ncomputation/communication overhead on heterogeneous, resource-constrained\nmobile/IoT devices is challenging. To address this problem, this paper proposes\nFedX, a novel adaptive model decomposition and quantization FL system for IoT.\nTo balance utility with resource constraints on IoT devices, FedX decomposes a\nglobal FL model into different sub-networks with adaptive numbers of quantized\nbits for different devices. The key idea is that a device with fewer resources\nreceives a smaller sub-network for lower overhead but utilizes a larger number\nof quantized bits for higher model utility, and vice versa. The quantization\noperations in FedX are done at the server to reduce the computational load on\ndevices. FedX iteratively minimizes the losses in the devices' local data and\nin the server's public data using quantized sub-networks under a regularization\nterm, and thus it maximizes the benefits of combining FL with model\nquantization through knowledge sharing among the server and devices in a\ncost-effective training process. Extensive experiments show that FedX\nsignificantly improves quantization times by up to 8.43X, on-device computation\ntime by 1.5X, and total end-to-end training time by 1.36X, compared with\nbaseline FL systems. We guarantee the global model convergence theoretically\nand validate local model convergence empirically, highlighting FedX's\noptimization efficiency.", "AI": {"tldr": "FedX \u662f\u4e00\u79cd\u81ea\u9002\u5e94\u6a21\u578b\u5206\u89e3\u548c\u91cf\u5316\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u65e8\u5728\u5728\u8d44\u6e90\u53d7\u9650\u7684 IoT \u8bbe\u5907\u4e0a\u63d0\u9ad8\u6a21\u578b\u6548\u7528\u548c\u6548\u7387\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u5728\u5f02\u6784\u3001\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8/IoT \u8bbe\u5907\u4e0a\u5177\u6709\u826f\u597d\u6a21\u578b\u6548\u7528\u548c\u4f4e\u8ba1\u7b97/\u901a\u4fe1\u5f00\u9500\u7684 FL \u7cfb\u7edf\u3002", "method": "FedX \u5c06\u5168\u5c40\u6a21\u578b\u5206\u89e3\u4e3a\u5b50\u7f51\u7edc\uff0c\u6839\u636e\u8bbe\u5907\u8d44\u6e90\u81ea\u9002\u5e94\u8c03\u6574\u91cf\u5316\u4f4d\u6570\uff0c\u670d\u52a1\u5668\u7aef\u8fdb\u884c\u91cf\u5316\u64cd\u4f5c\uff0c\u901a\u8fc7\u8fed\u4ee3\u6700\u5c0f\u5316\u635f\u5931\u6765\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a FedX \u6539\u5584\u4e86\u91cf\u5316\u65f6\u95f4\uff08\u6700\u9ad8 8.43 \u500d\uff09\u3001\u8bbe\u5907\u8ba1\u7b97\u65f6\u95f4\uff081.5 \u500d\uff09\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\uff081.36 \u500d\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "FedX \u901a\u8fc7\u7ed3\u5408 FL \u548c\u6a21\u578b\u91cf\u5316\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u7406\u8bba\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u6a21\u578b\u6536\u655b\u3002"}}
{"id": "2504.12436", "pdf": "https://arxiv.org/pdf/2504.12436", "abs": "https://arxiv.org/abs/2504.12436", "authors": ["Nairouz Mrabah", "Nicolas Richet", "Ismail Ben Ayed", "\u00c9ric Granger"], "title": "Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation", "categories": ["cs.CV", "cs.AI", "I.4.8; I.5.1; G.1.6"], "comment": "Under review", "summary": "Adapting Vision-Language Models (VLMs) to new domains with few labeled\nsamples remains a significant challenge due to severe overfitting and\ncomputational constraints. State-of-the-art solutions, such as low-rank\nreparameterization, mitigate these issues but often struggle with\ngeneralization and require extensive hyperparameter tuning. In this paper, a\nnovel Sparse Optimization (SO) framework is proposed. Unlike low-rank\napproaches that typically constrain updates to a fixed subspace, our SO method\nleverages high sparsity to dynamically adjust very few parameters. We introduce\ntwo key paradigms. First, we advocate for \\textit{local sparsity and global\ndensity}, which updates a minimal subset of parameters per iteration while\nmaintaining overall model expressiveness. As a second paradigm, we advocate for\n\\textit{local randomness and global importance}, which sparsifies the gradient\nusing random selection while pruning the first moment based on importance. This\ncombination significantly mitigates overfitting and ensures stable adaptation\nin low-data regimes. Extensive experiments on 11 diverse datasets show that SO\nachieves state-of-the-art few-shot adaptation performance while reducing memory\noverhead.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u4e0b\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8fc7\u62df\u5408\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5230\u65b0\u9886\u57df\u65f6\uff0c\u9762\u4e34\u4e25\u91cd\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u73b0\u6709\u4f4e\u79e9\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u548c\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u4f18\u5316\uff08SO\uff09\u6846\u67b6\uff0c\u5305\u62ec\u5c40\u90e8\u7a00\u758f\u4e0e\u5168\u5c40\u7a20\u5bc6\u3001\u5c40\u90e8\u968f\u673a\u6027\u4e0e\u5168\u5c40\u91cd\u8981\u6027\u4e24\u4e2a\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c11\u91cf\u53c2\u6570\u7f13\u89e3\u8fc7\u62df\u5408\u3002", "result": "\u572811\u4e2a\u591a\u6837\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSO\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c11\u6837\u672c\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "SO\u65b9\u6cd5\u663e\u8457\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u786e\u4fdd\u4f4e\u6570\u636e\u60c5\u5883\u4e0b\u7684\u7a33\u5b9a\u9002\u5e94\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.12850", "pdf": "https://arxiv.org/pdf/2504.12850", "abs": "https://arxiv.org/abs/2504.12850", "authors": ["Khaled SH. Raslan", "Almohammady S. Alsharkawy", "K. R. Raslan"], "title": "iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise to Improve Imbalanced Data Classification", "categories": ["cs.LG"], "comment": null, "summary": "Classifying imbalanced datasets remains a significant challenge in machine\nlearning, particularly with big data where instances are unevenly distributed\namong classes, leading to class imbalance issues that impact classifier\nperformance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses\nthis challenge by generating new instances for the under-represented minority\nclass, it faces obstacles in the form of noise and outliers during the creation\nof new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses\nthe limitations of SMOTE by first cleansing the data from noise points. This\nprocess involves employing feature selection using a random forest to identify\nthe most valuable features, followed by applying the Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm to detect outliers\nbased on the selected features. The identified outliers from the minority\nclasses are then removed, creating a refined dataset for subsequent\noversampling using the hybrid approach called iHHO-SMOTe. The comprehensive\nexperiments across diverse datasets demonstrate the exceptional performance of\nthe proposed model, with an AUC score exceeding 0.99, a high G-means score of\n0.99 highlighting its robustness, and an outstanding F1-score consistently\nexceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a\nformidable contender in addressing imbalanced datasets, focusing on noise\nreduction and outlier handling for improved classification models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faiHHO-SMOTe\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6e05\u9664\u548c\u5f02\u5e38\u503c\u5904\u7406\u6539\u8fdbSMOTE\uff0c\u9488\u5bf9\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u5206\u7c7b\u95ee\u9898\uff0c\u5b9e\u9a8c\u6027\u80fd\u5353\u8d8a\u3002", "motivation": "\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u5206\u7c7b\u6311\u6218\uff0cSMOTE\u5b58\u5728\u566a\u58f0\u548c\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0cDBSCAN\u68c0\u6d4b\u5e76\u79fb\u9664\u5c11\u6570\u7c7b\u5f02\u5e38\u503c\uff0c\u7136\u540e\u5e94\u7528iHHO-SMOTe\u6df7\u5408\u8fc7\u91c7\u6837\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAUC\u8d85\u8fc70.99\uff0cG-means\u4e3a0.99\uff0cF1-score consistently\u8d85\u8fc70.967\u3002", "conclusion": "Cleansed iHHO-SMOTe\u662f\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u5f3a\u6709\u529b\u65b9\u6cd5\uff0c\u5f3a\u8c03\u566a\u58f0\u51cf\u5c11\u548c\u5f02\u5e38\u503c\u5904\u7406\u4ee5\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2504.12875", "pdf": "https://arxiv.org/pdf/2504.12875", "abs": "https://arxiv.org/abs/2504.12875", "authors": ["Phung Lai", "Guanxiong Liu", "Hai Phan", "Issa Khalil", "Abdallah Khreishah", "Xintao Wu"], "title": "A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training using\ndecentralized private data from multiple clients. While FL has shown robustness\nagainst poisoning attacks with basic defenses, our research reveals new\nvulnerabilities stemming from non-independent and identically distributed\n(non-IID) data among clients. These vulnerabilities pose a substantial risk of\nmodel poisoning in real-world FL scenarios.\n  To demonstrate such vulnerabilities, we develop a novel collaborative\nbackdoor poisoning attack called CollaPois. In this attack, we distribute a\nsingle pre-trained model infected with a Trojan to a group of compromised\nclients. These clients then work together to produce malicious gradients,\ncausing the FL model to consistently converge towards a low-loss region\ncentered around the Trojan-infected model. Consequently, the impact of the\nTrojan is amplified, especially when the benign clients have diverse local data\ndistributions and scattered local gradients. CollaPois stands out by achieving\nits goals while involving only a limited number of compromised clients, setting\nit apart from existing attacks. Also, CollaPois effectively avoids noticeable\nshifts or degradation in the FL model's performance on legitimate data samples,\nallowing it to operate stealthily and evade detection by advanced robust FL\nalgorithms.\n  Thorough theoretical analysis and experiments conducted on various benchmark\ndatasets demonstrate the superiority of CollaPois compared to state-of-the-art\nbackdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,\nespecially in scenarios where clients possess diverse data distributions.\nMoreover, the results show that CollaPois remains effective even when involving\na small number of compromised clients. Notably, clients whose local data is\nclosely aligned with compromised clients experience higher risks of backdoor\ninfections.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165CollaPois\uff0c\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u4e2d\u5229\u7528\u975eIID\u6570\u636e\u8fdb\u884c\u534f\u4f5c\u540e\u95e8\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u80fd\u5728\u5c11\u91cf\u53d7\u635f\u5ba2\u6237\u7aef\u4e0b\u9690\u853d\u5730\u6bd2\u5316\u6a21\u578b\u3002", "motivation": "\u63ed\u793a\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5e26\u6765\u7684\u65b0\u6f0f\u6d1e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u4e2d\u6bd2\u98ce\u9669\u3002", "method": "\u5f00\u53d1CollaPois\u653b\u51fb\uff0c\u5c06\u6728\u9a6c\u611f\u67d3\u6a21\u578b\u5206\u53d1\u7ed9\u53d7\u635f\u5ba2\u6237\u7aef\uff0c\u534f\u4f5c\u4ea7\u751f\u6076\u610f\u68af\u5ea6\uff0c\u4f7f\u6a21\u578b\u6536\u655b\u5230\u611f\u67d3\u533a\u57df\u3002", "result": "CollaPois\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\uff0c\u80fd\u7ed5\u8fc7\u9632\u5fa1\uff0c\u5c24\u5176\u5728\u6570\u636e\u5206\u5e03\u591a\u6837\u5316\u573a\u666f\u4e0b\uff0c\u4e14\u5c11\u91cf\u53d7\u635f\u5ba2\u6237\u7aef\u5373\u53ef\u6709\u6548\u3002", "conclusion": "\u7a81\u663e\u8054\u90a6\u5b66\u4e60\u5b89\u5168\u98ce\u9669\uff0c\u8bc1\u660eCollaPois\u5728\u975eIID\u6570\u636e\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12459", "pdf": "https://arxiv.org/pdf/2504.12459", "abs": "https://arxiv.org/abs/2504.12459", "authors": ["Jack Merullo", "Noah A. Smith", "Sarah Wiegreffe", "Yanai Elazar"], "title": "On Linear Representations and Pretraining Data Frequency in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u9884\u8bad\u7ec3\u6570\u636e\u9891\u7387\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u7ebf\u6027\u8868\u793a\uff0c\u5e76\u53d1\u73b0\u76f8\u5173\u6027\u548c\u4e00\u79cd\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u8bed\u8a00\u6a21\u578b\u8868\u793a\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7ebf\u6027\u8868\u793a\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u672f\u8bed\u9891\u7387\u4e0e\u7ebf\u6027\u8868\u793a\u7684\u5173\u8054\uff0c\u4f7f\u7528OLMo-7B\u548cGPT-J\u6a21\u578b\uff0c\u5e76\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u672f\u8bed\u9891\u7387\u3002", "result": "\u53d1\u73b0\u4e3b\u8bed\u548c\u5bbe\u8bed\u5171\u73b0\u9891\u7387\u5206\u522b\u81f3\u5c111k\u548c2k\u65f6\uff0c\u7ebf\u6027\u8868\u793a\u5f62\u6210\uff1b\u56de\u5f52\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u9884\u6d4b\u51c6\u786e\u3002", "conclusion": "\u7ebf\u6027\u8868\u793a\u5f3a\u5ea6\u53cd\u6620\u9884\u8bad\u7ec3\u8bed\u6599\u4fe1\u606f\uff0c\u53ef\u901a\u8fc7\u64cd\u7eb5\u6570\u636e\u9891\u7387\u63a7\u5236\u6a21\u578b\u884c\u4e3a\u3002"}}
{"id": "2504.12880", "pdf": "https://arxiv.org/pdf/2504.12880", "abs": "https://arxiv.org/abs/2504.12880", "authors": ["Lukas Rauch", "Ilyass Moummad", "Ren\u00e9 Heinrich", "Alexis Joly", "Bernhard Sick", "Christoph Scholz"], "title": "Can Masked Autoencoders Also Listen to Birds?", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.", "AI": {"tldr": "Bird-MAE \u662f\u4e00\u79cd\u9488\u5bf9\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7b\u7684\u4e13\u7528\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u5728 BirdSet \u4e0a\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u539f\u578b\u63a2\u6d4b\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u901a\u7528 Masked Autoencoders \u65e0\u6cd5\u6355\u83b7\u751f\u7269\u58f0\u5b66\u76d1\u6d4b\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u7ec6\u7c92\u5ea6\u58f0\u5b66\u7279\u5f81\uff0c\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7b\u5bf9\u73af\u5883\u5065\u5eb7\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165 Bird-MAE \u5728 BirdSet \u4e0a\u9884\u8bad\u7ec3\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u51bb\u7ed3\u8868\u793a\u7684\u8c03\u6574\uff0c\u5e76\u63d0\u51fa\u539f\u578b\u63a2\u6d4b\u65b9\u6cd5\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u7684\u5229\u7528\u65b9\u5f0f\u3002", "result": "Bird-MAE \u5728 BirdSet \u6240\u6709\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\u5927\u5e45\u63d0\u5347\uff1b\u539f\u578b\u63a2\u6d4b\u6bd4\u7ebf\u6027\u63a2\u6d4b\u63d0\u9ad8\u9ad8\u8fbe 37% \u7684 MAP\uff0c\u5e76\u5c06\u4e0e\u5fae\u8c03\u7684\u5dee\u8ddd\u7f29\u5c0f\u5230\u5e73\u5747 3%\u3002", "conclusion": "Bird-MAE \u548c\u539f\u578b\u63a2\u6d4b\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u9886\u57df\u7279\u5b9a\u97f3\u9891\u4efb\u52a1\u4e2d\uff0c\u4e13\u7528\u6a21\u578b\u548c\u9ad8\u6548\u63a2\u6d4b\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2504.12883", "pdf": "https://arxiv.org/pdf/2504.12883", "abs": "https://arxiv.org/abs/2504.12883", "authors": ["Tom Jacobs", "Chao Zhou", "Rebekka Burkholz"], "title": "Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?", "categories": ["cs.LG"], "comment": "26 pages, 16 figures", "summary": "Implicit bias plays an important role in explaining how overparameterized\nmodels generalize well. Explicit regularization like weight decay is often\nemployed in addition to prevent overfitting. While both concepts have been\nstudied separately, in practice, they often act in tandem. Understanding their\ninterplay is key to controlling the shape and strength of implicit bias, as it\ncan be modified by explicit regularization. To this end, we incorporate\nexplicit regularization into the mirror flow framework and analyze its lasting\neffects on the geometry of the training dynamics, covering three distinct\neffects: positional bias, type of bias, and range shrinking. Our analytical\napproach encompasses a broad class of problems, including sparse coding, matrix\nsensing, single-layer attention, and LoRA, for which we demonstrate the utility\nof our insights. To exploit the lasting effect of regularization and highlight\nthe potential benefit of dynamic weight decay schedules, we propose to switch\noff weight decay during training, which can improve generalization, as we\ndemonstrate in experiments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u663e\u5f0f\u6b63\u5219\u5316\u5982\u4f55\u5f71\u54cd\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u7684\u9690\u5f0f\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u955c\u50cf\u6d41\u6846\u67b6\u5206\u6790\u5176\u52a8\u6001\u6548\u679c\uff0c\u63d0\u51fa\u52a8\u6001\u6743\u91cd\u8870\u51cf\u4ee5\u63d0\u5347\u6cdb\u5316\u3002", "motivation": "\u7406\u89e3\u9690\u5f0f\u504f\u5dee\u4e0e\u663e\u5f0f\u6b63\u5219\u5316\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u63a7\u5236\u6a21\u578b\u7684\u6cdb\u5316\u504f\u5dee\u5f62\u72b6\u548c\u5f3a\u5ea6\u3002", "method": "\u5c06\u663e\u5f0f\u6b63\u5219\u5316\u878d\u5165\u955c\u50cf\u6d41\u6846\u67b6\uff0c\u5206\u6790\u5176\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u6301\u4e45\u5f71\u54cd\uff0c\u5305\u62ec\u4f4d\u7f6e\u504f\u5dee\u3001\u504f\u5dee\u7c7b\u578b\u548c\u8303\u56f4\u6536\u7f29\u3002", "result": "\u5728\u7a00\u758f\u7f16\u7801\u3001\u77e9\u9635\u611f\u77e5\u3001\u5355\u5c42\u6ce8\u610f\u529b\u53caLoRA\u7b49\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6d1e\u89c1\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\u52a8\u6001\u5173\u95ed\u6743\u91cd\u8870\u51cf\u53ef\u6539\u5584\u6cdb\u5316\u3002", "conclusion": "\u5efa\u8bae\u4f7f\u7528\u52a8\u6001\u6743\u91cd\u8870\u51cf\u8c03\u5ea6\uff0c\u5229\u7528\u6b63\u5219\u5316\u7684\u6301\u4e45\u6548\u679c\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.12474", "pdf": "https://arxiv.org/pdf/2504.12474", "abs": "https://arxiv.org/abs/2504.12474", "authors": ["Azadeh Beiranvand", "Seyed Mehdi Vahidipour"], "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBiGTex\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\u7684\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u6587\u672c\u5c5e\u6027\u56fe\u9700\u8981\u540c\u65f6\u6355\u6349\u8282\u70b9\u6587\u672c\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u56fe\u7684\u7ed3\u6784\u4f9d\u8d56\u6027\uff0c\u4f46GNN\u65e0\u6cd5\u5904\u7406\u6587\u672c\uff0cLLM\u5ffd\u7565\u56fe\u7ed3\u6784\u3002", "method": "\u63d0\u51faBiGTex\u67b6\u6784\uff0c\u4f7f\u7528\u5806\u53e0\u7684\u56fe-\u6587\u672c\u878d\u5408\u5355\u5143\u5b9e\u73b0\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u5e76\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08LoRA\uff09\u8bad\u7ec3\uff0c\u4fdd\u6301LLM\u51bb\u7ed3\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBiGTex\u5728\u8282\u70b9\u5206\u7c7b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u6cdb\u5316\u5230\u94fe\u63a5\u9884\u6d4b\uff1b\u6d88\u878d\u7814\u7a76\u5f3a\u8c03\u4e86\u8f6f\u63d0\u793a\u548c\u53cc\u5411\u6ce8\u610f\u529b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "BiGTex\u6709\u6548\u6574\u5408GNN\u548cLLM\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u5c5e\u6027\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2504.12916", "pdf": "https://arxiv.org/pdf/2504.12916", "abs": "https://arxiv.org/abs/2504.12916", "authors": ["Nischal Mainali", "Lucas Teixeira"], "title": "Exact Learning Dynamics of In-Context Learning in Linear Transformers and Its Application to Non-Linear Transformers", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": "10 pages, 7 figures", "summary": "Transformer models exhibit remarkable in-context learning (ICL), adapting to\nnovel tasks from examples within their context, yet the underlying mechanisms\nremain largely mysterious. Here, we provide an exact analytical\ncharacterization of ICL emergence by deriving the closed-form stochastic\ngradient descent (SGD) dynamics for a simplified linear transformer performing\nregression tasks. Our analysis reveals key properties: (1) a natural separation\nof timescales directly governed by the input data's covariance structure,\nleading to staged learning; (2) an exact description of how ICL develops,\nincluding fixed points corresponding to learned algorithms and conservation\nlaws constraining the dynamics; and (3) surprisingly nonlinear learning\nbehavior despite the model's linearity. We hypothesize this phenomenology\nextends to non-linear models. To test this, we introduce theory-inspired\nmacroscopic measures (spectral rank dynamics, subspace stability) and use them\nto provide mechanistic explanations for (1) the sudden emergence of ICL in\nattention-only networks and (2) delayed generalization (grokking) in modular\narithmetic models. Our work offers an exact dynamical model for ICL and\ntheoretically grounded tools for analyzing complex transformer training.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u4e3a\u7b80\u5316\u7ebf\u6027Transformer\u63a8\u5bfc\u5c01\u95ed\u5f62\u5f0f\u7684SGD\u52a8\u6001\uff0c\u5206\u6790\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u51fa\u73b0\uff0c\u63ed\u793a\u5173\u952e\u673a\u5236\uff0c\u5e76\u5e94\u7528\u4e8e\u89e3\u91ca\u5176\u4ed6\u6a21\u578b\u7684\u73b0\u8c61\u3002", "motivation": "\u63ed\u793aTransformer\u6a21\u578b\u4e2dICL\u7684\u795e\u79d8\u5e95\u5c42\u673a\u5236\u3002", "method": "\u63a8\u5bfc\u7ebf\u6027Transformer\u7684SGD\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u7406\u8bba\u542f\u53d1\u7684\u5b8f\u89c2\u6d4b\u91cf\u5982\u8c31\u79e9\u52a8\u6001\u548c\u5b50\u7a7a\u95f4\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u3001\u5206\u9636\u6bb5\u5b66\u4e60\u3001\u56fa\u5b9a\u70b9\u3001\u5b88\u6052\u5b9a\u5f8b\u548c\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u5e76\u89e3\u91caICL\u7684\u7a81\u7136\u51fa\u73b0\u548c\u5ef6\u8fdf\u6cdb\u5316\u3002", "conclusion": "\u63d0\u4f9b\u7cbe\u786e\u7684ICL\u52a8\u6001\u6a21\u578b\u548c\u7406\u8bba\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790Transformer\u8bad\u7ec3\u3002"}}
{"id": "2504.12476", "pdf": "https://arxiv.org/pdf/2504.12476", "abs": "https://arxiv.org/abs/2504.12476", "authors": ["Andreas Jungherr", "Adrian Rauchfleisch"], "title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Recent advances in generative Artificial Intelligence have raised public\nawareness, shaping expectations and concerns about their societal implications.\nCentral to these debates is the question of AI alignment -- how well AI systems\nmeet public expectations regarding safety, fairness, and social values.\nHowever, little is known about what people expect from AI-enabled systems and\nhow these expectations differ across national contexts. We present evidence\nfrom two surveys of public preferences for key functional features of\nAI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We\nexamine support for four types of alignment in AI moderation: accuracy and\nreliability, safety, bias mitigation, and the promotion of aspirational\nimaginaries. U.S. respondents report significantly higher AI use and\nconsistently greater support for all alignment features, reflecting broader\ntechnological openness and higher societal involvement with AI. In both\ncountries, accuracy and safety enjoy the strongest support, while more\nnormatively charged goals -- like fairness and aspirational imaginaries --\nreceive more cautious backing, particularly in Germany. We also explore how\nindividual experience with AI, attitudes toward free speech, political\nideology, partisan affiliation, and gender shape these preferences. AI use and\nfree speech support explain more variation in Germany. In contrast, U.S.\nresponses show greater attitudinal uniformity, suggesting that higher exposure\nto AI may consolidate public expectations. These findings contribute to debates\non AI governance and cross-national variation in public preferences. More\nbroadly, our study demonstrates the value of empirically grounding AI alignment\ndebates in public attitudes and of explicitly developing normatively grounded\nexpectations into theoretical and policy discussions on the governance of\nAI-generated content.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5fb7\u56fd\u548c\u7f8e\u56fd\u516c\u4f17\u5bf9AI\u5bf9\u9f50\u7279\u5f81\u7684\u504f\u597d\uff0c\u53d1\u73b0\u7f8e\u56fd\u652f\u6301\u5ea6\u66f4\u9ad8\uff0c\u51c6\u786e\u6027\u548c\u5b89\u5168\u6700\u53d7\u91cd\u89c6\u3002", "motivation": "AI\u8fdb\u5c55\u5f15\u53d1\u516c\u4f17\u5bf9\u5b89\u5168\u3001\u516c\u5e73\u548c\u793e\u4f1a\u4ef7\u503c\u7684\u671f\u671b\uff0c\u9700\u8981\u4e86\u89e3\u8de8\u56fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5728\u5fb7\u56fd\uff08n=1800\uff09\u548c\u7f8e\u56fd\uff08n=1756\uff09\u8fdb\u884c\u7684\u4e24\u9879\u8c03\u67e5\uff0c\u8003\u5bdfAI\u8c03\u8282\u7684\u56db\u79cd\u5bf9\u9f50\u7c7b\u578b\uff1a\u51c6\u786e\u6027\u3001\u5b89\u5168\u3001\u504f\u89c1\u7f13\u89e3\u548c\u7406\u60f3\u5316\u60f3\u8c61\u3002", "result": "\u7f8e\u56fd\u53d7\u8bbf\u8005AI\u4f7f\u7528\u66f4\u9ad8\uff0c\u652f\u6301\u6240\u6709\u5bf9\u9f50\u7279\u5f81\uff1b\u51c6\u786e\u6027\u548c\u5b89\u5168\u6700\u53d7\u6b22\u8fce\uff0c\u5fb7\u56fd\u66f4\u8c28\u614e\uff1bAI\u4f7f\u7528\u3001\u8a00\u8bba\u81ea\u7531\u7b49\u56e0\u7d20\u5f71\u54cd\u504f\u597d\u3002", "conclusion": "\u4e3aAI\u6cbb\u7406\u8fa9\u8bba\u63d0\u4f9b\u8bc1\u636e\uff0c\u5f3a\u8c03\u57fa\u4e8e\u516c\u4f17\u6001\u5ea6\u7684\u7406\u8bba\u548c\u653f\u7b56\u8ba8\u8bba\u3002"}}
{"id": "2504.12918", "pdf": "https://arxiv.org/pdf/2504.12918", "abs": "https://arxiv.org/abs/2504.12918", "authors": ["Julien Pallage", "Antoine Lesage-Landry"], "title": "Sliced-Wasserstein Distance-based Data Selection", "categories": ["cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2410.21712", "summary": "We propose a new unsupervised anomaly detection method based on the\nsliced-Wasserstein distance for training data selection in machine learning\napproaches. Our filtering technique is interesting for decision-making\npipelines deploying machine learning models in critical sectors, e.g., power\nsystems, as it offers a conservative data selection and an optimal transport\ninterpretation. To ensure the scalability of our method, we provide two\nefficient approximations. The first approximation processes reduced-cardinality\nrepresentations of the datasets concurrently. The second makes use of a\ncomputationally light Euclidian distance approximation. Additionally, we open\nthe first dataset showcasing localized critical peak rebate demand response in\na northern climate. We present the filtering patterns of our method on\nsynthetic datasets and numerically benchmark our method for training data\nselection. Finally, we employ our method as part of a first forecasting\nbenchmark for our open-source dataset.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5207\u7247Wasserstein\u8ddd\u79bb\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u540c\u65f6\u5f00\u653e\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u5173\u952e\u9886\u57df\u5982\u7535\u529b\u7cfb\u7edf\u7684\u673a\u5668\u5b66\u4e60\u51b3\u7b56\u7ba1\u9053\u63d0\u4f9b\u4fdd\u5b88\u7684\u6570\u636e\u9009\u62e9\u548c\u6700\u4f18\u4f20\u8f93\u89e3\u91ca\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u5207\u7247Wasserstein\u8ddd\u79bb\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u5305\u62ec\u4e24\u4e2a\u9ad8\u6548\u8fd1\u4f3c\uff1a\u5904\u7406\u51cf\u5c11\u57fa\u6570\u7684\u6570\u636e\u96c6\u8868\u793a\u548c\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7684\u8f7b\u91cf\u8ba1\u7b97\u8fd1\u4f3c\u3002", "result": "\u7ed3\u679c\u5c55\u793a\u4e86\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8fc7\u6ee4\u6a21\u5f0f\u3001\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\u4e86\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u65b0\u6570\u636e\u96c6\u7684\u9884\u6d4b\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u7684\u53ef\u9760\u6027\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.12488", "pdf": "https://arxiv.org/pdf/2504.12488", "abs": "https://arxiv.org/abs/2504.12488", "authors": ["Mohi Reza", "Jeb Thomas-Mitchell", "Peter Dushniku", "Nathan Laundry", "Joseph Jay Williams", "Anastasia Kuzminykh"], "title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process", "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7; I.2.6; I.7.2"], "comment": null, "summary": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u548c\u8bbf\u8c08\uff0c\u63a2\u8ba8AI\u5bf9\u5199\u4f5c\u8fc7\u7a0b\u548c\u4f5c\u5bb6\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u7b56\u7565\u3002", "motivation": "\u586b\u8865\u5bf9AI\u8f85\u52a9\u5199\u4f5c\u5982\u4f55\u5f71\u54cd\u5199\u4f5c\u8fc7\u7a0b\u548c\u4f5c\u5bb6\u4ee3\u7406\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u7a7a\u767d\u3002", "method": "\u4f7f\u7528PRISMA\u65b9\u6cd5\u7cfb\u7edf\u56de\u987e109\u7bc7HCI\u8bba\u6587\uff0c\u5e76\u7ed3\u540815\u4f4d\u4e0d\u540c\u9886\u57df\u4f5c\u5bb6\u7684\u8bbf\u8c08\u3002", "result": "\u8bc6\u522b\u56db\u79cd\u8bbe\u8ba1\u7b56\u7565\uff08\u7ed3\u6784\u5316\u6307\u5bfc\u3001\u5f15\u5bfc\u63a2\u7d22\u3001\u4e3b\u52a8\u5171\u5199\u3001\u6279\u5224\u6027\u53cd\u9988\uff09\uff0c\u5e76\u63ed\u793a\u4f5c\u5bb6\u504f\u597d\u968f\u5199\u4f5c\u9636\u6bb5\u3001\u76ee\u6807\u548c\u4ef7\u503c\u89c2\u7684\u53d8\u5316\u3002", "conclusion": "\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5bfc\uff0c\u4ee5\u5f00\u53d1\u4ee5\u4eba\u4e3a\u672c\u7684AI\u5199\u4f5c\u5de5\u5177\u3002"}}
{"id": "2504.12921", "pdf": "https://arxiv.org/pdf/2504.12921", "abs": "https://arxiv.org/abs/2504.12921", "authors": ["Daniel Homm", "Patrick Carqueville", "Christian Eichhorn", "Thomas Weikert", "Thomas Menard", "David A. Plecher", "Chris Awai Easthope"], "title": "IdentiARAT: Toward Automated Identification of Individual ARAT Items from Wearable Sensors", "categories": ["cs.LG"], "comment": null, "summary": "This study explores the potential of using wrist-worn inertial sensors to\nautomate the labeling of ARAT (Action Research Arm Test) items. While the ARAT\nis commonly used to assess upper limb motor function, its limitations include\nsubjectivity and time consumption of clinical staff. By using IMU (Inertial\nMeasurement Unit) sensors and MiniROCKET as a time series classification\ntechnique, this investigation aims to classify ARAT items based on sensor\nrecordings. We test common preprocessing strategies to efficiently leverage\nincluded information in the data. Afterward, we use the best preprocessing to\nimprove the classification. The dataset includes recordings of 45 participants\nperforming various ARAT items. Results show that MiniROCKET offers a fast and\nreliable approach for classifying ARAT domains, although challenges remain in\ndistinguishing between individual resembling items. Future work may involve\nimproving classification through more advanced machine-learning models and data\nenhancements.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u8155\u6234\u5f0f\u60ef\u6027\u4f20\u611f\u5668\u548cMiniROCKET\u81ea\u52a8\u6807\u8bb0ARAT\u9879\u76ee\uff0c\u5c55\u793a\u4e86\u5feb\u901f\u5206\u7c7b\u7684\u4f18\u52bf\uff0c\u4f46\u76f8\u4f3c\u9879\u76ee\u533a\u5206\u6709\u6311\u6218\u3002", "motivation": "ARAT\u8bc4\u4f30\u4e3b\u89c2\u4e14\u8017\u8d39\u65f6\u95f4\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528IMU\u4f20\u611f\u5668\u548cMiniROCKET\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6280\u672f\uff0c\u6d4b\u8bd5\u9884\u5904\u7406\u7b56\u7565\uff0c\u6570\u636e\u96c6\u6765\u81ea45\u540d\u53c2\u4e0e\u8005\u7684\u8bb0\u5f55\u3002", "result": "MiniROCKET\u5728\u5206\u7c7bARAT\u9886\u57df\u65f6\u5feb\u901f\u53ef\u9760\uff0c\u4f46\u5bf9\u76f8\u4f3c\u9879\u76ee\u7684\u533a\u5206\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u4f7f\u7528\u66f4\u9ad8\u7ea7\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u6765\u6539\u8fdb\u5206\u7c7b\u3002"}}
{"id": "2504.12949", "pdf": "https://arxiv.org/pdf/2504.12949", "abs": "https://arxiv.org/abs/2504.12949", "authors": ["Zhenao Song"], "title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.", "AI": {"tldr": "RL-PINNs \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u5355\u8f6e\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u63d0\u9ad8 PINNs \u6c42\u89e3 PDE \u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "PINNs \u6027\u80fd\u4f9d\u8d56\u8bad\u7ec3\u70b9\u9009\u62e9\u7b56\u7565\uff0c\u4f20\u7edf\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\u9700\u591a\u8f6e\u91c7\u6837\u548c\u91cd\u590d\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u5728\u9ad8\u7ef4\u6216\u9ad8\u9636\u5bfc\u6570\u573a\u666f\u3002", "method": "\u63d0\u51fa RL-PINNs \u6846\u67b6\uff0c\u5c06\u81ea\u9002\u5e94\u91c7\u6837\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528 RL \u4ee3\u7406\u6700\u5927\u5316\u957f\u671f\u6548\u7528\u9009\u62e9\u70b9\uff0c\u91c7\u7528\u51fd\u6570\u53d8\u5f02\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u5ef6\u8fdf\u5956\u52b1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793a RL-PINNs \u5728\u5404\u79cd PDE \u57fa\u51c6\u4e0a\u51c6\u786e\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u91c7\u6837\u5f00\u9500\u5fae\u5c0f\uff0c\u53ef\u6269\u5c55\u5230\u9ad8\u7ef4\u548c\u9ad8\u9636\u95ee\u9898\u3002", "conclusion": "RL-PINNs \u63d0\u4f9b\u9ad8\u6548 PINNs \u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.12511", "pdf": "https://arxiv.org/pdf/2504.12511", "abs": "https://arxiv.org/abs/2504.12511", "authors": ["Shravan Chaudhari", "Trilokya Akula", "Yoon Kim", "Tom Blake"], "title": "Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we advance the study of AI-augmented reasoning in the context\nof Human-Computer Interaction (HCI), psychology and cognitive science, focusing\non the critical task of visual perception. Specifically, we investigate the\napplicability of Multimodal Large Language Models (MLLMs) in this domain. To\nthis end, we leverage established principles and explanations from psychology\nand cognitive science related to complexity in human visual perception. We use\nthem as guiding principles for the MLLMs to compare and interprete visual\ncontent. Our study aims to benchmark MLLMs across various explainability\nprinciples relevant to visual perception. Unlike recent approaches that\nprimarily employ advanced deep learning models to predict complexity metrics\nfrom visual content, our work does not seek to develop a mere new predictive\nmodel. Instead, we propose a novel annotation-free analytical framework to\nassess utility of MLLMs as cognitive assistants for HCI tasks, using visual\nperception as a case study. The primary goal is to pave the way for principled\nstudy in quantifying and evaluating the interpretability of MLLMs for\napplications in improving human reasoning capability and uncovering biases in\nexisting perception datasets annotated by humans.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u6807\u6ce8\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8bc4\u4f30\u5176\u5728\u89c6\u89c9\u611f\u77e5\u4e2d\u7684\u8ba4\u77e5\u8f85\u52a9\u4f5c\u7528\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u548c\u53d1\u73b0\u6570\u636e\u96c6\u504f\u5dee\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u63a8\u8fdbAI\u589e\u5f3a\u63a8\u7406\u5728HCI\u3001\u5fc3\u7406\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u89c6\u89c9\u611f\u77e5\u9886\u57df\uff0c\u901a\u8fc7\u5fc3\u7406\u548c\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u6307\u5bfcMLLMs\u7684\u6bd4\u8f83\u548c\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u6807\u6ce8\u5206\u6790\u6846\u67b6\uff0c\u4ee5\u89c6\u89c9\u611f\u77e5\u4e3a\u6848\u4f8b\uff0c\u8bc4\u4f30MLLMs\u4f5c\u4e3a\u8ba4\u77e5\u8f85\u52a9\u5de5\u5177\u7684\u6548\u7528\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30MLLMs\u5728\u89c6\u89c9\u611f\u77e5\u89e3\u91ca\u6027\u539f\u5219\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u4e3aMLLMs\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "\u8fd9\u79cd\u6846\u67b6\u6709\u52a9\u4e8e\u91cf\u5316MLLMs\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u9ad8\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u548c\u63ed\u793a\u73b0\u6709\u611f\u77e5\u6570\u636e\u96c6\u7684\u504f\u5dee\u3002"}}
{"id": "2504.12971", "pdf": "https://arxiv.org/pdf/2504.12971", "abs": "https://arxiv.org/abs/2504.12971", "authors": ["Shiwen Qin", "Gabriela Kadlecov\u00e1", "Martin Pil\u00e1t", "Shay B. Cohen", "Roman Neruda", "Elliot J. Crowley", "Jovita Lukasik", "Linus Ericsson"], "title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces", "categories": ["cs.LG", "cs.AI"], "comment": "Project page at: https://shiwenqin.github.io/TransferrableSurrogate/", "summary": "Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u63d0\u9ad8\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u6548\u7387\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u641c\u7d22\u7a7a\u95f4\u3002", "motivation": "\u89e3\u51b3NAS\u5728\u63a2\u7d22\u8868\u73b0\u529b\u5f3a\u641c\u7d22\u7a7a\u95f4\u4e0e\u9ad8\u6548\u8bc4\u4f30\u67b6\u6784\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\u3002", "method": "\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u7528\u96f6\u6210\u672c\u4ee3\u7406\u6307\u6807\u548c\u795e\u7ecf\u56fe\u7279\u5f81\uff08GRAF\uff09\u6216\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u3002", "result": "\u4ee3\u7406\u6a21\u578b\u5177\u6709\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u80fd\u8fc7\u6ee4\u4e0d\u826f\u67b6\u6784\u52a0\u901f\u641c\u7d22\uff0c\u5e76\u53ef\u76f4\u63a5\u7528\u4f5c\u641c\u7d22\u76ee\u6807\u5b9e\u73b0\u5de8\u5927\u52a0\u901f\u3002", "conclusion": "\u4ee3\u7406\u6a21\u578b\u663e\u8457\u63d0\u5347NAS\u7684\u641c\u7d22\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002"}}
{"id": "2504.12513", "pdf": "https://arxiv.org/pdf/2504.12513", "abs": "https://arxiv.org/abs/2504.12513", "authors": ["Chaitanya Patel", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "AdaVid: Adaptive Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025. Project Page: https://chaitanya100100.github.io/AdaVid/", "summary": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.", "AI": {"tldr": "AdaVid \u662f\u4e00\u79cd\u7075\u6d3b\u7684\u89c6\u9891\u7f16\u7801\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u53ef\u7528\u8d44\u6e90\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u89c6\u9891-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u89c6\u9891-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u4e14\u4ec5\u5904\u7406\u77ed\u89c6\u9891\u526a\u8f91\uff084-64 \u5e27\uff09\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u9002\u5e94\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u6548\u89c6\u9891\u7f16\u7801\u5668\u3002", "method": "\u5f15\u5165 AdaVid \u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u9002\u5e94 transformer \u5757\uff08\u53d7 Matryoshka \u8868\u793a\u5b66\u4e60\u542f\u53d1\uff09\uff0c\u5728 Ego4D \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff1b\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u5c42\u7f51\u7edc\u5904\u7406\u957f\u89c6\u9891\u3002", "result": "AdaVid-EgoVLP \u5728\u76f8\u540c\u6216\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5339\u914d\u6216\u8d85\u8d8a\u6807\u51c6 EgoVLP \u6027\u80fd\uff1b\u5728 Diving48 \u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b0\u66f4\u591a\u5e27\u5904\u7406\u800c\u4e0d\u8d85\u8ba1\u7b97\u9650\uff1b\u957f\u89c6\u9891\u57fa\u51c6\u4e0a\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "AdaVid \u6846\u67b6\u8bc1\u660e\u4e86\u5728\u8ba1\u7b97\u7ea6\u675f\u4e0b\u63d0\u9ad8\u89c6\u9891\u7f16\u7801\u5668\u6548\u7387\u548c\u9002\u5e94\u6027\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12984", "pdf": "https://arxiv.org/pdf/2504.12984", "abs": "https://arxiv.org/abs/2504.12984", "authors": ["Yaoyao Ding", "Bohan Hou", "Xiao Zhang", "Allan Lin", "Tianqi Chen", "Cody Yu Hao", "Yida Wang", "Gennady Pekhimenko"], "title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "18 pages, 15 figures", "summary": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u4efb\u610f\u4f4d\u5bbd\u4f4e\u7cbe\u5ea6\u6570\u636e\u7c7b\u578b\u7684GPU\u865a\u62df\u673a\uff0c\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4f4e\u7cbe\u5ea6\u5185\u6838\u53d7\u9650\u4e8e2\u7684\u5e42\u6b21\u65b9\u4f4d\u5bbd\uff0c\u4e14\u9ad8\u9636GPU\u7f16\u7a0b\u62bd\u8c61\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u4f18\u5316\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7ebf\u7a0b\u5757\u7ea7\u7f16\u7a0b\u6a21\u578b\u3001\u5206\u5c42\u5185\u5b58\u7a7a\u95f4\u3001\u65b0\u578b\u4ee3\u6570\u5e03\u5c40\u7cfb\u7edf\u548c\u591a\u79cd\u4f4e\u7cbe\u5ea6\u6570\u636e\u7c7b\u578b\u652f\u6301\u7684\u865a\u62df\u673a\uff0c\u5e76\u81ea\u52a8\u7f16\u8bd1\u4e3a\u9ad8\u6548GPU\u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u865a\u62df\u673a\u5728\u4f4e\u7cbe\u5ea6\u6570\u636e\u7c7b\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0eTriton\u3001Ladder\u3001QuantLLM\u548cMarlin\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u5206\u522b\u4e3a1.75x\u30012.61x\u30011.29x\u548c1.03x\u3002", "conclusion": "\u8be5\u865a\u62df\u673a\u9ad8\u6548\u652f\u6301\u5168\u8c31\u4f4e\u7cbe\u5ea6\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2504.12522", "pdf": "https://arxiv.org/pdf/2504.12522", "abs": "https://arxiv.org/abs/2504.12522", "authors": ["Alexander Shypula", "Shuo Li", "Botong Zhang", "Vishakh Padmakumar", "Kayo Yin", "Osbert Bastani"], "title": "Evaluating the Diversity and Quality of LLM Generated Content", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 Third Workshop on Deep Learning for Code", "summary": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.", "AI": {"tldr": "\u504f\u597d\u8c03\u6574\u51cf\u5c11\u8bcd\u6c47\u548c\u53e5\u6cd5\u591a\u6837\u6027\uff0c\u4f46\u63d0\u9ad8\u4e86\u6709\u6548\u8bed\u4e49\u591a\u6837\u6027\uff0c\u56e0\u4e3a\u5b83\u4ea7\u751f\u4e86\u66f4\u591a\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u504f\u597d\u8c03\u6574\u6280\u672f\uff08\u5982RLHF\u3001PPO\u3001GRPO\u3001DPO\uff09\u51cf\u5c11\u6a21\u578b\u591a\u6837\u6027\uff0c\u5728\u9700\u8981\u591a\u6837\u8f93\u51fa\u7684\u5e94\u7528\u4e2d\u5f62\u6210\u96be\u9898\u3002", "method": "\u5f15\u5165\u6d4b\u91cf\u6709\u6548\u8bed\u4e49\u591a\u6837\u6027\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u5f00\u653e\u5f0f\u4efb\u52a1\u8bc4\u4f30LLM\u3002", "result": "\u504f\u597d\u8c03\u6574\u6a21\u578b\u51cf\u5c11\u8bcd\u6c47\u548c\u53e5\u6cd5\u591a\u6837\u6027\uff0c\u4f46\u589e\u52a0\u6709\u6548\u8bed\u4e49\u591a\u6837\u6027\uff1b\u8f83\u5c0f\u6a21\u578b\u66f4\u53c2\u6570\u9ad8\u6548\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u9700\u8981\u591a\u6837\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u5e94\u7528\u6709\u542f\u793a\uff0c\u5e76\u533a\u5206\u4e86\u5f62\u5f0f\u548c\u5185\u5bb9\u591a\u6837\u6027\u3002"}}
{"id": "2504.12988", "pdf": "https://arxiv.org/pdf/2504.12988", "abs": "https://arxiv.org/abs/2504.12988", "authors": ["Yannis Montreuil", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faTop-k\u548cTop-k(x) Learning-to-Defer\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u4e0d\u786e\u5b9a\u9884\u6d4b\u5206\u914d\u7ed9\u591a\u4e2a\u4ee3\u7406\uff0c\u63d0\u9ad8\u51b3\u7b56\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709Learning-to-Defer\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4ee3\u7406\u9012\u4ea4\uff0c\u5728\u9ad8\u98ce\u9669\u573a\u666f\u9700\u8981\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\u65f6\u4e0d\u8db3\u591f\u3002", "method": "\u63d0\u51faTop-k L2D\u548c\u81ea\u9002\u5e94Top-k(x) L2D\u6846\u67b6\uff0c\u63a8\u5bfc\u65b0\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5176Bayes\u4e00\u81f4\u6027\u548c\u5176\u4ed6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u663e\u793a\u6846\u67b6\u6709\u6548\u6027\uff0c\u5e76\u5c06\u6a21\u578b\u7ea7\u8054\u89c6\u4e3a\u6846\u67b6\u7684\u7279\u4f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u4ee3\u7406\u5206\u914d\u548c\u81ea\u9002\u5e94\u673a\u5236\u63d0\u5347\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2504.12523", "pdf": "https://arxiv.org/pdf/2504.12523", "abs": "https://arxiv.org/abs/2504.12523", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165KUP\u57fa\u51c6\u548cMCT\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u65b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5c55\u793a\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5b9e\u4f53\u66ff\u6362\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u73b0\u5b9e\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u77e5\u8bc6\u66f4\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faKUP\u7ba1\u9053\u6a21\u62df\u77e5\u8bc6\u66f4\u65b0\uff0c\u5e76\u4f7f\u7528MCT\u8bad\u7ec3\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u66f4\u65b0\u8bed\u6599\u7684\u6807\u8bb0\u6765\u63d0\u5347\u8bb0\u5fc6\u548c\u63a8\u7406\u3002", "result": "KUP\u57fa\u51c6\u6311\u6218\u6027\u9ad8\uff0c\u6700\u597d\u6a21\u578b\u95f4\u63a5\u63a2\u6d4b\u6210\u7ee9\u4e0d\u8db32%\uff1bMCT\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u591a\u8fbe25.4%\u7684\u76f4\u63a5\u63a2\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u8bc1\u660eKUP\u96be\u5ea6\u5927\uff0cMCT\u6709\u6548\uff0c\u5f3a\u8c03\u77e5\u8bc6\u66f4\u65b0\u4e2d\u8bb0\u5fc6\u548c\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.12991", "pdf": "https://arxiv.org/pdf/2504.12991", "abs": "https://arxiv.org/abs/2504.12991", "authors": ["Yu Wang", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A Latent-Variable Study", "categories": ["cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0CoT\u5728\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u4f3c\u7684OOD\u6837\u672c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u76f8\u4f3c\u5ea6\u964d\u4f4e\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u52a8\u673a\u662f\u7531\u4e8eCoT\u7684\u6cdb\u5316\u80fd\u529b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6269\u5c55\u6f5c\u53d8\u91cf\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u4e24\u79cdOOD\u573a\u666f\uff1a\u6f5c\u53d8\u91cf\u7684\u6392\u5217\u7ec4\u5408\u548c\u5747\u5300\u7f29\u653e\u3002", "result": "\u7ed3\u679c\u663e\u793aCoT\u63a8\u7406\u5728\u6f5c\u53d8\u91cf\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u4f3c\u7684OOD\u6837\u672c\u4e0a\u6cdb\u5316\u6709\u6548\uff0c\u4f46\u5f53\u76f8\u4f3c\u5ea6\u51cf\u5c11\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u63d0\u4f9b\u4e86\u5bf9CoT\u63d0\u793a\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u6d1e\u89c1\uff0c\u5e76\u5efa\u8bae\u672a\u6765LLM\u5f00\u53d1\u66f4\u5177\u5f39\u6027\u7684\u63a8\u7406\u7b56\u7565\u3002"}}
{"id": "2504.13034", "pdf": "https://arxiv.org/pdf/2504.13034", "abs": "https://arxiv.org/abs/2504.13034", "authors": ["Yangxin Fan", "Haolai Che", "Yinghui Wu"], "title": "Inference-friendly Graph Compression for Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faIFGC\u56fe\u538b\u7f29\u65b9\u6848\uff0c\u4ee5\u52a0\u901fGNN\u5728\u5927\u578b\u56fe\u4e0a\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "GNN\u5728\u56fe\u5206\u6790\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u578b\u56fe\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faIFGC\uff0c\u5305\u62ec\u63a8\u7406\u7b49\u4ef7\u5173\u7cfb\u548c\u4e09\u79cd\u65b9\u6848\uff1aSPGC\u3001(\u03b1, r)-\u538b\u7f29\u548c\u951a\u5b9a\u538b\u7f29\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u5404\u79cd\u5927\u89c4\u6a21\u56fe\u4e0a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "IFGC\u6210\u529f\u52a0\u901fGNN\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002"}}
{"id": "2504.13075", "pdf": "https://arxiv.org/pdf/2504.13075", "abs": "https://arxiv.org/abs/2504.13075", "authors": ["Ruizhe Chen", "Dongyu Xue", "Xiangxin Zhou", "Zaixiang Zheng", "Xiangxiang Zeng", "Quanquan Gu"], "title": "An All-Atom Generative Model for Designing Protein Complexes", "categories": ["cs.LG"], "comment": null, "summary": "Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold. Despite these\ndevelopments, the study and modeling of multi-chain proteins remain largely\nuncharted, though they are vital for understanding biological functions.\nRecognizing the importance of these interactions, we introduce APM (All-Atom\nProtein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. Code will be released at https://github.com/bytedance/apm.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165APM\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u94fe\u86cb\u767d\u8d28\u5efa\u6a21\uff0c\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u94fe\u95f4\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u96f6\u5f00\u59cb\u8bbe\u8ba1\u86cb\u767d\u8d28\u590d\u5408\u7269\uff0c\u5e76\u5b9e\u73b0\u6298\u53e0\u548c\u9006\u6298\u53e0\u4efb\u52a1\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u5355\u94fe\u86cb\u767d\u8d28\u5efa\u6a21\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u591a\u94fe\u86cb\u767d\u8d28\u5efa\u6a21\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u591a\u94fe\u86cb\u767d\u8d28\u5bf9\u7406\u89e3\u751f\u7269\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6a21\u578b\u3002", "method": "\u5f00\u53d1APM\u6a21\u578b\uff0c\u6574\u5408\u539f\u5b50\u7ea7\u4fe1\u606f\uff0c\u5229\u7528\u591a\u94fe\u86cb\u767d\u8d28\u6570\u636e\uff0c\u7cbe\u786e\u5efa\u6a21\u94fe\u95f4\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u652f\u6301\u4ece\u96f6\u5f00\u59cb\u8bbe\u8ba1\u3001\u6298\u53e0\u548c\u9006\u6298\u53e0\u4efb\u52a1\u3002", "result": "APM\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u6027\u80fd\uff0c\u652f\u6301\u96f6\u6837\u672c\u91c7\u6837\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "APM\u6a21\u578b\u4e3a\u591a\u94fe\u86cb\u767d\u8d28\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5e76\u8ba1\u5212\u53d1\u5e03\u4ee3\u7801\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.12535", "pdf": "https://arxiv.org/pdf/2504.12535", "abs": "https://arxiv.org/abs/2504.12535", "authors": ["Andy Dimnaku", "Dominic Yurk", "Zhiyuan Gao", "Arun Padmanabhan", "Mandar Aras", "Yaser Abu-Mostafa"], "title": "Decision-based AI Visual Navigation for Cardiac Ultrasounds", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ultrasound imaging of the heart (echocardiography) is widely used to diagnose\ncardiac diseases. However, obtaining an echocardiogram requires an expert\nsonographer and a high-quality ultrasound imaging device, which are generally\nonly available in hospitals. Recently, AI-based navigation models and\nalgorithms have been used to aid novice sonographers in acquiring the\nstandardized cardiac views necessary to visualize potential disease\npathologies. These navigation systems typically rely on directional guidance to\npredict the necessary rotation of the ultrasound probe. This paper demonstrates\na novel AI navigation system that builds on a decision model for identifying\nthe inferior vena cava (IVC) of the heart. The decision model is trained\noffline using cardiac ultrasound videos and employs binary classification to\ndetermine whether the IVC is present in a given ultrasound video. The\nunderlying model integrates a novel localization algorithm that leverages the\nlearned feature representations to annotate the spatial location of the IVC in\nreal-time. Our model demonstrates strong localization performance on\ntraditional high-quality hospital ultrasound videos, as well as impressive\nzero-shot performance on lower-quality ultrasound videos from a more affordable\nButterfly iQ handheld ultrasound machine. This capability facilitates the\nexpansion of ultrasound diagnostics beyond hospital settings. Currently, the\nguidance system is undergoing clinical trials and is available on the Butterfly\niQ app.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u5bfc\u822a\u7cfb\u7edf\uff0c\u5e2e\u52a9\u65b0\u624b\u5728\u8d85\u58f0\u56fe\u50cf\u4e2d\u8bc6\u522b\u5fc3\u810f\u4e0b\u8154\u9759\u8109\uff08IVC\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u8bbe\u5907\u8d28\u91cf\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7531\u4e8e\u5fc3\u810f\u8d85\u58f0\u68c0\u67e5\u9700\u4e13\u5bb6\u548c\u9ad8\u54c1\u8d28\u8bbe\u5907\uff0c\u4ec5\u9650\u4e8e\u533b\u9662\uff0cAI\u7cfb\u7edf\u53ef\u8f85\u52a9\u65b0\u624b\u5e76\u6269\u5c55\u8bca\u65ad\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u5fc3\u810f\u8d85\u58f0\u89c6\u9891\u79bb\u7ebf\u8bad\u7ec3\u51b3\u7b56\u6a21\u578b\uff0c\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u68c0\u6d4bIVC\u5b58\u5728\uff0c\u5e76\u6574\u5408\u672c\u5730\u5316\u7b97\u6cd5\u5b9e\u65f6\u6807\u6ce8\u4f4d\u7f6e\u3002", "result": "\u6a21\u578b\u5728\u9ad8\u54c1\u8d28\u89c6\u9891\u4e0a\u663e\u793a\u5f3a\u672c\u5730\u5316\u6027\u80fd\uff0c\u5728\u4f4e\u54c1\u8d28\u624b\u6301\u8bbe\u5907\u89c6\u9891\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u51fa\u8272\u8868\u73b0\u3002", "conclusion": "\u6b64\u7cfb\u7edf\u4fc3\u8fdb\u8d85\u58f0\u8bca\u65ad\u4ece\u533b\u9662\u6269\u5c55\uff0c\u76ee\u524d\u6b63\u8fdb\u884c\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u53ef\u5728Butterfly iQ app\u4e0a\u4f7f\u7528\u3002"}}
{"id": "2504.13101", "pdf": "https://arxiv.org/pdf/2504.13101", "abs": "https://arxiv.org/abs/2504.13101", "authors": ["Patrik Reizinger", "Randall Balestriero", "David Klindt", "Wieland Brendel"], "title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bbe\uff0c\u901a\u8fc7\u6574\u5408\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u63d0\u51fa\u65b0\u6846\u67b6SITh\uff0c\u4ee5\u89e3\u91caSSL\u7684\u7ecf\u9a8c\u6210\u529f\u5e76\u6307\u660e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1SSL\u65b9\u6cd5\u591a\u6837\uff0c\u4f46\u8868\u793a\u53ef\u80fd\u6536\u655b\u5230\u76f8\u540c\u7406\u60f3\u72b6\u6001\uff0c\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u9700\u8981\u6865\u63a5\u7406\u8bba\u4e0e\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u5408\u6210\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u8bc1\u636e\uff0c\u5c55\u793aPRH\u5728SSL\u4e2d\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u63d0\u51fa\u6269\u5c55\u4e3a\u5947\u5f02\u53ef\u8bc6\u522b\u6027\u7406\u8bba\uff08SITh\uff09\u4ee5\u8986\u76d6\u6574\u4e2aSSL\u7ba1\u9053\u3002", "result": "\u8bc1\u660ePRH\u53ef\u51fa\u73b0\u5728SSL\u4e2d\uff0c\u4f46\u5f53\u524d\u7406\u8bba\u65e0\u6cd5\u89e3\u91ca\u7ecf\u9a8c\u6210\u529f\uff1bSITh\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u6846\u67b6\u3002", "conclusion": "SITh\u53ef\u63d0\u5347SSL\u7684\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u91cd\u70b9\u5305\u62ec\u8bad\u7ec3\u52a8\u6001\u3001\u6837\u672c\u5f71\u54cd\u548c\u5f52\u7eb3\u504f\u5dee\u3002"}}
{"id": "2504.12545", "pdf": "https://arxiv.org/pdf/2504.12545", "abs": "https://arxiv.org/abs/2504.12545", "authors": ["Benign John Ihugba", "Afsana Nasrin", "Ling Wu", "Lin Li", "Lijun Qian", "Xishuang Dong"], "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u67aa\u51fb\u4e8b\u4ef6\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002", "motivation": "\u5927\u89c4\u6a21\u67aa\u51fb\u4e8b\u4ef6\u4ea7\u751f\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u963b\u788d\u8c03\u67e5\u548c\u653f\u7b56\uff0c\u73b0\u6709\u7814\u7a76\u81ea\u52a8\u5316\u63d0\u53d6\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528LLM\u5982GPT-4o\u548co1-mini\u7684\u5c11\u6837\u672c\u63d0\u793a\uff0c\u4ece\u65b0\u95fb\u3001\u8b66\u62a5\u544a\u548c\u793e\u4ea4\u5a92\u4f53\u4e2d\u8fdb\u884cNER\u63d0\u53d6\u5b9e\u4f53\u3002", "result": "GPT-4o\u5728\u5fae\u7cbe\u5ea6\u3001\u5fae\u53ec\u56de\u548c\u5faeF1\u5206\u6570\u4e0a\u6700\u4f73\uff0co1-mini\u7ade\u4e89\u529b\u5f3a\uff0c\u589e\u52a0\u6837\u672c\u6570\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLM\u5728\u5c11\u6837\u672cNER\u4efb\u52a1\u4e2d\u6709\u6548\uff0cGPT-4o\u6700\u4f18\uff0co1-mini\u662f\u8d44\u6e90\u9ad8\u6548\u9009\u62e9\u3002"}}
{"id": "2504.13111", "pdf": "https://arxiv.org/pdf/2504.13111", "abs": "https://arxiv.org/abs/2504.13111", "authors": ["Kumar Manas", "Christian Schlauch", "Adrian Paschke", "Christian Wirth", "Nadja Klein"], "title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification", "categories": ["cs.LG", "cs.RO"], "comment": "17 Pages, 9 figures. Accepted to Robotics: Science and Systems(RSS),\n  2025", "summary": "Deep learning-based trajectory prediction models have demonstrated promising\ncapabilities in capturing complex interactions. However, their\nout-of-distribution generalization remains a significant challenge,\nparticularly due to unbalanced data and a lack of enough data and diversity to\nensure robustness and calibration. To address this, we propose SHIFT (Spectral\nHeteroscedastic Informed Forecasting for Trajectories), a novel framework that\nuniquely combines well-calibrated uncertainty modeling with informative priors\nderived through automated rule extraction. SHIFT reformulates trajectory\nprediction as a classification task and employs heteroscedastic\nspectral-normalized Gaussian processes to effectively disentangle epistemic and\naleatoric uncertainties. We learn informative priors from training labels,\nwhich are automatically generated from natural language driving rules, such as\nstop rules and drivability constraints, using a retrieval-augmented generation\nframework powered by a large language model. Extensive evaluations over the\nnuScenes dataset, including challenging low-data and cross-location scenarios,\ndemonstrate that SHIFT outperforms state-of-the-art methods, achieving\nsubstantial gains in uncertainty calibration and displacement metrics. In\nparticular, our model excels in complex scenarios, such as intersections, where\nuncertainty is inherently higher. Project page:\nhttps://kumarmanas.github.io/SHIFT/.", "AI": {"tldr": "SHIFT \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u4ece\u9a7e\u9a76\u89c4\u5219\u63d0\u53d6\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u63d0\u9ad8\u8f68\u8ff9\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6821\u51c6\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u6cdb\u5316\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6821\u51c6\u3002", "method": "\u5c06\u8f68\u8ff9\u9884\u6d4b\u91cd\u6784\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528\u5f02\u65b9\u5dee\u8c31\u5f52\u4e00\u5316\u9ad8\u65af\u8fc7\u7a0b\u5206\u79bb\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u4ece\u81ea\u7136\u8bed\u8a00\u9a7e\u9a76\u89c4\u5219\u4e2d\u5b66\u4e60\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u3001\u8de8\u4f4d\u7f6e\u548c\u590d\u6742\u4ea4\u53c9\u8def\u53e3\u573a\u666f\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u4f4d\u79fb\u6307\u6807\u3002", "conclusion": "SHIFT \u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.12546", "pdf": "https://arxiv.org/pdf/2504.12546", "abs": "https://arxiv.org/abs/2504.12546", "authors": ["Thomas \u00c5gotnes", "Rustam Galimullin", "Ken Satoh", "Satoshi Tojo"], "title": "Anonymous Public Announcements", "categories": ["cs.LO", "cs.AI", "cs.CR"], "comment": null, "summary": "We formalise the notion of an \\emph{anonymous public announcement} in the\ntradition of public announcement logic. Such announcements can be seen as\nin-between a public announcement from ``the outside\" (an announcement of\n$\\phi$) and a public announcement by one of the agents (an announcement of\n$K_a\\phi$): we get more information than just $\\phi$, but not (necessarily)\nabout exactly who made it. Even if such an announcement is prima facie\nanonymous, depending on the background knowledge of the agents it might reveal\nthe identity of the announcer: if I post something on a message board, the\ninformation might reveal who I am even if I don't sign my name. Furthermore,\nlike in the Russian Cards puzzle, if we assume that the announcer's intention\nwas to stay anonymous, that in fact might reveal more information. In this\npaper we first look at the case when no assumption about intentions are made,\nin which case the logic with an anonymous public announcement operator is\nreducible to epistemic logic. We then look at the case when we assume common\nknowledge of the intention to stay anonymous, which is both more complex and\nmore interesting: in several ways it boils down to the notion of a ``safe\"\nannouncement (again, similarly to Russian Cards). Main results include formal\nexpressivity results and axiomatic completeness for key logical languages.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f62\u5f0f\u5316\u4e86\u533f\u540d\u516c\u5171\u516c\u544a\u903b\u8f91\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u8ba4\u8bc6\u8bba\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u8bc1\u660e\u4e86\u8868\u8fbe\u6027\u548c\u516c\u7406\u5b8c\u5907\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u5904\u7406\u533f\u540d\u516c\u544a\u53ef\u80fd\u6cc4\u9732\u8eab\u4efd\u7684\u95ee\u9898\uff0c\u7c7b\u4f3c\u4e8e\u6d88\u606f\u677f\u6216\u4fc4\u7f57\u65af\u6251\u514b\u724c\u8c1c\u9898\uff0c\u4ecb\u4e8e\u6807\u51c6\u516c\u544a\u548c\u4ee3\u7406\u4eba\u516c\u544a\u4e4b\u95f4\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u65e0\u610f\u56fe\u5047\u8bbe\u7684\u60c5\u5f62\uff08\u53ef\u5f52\u7ea6\u5230\u8ba4\u8bc6\u8bba\u903b\u8f91\uff09\u548c\u5047\u8bbe\u610f\u56fe\u4fdd\u6301\u533f\u540d\u7684\u60c5\u5f62\uff08\u7c7b\u4f3c\u4e8e'\u5b89\u5168'\u516c\u544a\uff09\u3002", "result": "\u7ed3\u679c\u5305\u62ec\u5f62\u5f0f\u8868\u8fbe\u6027\u7ed3\u679c\u548c\u516c\u7406\u5b8c\u5907\u6027\u8bc1\u660e\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8fd9\u79cd\u5f62\u5f0f\u5316\u63d0\u5347\u4e86\u5bf9\u533f\u540d\u516c\u544a\u5728\u8ba4\u8bc6\u8bba\u903b\u8f91\u4e2d\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u89c1\u3002"}}
{"id": "2504.13112", "pdf": "https://arxiv.org/pdf/2504.13112", "abs": "https://arxiv.org/abs/2504.13112", "authors": ["Grigorios G Chrysos", "Yongtao Wu", "Razvan Pascanu", "Philip Torr", "Volkan Cevher"], "title": "Hadamard product in deep learning: Introduction, Advances and Challenges", "categories": ["cs.LG"], "comment": "Accepted in IEEE T-PAMI", "summary": "While convolution and self-attention mechanisms have dominated architectural\ndesign in deep learning, this survey examines a fundamental yet understudied\nprimitive: the Hadamard product. Despite its widespread implementation across\nvarious applications, the Hadamard product has not been systematically analyzed\nas a core architectural primitive. We present the first comprehensive taxonomy\nof its applications in deep learning, identifying four principal domains:\nhigher-order correlation, multimodal data fusion, dynamic representation\nmodulation, and efficient pairwise operations. The Hadamard product's ability\nto model nonlinear interactions with linear computational complexity makes it\nparticularly valuable for resource-constrained deployments and edge computing\nscenarios. We demonstrate its natural applicability in multimodal fusion tasks,\nsuch as visual question answering, and its effectiveness in representation\nmasking for applications including image inpainting and pruning. This\nsystematic review not only consolidates existing knowledge about the Hadamard\nproduct's role in deep learning architectures but also establishes a foundation\nfor future architectural innovations. Our analysis reveals the Hadamard product\nas a versatile primitive that offers compelling trade-offs between\ncomputational efficiency and representational power, positioning it as a\ncrucial component in the deep learning toolkit.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8c03\u67e5\u4e86Hadamard\u4e58\u79ef\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u5176\u5206\u7c7b\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u5377\u79ef\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e3b\u5bfc\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f46Hadamard\u4e58\u79ef\u4f5c\u4e3a\u57fa\u7840\u539f\u8bed\u672a\u88ab\u7cfb\u7edf\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u5168\u9762\u5ba1\u67e5\u5176\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u5206\u7c7b\uff0c\u5efa\u7acb\u4e86Hadamard\u4e58\u79ef\u5728\u56db\u4e2a\u4e3b\u8981\u9886\u57df\u7684\u5e94\u7528\uff1a\u9ad8\u9636\u76f8\u5173\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u52a8\u6001\u8868\u793a\u8c03\u5236\u548c\u9ad8\u6548\u6210\u5bf9\u64cd\u4f5c\u3002", "result": "\u8bc6\u522b\u4e86\u56db\u4e2a\u5e94\u7528\u9886\u57df\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u95ee\u7b54\u3001\u56fe\u50cf\u4fee\u590d\u548c\u526a\u679d\u7b49\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Hadamard\u4e58\u79ef\u662f\u4e00\u79cd\u591a\u529f\u80fd\u539f\u8bed\uff0c\u63d0\u4f9b\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u793a\u80fd\u529b\u7684\u826f\u597d\u6743\u8861\uff0c\u662f\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2504.12549", "pdf": "https://arxiv.org/pdf/2504.12549", "abs": "https://arxiv.org/abs/2504.12549", "authors": ["Iris Ma", "Ian Domingo", "Alberto Krone-Martins", "Pierre Baldi", "Cristina V. Lopes"], "title": "Memorization: A Close Look at Books", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Llama 3\u6a21\u578b\u548cprefix-prompting\u6280\u672f\uff0c\u4ece\u5c11\u91cftoken\u4e2d\u91cd\u6784\u4e66\u7c4d\uff0c\u63a2\u8ba8\u4e86\u6a21\u578b\u8bb0\u5fc6\u548c\u7f13\u89e3\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u8bc4\u4f30LLM\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u6574\u4e2a\u4e66\u7c4d\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u7814\u7a76\u7f13\u89e3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528prefix-prompting\u5728Llama 3 70B\u6a21\u578b\u4e0a\u8fdb\u884c\u81ea\u56de\u5f52\u91cd\u6784\uff0c\u5e76\u5206\u6790\u6743\u91cd\u53d8\u5316\u3002", "result": "\u7ed3\u679c\u663e\u793a\u67d0\u4e9b\u4e66\u7c4d\u53ef\u9ad8\u76f8\u4f3c\u5ea6\u91cd\u6784\uff0c\u63d0\u53d6\u7387\u4e0e\u4e66\u7c4d\u6d41\u884c\u5ea6\u76f8\u5173\uff0c\u7f13\u89e3\u7b56\u7565\u53ef\u88ab\u9006\u8f6c\uff0c\u4e3b\u8981\u7531\u5c11\u91cf\u6743\u91cd\u63a7\u5236\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5f53\u524d\u7f13\u89e3\u7b56\u7565\u6709\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u7814\u7a76\u5fae\u8c03\u5bf9\u8bb0\u5fc6\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.13113", "pdf": "https://arxiv.org/pdf/2504.13113", "abs": "https://arxiv.org/abs/2504.13113", "authors": ["Jason Zev Ludmir", "Sophia Rebello", "Jacob Ruiz", "Tirthak Patel"], "title": "Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum Autoencoders", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "Detecting mission-critical anomalous events and data is a crucial challenge\nacross various industries, including finance, healthcare, and energy. Quantum\ncomputing has recently emerged as a powerful tool for tackling several machine\nlearning tasks, but training quantum machine learning models remains\nchallenging, particularly due to the difficulty of gradient calculation. The\nchallenge is even greater for anomaly detection, where unsupervised learning\nmethods are essential to ensure practical applicability. To address these\nissues, we propose Quorum, the first quantum anomaly detection framework\ndesigned for unsupervised learning that operates without requiring any\ntraining.", "AI": {"tldr": "\u63d0\u51faQuorum\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u91cf\u5b50\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u884c\u4e1a\u5f02\u5e38\u4e8b\u4ef6\u68c0\u6d4b\u6311\u6218\u3002", "motivation": "\u68c0\u6d4b\u5173\u952e\u5f02\u5e38\u4e8b\u4ef6\u5728\u91d1\u878d\u3001\u533b\u7597\u548c\u80fd\u6e90\u7b49\u884c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u91cf\u5b50\u8ba1\u7b97\u867d\u5f3a\u5927\u4f46\u8bad\u7ec3\u56f0\u96be\uff0c\u5c24\u5176\u662f\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u63d0\u51faQuorum\u6846\u67b6\uff0c\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u6846\u67b6\u89e3\u51b3\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u68af\u5ea6\u8ba1\u7b97\u96be\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "Quorum\u6846\u67b6\u4e3a\u65e0\u76d1\u7763\u91cf\u5b50\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002"}}
{"id": "2504.12552", "pdf": "https://arxiv.org/pdf/2504.12552", "abs": "https://arxiv.org/abs/2504.12552", "authors": ["Alejandra Perez", "Han Zhang", "Yu-Chun Ku", "Lalithkumar Seenivasan", "Roger Soberanis", "Jose L. Porras", "Richard Day", "Jeff Jopling", "Peter Najjar", "Mathias Unberath"], "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u624b\u672f\u5ba4\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u6027\u80fd\u4e0e\u539f\u59cbRGB\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u3002", "motivation": "\u624b\u672f\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6210\u672c\u3001\u6539\u5584\u60a3\u8005\u7ed3\u679c\u5e76\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u751f\u6210\u53bb\u6807\u8bc6\u5316\u7684\u6570\u5b57\u5b6a\u751f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528SafeOR\u6a21\u578b\u5904\u7406\u5206\u5272\u63a9\u7801\u548c\u6df1\u5ea6\u56fe\u8fdb\u884c\u4e8b\u4ef6\u68c0\u6d4b\u3002", "result": "\u572838\u4e2a\u6a21\u62df\u624b\u672f\u8bd5\u9a8c\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u6027\u80fd\u4e0e\u539f\u59cbRGB\u89c6\u9891\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u6db5\u76d6\u4e94\u7c7b\u4e8b\u4ef6\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u624b\u672f\u5ba4\u5de5\u4f5c\u6d41\u7a0b\u5206\u6790\uff0c\u4fc3\u8fdb\u6570\u636e\u5171\u4eab\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.13116", "pdf": "https://arxiv.org/pdf/2504.13116", "abs": "https://arxiv.org/abs/2504.13116", "authors": ["Niamh Mimnagh", "Andrew Parnell", "Conor McAloon", "Jaden Carlson", "Maria Guelbenzu", "Jonas Brock", "Damien Barrett", "Guy McGrath", "Jamie Tratalos", "Rafael Moral"], "title": "Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced Herd-Level Data Using Machine Learning Algorithms", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication\nprogramme in Ireland, with the herd-level prevalence declining from 11.3% in\n2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the\ndevelopment of predictive models for targeted surveillance becomes increasingly\nimportant to mitigate the risk of disease re-emergence. In this study, we\nevaluate the performance of a range of machine learning algorithms, including\nbinary classification and anomaly detection techniques, for predicting\nBVD-positive herds using highly imbalanced herd-level data. We conduct an\nextensive simulation study to assess model performance across varying sample\nsizes and class imbalance ratios, incorporating resampling, class weighting,\nand appropriate evaluation metrics (sensitivity, positive predictive value,\nF1-score and AUC values). Random forests and XGBoost models consistently\noutperformed other methods, with the random forest model achieving the highest\nsensitivity and AUC across scenarios, including real-world prediction of 2023\nherd status, correctly identifying 219 of 250 positive herds while halving the\nnumber of herds that require compared to a blanket-testing strategy.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u9884\u6d4bBVD\u9633\u6027\u725b\u7fa4\uff0c\u4ee5\u4f18\u5316\u76d1\u6d4b\u7b56\u7565\uff0c\u51cf\u5c11\u6d4b\u8bd5\u9700\u6c42\u3002", "motivation": "\u7231\u5c14\u5170BVD\u6839\u9664\u7a0b\u5e8f\u6210\u529f\u540e\uff0c\u9700\u8981\u9884\u6d4b\u6a21\u578b\u9632\u6b62\u75be\u75c5\u590d\u53d1\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u8fdb\u884c\u6a21\u62df\u7814\u7a76\uff0c\u8bc4\u4f30\u4e0d\u540c\u6837\u672c\u5927\u5c0f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u968f\u673a\u68ee\u6797\u548cXGBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u968f\u673a\u68ee\u6797\u5728\u771f\u5b9e\u9884\u6d4b\u4e2d\u6b63\u786e\u8bc6\u522b219/250\u9633\u6027\u725b\u7fa4\uff0c\u5e76\u5c06\u9700\u8981\u6d4b\u8bd5\u7684\u725b\u7fa4\u6570\u91cf\u51cf\u534a\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5c24\u5176\u662f\u968f\u673a\u68ee\u6797\u53ef\u6709\u6548\u7528\u4e8eBVD\u76ee\u6807\u76d1\u6d4b\uff0c\u964d\u4f4e\u590d\u53d1\u98ce\u9669\u3002"}}
{"id": "2504.13142", "pdf": "https://arxiv.org/pdf/2504.13142", "abs": "https://arxiv.org/abs/2504.13142", "authors": ["Kristen Goebel", "Paola Pesantez-Cabrera", "Markus Keller", "Alan Fern"], "title": "Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165TAL\u8f6c\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8868\u578b\u6570\u636e\uff08\u5982\u82bd\u7834\u65e5\u671f\uff09\u6765\u63d0\u5347\u6c34\u679c\u4f5c\u7269\u6297\u5bd2\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5373\u4f7f\u6ca1\u6709\u76f4\u63a5\u6297\u5bd2\u6570\u636e\u3002", "motivation": "\u51b7\u6e29\u5ea6\u53ef\u80fd\u5bf9\u6c34\u679c\u4f5c\u7269\u9020\u6210\u4e25\u91cd\u971c\u51bb\u635f\u5bb3\uff0c\u6297\u5bd2\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u800c\u519c\u6c11\u6709\u4e30\u5bcc\u7684\u8868\u578b\u6570\u636e\u53ef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5229\u7528\u8f85\u52a9\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTAL\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u9009\u62e9\u548c\u5e73\u5747\uff0c\u5229\u7528\u6df1\u5ea6\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u4ece\u6709\u6297\u5bd2\u548c\u8868\u578b\u6570\u636e\u7684\u6e90\u4efb\u52a1\u8f6c\u79fb\u5230\u4ec5\u8868\u578b\u6570\u636e\u7684\u76ee\u6807\u4efb\u52a1\u3002", "result": "\u5728\u771f\u5b9e\u8461\u8404\u54c1\u79cd\u7684\u6570\u636e\u4e0a\uff0cTAL\u6846\u67b6\u4f7f\u7528\u8868\u578b\u6570\u636e\u663e\u8457\u6539\u5584\u4e86\u6297\u5bd2\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "TAL\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u7f3a\u4e4f\u6297\u5bd2\u6570\u636e\u65f6\uff0c\u901a\u8fc7\u5229\u7528\u8868\u578b\u6570\u636e\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.12563", "pdf": "https://arxiv.org/pdf/2504.12563", "abs": "https://arxiv.org/abs/2504.12563", "authors": ["Haris Riaz", "Sourav Bhabesh", "Vinayak Arannil", "Miguel Ballesteros", "Graham Horwood"], "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 17 figures. Preprint", "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMetaSynth\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u548c\u591a\u4e2a\u4e13\u5bb6LLM\u4ee3\u7406\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0LLM\u5728\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u3002", "motivation": "\u5408\u6210\u6570\u636e\u591a\u6837\u6027\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5728LLM\u9886\u57df\u9002\u5e94\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMetaSynth\uff0c\u4f7f\u7528\u5143\u63d0\u793a\u534f\u8c03\u591a\u4e2a'\u4e13\u5bb6'LLM\u4ee3\u7406\u534f\u4f5c\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u4f7f\u75282500\u4e07tokens\u5408\u6210\u6570\u636e\uff0c\u9002\u5e94Mistral-7B-v0.3\u5230\u91d1\u878d\u548c\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u6027\u80fd\u63d0\u5347\u8fbe4.08%\u548c13.75%\uff0c\u591a\u6837\u6027\u63a5\u8fd1\u9884\u8bad\u7ec3\u8bed\u6599\u3002", "conclusion": "\u5c11\u91cf\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u65e0\u9700\u771f\u5b9e\u6570\u636e\u5373\u53ef\u6709\u6548\u5b9e\u73b0\u9886\u57df\u9002\u5e94\u3002"}}
{"id": "2504.13151", "pdf": "https://arxiv.org/pdf/2504.13151", "abs": "https://arxiv.org/abs/2504.13151", "authors": ["Aaron Mueller", "Atticus Geiger", "Sarah Wiegreffe", "Dana Arad", "Iv\u00e1n Arcuschin", "Adam Belfki", "Yik Siu Chan", "Jaden Fiotto-Kaufman", "Tal Haklay", "Michael Hanna", "Jing Huang", "Rohan Gupta", "Yaniv Nikankin", "Hadas Orgad", "Nikhil Prakash", "Anja Reusch", "Aruna Sankaranarayanan", "Shun Shao", "Alessandro Stolfo", "Martin Tutek", "Amir Zur", "David Bau", "Yonatan Belinkov"], "title": "MIB: A Mechanistic Interpretability Benchmark", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "AI": {"tldr": "\u63d0\u51faMIB\u57fa\u51c6\u6d4b\u8bd5\u673a\u68b0\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u53d1\u73b0\u67d0\u4e9b\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u65b0\u673a\u68b0\u89e3\u91ca\u6027\u65b9\u6cd5\u662f\u5426\u771f\u6b63\u6539\u8fdb\uff0c\u5e76\u8ffd\u6c42\u6709\u610f\u4e49\u7684\u6301\u4e45\u8bc4\u4ef7\u6807\u51c6\u3002", "method": "\u63d0\u51faMIB\u57fa\u51c6\uff0c\u5305\u62ec\u7535\u8def\u5b9a\u4f4d\u548c\u56e0\u679c\u53d8\u91cf\u5b9a\u4f4d\u4e24\u4e2a\u8f68\u9053\uff0c\u6d89\u53ca\u56db\u9879\u4efb\u52a1\u548c\u4e94\u79cd\u6a21\u578b\uff0c\u6bd4\u8f83\u65b9\u6cd5\u5982\u5f52\u56e0\u4fee\u8865\u3001\u4fe1\u606f\u6d41\u8def\u7531\u3001\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u7b49\u3002", "result": "\u7535\u8def\u5b9a\u4f4d\u4e2d\uff0c\u5f52\u56e0\u548c\u63a9\u7801\u4f18\u5316\u65b9\u6cd5\u6700\u4f73\uff1b\u56e0\u679c\u53d8\u91cf\u5b9a\u4f4d\u4e2d\uff0c\u76d1\u7763DAS\u65b9\u6cd5\u6700\u4f73\uff0c\u800cSAE\u7279\u5f81\u4e0d\u4f18\u4e8e\u795e\u7ecf\u5143\u3002", "conclusion": "MIB\u80fd\u5b9e\u73b0\u65b9\u6cd5\u7684\u6709\u610f\u4e49\u6bd4\u8f83\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u9886\u57df\u8fdb\u6b65\u7684\u4fe1\u5fc3\u3002"}}
{"id": "2504.12576", "pdf": "https://arxiv.org/pdf/2504.12576", "abs": "https://arxiv.org/abs/2504.12576", "authors": ["Wentao Wu", "Xiao Wang", "Chenglong Li", "Bo Jiang", "Jin Tang", "Bin Luo", "Qi Liu"], "title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCM3AE\u6846\u67b6\uff0c\u7528\u4e8eRGB-Event\u611f\u77e5\u7684\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u878d\u5408\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u4e0eRGB\u5e27\u5efa\u7acb\u5f3a\u8fde\u63a5\uff0c\u9650\u5236\u591a\u6a21\u6001\u878d\u5408\u5e94\u7528\u3002", "method": "\u63d0\u51faCM3AE\u6846\u67b6\uff0c\u8f93\u5165RGB\u56fe\u50cf\u3001\u4e8b\u4ef6\u56fe\u50cf\u548c\u4e8b\u4ef6\u4f53\u7d20\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u878d\u5408\u91cd\u5efa\u6a21\u5757\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e94\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5b9e\u9a8c\u8bc1\u660eCM3AE\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "CM3AE\u6846\u67b6\u589e\u5f3a\u8de8\u6a21\u6001\u4fe1\u606f\u805a\u5408\u548c\u7406\u89e3\uff0c\u4e3a\u4e8b\u4ef6-based\u548cRGB-Event\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u7a33\u5065\u652f\u6301\u3002"}}
{"id": "2504.13173", "pdf": "https://arxiv.org/pdf/2504.13173", "abs": "https://arxiv.org/abs/2504.13173", "authors": ["Ali Behrouz", "Meisam Razaviyayn", "Peilin Zhong", "Vahab Mirrokni"], "title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u6982\u5ff5\u5316\u795e\u7ecf\u67b6\u6784\u57fa\u4e8e\u6ce8\u610f\u529b\u504f\u5dee\uff0c\u5f15\u5165Miras\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u65b0\u5e8f\u5217\u6a21\u578b\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8d85\u8d8aTransformer\u548c\u7ebf\u6027RNN\u3002", "motivation": "\u53d7\u4eba\u7c7b\u6ce8\u610f\u529b\u504f\u5dee\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u57fa\u7840\u6a21\u578b\u67b6\u6784\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u5c06\u67b6\u6784\u89c6\u4e3a\u5173\u8054\u8bb0\u5fc6\u6a21\u5757\uff0c\u5f15\u5165\u66ff\u4ee3\u6ce8\u610f\u529b\u504f\u5dee\u76ee\u6807\u548c\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u91cd\u91ca\u9057\u5fd8\u673a\u5236\uff0c\u63d0\u51faMiras\u6846\u67b6\uff0c\u5305\u62ec\u5173\u8054\u8bb0\u5fc6\u3001\u6ce8\u610f\u529b\u504f\u5dee\u3001\u4fdd\u7559\u95e8\u548c\u8bb0\u5fc6\u5b66\u4e60\u7b97\u6cd5\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u5f00\u53d1Moneta\u3001Yaad\u548cMemora\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMiras\u4e0d\u540c\u8bbe\u8ba1\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u56de\u5fc6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8aTransformer\u548c\u73b0\u4ee3\u7ebf\u6027RNN\uff0c\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u5e76\u884c\u8bad\u7ec3\u3002", "conclusion": "Miras\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u8bbe\u8ba1\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4e0d\u540c\u9009\u62e9\u53ef\u4ea7\u751f\u4e0d\u540c\u4f18\u52bf\u6a21\u578b\u3002"}}
{"id": "2504.13178", "pdf": "https://arxiv.org/pdf/2504.13178", "abs": "https://arxiv.org/abs/2504.13178", "authors": ["Evan Casey", "Tianyu Zhang", "Shu Ishida", "John Roger Thompson", "Amir Khasahmadi", "Joseph George Lambourne", "Pradeep Kumar Jayaraman", "Karl D. D. Willis"], "title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "categories": ["cs.LG"], "comment": null, "summary": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06LLM\u5bf9\u9f50\u6280\u672f\u5e94\u7528\u4e8eCAD\u5de5\u7a0b\u8349\u56fe\u7ea6\u675f\u751f\u6210\uff0c\u63d0\u9ad8\u7ea6\u675f\u5b8c\u6574\u7387\u81f393%\u3002", "motivation": "\u89e3\u51b3CAD\u8bbe\u8ba1\u5bf9\u9f50\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8bbe\u8ba1\u610f\u56fe\uff0c\u5bfc\u81f4\u7ea6\u675f\u4e0d\u8db3\u6216\u8fc7\u5ea6\u3002", "method": "\u4f7f\u7528\u7ea6\u675f\u6c42\u89e3\u5668\u53cd\u9988\u8bad\u7ec3\u73b0\u6709\u7ea6\u675f\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u5bf9\u9f50\u6280\u672f\u3002", "result": "\u7ea6\u675f\u5b8c\u6574\u7387\u8fbe93%\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u768434%\u548c\u65e0\u5bf9\u9f50\u76848.9%\u3002", "conclusion": "\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u7ea6\u675f\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4e3a\u8bed\u8a00\u4e0e\u8bbe\u8ba1\u9886\u57df\u5bf9\u9f50\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.12585", "pdf": "https://arxiv.org/pdf/2504.12585", "abs": "https://arxiv.org/abs/2504.12585", "authors": ["Liyi Zhang", "Veniamin Veselovsky", "R. Thomas McCoy", "Thomas L. Griffiths"], "title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e0a\u5931\u8d25\u7684\u539f\u56e0\u53ca\u5176\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u51cf\u5c11\u5bf9\u5148\u9a8c\u5206\u5e03\u7684\u4f9d\u8d56\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3LLMs\u7531\u4e8e\u5b66\u5230\u7684\u9690\u5f0f\u5148\u9a8c\u5206\u5e03\u5f71\u54cd\uff0c\u5bfc\u81f4\u5728\u8ba1\u6570\u6216\u7f29\u5199\u7b49\u4efb\u52a1\u4e2d\u54cd\u5e94\u4e0d\u5f53\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u63d0\u793a\u51cf\u5c11\u5148\u9a8c\u5f71\u54cd\u3001\u673a\u5236\u89e3\u91ca\u6027\u6280\u672f\u5b9a\u4f4d\u795e\u7ecf\u7f51\u7edc\u4e2d\u4e0e\u5148\u9a8c\u76f8\u5173\u7684\u5c42\uff0c\u4ee5\u53ca\u5bf9\u8fd9\u4e9b\u5c42\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u3002", "result": "\u7ed3\u679c\u663e\u793a\u63d0\u793a\u80fd\u663e\u8457\u6539\u5584\u6027\u80fd\uff0c\u8bc6\u522b\u51fa\u4e0e\u5148\u9a8c\u76f8\u5173\u7684\u5c42\uff0c\u5fae\u8c03\u540e\u5728\u4fdd\u7559\u4efb\u52a1\u4e0a\u8868\u73b0\u9ad8\uff0c\u4e14\u9519\u8bef\u4e0d\u518d\u4e0e\u5148\u9a8c\u76f8\u5173\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660eLLMs\u4e2d\u5305\u542b\u6b63\u786e\u54cd\u5e94\u4fe1\u606f\uff0c\u53ef\u80fd\u901a\u8fc7\u64cd\u7eb5\u5148\u9a8c\u4f9d\u8d56\u6765\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2504.12606", "pdf": "https://arxiv.org/pdf/2504.12606", "abs": "https://arxiv.org/abs/2504.12606", "authors": ["Changsheng Lv", "Mengshi Qi", "Zijian Fu", "Huadong Ma"], "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRobo-SGG\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u635f\u574f\u56fe\u50cf\u7684\u9c81\u68d2\u573a\u666f\u56fe\u751f\u6210\uff0c\u901a\u8fc7\u5e03\u5c40\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709SGG\u65b9\u6cd5\u5728\u635f\u574f\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u6838\u5fc3\u6311\u6218\u662f\u6e05\u6d01\u548c\u635f\u574f\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\u3002", "method": "\u63d0\u51faRobo-SGG\u6a21\u5757\uff0c\u5305\u62ec\u5e03\u5c40\u5bfc\u5411\u5f52\u4e00\u5316\u548c\u6062\u590d\u3001\u5b9e\u4f8b\u5f52\u4e00\u5316\u3001\u5e03\u5c40\u5bfc\u5411\u6062\u590d\u4ee5\u53ca\u5e03\u5c40\u5d4c\u5165\u7f16\u7801\u5668\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u3002", "result": "\u5728VG-C\u6570\u636e\u96c6\u4e0a\uff0cPredCls\u3001SGCls\u548cSGDet\u4efb\u52a1\u7684mR@50\u5206\u522b\u63d0\u9ad8\u4e865.6%\u30018.0%\u548c6.5%\uff0c\u5e76\u5728VG-C\u548cGQA-C\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Robo-SGG\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u573a\u666f\u56fe\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u8ba1\u5212\u53d1\u5e03\u6e90\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2504.12608", "pdf": "https://arxiv.org/pdf/2504.12608", "abs": "https://arxiv.org/abs/2504.12608", "authors": ["Mingwei Liu", "Juntao Li", "Ying Wang", "Xueying Du", "Zuoyu Ou", "Qiuyuan Chen", "Bingxu An", "Zhao Wei", "Yong Xu", "Fangming Zou", "Xin Peng", "Yiling Lou"], "title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7814\u7a76\u4e86LLM\u4e2d\u4ee3\u7801\u91cd\u590d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DeRep\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u4ee3\u7801\u91cd\u590d\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u8bfb\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a7619\u4e2aLLM\uff0c\u603b\u7ed3\u91cd\u590d\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684DeRep\u6280\u672f\u3002", "result": "DeRep\u51cf\u5c11\u91cd\u590d\u6307\u6807\u5e73\u5747\u63d0\u534791.3%\u81f393.5%\uff0cPass@1\u63d0\u9ad8208.3%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "DeRep\u6709\u6548\u964d\u4f4e\u4ee3\u7801\u91cd\u590d\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3002"}}
{"id": "2504.12338", "pdf": "https://arxiv.org/pdf/2504.12338", "abs": "https://arxiv.org/abs/2504.12338", "authors": ["David Anderson", "Michaela Anderson", "Margret Bjarnadottir", "Stephen Mahar", "Shriyan Reyya"], "title": "Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions", "categories": ["cs.CL", "cs.LG", "I.2.0"], "comment": "Paper and Online Supplement combined into one PDF. 26 pages. 2\n  figures", "summary": "There is a long history of building predictive models in healthcare using\ntabular data from electronic medical records. However, these models fail to\nextract the information found in unstructured clinical notes, which document\ndiagnosis, treatment, progress, medications, and care plans. In this study, we\ninvestigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical\nquestions about patients, when given access to the patient's discharge summary,\ncan support patient-level mortality prediction. Using data from 14,011\nfirst-time admissions to the Coronary Care or Cardiovascular Intensive Care\nUnits in the MIMIC-IV Note dataset, we implement a transparent framework that\nuses GPT responses as input features in logistic regression models. Our\nfindings demonstrate that GPT-based models alone can outperform models trained\non standard tabular data, and that combining both sources of information yields\neven greater predictive power, increasing AUC by an average of 5.1 percentage\npoints and increasing positive predictive value by 29.9 percent for the\nhighest-risk decile. These results highlight the value of integrating large\nlanguage models (LLMs) into clinical prediction tasks and underscore the\nbroader potential for using LLMs in any domain where unstructured text data\nremains an underutilized resource.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528GPT-4o-mini\u56de\u7b54\u57fa\u4e8e\u51fa\u9662\u603b\u7ed3\u7684\u4e34\u5e8a\u95ee\u9898\uff0c\u4ee5\u6539\u5584\u60a3\u8005\u6b7b\u4ea1\u7387\u9884\u6d4b\uff0c\u5e76\u663e\u793a\u4e0e\u8868\u683c\u6570\u636e\u7ed3\u5408\u65f6\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597\u9884\u6d4b\u6a21\u578b\u672a\u80fd\u5229\u7528\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u4fe1\u606f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u53d6\u548c\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u96c6\u4e2d\u768414,011\u4f8b\u9996\u6b21\u5165\u9662\u6570\u636e\uff0c\u6784\u5efa\u6846\u67b6\uff0c\u5c06GPT\u54cd\u5e94\u4f5c\u4e3alogistic\u56de\u5f52\u6a21\u578b\u7684\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u4e0e\u6807\u51c6\u8868\u683c\u6570\u636e\u6a21\u578b\u6bd4\u8f83\u3002", "result": "GPT-based\u6a21\u578b\u4f18\u4e8e\u6807\u51c6\u8868\u683c\u6570\u636e\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u79cd\u6570\u636e\u6e90\u540e\uff0cAUC\u5e73\u5747\u63d0\u9ad85.1\u4e2a\u767e\u5206\u70b9\uff0c\u6700\u9ad8\u98ce\u9669\u5341\u5206\u4f4d\u6570\u7684\u9633\u6027\u9884\u6d4b\u503c\u63d0\u9ad829.9%\u3002", "conclusion": "\u7a81\u663e\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5230\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5176\u4ed6\u5229\u7528\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u7684\u9886\u57df\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2504.12609", "pdf": "https://arxiv.org/pdf/2504.12609", "abs": "https://arxiv.org/abs/2504.12609", "authors": ["Tyler Ga Wei Lum", "Olivia Y. Lee", "C. Karen Liu", "Jeannette Bohg"], "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 13 figures", "summary": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io", "AI": {"tldr": "\u4e00\u4e2a\u6846\u67b6\u4f7f\u7528\u5355\u4e00RGB-D\u4eba\u7c7b\u89c6\u9891\uff0c\u901a\u8fc7\u6a21\u62df\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u6865\u63a5\u4eba\u673a\u5f62\u6001\u5dee\u5f02\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u8bad\u7ec3\u7684\u6269\u5c55\u6311\u6218\uff0c\u4eba\u7c7b\u89c6\u9891\u6613\u6536\u96c6\u4f46\u96be\u76f4\u63a5\u5e94\u7528\u3002", "method": "Human2Sim2Robot\u6846\u67b6\uff1a\u4ece\u89c6\u9891\u63d0\u53d6\u7269\u4f53\u4f4d\u59ff\u8f68\u8ff9\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u548c\u9884\u64cd\u4f5c\u624b\u90e8\u59ff\u52bf\u5f15\u5bfcRL\u8bad\u7ec3\u3002", "result": "\u5728\u6293\u53d6\u3001\u975e prehensile \u64cd\u4f5c\u548c\u591a\u6b65\u4efb\u52a1\u4e2d\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd555%\u548c68%\u3002", "conclusion": "\u8bc1\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u6865\u63a5\u4eba\u673a\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u8c03\u4f18\u3002"}}
{"id": "2504.12353", "pdf": "https://arxiv.org/pdf/2504.12353", "abs": "https://arxiv.org/abs/2504.12353", "authors": ["Shuo Shuo Liu", "Shikun Wang", "Yuxuan Chen", "Anil K. Rustgi", "Ming Yuan", "Jianhua Hu"], "title": "TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial Transcriptomics Data", "categories": ["q-bio.GN", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Background: Spatial transcriptomics have emerged as a powerful tool in\nbiomedical research because of its ability to capture both the spatial contexts\nand abundance of the complete RNA transcript profile in organs of interest.\nHowever, limitations of the technology such as the relatively low resolution\nand comparatively insufficient sequencing depth make it difficult to reliably\nextract real biological signals from these data. To alleviate this challenge,\nwe propose a novel transfer learning framework, referred to as TransST, to\nadaptively leverage the cell-labeled information from external sources in\ninferring cell-level heterogeneity of a target spatial transcriptomics data.\n  Results: Applications in several real studies as well as a number of\nsimulation settings show that our approach significantly improves existing\ntechniques. For example, in the breast cancer study, TransST successfully\nidentifies five biologically meaningful cell clusters, including the two\nsubgroups of cancer in situ and invasive cancer; in addition, only TransST is\nable to separate the adipose tissues from the connective issues among all the\nstudied methods.\n  Conclusions: In summary, the proposed method TransST is both effective and\nrobust in identifying cell subclusters and detecting corresponding driving\nbiomarkers in spatial transcriptomics data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransST\u7684\u8f6c\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u7684\u7ec6\u80de\u6c34\u5e73\u5f02\u8d28\u6027\u5206\u6790\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6280\u672f\u5b58\u5728\u5206\u8fa8\u7387\u4f4e\u548c\u6d4b\u5e8f\u6df1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u63d0\u53d6\u771f\u5b9e\u751f\u7269\u4fe1\u53f7\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5206\u6790\u80fd\u529b\u3002", "method": "\u63d0\u51faTransST\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5229\u7528\u5916\u90e8\u7ec6\u80de\u6807\u8bb0\u4fe1\u606f\uff0c\u63a8\u65ad\u76ee\u6807\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u7684\u7ec6\u80de\u5f02\u8d28\u6027\u3002", "result": "\u5728\u5b9e\u9645\u7814\u7a76\u548c\u6a21\u62df\u4e2d\uff0cTransST\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728\u4e73\u817a\u764c\u7814\u7a76\u4e2d\u6210\u529f\u8bc6\u522b\u4e86\u4e94\u4e2a\u7ec6\u80de\u7c07\uff0c\u5e76\u533a\u5206\u4e86\u8102\u80aa\u7ec4\u7ec7\u548c\u7ed3\u7f14\u7ec4\u7ec7\u3002", "conclusion": "TransST\u5728\u8bc6\u522b\u7ec6\u80de\u4e9a\u7fa4\u548c\u68c0\u6d4b\u9a71\u52a8\u751f\u7269\u6807\u5fd7\u7269\u65b9\u9762\u6709\u6548\u4e14\u7a33\u5065\u3002"}}
{"id": "2504.12637", "pdf": "https://arxiv.org/pdf/2504.12637", "abs": "https://arxiv.org/abs/2504.12637", "authors": ["Linda He", "Jue Wang", "Maurice Weber", "Shang Zhu", "Ben Athiwaratkun", "Ce Zhang"], "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 5 figures", "summary": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7RoPE\u7f29\u653e\u8bad\u7ec3\u6269\u5c55LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f3100\u4e07tokens\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u822c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "LLM\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5f00\u6e90\u5de5\u4f5c\u548c\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u540e\u8bad\u7ec3\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\u548c\u9010\u6b65RoPE\u7f29\u653e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728RULER\u548cInfiniteBench\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4fdd\u6301\u4e00\u822c\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u3002"}}
{"id": "2504.12368", "pdf": "https://arxiv.org/pdf/2504.12368", "abs": "https://arxiv.org/abs/2504.12368", "authors": ["Babak Ghassemi", "Cassio Fraga-Dantas", "Raffaele Gaetano", "Dino Ienco", "Omid Ghorbanzadeh", "Emma Izquierdo-Verdiguier", "Francesco Vuolo"], "title": "Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Land use and land cover mapping from Earth Observation (EO) data is a\ncritical tool for sustainable land and resource management. While advanced\nmachine learning and deep learning algorithms excel at analyzing EO imagery\ndata, they often overlook crucial geospatial metadata information that could\nenhance scalability and accuracy across regional, continental, and global\nscales. To address this limitation, we propose BRIDGE-LC (Bi-level\nRepresentation Integration for Disentangled GEospatial Land Cover), a novel\ndeep learning framework that integrates multi-scale geospatial information into\nthe land cover classification process. By simultaneously leveraging\nfine-grained (latitude/longitude) and coarse-grained (biogeographical region)\nspatial information, our lightweight multi-layer perceptron architecture learns\nfrom both during training but only requires fine-grained information for\ninference, allowing it to disentangle region-specific from region-agnostic land\ncover features while maintaining computational efficiency. To assess the\nquality of our framework, we use an open-access in-situ dataset and adopt\nseveral competing classification approaches commonly considered for large-scale\nland cover mapping. We evaluated all approaches through two scenarios: an\nextrapolation scenario in which training data encompasses samples from all\nbiogeographical regions, and a leave-one-region-out scenario where one region\nis excluded from training. We also explore the spatial representation learned\nby our model, highlighting a connection between its internal manifold and the\ngeographical information used during training. Our results demonstrate that\nintegrating geospatial information improves land cover mapping performance,\nwith the most substantial gains achieved by jointly leveraging both fine- and\ncoarse-grained spatial information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBRIDGE-LC\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5c3a\u5ea6\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u6539\u5584\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5ffd\u7565\u5730\u7406\u7a7a\u95f4\u5143\u6570\u636e\uff0c\u5bfc\u81f4\u8de8\u5c3a\u5ea6\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faBRIDGE-LC\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u5668\u6574\u5408\u7ec6\u7c92\u5ea6\uff08\u7ecf\u7eac\u5ea6\uff09\u548c\u7c97\u7c92\u5ea6\uff08\u751f\u7269\u5730\u7406\u533a\u57df\uff09\u7a7a\u95f4\u4fe1\u606f\uff0c\u8bad\u7ec3\u65f6\u5229\u7528\u4e24\u8005\uff0c\u63a8\u7406\u4ec5\u9700\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6574\u5408\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8054\u5408\u7ec6\u7c97\u7c92\u5ea6\u4fe1\u606f\u83b7\u5f97\u6700\u5927\u6536\u76ca\uff0c\u5728\u5916\u63a8\u548c\u7559\u4e00\u533a\u57df\u51fa\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6574\u5408\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u53ef\u663e\u8457\u63d0\u9ad8\u571f\u5730\u8986\u76d6\u6620\u5c04\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663", "abs": "https://arxiv.org/abs/2504.12663", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165Persona-judge\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e2a\u6027\u5316\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f7f\u7528\u6a21\u578b\u5185\u5728\u504f\u597d\u5224\u65ad\u80fd\u529b\u5b9e\u73b0\u9ad8\u6548\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5956\u52b1\u4fe1\u53f7\u548c\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u9002\u5e94\u591a\u6837\u4eba\u7c7b\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u65b9\u6848\u3002", "method": "Persona-judge\u91c7\u7528\u9274\u522b\u6027\u8303\u5f0f\uff1a\u8349\u7a3f\u6a21\u578b\u57fa\u4e8e\u7ed9\u5b9a\u504f\u597d\u751f\u6210\u5019\u9009\u6807\u8bb0\uff0c\u5224\u65ad\u6a21\u578b\u4f53\u73b0\u53e6\u4e00\u504f\u597d\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u800c\u4e0d\u901a\u8fc7\u5916\u90e8\u5956\u52b1\u4f18\u5316\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPersona-judge\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fd9\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u7684\u5b9a\u5236\u5bf9\u9f50\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2504.12374", "pdf": "https://arxiv.org/pdf/2504.12374", "abs": "https://arxiv.org/abs/2504.12374", "authors": ["Namu Kroupa", "G\u00e1bor Cs\u00e1nyi", "Will Handley"], "title": "Resonances in reflective Hamiltonian Monte Carlo", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "math.DS"], "comment": null, "summary": "In high dimensions, reflective Hamiltonian Monte Carlo with inexact\nreflections exhibits slow mixing when the particle ensemble is initialised from\na Dirac delta distribution and the uniform distribution is targeted. By\nquantifying the instantaneous non-uniformity of the distribution with the\nSinkhorn divergence, we elucidate the principal mechanisms underlying the\nmixing problems. In spheres and cubes, we show that the collective motion\ntransitions between fluid-like and discretisation-dominated behaviour, with the\ncritical step size scaling as a power law in the dimension. In both regimes,\nthe particles can spontaneously unmix, leading to resonances in the particle\ndensity and the aforementioned problems. Additionally, low-dimensional toy\nmodels of the dynamics are constructed which reproduce the dominant features of\nthe high-dimensional problem. Finally, the dynamics is contrasted with the\nexact Hamiltonian particle flow and tuning practices are discussed.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u9ad8\u7ef4\u4e0b\u53cd\u5c04Hamiltonian Monte Carlo\u65b9\u6cd5\u7684\u6162\u6df7\u5408\u95ee\u9898\uff0c\u4f7f\u7528Sinkhorn divergence\u5206\u6790\u673a\u5236\uff0c\u5e76\u5728\u7403\u4f53\u548c\u7acb\u65b9\u4f53\u4e2d\u7814\u7a76\u884c\u4e3a\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u91ca\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\uff0c\u4eceDirac delta\u5206\u5e03\u521d\u59cb\u5316\u5e76\u9488\u5bf9\u5747\u5300\u5206\u5e03\u65f6\uff0c\u7c92\u5b50\u7fa4\u7684\u6162\u6df7\u5408\u95ee\u9898\u53ca\u5176\u6f5c\u5728\u673a\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7528Sinkhorn divergence\u91cf\u5316\u975e\u5747\u5300\u6027\uff0c\u5206\u6790\u96c6\u4f53\u8fd0\u52a8\u5728\u6d41\u4f53\u72b6\u548c\u79bb\u6563\u5316\u4e3b\u5bfc\u884c\u4e3a\u95f4\u7684\u8fc7\u6e21\uff0c\u6784\u5efa\u4f4e\u7ef4\u73a9\u5177\u6a21\u578b\uff0c\u5e76\u4e0e\u7cbe\u786eHamiltonian\u7c92\u5b50\u6d41\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e34\u754c\u6b65\u957f\u968f\u7ef4\u5ea6\u5448\u5e42\u5f8b\u7f29\u653e\uff0c\u7c92\u5b50\u53ef\u80fd\u81ea\u53d1\u5206\u79bb\u5bfc\u81f4\u5bc6\u5ea6\u5171\u632f\uff0c\u73a9\u5177\u6a21\u578b\u518d\u73b0\u4e86\u9ad8\u7ef4\u95ee\u9898\u7684\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u7ed3\u8bba\u5bf9\u6bd4\u4e86\u52a8\u529b\u5b66\u4e0e\u7cbe\u786e\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u8c03\u4f18\u5b9e\u8df5\u4ee5\u6539\u5584\u6df7\u5408\u95ee\u9898\u3002"}}
{"id": "2504.12672", "pdf": "https://arxiv.org/pdf/2504.12672", "abs": "https://arxiv.org/abs/2504.12672", "authors": ["Belinda Trotta", "Robert Johnson", "Catherine de Burgh-Day", "Debra Hudson", "Esteban Abellan", "James Canvin", "Andrew Kelly", "Daniel Mentiplay", "Benjamin Owen", "Jennifer Whelan"], "title": "Post-processing improves accuracy of Artificial Intelligence weather forecasts", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.", "AI": {"tldr": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u5c06\u7edf\u8ba1\u540e\u5904\u7406\u65b9\u6cd5\u5e94\u7528\u4e8eAI\u5929\u6c14\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u80fd\u6539\u5584AI\u548c\u4f20\u7edfNWP\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u652f\u6301\u4f4e\u98ce\u9669\u6574\u5408AI\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\u3002", "motivation": "AI\u5929\u6c14\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u504f\u5dee\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e0e\u4f20\u7edfNWP\u7c7b\u4f3c\uff0c\u9700\u8981\u540e\u5904\u7406\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6c14\u8c61\u5c40\u7684IMPROVER\u7cfb\u7edf\u5bf9ECMWF\u7684AIFS\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5e76\u4e0eHRES\u548cENS\u6a21\u578b\u7684\u8f93\u51fa\u8fdb\u884c\u6bd4\u8f83\uff0c\u65e0\u9700\u4fee\u6539\u914d\u7f6e\u3002", "result": "\u540e\u5904\u7406\u4f7fAIFS\u7684\u51c6\u786e\u6027\u4e0e\u4f20\u7edfNWP\u76f8\u5f53\u6539\u5584\uff1b\u5c06AIFS\u4e0eNWP\u6df7\u5408\u63d0\u5347\u6574\u4f53\u9884\u6d4b\u6280\u80fd\u3002", "conclusion": "\u7edf\u8ba1\u540e\u5904\u7406\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8eAI\u6a21\u578b\uff0c\u5141\u8bb8\u6c14\u8c61\u4e2d\u5fc3\u4ee5\u4f4e\u98ce\u9669\u3001\u6e10\u8fdb\u65b9\u5f0f\u6574\u5408AI\u9884\u6d4b\u3002"}}
{"id": "2504.12389", "pdf": "https://arxiv.org/pdf/2504.12389", "abs": "https://arxiv.org/abs/2504.12389", "authors": ["Nayoung Lee", "Minsoo Shin", "Asel Sagingalieva", "Ayush Joshi Tripathi", "Karan Pinto", "Alexey Melnikov"], "title": "Predictive control of blast furnace temperature in steelmaking with hybrid depth-infused quantum neural networks", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Accurate prediction and stabilization of blast furnace temperatures are\ncrucial for optimizing the efficiency and productivity of steel production.\nTraditional methods often struggle with the complex and non-linear nature of\nthe temperature fluctuations within blast furnaces. This paper proposes a novel\napproach that combines hybrid quantum machine learning with pulverized coal\ninjection control to address these challenges. By integrating classical machine\nlearning techniques with quantum computing algorithms, we aim to enhance\npredictive accuracy and achieve more stable temperature control. For this we\nutilized a unique prediction-based optimization method. Our method leverages\nquantum-enhanced feature space exploration and the robustness of classical\nregression models to forecast temperature variations and optimize pulverized\ncoal injection values. Our results demonstrate a significant improvement in\nprediction accuracy over 25 percent and our solution improved temperature\nstability to +-7.6 degrees of target range from the earlier variance of +-50\ndegrees, highlighting the potential of hybrid quantum machine learning models\nin industrial steel production applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u6df7\u5408\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u548c\u7c89\u7164\u7070\u6ce8\u5165\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u9ad8\u7089\u6e29\u5ea6\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7089\u6e29\u5ea6\u6ce2\u52a8\u590d\u6742\u3001\u975e\u7ebf\u6027\u7279\u6027\uff0c\u51c6\u786e\u9884\u6d4b\u548c\u7a33\u5b9a\u6e29\u5ea6\u5bf9\u4f18\u5316\u94a2\u94c1\u751f\u4ea7\u6548\u7387\u548c\u751f\u4ea7\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6574\u5408\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7b97\u6cd5\uff0c\u5229\u7528\u91cf\u5b50\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\u63a2\u7d22\u548c\u7ecf\u5178\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u6e29\u5ea6\u53d8\u5316\uff0c\u5e76\u4f18\u5316\u7c89\u7164\u7070\u6ce8\u5165\u503c\u3002", "result": "\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u8d85\u8fc725%\uff0c\u6e29\u5ea6\u7a33\u5b9a\u6027\u4ece\u00b150\u5ea6\u6539\u5584\u5230\u00b17.6\u5ea6\u3002", "conclusion": "\u7a81\u663e\u6df7\u5408\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u5de5\u4e1a\u94a2\u94c1\u751f\u4ea7\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12673", "pdf": "https://arxiv.org/pdf/2504.12673", "abs": "https://arxiv.org/abs/2504.12673", "authors": ["Singon Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faACoRN\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5fae\u8c03\uff0c\u63d0\u9ad8RAG\u4e2d\u6458\u8981\u538b\u7f29\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u663e\u793aEM\u548cF1\u5206\u6570\u63d0\u5347\u3002", "motivation": "\u68c0\u7d22\u6587\u6863\u4e2d\u5b58\u5728\u65e0\u5173\u6216\u8bef\u5bfc\u4fe1\u606f\uff0c\u5bfc\u81f4\u6458\u8981\u538b\u7f29\u9057\u6f0f\u5173\u952e\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u6ce8\u610f\u529b\u5206\u6563\u3002", "method": "\u63d0\u51faACoRN\uff0c\u5305\u62ec\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u5bf9\u68c0\u7d22\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5fae\u8c03\u751f\u6210\u4ee5\u76f4\u63a5\u652f\u6301\u7b54\u6848\u7684\u5173\u952e\u4fe1\u606f\u6458\u8981\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528ACoRN\u8bad\u7ec3\u7684T5-large\u6539\u5584\u4e86EM\u548cF1\u5206\u6570\uff0c\u4fdd\u7559\u7b54\u6848\u5b57\u7b26\u4e32\uff0c\u5728\u566a\u58f0\u6587\u6863\u591a\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "ACoRN\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u53ef\u63d0\u5347RAG\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.12392", "pdf": "https://arxiv.org/pdf/2504.12392", "abs": "https://arxiv.org/abs/2504.12392", "authors": ["Aleix Alcacer", "Irene Epifanio", "Sebastian Mair", "Morten M\u00f8rup"], "title": "A Survey on Archetypal Analysis", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": "20 pages, 13 figures, under review", "summary": "Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and\nLeo Breiman as a computational procedure to extract the distinct aspects called\narchetypes in observations with each observational record approximated as a\nmixture (i.e., convex combination) of these archetypes. AA thereby provides\nstraightforward, interpretable, and explainable representations for feature\nextraction and dimensionality reduction, facilitating the understanding of the\nstructure of high-dimensional data with wide applications throughout the\nsciences. However, AA also faces challenges, particularly as the associated\noptimization problem is non-convex. This survey provides researchers and data\nmining practitioners an overview of methodologies and opportunities that AA has\nto offer surveying the many applications of AA across disparate fields of\nscience, as well as best practices for modeling data using AA and limitations.\nThe survey concludes by explaining important future research directions\nconcerning AA.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6982\u8ff0\u4e86\u539f\u578b\u5206\u6790\uff08AA\uff09\u7684\u6982\u5ff5\u3001\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63d0\u4f9bAA\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u6570\u636e\u6316\u6398\u4ece\u4e1a\u8005\u7406\u89e3\u5176\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u9650\u5236\u3002", "method": "\u901a\u8fc7\u8c03\u67e5AA\u7684\u65b9\u6cd5\u5b66\u3001\u5e94\u7528\u9886\u57df\u548c\u5efa\u6a21\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u603b\u7ed3\u4e86AA\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3001\u4f18\u52bf\u548c\u6311\u6218\u3002", "conclusion": "\u89e3\u91caAA\u7684\u91cd\u8981\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.12681", "pdf": "https://arxiv.org/pdf/2504.12681", "abs": "https://arxiv.org/abs/2504.12681", "authors": ["Kun-Woo Kim", "Ji-Hoon Park", "Ju-Min Han", "Seong-Whan Lee"], "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faGRAIL\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u57df\u573a\u666f\u4e0b\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5220\u9664\u654f\u611f\u4fe1\u606f\uff0c\u540c\u65f6\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u654f\u611f\u4fe1\u606f\u5f15\u53d1\u9690\u79c1\u548c\u6cd5\u5f8b\u95ee\u9898\uff0c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u7684\u5355\u57df\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u591a\u57df\u77e5\u8bc6\u4ea4\u7ec7\u95ee\u9898\u3002", "method": "\u63d0\u51faGRAIL\u6846\u67b6\uff0c\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u533a\u5206\u5220\u9664\u548c\u4fdd\u7559\u8303\u56f4\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u53c2\u6570\u5b9a\u4f4d\u7b56\u7565\u9009\u62e9\u6027\u5730\u79fb\u9664\u76ee\u6807\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGRAIL\u7684\u5220\u9664\u6210\u529f\u7387\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u77e5\u8bc6\u4fdd\u7559\u6210\u529f\u7387\u63d0\u9ad817%\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7ba1\u7406\u5927\u8bed\u8a00\u6a21\u578b\u654f\u611f\u4fe1\u606f\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2504.12412", "pdf": "https://arxiv.org/pdf/2504.12412", "abs": "https://arxiv.org/abs/2504.12412", "authors": ["Benjamin Krummenacher", "Jonas Frey", "Turcan Tuna", "Olga Vysotska", "Marco Hutter"], "title": "Diffusion Based Robust LiDAR Place Recognition", "categories": ["cs.RO", "cs.LG"], "comment": "accepted for ICRA 2025", "summary": "Mobile robots on construction sites require accurate pose estimation to\nperform autonomous surveying and inspection missions. Localization in\nconstruction sites is a particularly challenging problem due to the presence of\nrepetitive features such as flat plastered walls and perceptual aliasing due to\napartments with similar layouts inter and intra floors. In this paper, we focus\non the global re-positioning of a robot with respect to an accurate scanned\nmesh of the building solely using LiDAR data. In our approach, a neural network\nis trained on synthetic LiDAR point clouds generated by simulating a LiDAR in\nan accurate real-life large-scale mesh. We train a diffusion model with a\nPointNet++ backbone, which allows us to model multiple position candidates from\na single LiDAR point cloud. The resulting model can successfully predict the\nglobal position of LiDAR in confined and complex sites despite the adverse\neffects of perceptual aliasing. The learned distribution of potential global\npositions can provide multi-modal position distribution. We evaluate our\napproach across five real-world datasets and show the place recognition\naccuracy of 77% +/-2m on average while outperforming baselines at a factor of 2\nin mean error.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528LiDAR\u6570\u636e\u548c\u6269\u6563\u6a21\u578b\u5728\u5efa\u7b51\u5de5\u5730\u7cbe\u786e\u91cd\u65b0\u5b9a\u4f4d\u673a\u5668\u4eba\u7684\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u5b58\u5728\u611f\u77e5\u6df7\u6dc6\uff0c\u5b9e\u73b0\u4e8677%\u7684\u4f4d\u7f6e\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5efa\u7b51\u5de5\u5730\u673a\u5668\u4eba\u5b9a\u4f4d\u9762\u4e34\u91cd\u590d\u7279\u5f81\u548c\u611f\u77e5\u6df7\u6dc6\u7684\u6311\u6218\uff0c\u9700\u8981\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u6765\u652f\u6301\u81ea\u4e3b\u4efb\u52a1\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8ePointNet++\u9aa8\u5e72\u7684\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u5408\u6210LiDAR\u70b9\u4e91\u6570\u636e\uff0c\u4ece\u5355\u4e2a\u70b9\u4e91\u9884\u6d4b\u591a\u4e2a\u4f4d\u7f6e\u5019\u9009\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u4f4d\u7f6e\u8bc6\u522b\u51c6\u786e\u7387\u5e73\u5747\u4e3a77%\u00b12m\uff0c\u5e73\u5747\u8bef\u5dee\u6bd4\u57fa\u7ebf\u4f4e2\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u5168\u5c40\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u4f4d\u7f6e\u5206\u5e03\u3002"}}
{"id": "2504.12711", "pdf": "https://arxiv.org/pdf/2504.12711", "abs": "https://arxiv.org/abs/2504.12711", "authors": ["Xin Li", "Yeying Jin", "Xin Jin", "Zongwei Wu", "Bingchen Li", "Yufei Wang", "Wenhan Yang", "Yu Li", "Zhibo Chen", "Bihan Wen", "Robby T. Tan", "Radu Timofte", "Qiyu Rong", "Hongyuan Jing", "Mengmeng Zhang", "Jinglong Li", "Xiangyu Lu", "Yi Ren", "Yuting Liu", "Meng Zhang", "Xiang Chen", "Qiyuan Guan", "Jiangxin Dong", "Jinshan Pan", "Conglin Gou", "Qirui Yang", "Fangpu Zhang", "Yunlong Lin", "Sixiang Chen", "Guoxi Huang", "Ruirui Lin", "Yan Zhang", "Jingyu Yang", "Huanjing Yue", "Jiyuan Chen", "Qiaosi Yi", "Hongjun Wang", "Chenxi Xie", "Shuai Li", "Yuhui Wu", "Kaiyi Ma", "Jiakui Hu", "Juncheng Li", "Liwen Pan", "Guangwei Gao", "Wenjie Li", "Zhenyu Jin", "Heng Guo", "Zhanyu Ma", "Yubo Wang", "Jinghua Wang", "Wangzhi Xing", "Anjusree Karnavar", "Diqi Chen", "Mohammad Aminul Islam", "Hao Yang", "Ruikun Zhang", "Liyuan Pan", "Qianhao Luo", "XinCao", "Han Zhou", "Yan Min", "Wei Dong", "Jun Chen", "Taoyi Wu", "Weijia Dou", "Yu Wang", "Shengjie Zhao", "Yongcheng Huang", "Xingyu Han", "Anyan Huang", "Hongtao Wu", "Hong Wang", "Yefeng Zheng", "Abhijeet Kumar", "Aman Kumar", "Marcos V. Conde", "Paula Garrido", "Daniel Feijoo", "Juan C. Benito", "Guanglu Dong", "Xin Lin", "Siyuan Liu", "Tianheng Zheng", "Jiayu Zhong", "Shouyi Wang", "Xiangtai Li", "Lanqing Guo", "Lu Qi", "Chao Ren", "Shuaibo Wang", "Shilong Zhang", "Wanyu Zhou", "Yunze Wu", "Qinzhong Tan", "Jieyuan Pei", "Zhuoxuan Li", "Jiayu Wang", "Haoyu Bian", "Haoran Sun", "Subhajit Paul", "Ni Tang", "Junhao Huang", "Zihan Cheng", "Hongyun Zhu", "Yuehan Wu", "Kaixin Deng", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhizun Luo", "Zeyu Xiao", "Zhuoyuan Li", "Nguyen Pham Hoang Le", "An Dinh Thien", "Son T. Luu", "Kiet Van Nguyen", "Ronghua Xu", "Xianmin Tian", "Weijian Zhou", "Jiacheng Zhang", "Yuqian Chen", "Yihang Duan", "Yujie Wu", "Suresh Raikwar", "Arsh Garg", "Kritika", "Jianhua Zheng", "Xiaoshan Ma", "Ruolin Zhao", "Yongyu Yang", "Yongsheng Liang", "Guiming Huang", "Qiang Li", "Hongbin Zhang", "Xiangyu Zheng", "A. N. Rajagopalan"], "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams", "summary": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.", "AI": {"tldr": "This paper reviews the NTIRE 2025 Challenge on raindrop removal for dual-focused images, introducing a new diverse dataset and reporting state-of-the-art results from participants.", "motivation": "To address limitations of existing deraining datasets by providing a more diverse and challenging one that includes various lighting and focus conditions, and to establish a new benchmark for raindrop removal.", "method": "Organizing a challenge with the collection and curation of the Raindrop Clarity dataset, which includes subsets for training, validation, and testing, and evaluating participant submissions.", "result": "361 participants joined, 32 teams submitted valid solutions achieving state-of-the-art performance on the Raindrop Clarity dataset.", "conclusion": "The challenge successfully establishes a new benchmark for raindrop removal under varying conditions, with project details available online."}}
{"id": "2504.12714", "pdf": "https://arxiv.org/pdf/2504.12714", "abs": "https://arxiv.org/abs/2504.12714", "authors": ["Kunal Jha", "Wilka Carvalho", "Yancheng Liang", "Simon S. Du", "Max Kleiman-Weiner", "Natasha Jaques"], "title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted to CogSci 2025, In-review for ICML 2025", "summary": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u901a\u8fc7\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8bad\u7ec3AI\u4ee3\u7406\uff0c\u5b9e\u73b0\u96f6-shot\u534f\u8c03\u80fd\u529b\uff0c\u5f15\u5165Cross-Environment Cooperation (CEC)\u8303\u5f0f\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4e0e\u4eba\u7c7b\u5408\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u96f6-shot\u534f\u8c03\u662f\u4eba\u7c7b\u517c\u5bb9AI\u7684\u5173\u952e\uff0c\u5148\u524d\u5de5\u4f5c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u9002\u5e94\u65b0\u4f19\u4f34\u548c\u95ee\u9898\u7684\u4ee3\u7406\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u73af\u5883\u5206\u5e03\u4e0a\u8bad\u7ec3\u5355\u4e00\u4f19\u4f34\uff0c\u5f15\u5165CEC\u8303\u5f0f\uff0c\u5e76\u5229\u7528Jax-based\u7a0b\u5e8f\u751f\u6210\u5668\u521b\u5efa\u5927\u91cf\u534f\u8c03\u6311\u6218\u3002", "result": "CEC\u5728\u4e0e\u771f\u5b9e\u4eba\u7c7b\u5408\u4f5c\u65f6\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4ee3\u7406\u53d1\u5c55\u51fa\u901a\u7528\u89c4\u8303\uff0c\u6709\u6548\u652f\u6301\u4e0d\u540c\u4f19\u4f34\u7684\u534f\u4f5c\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8de8\u73af\u5883\u5b66\u4e60\uff0c\u4ee3\u7406\u53ef\u8bbe\u8ba1\u51fa\u65e0\u9700\u4eba\u7c7b\u6570\u636e\u5373\u53ef\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u901a\u7528\u5408\u4f5c\u7cfb\u7edf\u3002"}}
{"id": "2504.12717", "pdf": "https://arxiv.org/pdf/2504.12717", "abs": "https://arxiv.org/abs/2504.12717", "authors": ["Shin'ya Yamaguchi", "Dewei Feng", "Sekitoshi Kanai", "Kazuki Adachi", "Daiki Chijiwa"], "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine", "summary": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faCLIP-Refine\uff0c\u4e00\u79cd\u540e\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc71\u4e2aepoch\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u51cf\u5c11CLIP\u6a21\u578b\u7684\u6a21\u6001\u95f4\u5dee\u8ddd\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6709\u51fa\u8272\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f46\u6a21\u6001\u95f4\u5dee\u8ddd\u9650\u5236\u4e86\u8868\u73b0\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u6210\u672c\u6216\u6027\u80fd\u4e0a\u5b58\u5728\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u968f\u673a\u7279\u5f81\u5bf9\u9f50(RaFA)\u548c\u6df7\u5408\u5bf9\u6bd4\u84b8\u998f(HyCD)\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7279\u5f81\u4e0e\u968f\u673a\u53c2\u8003\u5411\u91cf\u7684\u8ddd\u79bb\uff0c\u5e76\u7ed3\u5408\u771f\u5b9e\u6807\u7b7e\u548c\u9884\u8bad\u7ec3CLIP\u8f93\u51fa\u751f\u6210\u6df7\u5408\u8f6f\u6807\u7b7e\u6765\u5bf9\u9f50\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCLIP-Refine\u51cf\u8f7b\u4e86\u6a21\u6001\u95f4\u5dee\u8ddd\uff0c\u5e76\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\u6709\u6548\u3002", "conclusion": "CLIP-Refine\u6210\u529f\u7f13\u89e3\u6a21\u6001\u95f4\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u9ad8\u6210\u672c\u8bad\u7ec3\u3002"}}
{"id": "2504.12718", "pdf": "https://arxiv.org/pdf/2504.12718", "abs": "https://arxiv.org/abs/2504.12718", "authors": ["Walid Rehamnia", "Alexandra Getmanskaya", "Evgeniy Vasilyev", "Vadim Turlapov"], "title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"], "comment": "32 pages, 15 figures, 3 tables, 42 references", "summary": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u591a\u7ea7\u5206\u5272\u65b9\u6cd5TUMLS\uff0c\u7528\u4e8e\u6570\u5b57\u75c5\u7406\u5b66\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u6807\u6ce8\u52b3\u52a8\u5bc6\u96c6\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u4ee5\u53ca\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bfc\u81f4\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u4ee3\u8868\u6027\u8865\u4e01\uff0c\u8fdb\u884c\u65e0\u76d1\u7763\u7684\u6838\u5206\u5272\uff0c\u5e76\u6574\u5408\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u3002", "result": "\u5728UPENN-GBM\u6570\u636e\u96c6\u4e0a\uff0cAE\u7684MSE\u4e3a0.0016\uff1b\u5728MoNuSeg\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u657077.46%\u3001Jaccard\u5206\u657063.35%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86TUMLS\u5728\u63a8\u8fdb\u6570\u5b57\u75c5\u7406\u5b66\u9886\u57df\u7684\u6548\u80fd\uff0c\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6d41\u7684\u901f\u5ea6\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2504.12480", "pdf": "https://arxiv.org/pdf/2504.12480", "abs": "https://arxiv.org/abs/2504.12480", "authors": ["Keshav Srinivasan", "Dietmar Plenz", "Michelle Girvan"], "title": "Boosting Reservoir Computing with Brain-inspired Adaptive Dynamics", "categories": ["cs.NE", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Reservoir computers (RCs) provide a computationally efficient alternative to\ndeep learning while also offering a framework for incorporating brain-inspired\ncomputational principles. By using an internal neural network with random,\nfixed connections$-$the 'reservoir'$-$and training only the output weights, RCs\nsimplify the training process but remain sensitive to the choice of\nhyperparameters that govern activation functions and network architecture.\nMoreover, typical RC implementations overlook a critical aspect of neuronal\ndynamics: the balance between excitatory and inhibitory (E-I) signals, which is\nessential for robust brain function. We show that RCs characteristically\nperform best in balanced or slightly over-inhibited regimes, outperforming\nexcitation-dominated ones. To reduce the need for precise hyperparameter\ntuning, we introduce a self-adapting mechanism that locally adjusts E/I balance\nto achieve target neuronal firing rates, improving performance by up to 130% in\ntasks like memory capacity and time series prediction compared with globally\ntuned RCs. Incorporating brain-inspired heterogeneity in target neuronal firing\nrates further reduces the need for fine-tuning hyperparameters and enables RCs\nto excel across linear and non-linear tasks. These results support a shift from\nstatic optimization to dynamic adaptation in reservoir design, demonstrating\nhow brain-inspired mechanisms improve RC performance and robustness while\ndeepening our understanding of neural computation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u5174\u594b-\u6291\u5236\u5e73\u8861\u673a\u5236\u6765\u63d0\u5347\u6c34\u5e93\u8ba1\u7b97\u673a\uff08RC\uff09\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u8d85\u53c2\u6570\u8c03\u6574\u9700\u6c42\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "RC\u8ba1\u7b97\u9ad8\u6548\u4f46\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u4e14\u5ffd\u7565\u4e86\u8111\u4e2d\u5174\u594b-\u6291\u5236\u5e73\u8861\u7684\u91cd\u8981\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8111\u542f\u53d1\u673a\u5236\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u5f15\u5165\u5c40\u90e8\u8c03\u6574\u5174\u594b/\u6291\u5236\u5e73\u8861\u7684\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5e76\u6dfb\u52a0\u795e\u7ecf\u5143\u653e\u7535\u7387\u7684\u5f02\u8d28\u6027\uff0c\u4ee5\u51cf\u5c11\u8d85\u53c2\u6570\u5fae\u8c03\u3002", "result": "RC\u5728\u5e73\u8861\u6216\u8f7b\u5fae\u8fc7\u6291\u5236\u72b6\u6001\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u81ea\u9002\u5e94\u673a\u5236\u4f7f\u6027\u80fd\u63d0\u9ad8\u9ad8\u8fbe130%\uff0c\u5e76\u5728\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edfRC\u3002", "conclusion": "\u652f\u6301\u6c34\u5e93\u8bbe\u8ba1\u4ece\u9759\u6001\u4f18\u5316\u8f6c\u5411\u52a8\u6001\u9002\u5e94\uff0c\u589e\u5f3a\u4e86\u5bf9\u795e\u7ecf\u8ba1\u7b97\u7684\u7406\u89e3\u548cRC\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.12722", "pdf": "https://arxiv.org/pdf/2504.12722", "abs": "https://arxiv.org/abs/2504.12722", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165SimUSER\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u7528\u6237\u4ee3\u7406\u8bc4\u4f30\u63a8\u8350\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u771f\u5b9e\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u5b58\u5728\u79bb\u7ebf\u6307\u6807\u4e0e\u5728\u7ebf\u884c\u4e3a\u8131\u8282\u7684\u95ee\u9898\uff0c\u4e14\u771f\u5b9e\u7528\u6237\u6570\u636e\u7a00\u7f3a\u5e76\u53d7\u9690\u79c1\u9650\u5236\u3002", "method": "\u5f00\u53d1SimUSER\u6846\u67b6\uff0c\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u63d0\u53d6\u4e00\u81f4\u6027\u4eba\u683c\uff0c\u6784\u5efa\u5177\u6709\u4eba\u683c\u3001\u8bb0\u5fc6\u3001\u611f\u77e5\u548c\u5927\u8111\u6a21\u5757\u7684\u4ee3\u7406\u7528\u6237\uff0c\u4e0e\u63a8\u8350\u7cfb\u7edf\u4ea4\u4e92\u3002", "result": "SimUSER\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u63a5\u8fd1\u771f\u5b9e\u4eba\u7c7b\uff1b\u5b9e\u9a8c\u63a2\u8ba8\u4e86\u7f29\u7565\u56fe\u5bf9\u70b9\u51fb\u7387\u3001\u66dd\u5149\u6548\u5e94\u548c\u8bc4\u8bba\u5bf9\u7528\u6237\u53c2\u4e0e\u7684\u5f71\u54cd\uff1b\u4f18\u5316\u53c2\u6570\u540e\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u901a\u8fc7SimUSER\u6846\u67b6\u7684\u79bb\u7ebfA/B\u6d4b\u8bd5\uff0c\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u4e2d\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u6539\u5584\u3002"}}
{"id": "2504.12519", "pdf": "https://arxiv.org/pdf/2504.12519", "abs": "https://arxiv.org/abs/2504.12519", "authors": ["Dmitry Yarotsky"], "title": "Corner Gradient Descent", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We consider SGD-type optimization on infinite-dimensional quadratic problems\nwith power law spectral conditions. It is well-known that on such problems\ndeterministic GD has loss convergence rates $L_t=O(t^{-\\zeta})$, which can be\nimproved to $L_t=O(t^{-2\\zeta})$ by using Heavy Ball with a non-stationary\nJacobi-based schedule (and the latter rate is optimal among fixed schedules).\nHowever, in the mini-batch Stochastic GD setting, the sampling noise causes the\nJacobi HB to diverge; accordingly no $O(t^{-2\\zeta})$ algorithm is known. In\nthis paper we show that rates up to $O(t^{-2\\zeta})$ can be achieved by a\ngeneralized stationary SGD with infinite memory. We start by identifying\ngeneralized (S)GD algorithms with contours in the complex plane. We then show\nthat contours that have a corner with external angle $\\theta\\pi$ accelerate the\nplain GD rate $O(t^{-\\zeta})$ to $O(t^{-\\theta\\zeta})$. For deterministic GD,\nincreasing $\\theta$ allows to achieve rates arbitrarily close to\n$O(t^{-2\\zeta})$. However, in Stochastic GD, increasing $\\theta$ also amplifies\nthe sampling noise, so in general $\\theta$ needs to be optimized by balancing\nthe acceleration and noise effects. We prove that the optimal rate is given by\n$\\theta_{\\max}=\\min(2,\\nu,\\tfrac{2}{\\zeta+1/\\nu})$, where $\\nu,\\zeta$ are the\nexponents appearing in the capacity and source spectral conditions.\nFurthermore, using fast rational approximations of the power functions, we show\nthat ideal corner algorithms can be efficiently approximated by finite-memory\nalgorithms, and demonstrate their practical efficiency on a synthetic problem\nand MNIST.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u5728\u65e0\u9650\u7ef4\u4e8c\u6b21\u95ee\u9898\u4e0a\uff0c\u901a\u8fc7\u65e0\u9650\u8bb0\u5fc6\u7684\u5e7f\u4e49SGD\uff0c\u53ef\u4ee5\u8fbe\u5230O(t^{-2\u03b6})\u7684\u6536\u655b\u7387\u3002", "motivation": "\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4e2d\uff0cJacobi HB\u4f1a\u53d1\u6563\uff0c\u65e0\u6cd5\u8fbe\u5230O(t^{-2\u03b6})\u7684\u6536\u655b\u7387\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u590d\u5e73\u9762\u8f6e\u5ed3\u4f18\u5316\u53c2\u6570\u03b8\uff0c\u5e73\u8861\u52a0\u901f\u548c\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5feb\u901f\u6709\u7406\u903c\u8fd1\u5b9e\u73b0\u6709\u9650\u8bb0\u5fc6\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u6536\u655b\u7387\u03b8_max=min(2,\u03bd,2/(\u03b6+1/\u03bd))\uff0c\u5e76\u5728\u5408\u6210\u95ee\u9898\u548cMNIST\u4e0a\u9a8c\u8bc1\u4e86\u6548\u7387\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u8fd1\u4f3c\u5b9e\u73b0\uff0c\u5e76\u663e\u8457\u6539\u5584\u968f\u673a\u4f18\u5316\u4e2d\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2504.12734", "pdf": "https://arxiv.org/pdf/2504.12734", "abs": "https://arxiv.org/abs/2504.12734", "authors": ["Yongrui Chen", "Junhao He", "Linbo Fu", "Shenyu Zhang", "Rihui Jin", "Xinbang Dai", "Jiaqi Li", "Dehai Min", "Nan Hu", "Yuxin Zhang", "Guilin Qi", "Yi Huang", "Tongtong Wu"], "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.", "AI": {"tldr": "Pandora\u6846\u67b6\u901a\u8fc7Python\u7684Pandas API\u548cLLM\u751f\u6210\u63a8\u7406\u6b65\u9aa4\u53ca\u4ee3\u7801\uff0c\u7edf\u4e00\u77e5\u8bc6\u8868\u793a\uff0c\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709USKR\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u6216\u81ea\u5b9a\u4e49\u8868\u793a\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4efb\u52a1\u95f4\u77e5\u8bc6\u8f6c\u79fb\u6216\u4e0eLLM\u5bf9\u9f50\uff0c\u4ece\u800c\u9650\u5236\u6027\u80fd\u3002", "method": "\u63d0\u51faPandora\u6846\u67b6\uff0c\u4f7f\u7528Pandas API\u7edf\u4e00\u77e5\u8bc6\u8868\u793a\uff0cLLM\u751f\u6210\u6587\u672c\u63a8\u7406\u548c\u53ef\u6267\u884cPython\u4ee3\u7801\uff0c\u5e76\u4ece\u8bad\u7ec3\u793a\u4f8b\u8bb0\u5fc6\u4e2d\u62bd\u53d6\u6f14\u793a\u4ee5\u4fc3\u8fdb\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u6d89\u53ca\u4e09\u4e2aSKR\u4efb\u52a1\u4e2d\uff0cPandora\u4f18\u4e8e\u73b0\u6709\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u4e0e\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "Pandora\u6846\u67b6\u8bc1\u660e\u4e86\u7edf\u4e00\u77e5\u8bc6\u8868\u793a\u548cLLM\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86USKR\u6027\u80fd\u3002"}}
{"id": "2504.12735", "pdf": "https://arxiv.org/pdf/2504.12735", "abs": "https://arxiv.org/abs/2504.12735", "authors": ["Lidong Zhai", "Zhijie Qiu", "Xizhong Guo", "Jiaqi Li"], "title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e03\u5c42\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8eAI\u827a\u672f\u521b\u4f5c\u4e2d\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u6539\u5584\u534f\u4f5c\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u9488\u5bf9\u591a\u4ee3\u7406\u7cfb\u7edf\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u95ee\u9898\uff0c\u5982\u534f\u4f5c\u6548\u7387\u3001\u89d2\u8272\u5206\u914d\u3001\u73af\u5883\u9002\u5e94\u548c\u4efb\u52a1\u5e76\u884c\u6027\u3002", "method": "\u63d0\u51fa'Academy of Athens'\u4e03\u5c42\u6846\u67b6\uff0c\u5305\u62ec\u591a\u4ee3\u7406\u534f\u4f5c\u3001\u5355\u4ee3\u7406\u591a\u89d2\u8272\u7b49\u5c42\u7ea7\uff0c\u5e76\u901a\u8fc7\u827a\u672f\u521b\u4f5c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u5728\u4efb\u52a1\u534f\u4f5c\u3001\u8de8\u573a\u666f\u9002\u5e94\u548c\u6a21\u578b\u878d\u5408\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u6311\u6218\u548c\u672a\u6765\u6280\u672f\u3002", "conclusion": "\u6846\u67b6\u4e3aAI\u827a\u672f\u521b\u4f5c\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u63d0\u4f9b\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u827a\u672f\u521b\u65b0\u5e94\u7528\u3002"}}
{"id": "2504.12528", "pdf": "https://arxiv.org/pdf/2504.12528", "abs": "https://arxiv.org/abs/2504.12528", "authors": ["Carlos Misael Madrid Padilla", "Shitao Fan", "Lizhen Lin"], "title": "Robust and Scalable Variational Bayes", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We propose a robust and scalable framework for variational Bayes (VB) that\neffectively handles outliers and contamination of arbitrary nature in large\ndatasets. Our approach divides the dataset into disjoint subsets, computes the\nposterior for each subset, and applies VB approximation independently to these\nposteriors. The resulting variational posteriors with respect to the subsets\nare then aggregated using the geometric median of probability measures,\ncomputed with respect to the Wasserstein distance. This novel aggregation\nmethod yields the Variational Median Posterior (VM-Posterior) distribution. We\nrigorously demonstrate that the VM-Posterior preserves contraction properties\nakin to those of the true posterior, while accounting for approximation errors\nor the variational gap inherent in VB methods. We also provide provable\nrobustness guarantee of the VM-Posterior. Furthermore, we establish a\nvariational Bernstein-von Mises theorem for both multivariate Gaussian\ndistributions with general covariance structures and the mean-field variational\nfamily. To facilitate practical implementation, we adapt existing algorithms\nfor computing the VM-Posterior and evaluate its performance through extensive\nnumerical experiments. The results highlight its robustness and scalability,\nmaking it a reliable tool for Bayesian inference in the presence of complex,\ncontaminated datasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u53d8\u5206\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u7684\u5f02\u5e38\u503c\u548c\u6c61\u67d3\uff0c\u901a\u8fc7\u5b50\u96c6\u805a\u5408\u548cWasserstein\u8ddd\u79bb\u4e2d\u4f4d\u6570\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u53d8\u5206\u8d1d\u53f6\u65af\u5bf9\u5f02\u5e38\u503c\u548c\u6c61\u67d3\u7684\u654f\u611f\u6027\uff0c\u63d0\u9ad8\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u6570\u636e\u96c6\u5206\u6210\u5b50\u96c6\uff0c\u72ec\u7acb\u8ba1\u7b97\u53d8\u5206\u540e\u9a8c\uff0c\u5e76\u4f7f\u7528Wasserstein\u8ddd\u79bb\u7684\u51e0\u4f55\u4e2d\u4f4d\u6570\u805a\u5408\u5f97\u5230\u53d8\u5206\u4e2d\u4f4d\u6570\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u7ed3\u679c\u8bc1\u660e\u4e86\u6536\u7f29\u6027\u8d28\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5efa\u7acb\u4e86\u53d8\u5206Bernstein-von Mises\u5b9a\u7406\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8be5\u6846\u67b6\u4e3a\u6c61\u67d3\u6570\u636e\u96c6\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2504.12755", "pdf": "https://arxiv.org/pdf/2504.12755", "abs": "https://arxiv.org/abs/2504.12755", "authors": ["Anurag Maurya", "Tashmoy Ghosh", "Ravi Prakash"], "title": "Trajectory Adaptation using Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to CoRL LangRob workshop 2024", "summary": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u9884\u8bad\u7ec3LLM\u6765\u9002\u5e94\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u6839\u636e\u4eba\u7c7b\u6307\u4ee4\u751f\u6210\u4ee3\u7801\u7b56\u7565\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\uff0c\u5e76\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9002\u5e94\u673a\u5668\u4eba\u8f68\u8ff9\u4ee5\u54cd\u5e94\u4eba\u7c7b\u6307\u4ee4\u662f\u5b9e\u73b0\u66f4\u76f4\u89c2\u548c\u53ef\u6269\u5c55\u7684\u4eba\u673a\u4ea4\u4e92\u6240\u5fc5\u9700\u7684\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7075\u6d3b\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3LLM\u901a\u8fc7\u751f\u6210\u4ee3\u7801\u4f5c\u4e3a\u7b56\u7565\u6765\u9002\u5e94\u901a\u7528\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4e0d\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5728Pybullet\u548cGazebo\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u673a\u5668\u4eba\u673a\u68b0\u81c2\u3001\u822a\u7a7a\u5668\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660eLLM\u80fd\u591f\u6210\u529f\u9002\u5e94\u590d\u6742\u7684\u4eba\u7c7b\u6307\u4ee4\u3002", "conclusion": "\u4e0e\u73b0\u6709\u57fa\u4e8e\u7279\u5f81\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53cd\u9988\u673a\u5236\u3002"}}
{"id": "2504.12757", "pdf": "https://arxiv.org/pdf/2504.12757", "abs": "https://arxiv.org/abs/2504.12757", "authors": ["Sonu Kumar", "Anubhav Girdhar", "Ritesh Patil", "Divyansh Tripathi"], "title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86MCP Guardian\u6846\u67b6\uff0c\u4ee5\u589e\u5f3aMCP\u534f\u8bae\u7684\u5b89\u5168\u6027\uff0c\u786e\u4fddAI\u5e94\u7528\u7684\u5b89\u5168\u6570\u636e\u8bbf\u95ee\u3002", "motivation": "AI\u7cfb\u7edf\u53d7\u9650\u4e8e\u6570\u636e\u5b64\u5c9b\uff0c\u96c6\u6210\u56f0\u96be\uff0c\u4e14MCP\u5f15\u5165\u4e86\u6076\u610f\u670d\u52a1\u5668\u7b49\u98ce\u9669\uff0c\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u63d0\u51faMCP Guardian\u6846\u67b6\uff0c\u5305\u62ec\u8ba4\u8bc1\u3001\u901f\u7387\u9650\u5236\u3001\u65e5\u5fd7\u8bb0\u5f55\u3001\u8ffd\u8e2a\u548cWAF\u626b\u63cf\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u573a\u666f\u548c\u5b9e\u8bc1\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660eMCP Guardian\u6709\u6548\u7f13\u89e3\u653b\u51fb\uff0c\u5e76\u4ee5\u6700\u5c0f\u5f00\u9500\u5b9e\u73b0\u7a33\u5065\u76d1\u7763\u3002", "conclusion": "\u4fc3\u8fdbAI\u52a9\u624b\u7684\u53ef\u6269\u5c55\u5b89\u5168\u6570\u636e\u8bbf\u95ee\uff0c\u5f3a\u8c03\u9632\u5fa1\u6df1\u5ea6\u65b9\u6cd5\u5728AI\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.12773", "pdf": "https://arxiv.org/pdf/2504.12773", "abs": "https://arxiv.org/abs/2504.12773", "authors": ["Yicheng Pan", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Jun Du", "Jianshu Zhang", "Quan Liu", "Jianqing Gao", "Feng Ma"], "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faGeoGen\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u51e0\u4f55\u63a8\u7406\u6570\u636e\uff0c\u5e76\u8bad\u7ec3GeoLogic\u6a21\u578b\uff0c\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u4e2d\u56e0\u7f3a\u4e4f\u7cbe\u786e\u9010\u6b65\u6570\u636e\u548c\u4e25\u91cd\u5e7b\u89c9\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faGeoGen\u7ba1\u9053\u5229\u7528\u7b26\u53f7\u63a8\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u9898\u7b54\u6848\u5bf9\uff0c\u5e76\u8bad\u7ec3GeoLogic\u6a21\u578b\u4f5c\u4e3a\u8bed\u8a00\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u6865\u6881\uff0c\u9a8c\u8bc1\u8f93\u51fa\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u51e0\u4f55\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2504.12573", "pdf": "https://arxiv.org/pdf/2504.12573", "abs": "https://arxiv.org/abs/2504.12573", "authors": ["Yuning Zhou", "Henry Badgery", "Matthew Read", "James Bailey", "Catherine Davey"], "title": "Parsimonious Dataset Construction for Laparoscopic Cholecystectomy Structure Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "IEEE EMBS ISC Australia 2022", "summary": "Labeling has always been expensive in the medical context, which has hindered\nrelated deep learning application. Our work introduces active learning in\nsurgical video frame selection to construct a high-quality, affordable\nLaparoscopic Cholecystectomy dataset for semantic segmentation. Active learning\nallows the Deep Neural Networks (DNNs) learning pipeline to include the dataset\nconstruction workflow, which means DNNs trained by existing dataset will\nidentify the most informative data from the newly collected data. At the same\ntime, DNNs' performance and generalization ability improve over time when the\nnewly selected and annotated data are included in the training data. We\nassessed different data informativeness measurements and found the deep\nfeatures distances select the most informative data in this task. Our\nexperiments show that with half of the data selected by active learning, the\nDNNs achieve almost the same performance with 0.4349 mean Intersection over\nUnion (mIoU) compared to the same DNNs trained on the full dataset (0.4374\nmIoU) on the critical anatomies and surgical instruments.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u624b\u672f\u89c6\u9891\u5e27\uff0c\u6784\u5efa\u4f4e\u6210\u672c\u9ad8\u8d28\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u4ec5\u7528\u4e00\u534a\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u51e0\u4e4e\u76f8\u540c\u6027\u80fd\uff08mIoU 0.4349 vs 0.4374\uff09\u3002", "motivation": "\u533b\u7597\u9886\u57df\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u9ad8\u6548\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u6570\u636e\u3002", "method": "\u5f15\u5165\u4e3b\u52a8\u5b66\u4e60\uff0cDNNs\u57fa\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u8bc6\u522b\u65b0\u6570\u636e\u4e2d\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e27\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u6d4b\u91cf\u65b9\u6cd5\uff0c\u53d1\u73b0\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u6700\u6709\u6548\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u4e00\u534a\u6570\u636e\uff0cDNNs\u5728\u5173\u952e\u89e3\u5256\u548c\u5668\u68b0\u4e0a\u7684mIoU\u4e3a0.4349\uff0c\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76840.4374\u51e0\u4e4e\u76f8\u5f53\u3002", "conclusion": "\u4e3b\u52a8\u5b66\u4e60\u80fd\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u9ad8DNNs\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.12777", "pdf": "https://arxiv.org/pdf/2504.12777", "abs": "https://arxiv.org/abs/2504.12777", "authors": ["James Rudd-Jones", "Mirco Musolesi", "Mar\u00eda P\u00e9rez-Ortiz"], "title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis", "categories": ["cs.MA", "cs.AI"], "comment": "Published in AAMAS'25 Blue Sky Ideas Track", "summary": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u589e\u5f3a\u6c14\u5019\u6a21\u62df\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6c14\u5019\u653f\u7b56\u5408\u6210\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u52a8\u6001\u6311\u6218\u3002", "motivation": "\u6c14\u5019\u653f\u7b56\u5f00\u53d1\u9762\u4e34\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u3001\u590d\u6742\u7cfb\u7edf\u52a8\u6001\u548c\u5229\u76ca\u76f8\u5173\u8005\u51b2\u7a81\u7684\u6311\u6218\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5728\u653f\u7b56\u5408\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e0e\u6c14\u5019\u6a21\u62df\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u653f\u7b56\u8def\u5f84\u3002", "result": "\u8bc6\u522b\u4e86MARL\u5728\u6c14\u5019\u6a21\u62df\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u653f\u7b56\u63a2\u7d22\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u66f4\u9ad8\u7ea7\u7684\u653f\u7b56\u63a2\u7d22\u5960\u5b9a\u57fa\u7840\uff0c\u4f46\u627f\u8ba4\u4e86\u9650\u5236\u548c\u672a\u6765\u7814\u7a76\u9700\u6c42\u3002"}}
{"id": "2504.12575", "pdf": "https://arxiv.org/pdf/2504.12575", "abs": "https://arxiv.org/abs/2504.12575", "authors": ["Timothy Proctor", "Anh Tran", "Xingxin Liu", "Aditya Dhumuntarao", "Stefan Seritan", "Alaina Green", "Norbert M Linke"], "title": "Featuremetric benchmarking: Quantum computer benchmarks based on circuit features", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Benchmarks that concisely summarize the performance of many-qubit quantum\ncomputers are essential for measuring progress towards the goal of useful\nquantum computation. In this work, we present a benchmarking framework that is\nbased on quantifying how a quantum computer's performance on quantum circuits\nvaries as a function of features of those circuits, such as circuit depth,\nwidth, two-qubit gate density, problem input size, or algorithmic depth. Our\nfeaturemetric benchmarking framework generalizes volumetric benchmarking -- a\nwidely-used methodology that quantifies performance versus circuit width and\ndepth -- and we show that it enables richer and more faithful models of quantum\ncomputer performance. We demonstrate featuremetric benchmarking with example\nbenchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to\nproduce performance summaries from the data using Gaussian process regression.\nOur data analysis methods are also of interest in the special case of\nvolumetric benchmarking, as they enable the creation of intuitive\ntwo-dimensional capability regions using data from few circuits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7535\u8def\u7279\u5f81\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u91cf\u5b50\u4f4d\u91cf\u5b50\u8ba1\u7b97\u673a\u6027\u80fd\uff0c\u63a8\u5e7f\u4e86\u4f53\u79ef\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "motivation": "\u9700\u8981\u7b80\u6d01\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u603b\u7ed3\u591a\u91cf\u5b50\u4f4d\u91cf\u5b50\u8ba1\u7b97\u673a\u6027\u80fd\uff0c\u4ee5\u8861\u91cf\u5b9e\u73b0\u6709\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fafeaturemetric\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u91cf\u5316\u6027\u80fd\u968f\u7535\u8def\u6df1\u5ea6\u3001\u5bbd\u5ea6\u3001\u4e8c\u91cf\u5b50\u4f4d\u95e8\u5bc6\u5ea6\u7b49\u95ee\u9898\u53d8\u5316\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5206\u6790\u6570\u636e\uff0c\u5e76\u5728IBM Q\u548cIonQ\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u3002", "result": "\u5c55\u793a\u4e86\u6846\u67b6\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6027\u80fd\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u5c11\u91cf\u7535\u8def\u6570\u636e\u521b\u5efa\u76f4\u89c2\u7684\u4e8c\u7ef4\u80fd\u529b\u533a\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u548c\u603b\u7ed3\u91cf\u5b50\u8ba1\u7b97\u673a\u6027\u80fd\uff0c\u4fc3\u8fdb\u91cf\u5b50\u8ba1\u7b97\u53d1\u5c55\u3002"}}
{"id": "2504.12778", "pdf": "https://arxiv.org/pdf/2504.12778", "abs": "https://arxiv.org/abs/2504.12778", "authors": ["Yuxuan Zong", "Benjamin Piwowarski"], "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted at SIGIR 2025 Full Paper Track", "summary": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u4fee\u526aColBERT\u6a21\u578b\u6807\u8bb0\uff0c\u4e0d\u5f71\u54cd\u68c0\u7d22\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4f7f\u7528\u4ec530%\u7684\u6807\u8bb0\u5373\u53ef\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "ColBERT\u6a21\u578b\u9700\u5927\u91cf\u5185\u5b58\u5b58\u50a8\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u73b0\u6709\u7684\u4fee\u526a\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u4e0d\u5f71\u54cd\u68c0\u7d22\u5206\u6570\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u6b63\u5219\u5316\u635f\u5931\u548c\u4e24\u79cd\u4fee\u526a\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fee\u526a\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\uff0c\u4f7f\u7528\u4ec530%\u7684\u6807\u8bb0\u5373\u53ef\u4fdd\u7559ColBERT\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fee\u526a\u6807\u8bb0\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6548\u679c\u3002"}}
{"id": "2504.12782", "pdf": "https://arxiv.org/pdf/2504.12782", "abs": "https://arxiv.org/abs/2504.12782", "authors": ["Leyang Li", "Shilin Lu", "Yan Ren", "Adams Wai-Kin Kong"], "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": "Preprint", "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faANT\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5f15\u5bfc\u53bb\u566a\u8f68\u8ff9\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4f26\u7406\u90e8\u7f72\uff0c\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1aanchor-free\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u4f2a\u50cf\uff0canchor-based\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u951a\u6982\u5ff5\u9009\u62e9\u3002", "method": "ANT\u9006\u8f6cclassifier-free guidance\u7684\u6761\u4ef6\u65b9\u5411\uff0c\u4f7f\u7528trajectory-aware\u76ee\u6807\u51fd\u6570\u548c\u589e\u5f3a\u6743\u91cd\u663e\u7740\u6027\u56fe\u8fdb\u884c\u7cbe\u786e\u6982\u5ff5\u64e6\u9664\u3002", "result": "\u5b9e\u9a8c\u663e\u793aANT\u5728\u5355\u591a\u6982\u5ff5\u64e6\u9664\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b89\u5168\u56fe\u50cf\u800c\u4e0d\u964d\u4f4e\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "conclusion": "ANT\u63d0\u4f9b\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4f26\u7406\u90e8\u7f72\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u3002"}}
{"id": "2504.12625", "pdf": "https://arxiv.org/pdf/2504.12625", "abs": "https://arxiv.org/abs/2504.12625", "authors": ["Jun Fan", "Zheng-Chu Guo", "Lei Shi"], "title": "Spectral Algorithms under Covariate Shift", "categories": ["stat.ML", "cs.LG", "68Q32, 68T05, 62J02"], "comment": null, "summary": "Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u8c31\u7b97\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u52a0\u6743\u8c31\u7b97\u6cd5\u6539\u5584\u6536\u655b\u7387\u3002", "motivation": "\u4e3a\u4e86\u52a0\u6df1\u5bf9\u8c31\u7b97\u6cd5\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u4e0d\u540c\u7684\u771f\u5b9e\u573a\u666f\u4e0b\u6027\u80fd\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u534f\u53d8\u91cf\u504f\u79fb\u3002", "method": "\u5206\u6790\u8c31\u7b97\u6cd5\u7684\u6cdb\u5316\u8bef\u5dee\uff0c\u5728\u5bc6\u5ea6\u6bd4\u6709\u754c\u65f6\u8bc1\u660eminimax\u6700\u4f18\u6027\uff1b\u5728\u65e0\u754c\u65f6\uff0c\u63d0\u51fa\u52a0\u6743\u8c31\u7b97\u6cd5\u548c\u6743\u91cd\u88c1\u526a\u6280\u672f\u3002", "result": "\u8c31\u7b97\u6cd5\u5728\u5bc6\u5ea6\u6bd4\u6709\u754c\u65f6\u8fbe\u5230minimax\u6700\u4f18\uff1b\u5728\u65e0\u754c\u65f6\u53ef\u80fd\u6b21\u4f18\uff0c\u52a0\u6743\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f18\u6536\u655b\u7387\u3002", "conclusion": "\u89e3\u51b3\u4e86\u65e0\u754c\u5bc6\u5ea6\u6bd4\u4e0b\u7684\u6b21\u4f18\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2504.12683", "pdf": "https://arxiv.org/pdf/2504.12683", "abs": "https://arxiv.org/abs/2504.12683", "authors": ["Cristina Anton", "Roy Shivam Ram Shreshtth"], "title": "Cluster weighted models with multivariate skewed distributions for functional data", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u529f\u80fd\u7ebf\u6027\u56de\u5f52\u548c\u504f\u659c\u5206\u5e03\u7684\u529f\u80fd\u6570\u636e\u805a\u7c7b\u65b9\u6cd5funWeightClustSkew\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u6269\u5c55\u529f\u80fd\u9ad8\u7ef4\u6570\u636e\u805a\u7c7b\u65b9\u6cd5funHDDC\u5230\u529f\u80fd\u6570\u636e\uff0c\u5e76\u5f15\u5165\u504f\u659c\u5206\u5e03\u4ee5\u5904\u7406\u6570\u636e\u504f\u659c\u3002", "method": "\u57fa\u4e8e\u529f\u80fd\u7ebf\u6027\u56de\u5f52\u6df7\u5408\u6a21\u578b\u548c\u65b9\u5dee-\u4f3d\u9a6c\u3001\u504f\u659c-t\u3001\u6b63\u6001-\u9006\u9ad8\u65af\u5206\u5e03\uff0c\u6784\u5efa\u4e86funWeightClustSkew\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528EM\u7b97\u6cd5\u4f30\u8ba1\u53c2\u6570\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u7a7a\u6c14\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u504f\u659c\u529f\u80fd\u6570\u636e\u7684\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.12807", "pdf": "https://arxiv.org/pdf/2504.12807", "abs": "https://arxiv.org/abs/2504.12807", "authors": ["Ach Khozaimi", "Isnani Darti", "Syaiful Anam", "Wuryansari Muharini Kusumawinahyu"], "title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408Dense-UNet201\u6a21\u578b\uff0c\u4f7f\u7528SMO\u7b97\u6cd5\u4f18\u5316\uff0c\u7528\u4e8ePap\u6d82\u7247\u56fe\u50cf\u5206\u5272\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u5e76\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5206\u5272\u6a21\u578b\u5728\u5904\u7406Pap\u6d82\u7247\u56fe\u50cf\u590d\u6742\u7ec6\u80de\u7ed3\u6784\u548c\u53d8\u5f02\u65f6\u7684\u6311\u6218\uff0c\u4ee5\u6539\u5584\u5bab\u9888\u764c\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u5c06\u9884\u8bad\u7ec3DenseNet201\u4f5c\u4e3aU-Net\u7f16\u7801\u5668\uff0c\u5e76\u4f7f\u7528\u4fee\u6539\u540e\u7684SMO\u7b97\u6cd5\u4f18\u5316\uff0c\u91c7\u7528SIPaKMeD\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "SMO Dense-UNet201\u8fbe\u5230\u4e8696.16%\u7684\u51c6\u786e\u7387\u300191.63%\u7684IoU\u548c95.63%\u7684Dice\u7cfb\u6570\uff0c\u4f18\u4e8eU-Net\u3001Res-UNet50\u548cEfficient-UNetB0\u3002", "conclusion": "\u5f3a\u8c03\u56fe\u50cf\u9884\u5904\u7406\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u5728\u63d0\u5347\u533b\u7597\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5bab\u9888\u7ec6\u80de\u5206\u5272\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002"}}
{"id": "2504.12695", "pdf": "https://arxiv.org/pdf/2504.12695", "abs": "https://arxiv.org/abs/2504.12695", "authors": ["Tempei Kabayama", "Motomasa Komuro", "Yasuo Kuniyoshi", "Kazuyuki Aihara", "Kohei Nakajima"], "title": "Attractor-merging Crises and Intermittency in Reservoir Computing", "categories": ["nlin.CD", "cs.LG", "cs.NE", "math.DS"], "comment": "20 pages, 15 figures", "summary": "Reservoir computing can embed attractors into random neural networks (RNNs),\ngenerating a ``mirror'' of a target attractor because of its inherent\nsymmetrical constraints. In these RNNs, we report that an attractor-merging\ncrisis accompanied by intermittency emerges simply by adjusting the global\nparameter. We further reveal its underlying mechanism through a detailed\nanalysis of the phase-space structure and demonstrate that this bifurcation\nscenario is intrinsic to a general class of RNNs, independent of training data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u8c03\u6574\u968f\u673a\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u7684\u5168\u5c40\u53c2\u6570\uff0c\u53ef\u5f15\u53d1\u5438\u5f15\u5b50\u5408\u5e76\u5371\u673a\u548c\u95f4\u6b47\u6027\uff0c\u8fd9\u79cd\u73b0\u8c61\u662fRNNs\u7684\u5185\u5728\u7279\u6027\uff0c\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u63a2\u8ba8Reservoir computing\u5728RNNs\u4e2d\u5d4c\u5165\u5438\u5f15\u5b50\u5e76\u751f\u6210\u955c\u50cf\u5438\u5f15\u5b50\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5bf9\u79f0\u7ea6\u675f\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u5168\u5c40\u53c2\u6570\u5e76\u5bf9\u76f8\u7a7a\u95f4\u7ed3\u6784\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u63ed\u793a\u5206\u53c9\u673a\u5236\u3002", "result": "\u89c2\u5bdf\u5230\u5438\u5f15\u5b50\u5408\u5e76\u5371\u673a\u4f34\u968f\u95f4\u6b47\u6027\u51fa\u73b0\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u5206\u53c9\u573a\u666f\u9002\u7528\u4e8e\u4e00\u822cRNNs\uff0c\u4e0e\u8bad\u7ec3\u6570\u636e\u65e0\u5173\u3002", "conclusion": "\u8fd9\u79cd\u5206\u53c9\u73b0\u8c61\u662fRNNs\u7684\u56fa\u6709\u7279\u6027\uff0c\u4e0d\u53d7\u8bad\u7ec3\u6570\u636e\u5f71\u54cd\u3002"}}
{"id": "2504.12817", "pdf": "https://arxiv.org/pdf/2504.12817", "abs": "https://arxiv.org/abs/2504.12817", "authors": ["Nassim Belmecheri", "Arnaud Gotlieb", "Nadjib Lazaar", "Helge Spieker"], "title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)", "summary": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGNN\u4e0eQXGs\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u573a\u666f\u7406\u89e3\u662f\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u57fa\u7840\uff0c\u5148\u524d\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u4e00\u5173\u7cfb\u94fe\uff0c\u5ffd\u7565\u6574\u4f53\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51fa\u65b0\u578bGNN\u67b6\u6784\uff0c\u5904\u7406\u6574\u4e2a\u56fe\u7ed3\u6784\u4ee5\u8bc6\u522b\u4ea4\u901a\u573a\u666f\u76f8\u5173\u5bf9\u8c61\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u5e76\u8003\u8651\u65f6\u7a7a\u5173\u7cfb\u3002", "conclusion": "\u5c55\u793a\u4e86\u5b9a\u6027\u8868\u793a\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5728\u53ef\u89e3\u91ca\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12700", "pdf": "https://arxiv.org/pdf/2504.12700", "abs": "https://arxiv.org/abs/2504.12700", "authors": ["Robert de Mello Koch", "Animik Ghosh"], "title": "A Two-Phase Perspective on Deep Learning Dynamics", "categories": ["hep-th", "cond-mat.dis-nn", "cs.LG"], "comment": "17 pages, 6 figures", "summary": "We propose that learning in deep neural networks proceeds in two phases: a\nrapid curve fitting phase followed by a slower compression or coarse graining\nphase. This view is supported by the shared temporal structure of three\nphenomena: grokking, double descent and the information bottleneck, all of\nwhich exhibit a delayed onset of generalization well after training error\nreaches zero. We empirically show that the associated timescales align in two\nrather different settings. Mutual information between hidden layers and input\ndata emerges as a natural progress measure, complementing circuit-based metrics\nsuch as local complexity and the linear mapping number. We argue that the\nsecond phase is not actively optimized by standard training algorithms and may\nbe unnecessarily prolonged. Drawing on an analogy with the renormalization\ngroup, we suggest that this compression phase reflects a principled form of\nforgetting, critical for generalization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6709\u4e24\u4e2a\u9636\u6bb5\uff1a\u5feb\u901f\u62df\u5408\u548c\u7f13\u6162\u538b\u7f29\uff0c\u5e76\u901a\u8fc7grokking\u3001double descent\u548c\u4fe1\u606f\u74f6\u9888\u73b0\u8c61\u652f\u6301\u5ef6\u8fdf\u6cdb\u5316\u89c2\u70b9\u3002", "motivation": "\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6cdb\u5316\u5ef6\u8fdf\u7684\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u73b0\u8c61\u7684\u5171\u4eab\u65f6\u95f4\u7ed3\u6784\u6765\u652f\u6301\u8fd9\u4e00\u89c2\u70b9\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5728\u4e24\u4e2a\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u5206\u6790\u73b0\u8c61\uff0c\u4f7f\u7528\u4e92\u4fe1\u606f\u4f5c\u4e3a\u8fdb\u5ea6\u6307\u6807\uff0c\u5e76\u4e0e\u7535\u8def-based\u6307\u6807\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u73b0\u8c61\u7684\u65f6\u95f4\u5c3a\u5ea6\u5bf9\u9f50\uff0c\u4e92\u4fe1\u606f\u4f5c\u4e3a\u81ea\u7136\u8fdb\u5ea6\u6307\u6807\uff0c\u5e76\u8bba\u8bc1\u538b\u7f29\u9636\u6bb5\u5bf9\u6cdb\u5316\u91cd\u8981\u3002", "conclusion": "\u5efa\u8bae\u538b\u7f29\u9636\u6bb5\u662f\u4e00\u79cd\u539f\u5219\u6027\u7684\u9057\u5fd8\u5f62\u5f0f\uff0c\u5bf9\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53ef\u80fd\u88ab\u6807\u51c6\u8bad\u7ec3\u7b97\u6cd5\u4e0d\u5fc5\u8981\u5730\u5ef6\u957f\u3002"}}
{"id": "2504.12833", "pdf": "https://arxiv.org/pdf/2504.12833", "abs": "https://arxiv.org/abs/2504.12833", "authors": ["Elior Benarous", "Yilun Du", "Heng Yang"], "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u4e0e\u7528\u6237\u6307\u4ee4\u7684\u5bf9\u9f50\u6027\u548c\u771f\u5b9e\u6027\uff0c\u53ea\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u7f16\u8f91\u4e2d\u7ed3\u6784\u4fdd\u7559\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5e76\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "method": "\u5f15\u5165\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u63d0\u793a\u548c\u4ec55\u5f20\u53c2\u8003\u56fe\u50cf\uff0c\u572810\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u5185\u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u7cbe\u786e\u7f16\u8f91\uff0c\u4fdd\u6301\u4e86\u65e0\u5173\u533a\u57df\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u63d0\u5347\u4e86\u6a21\u62df\u73af\u5883\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u7528\u6237\u7f16\u8f91\u8fc7\u7a0b\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.12856", "pdf": "https://arxiv.org/pdf/2504.12856", "abs": "https://arxiv.org/abs/2504.12856", "authors": ["Yifeng Cheng", "Juan Du"], "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO", "I.5.4"], "comment": null, "summary": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa3D-PNAS\u65b9\u6cd5\uff0c\u4f7f\u7528Perlin\u566a\u58f0\u751f\u62103D\u8868\u9762\u5f02\u5e38\uff0c\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u771f\u5b9e\u7f3a\u9677\u6837\u672c\u7a00\u7f3a\uff0c3D\u6570\u636e\u5229\u7528\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d223D\u5f02\u5e38\u751f\u6210\u4ee5\u63d0\u5347\u8868\u9762\u8d28\u91cf\u68c0\u67e5\u3002", "method": "\u57fa\u4e8ePerlin\u566a\u58f0\u548c\u8868\u9762\u53c2\u6570\u5316\uff0c\u5c06\u70b9\u4e91\u6295\u5f71\u52302D\u5e73\u9762\uff0c\u91c7\u6837\u591a\u5c3a\u5ea6\u566a\u58f0\uff0c\u5e76\u6cbf\u6cd5\u7ebf\u65b9\u5411\u6270\u52a8\u70b9\u4e91\u751f\u6210\u5f02\u5e38\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u53c2\u6570\uff08\u5982\u566a\u58f0\u89c4\u6a21\u3001\u6270\u52a8\u5f3a\u5ea6\uff09\u53ef\u7cbe\u7ec6\u63a7\u5236\u5f02\u5e38\u591a\u6837\u6027\uff0c\u8de8\u7c7b\u522b\u751f\u6210\u4e00\u81f4\u4e14\u51e0\u4f55\u5408\u7406\u7684\u7f3a\u9677\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e863D\u5f02\u5e38\u751f\u6210\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4ee3\u7801\u5e93\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.12758", "pdf": "https://arxiv.org/pdf/2504.12758", "abs": "https://arxiv.org/abs/2504.12758", "authors": ["Kyriakos Stylianopoulos", "George C. Alexandropoulos"], "title": "Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining", "categories": ["eess.SP", "cs.LG"], "comment": "Submitted to IEEE SPAWC 2025", "summary": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86XL MIMO\u7cfb\u7edf\u53ef\u50cf\u795e\u7ecf\u7f51\u7edc\u4e00\u6837\u5145\u5f53\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u65e0\u7ebf\u63a8\u7406\uff0c\u5e76\u5177\u6709\u4f4e\u590d\u6742\u5ea6\u3002", "motivation": "\u52a8\u673a\u662f\u51cf\u5c11\u65e0\u7ebf\u7cfb\u7edf\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u5c06MIMO\u89c6\u4e3a\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u65e0\u9700\u6570\u5b57\u5904\u7406\u3002", "method": "\u65b9\u6cd5\u662f\u5c06XL MIMO\u4fe1\u9053\u7cfb\u6570\u89c6\u4e3a\u9690\u85cf\u5c42\u8282\u70b9\uff0c\u6a21\u62df\u7ec4\u5408\u5668\u4f5c\u4e3a\u8f93\u51fa\u5c42\uff0c\u91c7\u7528ELM\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u548c\u6570\u503c\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u6bd4\uff0c\u6027\u80fd\u76f8\u5f53\u4f46\u590d\u6742\u5ea6\u4f4e\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\u548c\u9ad8\u6548\u5206\u7c7b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5efa\u8bae\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u8d85\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u89c6\u4e3a\u795e\u7ecf\u7f51\u7edc\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u65e0\u7ebf\u8bbe\u5907\u3002"}}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867", "abs": "https://arxiv.org/abs/2504.12867", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "AI": {"tldr": "This paper introduces EmoVoice, an emotion-controllable TTS model using LLMs for fine-grained emotion control, along with a new dataset and improved evaluation methods.", "motivation": "To address the limitations in emotional expression of existing TTS models, enabling more natural and human-like speech synthesis.", "method": "Proposes EmoVoice model with LLM-based emotion control and parallel phoneme-audio output design, and introduces EmoVoice-DB dataset with fine-grained emotion labels.", "result": "Achieves state-of-the-art performance on English and Chinese test sets, and explores emotion evaluation metrics using multimodal LLMs.", "conclusion": "Advances TTS technology by providing an effective emotion control mechanism and open resources for future research."}}
{"id": "2504.12891", "pdf": "https://arxiv.org/pdf/2504.12891", "abs": "https://arxiv.org/abs/2504.12891", "authors": ["Vicent Briva-Iglesias"], "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8AI\u4ee3\u7406\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6f5c\u529b\uff0c\u7126\u70b9\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6cd5\u5f8bMT\u8bd5\u70b9\u7814\u7a76\u5c55\u793a\u5176\u4f18\u52bf\u3002", "motivation": "AI\u4ee3\u7406\u5728\u5404\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u673a\u5668\u7ffb\u8bd1\u5e94\u7528\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u63d0\u5347\u591a\u8bed\u79cd\u6570\u5b57\u901a\u4fe1\u3002", "method": "\u8fdb\u884c\u6cd5\u5f8b\u673a\u5668\u7ffb\u8bd1\u8bd5\u70b9\u7814\u7a76\uff0c\u4f7f\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u7ffb\u8bd1\u3001 adequacy review\u3001 fluency review \u548c\u6700\u7ec8\u7f16\u8f91\u56db\u4e2a\u4e13\u95e8\u4ee3\u7406\u3002", "result": "\u591a\u4ee3\u7406\u7cfb\u7edf\u53ef\u663e\u8457\u63d0\u9ad8\u9886\u57df\u9002\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\uff0c\u7ffb\u8bd1\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u6216\u5355\u4ee3\u7406\u7cfb\u7edf\u3002", "conclusion": "\u4e3a\u672a\u6765\u591a\u4ee3\u7406\u5728MT\u4e2d\u7684\u5e94\u7528\u548c\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u5206\u4eab\u7cfb\u7edf\u6f14\u793a\u3002"}}
{"id": "2504.12898", "pdf": "https://arxiv.org/pdf/2504.12898", "abs": "https://arxiv.org/abs/2504.12898", "authors": ["Zhouhao Sun", "Xiao Ding", "Li Du", "Yunpeng Xu", "Yixuan Ma", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u589e\u76ca\u5f15\u5bfc\u7684\u56e0\u679c\u5e72\u9884\u53bb\u504f\u6846\u67b6\uff08IGCIDB\uff09\uff0c\u7528\u4e8e\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u6355\u83b7\u6570\u636e\u96c6\u504f\u5dee\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u7684\u53bb\u504f\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u5f15\u5bfc\u7684\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\u81ea\u52a8\u5e73\u8861\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u7136\u540e\u901a\u8fc7\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3LLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIGCIDB\u80fd\u6709\u6548\u53bb\u504fLLM\uff0c\u63d0\u9ad8\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IGCIDB\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u673a\u5236\u548c\u4fe1\u606f\u7406\u8bba\uff0c\u6210\u529f\u63d0\u5347\u4e86LLM\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2504.12911", "pdf": "https://arxiv.org/pdf/2504.12911", "abs": "https://arxiv.org/abs/2504.12911", "authors": ["Chengyi Ju", "Weijie Shi", "Chengzhong Liu", "Jiaming Ji", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "Benchmarking Multi-National Value Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165NaVAB\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4e0e\u4e94\u4e2a\u56fd\u5bb6\uff08\u4e2d\u56fd\u3001\u7f8e\u56fd\u3001\u82f1\u56fd\u3001\u6cd5\u56fd\u3001\u5fb7\u56fd\uff09\u4ef7\u503c\u89c2\u7684\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u4f26\u7406\u5ba1\u67e5\uff0c\u4f46\u672a\u80fd\u6355\u6349\u56fd\u5bb6\u4ef7\u503c\u89c2\u7684\u591a\u6837\u6027\uff0c\u4e14\u57fa\u51c6\u4f9d\u8d56\u624b\u52a8\u95ee\u5377\uff0c\u4e0d\u6613\u6269\u5c55\u3002", "method": "\u5f00\u53d1NaVAB\u57fa\u51c6\uff0c\u5305\u62ec\u56fd\u5bb6\u4ef7\u503c\u89c2\u63d0\u53d6\u7ba1\u9053\uff1a\u6307\u4ee4\u6807\u8bb0\u5efa\u6a21\u3001\u7b5b\u9009\u76f8\u5173\u4e3b\u9898\u548c\u5e26\u51b2\u7a81\u51cf\u5c11\u673a\u5236\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u5b58\u5728\u4e0d\u4e00\u81f4\u573a\u666f\uff0c\u5e76\u8bc1\u660eNaVAB\u53ef\u4e0e\u5bf9\u9f50\u6280\u672f\u7ed3\u5408\uff0c\u51cf\u5c11\u4ef7\u503c\u89c2\u95ee\u9898\u3002", "conclusion": "NaVAB\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u7f13\u89e3LLM\u4e0e\u56fd\u5bb6\u4ef7\u503c\u89c2\u7684\u4e0d\u4e00\u81f4\u3002"}}
{"id": "2504.12860", "pdf": "https://arxiv.org/pdf/2504.12860", "abs": "https://arxiv.org/abs/2504.12860", "authors": ["C. Revelas", "O. Boldea", "B. J. M. Werker"], "title": "When do Random Forests work?", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u968f\u673a\u68ee\u6797\u4e2d\u968f\u673a\u5316\u5206\u88c2\u65b9\u5411\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4fe1\u566a\u6bd4\u548c\u6570\u636e\u7279\u6027\uff0c\u53d1\u73b0\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u76f8\u5173\u534f\u53d8\u91cf\u60c5\u51b5\u4e0b\u66f4\u6709\u6548\u3002", "motivation": "\u6574\u5408\u5148\u524d\u63d0\u51fa\u7684\u53bb\u76f8\u5173\u548c\u6b63\u5219\u5316\u6548\u679c\uff0c\u5bf9\u968f\u673a\u68ee\u6797\u968f\u673a\u5316\u7684\u6709\u6548\u6027\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5e38\u89c1\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u7cfb\u7edf\u5206\u6790\u548c\u6a21\u62df\u7814\u7a76\uff0c\u8003\u5bdf\u4e0d\u540c\u4fe1\u566a\u6bd4\u548c\u6570\u636e\u7279\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u65b9\u5dee\u51cf\u5c11\u968f\u4fe1\u566a\u6bd4\u589e\u52a0\uff1b\u968f\u673a\u68ee\u6797\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u4f18\u4e8ebagging\uff1b\u968f\u673a\u5316\u53ef\u80fd\u589e\u52a0\u504f\u7f6e\uff08\u80a5\u5c3e\u5206\u5e03\uff09\uff1b\u5728\u65e0\u5173\u534f\u53d8\u91cf\u4e0b\u65e0\u6548\uff1b\u5728\u76f8\u5173\u534f\u53d8\u91cf\u4e0b\u6709\u6548\uff1b\u5e73\u5747\u53ef\u51cf\u5c11\u504f\u7f6e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u968f\u673a\u68ee\u6797\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u534f\u53d8\u91cf\u76f8\u5173\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2504.12951", "pdf": "https://arxiv.org/pdf/2504.12951", "abs": "https://arxiv.org/abs/2504.12951", "authors": ["Nearchos Potamitis", "Akhil Arora"], "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691", "summary": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u53cd\u9988\u7684\u91cd\u8bd5\u673a\u5236\uff0c\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u7b80\u5355\u65b9\u6cd5\u53ef\u80fd\u4f18\u4e8e\u590d\u6742\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fed\u4ee3\u63a8\u7406\u7b56\u7565\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6210\u672c\u9ad8\u95ee\u9898\u3002", "method": "\u5f15\u5165\u2018retrials without feedback\u2019\u6982\u5ff5\uff0c\u5141\u8bb8LLM\u5728\u8bc6\u522b\u9519\u8bef\u7b54\u6848\u65f6\u91cd\u8bd5\uff0c\u4e0d\u9700\u81ea\u53cd\u6216\u53cd\u9988\u3002", "result": "\u7b80\u5355\u91cd\u8bd5\u65b9\u6cd5\u5f80\u5f80\u4f18\u4e8e\u590d\u6742\u63a8\u7406\u6846\u67b6\u3002", "conclusion": "\u6311\u6218\u590d\u6742\u7b56\u7565\u7684\u5fc5\u8981\u6027\uff0c\u5efa\u8bae\u7b80\u5355\u65b9\u6cd5\u53ef\u80fd\u8db3\u591f\u3002"}}
{"id": "2504.12922", "pdf": "https://arxiv.org/pdf/2504.12922", "abs": "https://arxiv.org/abs/2504.12922", "authors": ["Morenikeji Neri", "Nicholas Pischke", "Thomas Powell"], "title": "On the asymptotic behaviour of stochastic processes, with applications to supermartingale convergence, Dvoretzky's approximation theorem, and stochastic quasi-Fej\u00e9r monotonicity", "categories": ["math.OC", "cs.LG", "math.LO", "math.PR"], "comment": "41 pages", "summary": "We prove a novel and general result on the asymptotic behavior of stochastic\nprocesses which conform to a certain relaxed supermartingale condition. Our\nresult provides quantitative information in the form of an explicit and\neffective construction of a rate of convergence for this process, both in mean\nand almost surely, that is moreover highly uniform in the sense that it only\ndepends on very few data of the surrounding objects involved in the iteration.\nWe then apply this result to derive new quantitative versions of well-known\nconcepts and theorems from stochastic approximation, in particular providing\neffective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's\nconvergence theorem, as well as the convergence of stochastic quasi-Fej\\'er\nmonotone sequences, the latter of which formulated in a novel and highly\ngeneral metric context. We utilize the classic and widely studied Robbins-Monro\nprocedure as a template to evaluate our quantitative results and their\napplicability in greater detail. We conclude by illustrating the breadth of\npotential further applications with a brief discussion on a variety of other\nwell-known iterative procedures from stochastic approximation, covering a range\nof different applied scenarios to which our methods can be immediately applied.\nThroughout, we isolate and discuss special cases of our results which even\nallow for the construction of fast, and in particular linear, rates.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc1\u660e\u4e86\u4e00\u4e2a\u5173\u4e8e\u7b26\u5408\u677e\u5f1b\u8d85\u9785\u6761\u4ef6\u7684\u968f\u673a\u8fc7\u7a0b\u6e10\u8fd1\u884c\u4e3a\u7684\u65b0\u9896\u4e00\u822c\u7ed3\u679c\uff0c\u63d0\u4f9b\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u5e76\u5e94\u7528\u4e8e\u968f\u673a\u903c\u8fd1\u7684\u591a\u4e2a\u5b9a\u7406\u3002", "motivation": "\u52a8\u673a\u662f\u63d0\u4f9b\u5b9a\u91cf\u6536\u655b\u4fe1\u606f\uff0c\u6539\u8fdb\u73b0\u6709\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7ecf\u5178\u5b9a\u7406\u7684\u6536\u655b\u5206\u6790\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u8bc1\u660e\u4e00\u4e2a\u4e00\u822c\u5b9a\u7406\uff0c\u6784\u5efa\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5982Robbins-Siegmund\u5b9a\u7406\u3001Dvoretzky\u5b9a\u7406\u548c\u968f\u673a\u51c6-Fej\u00e9r\u5355\u8c03\u5e8f\u5217\u7684\u5177\u4f53\u573a\u666f\u3002", "result": "\u7ed3\u679c\u5305\u62ec\u4e86\u968f\u673a\u903c\u8fd1\u6982\u5ff5\u7684\u5b9a\u91cf\u7248\u672c\u3001\u6709\u6548\u6536\u655b\u901f\u7387\u7684\u6784\u9020\uff0c\u4ee5\u53ca\u5728Robbins-Monro\u7a0b\u5e8f\u7b49\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u7ed3\u8bba\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3001\u8fdb\u4e00\u6b65\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u7279\u6b8a\u60c5\u51b5\u4e0b\u5feb\u901f\u7ebf\u6027\u6536\u655b\u7387\u7684\u6784\u5efa\u3002"}}
{"id": "2504.12961", "pdf": "https://arxiv.org/pdf/2504.12961", "abs": "https://arxiv.org/abs/2504.12961", "authors": ["Zhouyang Jiang", "Bin Zhang", "Airong Wei", "Zhiwei Xu"], "title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?", "categories": ["cs.MA", "cs.AI"], "comment": "9 pages, 7 figures", "summary": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faQLLM\u7b97\u6cd5\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u4fe1\u7528\u5206\u914d\u51fd\u6570\uff0c\u89e3\u51b3\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u6311\u6218\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u4ef7\u503c\u5206\u89e3\u65b9\u6cd5\u5b58\u5728\u8d21\u732e\u5f52\u56e0\u4e0d\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u548c\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u53ef\u4f38\u7f29\u6027\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165TFCAF\u6982\u5ff5\uff0c\u5e76\u4f7f\u7528coder-evaluator\u6846\u67b6\u6307\u5bfcLLM\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u4fe1\u7528\u5206\u914d\u4ee3\u7801\u3002", "result": "\u5728\u6807\u51c6MARL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQLLM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u51fa\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u517c\u5bb9\u6027\u3002", "conclusion": "QLLM\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u591a\u4ee3\u7406\u573a\u666f\u3002"}}
{"id": "2504.12939", "pdf": "https://arxiv.org/pdf/2504.12939", "abs": "https://arxiv.org/abs/2504.12939", "authors": ["Robin Hesse", "Jonas Fischer", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Disentangling Polysemantic Channels in Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 Workshop on Mechanistic Interpretability for\n  Vision (MIV). Code: https://github.com/visinf/disentangle-channels", "summary": "Mechanistic interpretability is concerned with analyzing individual\ncomponents in a (convolutional) neural network (CNN) and how they form larger\ncircuits representing decision mechanisms. These investigations are challenging\nsince CNNs frequently learn polysemantic channels that encode distinct\nconcepts, making them hard to interpret. To address this, we propose an\nalgorithm to disentangle a specific kind of polysemantic channel into multiple\nchannels, each responding to a single concept. Our approach restructures\nweights in a CNN, utilizing that different concepts within the same channel\nexhibit distinct activation patterns in the previous layer. By disentangling\nthese polysemantic features, we enhance the interpretability of CNNs,\nultimately improving explanatory techniques such as feature visualizations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u79bbCNN\u4e2d\u591a\u4e49\u901a\u9053\u7684\u7279\u5f81\uff0c\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "CNN\u7ecf\u5e38\u5b66\u4e60\u591a\u4e49\u901a\u9053\uff0c\u8fd9\u4e9b\u901a\u9053\u7f16\u7801\u591a\u4e2a\u4e0d\u540c\u6982\u5ff5\uff0c\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u7b97\u6cd5\u901a\u8fc7\u91cd\u6784\u6743\u91cd\uff0c\u5229\u7528\u4e0d\u540c\u6982\u5ff5\u5728\u4e0a\u4e00\u5c42\u4e0d\u540c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u6765\u5206\u79bb\u591a\u4e49\u7279\u5f81\u3002", "result": "\u5206\u79bb\u591a\u4e49\u7279\u5f81\u540e\uff0c\u589e\u5f3a\u4e86CNN\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6539\u5584\u4e86\u7279\u5f81\u53ef\u89c6\u5316\u7b49\u89e3\u91ca\u6280\u672f\u3002", "conclusion": "\u6700\u7ec8\u63d0\u9ad8\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u673a\u5236\u7684\u7406\u89e3\u3002"}}
{"id": "2504.12977", "pdf": "https://arxiv.org/pdf/2504.12977", "abs": "https://arxiv.org/abs/2504.12977", "authors": ["Maksim Vishnevskiy"], "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "comment": "12 pages, no figures", "summary": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d77\u5fb7\u683c\u5c14\u6839\u672c\u672c\u4f53\u8bba\u7684IT\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b58\u5728\u8bed\u8a00\u548c\u8303\u7574\u8bed\u8a00\u5206\u6790\u7528\u6237\u67e5\u8be2\uff0c\u63d0\u4f9b\u66f4\u6df1\u5c42\u6d1e\u89c1\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4ec5\u9650\u4e8e\u8303\u7574\u5206\u6790\uff0c\u65e0\u6cd5\u63ed\u793a\u672c\u4f53\u8bba\u6a21\u5f0f\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6d77\u5fb7\u683c\u5c14\u65b9\u6cd5\u89e3\u51b3IT\u4ea4\u4e92\u4e2d\u7684\u903b\u8f91\u9677\u9631\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e24\u4e2a\u6a21\u6001\u8bed\u8a00\uff0c\u901a\u8fc7\u73b0\u8c61\u5b66\u5f52\u7ea6\u6a21\u5757\u6865\u63a5\uff0c\u5206\u6790\u67e5\u8be2\u3001\u8bc6\u522b\u7ed3\u6784\uff0c\u5e76\u5305\u62ec\u67b6\u6784\u3001\u5b9e\u73b0\u548c\u7528\u4f8b\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u80fd\u8bc6\u522b\u9012\u5f52\u6a21\u5f0f\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6d1e\u89c1\uff0c\u5904\u7406IT\u9690\u55bb\uff0c\u5e76\u4e3a\u901a\u7528\u67e5\u8be2\u5de5\u5177\u94fa\u8def\u3002", "conclusion": "\u6b63\u5f0f\u5316\u5b58\u5728\u8bed\u8a00\u53ef\u5b9e\u73b0\u7cfb\u7edf\u5b8c\u6574\u6027\uff0c\u8bba\u6587\u8ba8\u8bba\u4e86\u4f18\u52bf\u548c\u9650\u5236\u3002"}}
{"id": "2504.12966", "pdf": "https://arxiv.org/pdf/2504.12966", "abs": "https://arxiv.org/abs/2504.12966", "authors": ["Yanmei Wang", "Xiyao Liu", "Fupeng Chu", "Zhi Han"], "title": "Vision and Language Integration for Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVLCA\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bed\u8a00\u7a7a\u95f4\u4f5c\u4e3a\u6865\u6881\u8fde\u63a5\u89c6\u89c9\u57df\uff0c\u5b9e\u73b0\u57df\u6cdb\u5316\uff0c\u901a\u8fc7\u5bf9\u9f50\u8bed\u4e49\u548c\u89c6\u89c9\u8868\u793a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u56fe\u50cf\u57df\u95f4\u5dee\u8ddd\uff0c\u96be\u4ee5\u627e\u5230\u53ef\u9760\u7684\u5171\u540c\u7279\u5f81\u7a7a\u95f4\uff0c\u800c\u8bed\u8a00\u5177\u6709\u66f4\u5168\u9762\u7684\u8bed\u4e49\u8868\u8fbe\u5143\u7d20\uff0c\u56e0\u6b64\u63d0\u51fa\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u6865\u6881\u3002", "method": "\u5728\u8bed\u8a00\u7a7a\u95f4\u5229\u7528\u8bcd\u5411\u91cf\u8ddd\u79bb\u6355\u83b7\u7c7b\u522b\u5173\u7cfb\uff1b\u5728\u89c6\u89c9\u7a7a\u95f4\u901a\u8fc7\u4f4e\u79e9\u903c\u8fd1\u63a2\u7d22\u5171\u540c\u6a21\u5f0f\uff1b\u6700\u7ec8\u901a\u8fc7\u591a\u6a21\u6001\u7a7a\u95f4\u5bf9\u9f50\u8bed\u8a00\u548c\u89c6\u89c9\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u8a00\u8bed\u4e49\u6865\u63a5\u89c6\u89c9\u57df\uff0c\u6539\u8fdb\u4e86\u57df\u6cdb\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2504.12982", "pdf": "https://arxiv.org/pdf/2504.12982", "abs": "https://arxiv.org/abs/2504.12982", "authors": ["Jiatai Wang", "Zhiwei Xu", "Di Jin", "Xuewen Yang", "Tao Li"], "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5206\u6790LLM\u5904\u7406\u77e5\u8bc6\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51faSwin-VIB\u6846\u67b6\u6765\u6539\u5584\u54cd\u5e94\u751f\u6210\uff0c\u5b9e\u9a8c\u663e\u793a\u5355\u9009\u4efb\u52a1\u51c6\u786e\u7387\u81f3\u5c11\u63d0\u53477.54%\u3002", "motivation": "LLM\u5728\u5185\u90e8\u8bb0\u5fc6\u548c\u5916\u90e8\u68c0\u7d22\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u77e5\u8bc6\u51b2\u7a81\uff0c\u5bfc\u81f4\u54cd\u5e94\u4e0d\u53ef\u9760\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u4ece\u4fe1\u606f\u8bba\u89c6\u89d2\u5206\u6790\u51b2\u7a81\uff0c\u5e76\u63d0\u51faSwin-VIB\u6846\u67b6\uff0c\u6574\u5408\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u6a21\u578b\u7528\u4e8e\u81ea\u9002\u5e94\u4fe1\u606f\u589e\u5f3a\u548c\u5f15\u5bfcLLM\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u5e76\u5728\u5355\u9009\u3001\u5f00\u653e\u5f0fQA\u548cRAG\u4efb\u52a1\u4e2d\u663e\u793aSwin-VIB\u7684\u6709\u6548\u6027\uff0c\u5355\u9009\u4efb\u52a1\u51c6\u786e\u7387\u81f3\u5c11\u63d0\u9ad87.54%\u3002", "conclusion": "Swin-VIB\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u77e5\u8bc6\u51b2\u7a81\uff0c\u63d0\u5347LLM\u5728\u54cd\u5e94\u751f\u6210\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.12989", "pdf": "https://arxiv.org/pdf/2504.12989", "abs": "https://arxiv.org/abs/2504.12989", "authors": ["Theshani Nuradha", "Mark M. Wilde"], "title": "Query Complexity of Classical and Quantum Channel Discrimination", "categories": ["quant-ph", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": "22 pages; see also the independent work \"Sampling complexity of\n  quantum channel discrimination\" DOI 10.1088/1572-9494/adcb9e", "summary": "Quantum channel discrimination has been studied from an information-theoretic\nperspective, wherein one is interested in the optimal decay rate of error\nprobabilities as a function of the number of unknown channel accesses. In this\npaper, we study the query complexity of quantum channel discrimination, wherein\nthe goal is to determine the minimum number of channel uses needed to reach a\ndesired error probability. To this end, we show that the query complexity of\nbinary channel discrimination depends logarithmically on the inverse error\nprobability and inversely on the negative logarithm of the (geometric and\nHolevo) channel fidelity. As a special case of these findings, we precisely\ncharacterize the query complexity of discriminating between two classical\nchannels. We also provide lower and upper bounds on the query complexity of\nbinary asymmetric channel discrimination and multiple quantum channel\ndiscrimination. For the former, the query complexity depends on the geometric\nR\\'enyi and Petz R\\'enyi channel divergences, while for the latter, it depends\non the negative logarithm of (geometric and Uhlmann) channel fidelity. For\nmultiple channel discrimination, the upper bound scales as the logarithm of the\nnumber of channels.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u91cf\u5b50\u901a\u9053\u9274\u522b\u7684\u67e5\u8be2\u590d\u6742\u5ea6\uff0c\u7126\u70b9\u662f\u8fbe\u5230\u671f\u671b\u9519\u8bef\u6982\u7387\u6240\u9700\u7684\u6700\u5c0f\u901a\u9053\u4f7f\u7528\u6b21\u6570\uff0c\u5e76\u63d0\u4f9b\u4e8c\u5143\u548c\u591a\u901a\u9053\u9274\u522b\u7684\u754c\u9650\u3002", "motivation": "\u52a8\u673a\u662f\u63a2\u8ba8\u67e5\u8be2\u590d\u6742\u5ea6\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u4fe1\u606f\u8bba\u89c6\u89d2\u4e0b\u9519\u8bef\u6982\u7387\u7684\u8870\u51cf\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bc1\u660e\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u9519\u8bef\u6982\u7387\u548c\u901a\u9053\u4fdd\u771f\u5ea6\u7684\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u7279\u5316\u5230\u7ecf\u5178\u901a\u9053\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e8c\u5143\u901a\u9053\u9274\u522b\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u5bf9\u9519\u8bef\u6982\u7387\u5012\u6570\u5bf9\u6570\u4f9d\u8d56\uff0c\u5e76\u4f9d\u8d56\u901a\u9053\u4fdd\u771f\u5ea6\u7684\u8d1f\u5bf9\u6570\uff1b\u591a\u901a\u9053\u9274\u522b\u4e0a\u754c\u968f\u901a\u9053\u6570\u5bf9\u6570\u589e\u957f\u3002", "conclusion": "\u7ed3\u8bba\u662f\u67e5\u8be2\u590d\u6742\u5ea6\u53ef\u901a\u8fc7\u901a\u9053\u53d1\u6563\u548c\u4fdd\u771f\u5ea6\u8868\u5f81\uff0c\u63d0\u9ad8\u4e86\u901a\u9053\u9274\u522b\u7684\u6548\u7387\u5206\u6790\u3002"}}
{"id": "2504.13044", "pdf": "https://arxiv.org/pdf/2504.13044", "abs": "https://arxiv.org/abs/2504.13044", "authors": ["Farhan Khodaee", "Rohola Zandie", "Yufan Xia", "Elazer R. Edelman"], "title": "The Dissipation Theory of Aging: A Quantitative Analysis Using a Cellular Aging Map", "categories": ["q-bio.QM", "cs.LG", "physics.bio-ph"], "comment": null, "summary": "We propose a new theory for aging based on dynamical systems and provide a\ndata-driven computational method to quantify the changes at the cellular level.\nWe use ergodic theory to decompose the dynamics of changes during aging and\nshow that aging is fundamentally a dissipative process within biological\nsystems, akin to dynamical systems where dissipation occurs due to\nnon-conservative forces. To quantify the dissipation dynamics, we employ a\ntransformer-based machine learning algorithm to analyze gene expression data,\nincorporating age as a token to assess how age-related dissipation is reflected\nin the embedding space. By evaluating the dynamics of gene and age embeddings,\nwe provide a cellular aging map (CAM) and identify patterns indicative of\ndivergence in gene embedding space, nonlinear transitions, and entropy\nvariations during aging for various tissues and cell types. Our results provide\na novel perspective on aging as a dissipative process and introduce a\ncomputational framework that enables measuring age-related changes with\nmolecular resolution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u8870\u8001\u65b0\u7406\u8bba\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u91cf\u5316\u7ec6\u80de\u53d8\u5316\uff0c\u5c06\u8870\u8001\u89c6\u4e3a\u8017\u6563\u8fc7\u7a0b\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u63d0\u51fa\u8870\u8001\u65b0\u7406\u8bba\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u91cf\u5316\u7ec6\u80de\u6c34\u5e73\u7684\u6539\u53d8\u3002", "method": "\u4f7f\u7528\u904d\u5386\u7406\u8bba\u5206\u89e3\u8870\u8001\u52a8\u6001\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u57fa\u56e0\u8868\u8fbe\u6570\u636e\uff0c\u5c06\u5e74\u9f84\u4f5c\u4e3a\u6807\u8bb0\uff0c\u521b\u5efa\u7ec6\u80de\u8870\u8001\u56fe\uff08CAM\uff09\u3002", "result": "\u7ed3\u679c\u63d0\u4f9b\u4e86\u7ec6\u80de\u8870\u8001\u56fe\uff0c\u8bc6\u522b\u4e86\u57fa\u56e0\u5d4c\u5165\u7a7a\u95f4\u7684\u53d1\u6563\u3001\u975e\u7ebf\u6027\u8f6c\u53d8\u548c\u71b5\u53d8\u5316\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7ec4\u7ec7\u548c\u7ec6\u80de\u7c7b\u578b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63d0\u4f9b\u8870\u8001\u4f5c\u4e3a\u8017\u6563\u8fc7\u7a0b\u7684\u65b0\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u8ba1\u7b97\u6846\u67b6\u4ee5\u5206\u5b50\u5206\u8fa8\u7387\u6d4b\u91cf\u5e74\u9f84\u76f8\u5173\u53d8\u5316\u3002"}}
{"id": "2504.12996", "pdf": "https://arxiv.org/pdf/2504.12996", "abs": "https://arxiv.org/abs/2504.12996", "authors": ["Saransh Agrawal", "Kuan-Hao Huang"], "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025", "summary": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76ee\u6807\u6027\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u5206\u6790\u548c\u5c42\u7ea7\u4f18\u5316\u9009\u62e9\u6027\u5730\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5173\u8054\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u4f1a\u8bb0\u5fc6\u654f\u611f\u4fe1\u606f\uff0c\u516c\u5f00\u90e8\u7f72\u65f6\u5b58\u5728\u98ce\u9669\uff0c\u800c\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u964d\u4f4e\u6574\u4f53\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u6027\u5730\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5173\u8054\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7ed3\u5408\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u548c\u5c42\u7ea7\u7279\u5b9a\u4f18\u5316\uff0c\u901a\u8fc7\u5728OLMo\u67b6\u6784\u4e0a\u7684\u56e0\u679c\u8ffd\u8e2a\u5b9e\u9a8c\u8bc6\u522b\u5173\u952e\u5c42\uff0c\u5e76\u4f7f\u7528\u7ea6\u675f\u4f18\u5316\u51bb\u7ed3\u4e0a\u5c42\uff0c\u5728\u4e0b\u5c42\u5e94\u7528\u8054\u5408\u635f\u5931\u51fd\u6570\u6700\u5927\u5316\u9057\u5fd8\u96c6\u635f\u5931\u5e76\u6700\u5c0f\u5316\u4fdd\u7559\u96c6\u504f\u5dee\u3002", "result": "\u57281B\u6a21\u578b\u8f68\u9053\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u4fdd\u630188%\u7684\u57fa\u7ebfMMLU\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u56e0\u679c\u4fe1\u606f\u5c42\u4f18\u5316\u5728LLM\u9057\u5fd8\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u786e\u7acb\u4e86\u57fa\u4e8e\u56e0\u679c\u7684\u5c42\u4f18\u5316\u4f5c\u4e3aLLM\u9057\u5fd8\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u4e3a\u89e3\u51b3AI\u6570\u636e\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2504.13061", "pdf": "https://arxiv.org/pdf/2504.13061", "abs": "https://arxiv.org/abs/2504.13061", "authors": ["Linkang Du", "Zheng Zhu", "Min Chen", "Zhou Su", "Shouling Ji", "Peng Cheng", "Jiming Chen", "Zhikun Zhang"], "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "To appear in the ACM Web Conference 2025, Sydney, Australia", "summary": "Text-to-image models based on diffusion processes, such as DALL-E, Stable\nDiffusion, and Midjourney, are capable of transforming texts into detailed\nimages and have widespread applications in art and design. As such, amateur\nusers can easily imitate professional-level paintings by collecting an artist's\nwork and fine-tuning the model, leading to concerns about artworks' copyright\ninfringement. To tackle these issues, previous studies either add visually\nimperceptible perturbation to the artwork to change its underlying styles\n(perturbation-based methods) or embed post-training detectable watermarks in\nthe artwork (watermark-based methods). However, when the artwork or the model\nhas been published online, i.e., modification to the original artwork or model\nretraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the\ntext-to-image generation model. The general idea of ArtistAuditor is to\nidentify if a suspicious model has been finetuned using the artworks of\nspecific artists by analyzing the features related to the style. Concretely,\nArtistAuditor employs a style extractor to obtain the multi-granularity style\nrepresentations and treats artworks as samplings of an artist's style. Then,\nArtistAuditor queries a trained discriminator to gain the auditing decisions.\nThe experimental results on six combinations of models and datasets show that\nArtistAuditor can achieve high AUC values (> 0.937). By studying\nArtistAuditor's transferability and core modules, we provide valuable insights\ninto the practical implementation. Finally, we demonstrate the effectiveness of\nArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor\nis open-sourced at https://github.com/Jozenn/ArtistAuditor.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faArtistAuditor\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u98ce\u683c\u7279\u5f81\u68c0\u6d4b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u662f\u5426\u4f7f\u7528\u7279\u5b9a\u827a\u672f\u5bb6\u7684\u4f5c\u54c1\u8fdb\u884c\u5fae\u8c03\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u827a\u672f\u4f5c\u54c1\u7248\u6743\u4fb5\u6743\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u4f5c\u54c1\u6216\u6a21\u578b\u5df2\u5728\u7ebf\u4e0a\u53d1\u5e03\uff0c\u65e0\u6cd5\u4fee\u6539\u65f6\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528\u98ce\u683c\u63d0\u53d6\u5668\u83b7\u53d6\u591a\u7c92\u5ea6\u98ce\u683c\u8868\u793a\uff0c\u5c06\u827a\u672f\u4f5c\u54c1\u89c6\u4e3a\u98ce\u683c\u91c7\u6837\uff0c\u5e76\u901a\u8fc7\u5224\u522b\u5668\u8fdb\u884c\u5ba1\u8ba1\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u516d\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u7ec4\u5408\u4e0aAUC\u503c\u8d85\u8fc70.937\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u8f6c\u79fb\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u7ed3\u8bba\u662fArtistAuditor\u6709\u6548\uff0c\u63d0\u4f9b\u5b9e\u9645insights\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13021", "pdf": "https://arxiv.org/pdf/2504.13021", "abs": "https://arxiv.org/abs/2504.13021", "authors": ["Petr Jahoda", "Jan Cech"], "title": "Pose and Facial Expression Transfer by using StyleGAN", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVWW 2024. Presented in Terme Olimia, Slovenia", "summary": "We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.", "AI": {"tldr": "\u4e00\u79cd\u4f7f\u7528StyleGAN2\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u9762\u90e8\u56fe\u50cf\u4e4b\u95f4\u8f6c\u79fb\u59ff\u52bf\u548c\u8868\u60c5\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u7684\u59ff\u52bf\u548c\u8868\u60c5\u8f6c\u79fb\uff0c\u4ece\u800c\u9ad8\u6548\u4e14\u53ef\u63a7\u5730\u5408\u6210\u9762\u90e8\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7f16\u7801\u5668\u548c\u6620\u5c04\u7f51\u7edc\uff0c\u5c06\u8f93\u5165\u6295\u5f71\u5230StyleGAN2\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u89c6\u9891\u5e8f\u5217\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u80fd\u591f\u5408\u6210\u968f\u673a\u8eab\u4efd\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u5b9e\u73b0\u53ef\u63a7\u59ff\u52bf\u548c\u8868\u60c5\uff0c\u5e76\u8fbe\u5230\u63a5\u8fd1\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u65e0\u9700\u76d1\u7763\u7684\u6709\u6548\u9762\u90e8\u64cd\u4f5c\uff0c\u5177\u6709\u5b9e\u9645\u56fe\u50cf\u751f\u6210\u5e94\u7528\u3002"}}
{"id": "2504.13110", "pdf": "https://arxiv.org/pdf/2504.13110", "abs": "https://arxiv.org/abs/2504.13110", "authors": ["Margalit Glasgow", "Denny Wu", "Joan Bruna"], "title": "Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time", "categories": ["stat.ML", "cs.LG"], "comment": "70 pages", "summary": "We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u9879\u5f0f\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u65e0\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5747\u503c\u573a\u52a8\u6001\u4e0b\u7684\u903c\u8fd1\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5fae\u5206\u65b9\u7a0b\u754c\u5b9a\u5176\u754c\u9650\u3002", "motivation": "\u52a8\u673a\u662f\u7406\u89e3\u6709\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u903c\u8fd1\u65e0\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u5728\u7279\u5f81\u5b66\u4e60\u95ee\u9898\u4e2d\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528\u5747\u503c\u573a\u52a8\u6001\u6cbb\u7406\u7684\u5fae\u5206\u65b9\u7a0b\uff0c\u7ed3\u5408\u5c40\u90e8Hessian\u548c\u81ea\u534f\u53d8\u6027\u5c5e\u6027\u6765\u754c\u5b9a\u903c\u8fd1\u5dee\u8ddd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5177\u6709\u81ea\u534f\u53d8\u6027\u7684\u5355\u7d22\u5f15\u6a21\u578b\u4e2d\uff0c\u591a\u9879\u5f0f\u6570\u91cf\u7684\u795e\u7ecf\u5143\u8db3\u4ee5\u7d27\u5bc6\u903c\u8fd1\u5747\u503c\u573a\u52a8\u6001\uff0c\u6536\u655b\u65f6\u95f4\u4e3a\u7ef4\u5ea6\u7684\u591a\u9879\u5f0f\u589e\u957f\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u6709\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u6709\u6548\u6a21\u62df\u65e0\u9650\u5bbd\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2504.13035", "pdf": "https://arxiv.org/pdf/2504.13035", "abs": "https://arxiv.org/abs/2504.13035", "authors": ["WonJun Moon", "Cheol-Ho Cho", "Woojin Jun", "Minho Shim", "Taeoh Kim", "Inwoong Lee", "Dongyoon Wee", "Jae-Pil Heo"], "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u539f\u578b\u6846\u67b6\u6765\u5e73\u8861\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u68c0\u7d22\u7cfb\u7edf\u4e2d\uff0c\u540c\u65f6\u5b9e\u73b0\u641c\u7d22\u51c6\u786e\u6027\u548c\u6548\u7387\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u5728\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u4e2d\uff0c\u52a0\u5165\u66f4\u591a\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u53ef\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u539f\u578b\u5316\u7684PRVR\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u4e2d\u7684\u591a\u6837\u4e0a\u4e0b\u6587\u7f16\u7801\u6210\u56fa\u5b9a\u6570\u91cf\u7684\u539f\u578b\uff1b\u5f15\u5165\u7b56\u7565\u589e\u5f3a\u6587\u672c\u5173\u8054\u548c\u89c6\u9891\u7406\u89e3\uff1b\u4f7f\u7528\u4ea4\u53c9\u6a21\u6001\u548c\u5355\u6a21\u6001\u91cd\u5efa\u4efb\u52a1\uff1b\u91c7\u7528\u89c6\u9891\u6df7\u5408\u6280\u672f\u3002", "result": "\u5728TVR\u3001ActivityNet-Captions\u548cQVHighlights\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u800c\u4e0d\u727a\u7272\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86PRVR\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13122", "pdf": "https://arxiv.org/pdf/2504.13122", "abs": "https://arxiv.org/abs/2504.13122", "authors": ["Haojian Huang", "Haodong Chen", "Shengqiong Wu", "Meng Luo", "Jinlan Fu", "Xinya Du", "Hanwang Zhang", "Hao Fei"], "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "categories": ["cs.CV", "cs.LG"], "comment": "Code and Data: https://github.com/HaroldChen19/VistaDPO", "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "AI": {"tldr": "VistaDPO \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u89c6\u9891\u8bed\u8a00\u504f\u597d\u5bf9\u9f50\uff0c\u51cf\u5c11\u5927\u578b\u89c6\u9891\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u9891\u6a21\u578b\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e0d\u4e00\u81f4\u4ee5\u53ca\u89c6\u9891\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u5f15\u5165 VistaDPO \u6846\u67b6\uff0c\u5728\u5b9e\u4f8b\u3001\u65f6\u95f4\u548c\u611f\u77e5\u7ea7\u522b\u4f18\u5316\u504f\u597d\u5bf9\u9f50\uff0c\u5e76\u6784\u5efa 7.2K \u7684 VistaDPO-7k \u6570\u636e\u96c6\u3002", "result": "\u5728\u89c6\u9891\u5e7b\u89c9\u3001\u95ee\u7b54\u548c\u5b57\u5e55\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u7f13\u89e3\u89c6\u9891\u8bed\u8a00\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "conclusion": "VistaDPO \u6709\u6548\u6539\u5584\u73b0\u6709\u5927\u578b\u89c6\u9891\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2504.13037", "pdf": "https://arxiv.org/pdf/2504.13037", "abs": "https://arxiv.org/abs/2504.13037", "authors": ["Yundi Zhang", "Paul Hager", "Che Liu", "Suprosanna Shit", "Chen Chen", "Daniel Rueckert", "Jiazhen Pan"], "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.", "AI": {"tldr": "ViTa \u6a21\u578b\u6574\u5408\u5fc3\u810f\u78c1\u5171\u632f\u6210\u50cf\u548c\u60a3\u8005\u7ea7\u56e0\u7d20\uff0c\u63d0\u4f9b\u5168\u9762\u5fc3\u810f\u5065\u5eb7\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u3002", "motivation": "\u6574\u5408 CMR \u548c\u60a3\u8005\u7ea7\u5065\u5eb7\u56e0\u7d20\u4ee5\u5168\u9762\u7406\u89e3\u5fc3\u810f\u5065\u5eb7\u548c\u75be\u75c5\u98ce\u9669\u3002", "method": "\u4f7f\u7528 42,000 \u4e2a UK Biobank \u53c2\u4e0e\u8005\u6570\u636e\uff0c\u878d\u5408 3D+T cine \u56fe\u50cf\u548c\u8868\u683c\u60a3\u8005\u56e0\u7d20\uff0c\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u73b0\u4e86\u5fc3\u810f\u8868\u578b\u9884\u6d4b\u3001\u5206\u5272\u548c\u75be\u75c5\u5206\u7c7b\u7b49\u4efb\u52a1\uff0c\u652f\u6301\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "\u63a8\u8fdb\u901a\u7528\u60a3\u8005\u7279\u5b9a\u5fc3\u810f\u5065\u5eb7\u7406\u89e3\uff0c\u63d0\u9ad8\u4e34\u5e8a\u6548\u7528\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2504.13125", "pdf": "https://arxiv.org/pdf/2504.13125", "abs": "https://arxiv.org/abs/2504.13125", "authors": ["Varun Rao", "Youran Sun", "Mahendra Kumar", "Tejas Mutneja", "Agastya Mukherjee", "Haizhao Yang"], "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5e76\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6570\u636e\u89c4\u6a21\u5b9a\u5f8b\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Open FinLLM Leaderboard\u4f5c\u4e3a\u57fa\u51c6\uff0c\u57fa\u4e8eQwen2.5\u548cDeepseek-R1\u6a21\u578b\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u7b49\u6280\u672f\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5404\u79cd\u91d1\u878d\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u6d4b\u91cf\u4e86\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u89c4\u6a21\u5b9a\u5f8b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13042", "pdf": "https://arxiv.org/pdf/2504.13042", "abs": "https://arxiv.org/abs/2504.13042", "authors": ["Dachun Kai", "Yueyi Zhang", "Jin Wang", "Zeyu Xiao", "Zhiwei Xiong", "Xiaoyan Sun"], "title": "Event-Enhanced Blurry Video Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/", "summary": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.", "AI": {"tldr": "\u6211\u4eec\u4f7f\u7528\u4e8b\u4ef6\u4fe1\u53f7\u63d0\u5347\u6a21\u7cca\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u63d0\u51faEv-DeblurVSR\u7f51\u7edc\uff0c\u8fbe\u5230\u65b0SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524dBVSR\u65b9\u6cd5\u56e0\u8fd0\u52a8\u4fe1\u606f\u4e0d\u8db3\u548c\u9ad8\u9891\u7ec6\u8282\u7f3a\u5931\uff0c\u65e0\u6cd5\u6062\u590d\u6e05\u6670\u7ec6\u8282\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u6296\u52a8\u3002", "method": "\u5f15\u5165\u4e8b\u4ef6\u4fe1\u53f7\uff0c\u63d0\u51fa\u4e92\u60e0\u7279\u5f81\u53bb\u6a21\u7cca\u6a21\u5757\u548c\u6df7\u5408\u53ef\u53d8\u5f62\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u4e8b\u4ef6\u548c\u5149\u6d41\u4fe1\u606f\u6539\u5584\u8fd0\u52a8\u4f30\u8ba1\u548c\u7279\u5f81\u5904\u7406\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u65b0SOTA\uff0c\u6bd4FMA-Net\u51c6\u786e\u5ea6\u9ad82.59 dB\uff0c\u901f\u5ea6\u5feb7.28\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e8b\u4ef6\u4fe1\u53f7\u80fd\u663e\u8457\u63d0\u5347BVSR\u6027\u80fd\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13129", "pdf": "https://arxiv.org/pdf/2504.13129", "abs": "https://arxiv.org/abs/2504.13129", "authors": ["Jialuo Li", "Wenhao Chai", "Xingyu Fu", "Haiyang Xu", "Saining Xie"], "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web", "summary": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u79d1\u5b66\u77e5\u8bc6\u6574\u5408\u5230\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u9ad8\u56fe\u50cf\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u6574\u5408\u79d1\u5b66\u77e5\u8bc6\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165Science-T2I\u6570\u636e\u96c6\u548cSciScore\u5956\u52b1\u6a21\u578b\uff08\u57fa\u4e8e\u589e\u5f3a\u7684CLIP\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u63a9\u7801\u5728\u7ebf\u5fae\u8c03\u3002", "result": "SciScore\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u6c34\u5e73\u76f8\u5f53\uff0c\u5e76\u67095%\u7684\u6539\u8fdb\uff1b\u5c06\u5fae\u8c03\u65b9\u6cd5\u5e94\u7528\u4e8eFLUX\uff0cSciScore\u5f97\u5206\u63d0\u5347\u8d85\u8fc750%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u7684\u79d1\u5b66\u771f\u5b9e\u6027\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.13048", "pdf": "https://arxiv.org/pdf/2504.13048", "abs": "https://arxiv.org/abs/2504.13048", "authors": ["Haosheng Xu", "Dongheng Qian", "Zhixuan Liu", "Yadong Jiang", "Jing Wang"], "title": "Design Topological Materials by Reinforcement Fine-Tuned Generative Model", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Topological insulators (TIs) and topological crystalline insulators (TCIs)\nare materials with unconventional electronic properties, making their discovery\nhighly valuable for practical applications. However, such materials,\nparticularly those with a full band gap, remain scarce. Given the limitations\nof traditional approaches that scan known materials for candidates, we focus on\nthe generation of new topological materials through a generative model.\nSpecifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained\ngenerative model, thereby aligning the model's objectives with our material\ndesign goals. We demonstrate that ReFT is effective in enhancing the model's\nability to generate TIs and TCIs, with minimal compromise on the stability of\nthe generated materials. Using the fine-tuned model, we successfully identify a\nlarge number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a\nrepresentative example--a TI with a full band gap of 0.26 eV, ranking among the\nlargest known in this category.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u5f3a\u5316\u5fae\u8c03\u7684\u751f\u6210\u6a21\u578b\u751f\u6210\u65b0\u7684\u62d3\u6251\u7edd\u7f18\u4f53\u548c\u62d3\u6251\u6676\u4f53\u7edd\u7f18\u4f53\uff0c\u6210\u529f\u53d1\u73b0\u5982Ge\u2082Bi\u2082O\u2086\u8fd9\u6837\u7684\u65b0\u6750\u6599\u3002", "motivation": "\u62d3\u6251\u7edd\u7f18\u4f53\u548c\u62d3\u6251\u6676\u4f53\u7edd\u7f18\u4f53\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5177\u6709\u5b8c\u5168\u5e26\u9699\u7684\u6750\u6599\u7a00\u5c11\uff0c\u4f20\u7edf\u65b9\u6cd5\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5f00\u53d1\u65b0\u6750\u6599\u3002", "method": "\u5e94\u7528\u5f3a\u5316\u5fae\u8c03\uff08ReFT\uff09\u5bf9\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u4f7f\u6a21\u578b\u76ee\u6807\u4e0e\u6750\u6599\u8bbe\u8ba1\u76ee\u6807\u4e00\u81f4\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u751f\u6210\u5927\u91cf\u65b0\u62d3\u6251\u6750\u6599\uff0cGe\u2082Bi\u2082O\u2086\u4e3a\u4f8b\uff0c\u5177\u67090.26 eV\u7684\u5b8c\u5168\u5e26\u9699\uff0c\u662f\u5df2\u77e5\u6700\u5927\u5e26\u9699\u4e4b\u4e00\u3002", "conclusion": "ReFT\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u6750\u6599\u7a33\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u751f\u6210\u62d3\u6251\u6750\u6599\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134", "abs": "https://arxiv.org/abs/2504.13134", "authors": ["Anamika Lochab", "Ruqi Zhang"], "title": "Energy-Based Reward Models for Robust Language Model Alignment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f15\u5165Energy-Based Reward Model (EBRM)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u6846\u67b6\uff0c\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u504f\u597d\u548c\u6cdb\u5316\u5230\u672a\u89c1\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u4ee5\u5904\u7406\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5956\u52b1\u5206\u5e03\u3001\u51b2\u7a81\u611f\u77e5\u6570\u636e\u8fc7\u6ee4\u3001\u6807\u7b7e\u566a\u58f0\u611f\u77e5\u5bf9\u6bd4\u8bad\u7ec3\u548c\u6df7\u5408\u521d\u59cb\u5316\uff0c\u5b9e\u73b0\u540e\u5904\u7406\u589e\u5f3a\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u5728\u5173\u952e\u5b89\u5168\u4efb\u52a1\u4e2d\u63d0\u5347\u9ad8\u8fbe5.97%\uff0c\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u8bc1\u660e\u6539\u5584\u4e86\u5bf9\u9f50\u8d28\u91cf\u5e76\u5ef6\u8fdf\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002", "conclusion": "\u8bc1\u660eEBRM\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\u548c\u5bf9\u9f50\u7ba1\u9053\u3002"}}
{"id": "2504.13054", "pdf": "https://arxiv.org/pdf/2504.13054", "abs": "https://arxiv.org/abs/2504.13054", "authors": ["Yichao Feng", "Shuai Zhao", "Yueqiu Li", "Luwei Xiao", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6Self-Aspect Retrieval Enhanced Summary Generation\uff0c\u901a\u8fc7\u5d4c\u5165\u9a71\u52a8\u68c0\u7d22\u673a\u5236\u4f18\u5316\u65b9\u9762-based \u6458\u8981\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684token\u9650\u5236\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u65b9\u9762-based \u6458\u8981\u9762\u4e34\u4f20\u7edf\u65b9\u6cd5\u8d44\u6e90\u53d7\u9650\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\uff0c\u5e76\u5b58\u5728token\u9650\u5236\u548c\u5e7b\u89c9\u6311\u6218\u3002", "method": "\u63d0\u51faSelf-Aspect Retrieval Enhanced Summary Generation\u6846\u67b6\uff0c\u4f7f\u7528\u5d4c\u5165\u9a71\u52a8\u68c0\u7d22\u673a\u5236\u63d0\u53d6\u76f8\u5173\u6587\u672c\u6bb5\u843d\uff0c\u5220\u9664\u65e0\u5173\u90e8\u5206\uff0c\u786e\u4fdd\u57fa\u4e8e\u7ed9\u5b9a\u65b9\u9762\u7684\u8f93\u51fa\u4ee5\u4f18\u5316token\u4f7f\u7528\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u6709\u6548\u7f13\u89e3token\u9650\u5236\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u65b9\u9762-based \u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f mitigated \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6458\u8981\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.13139", "pdf": "https://arxiv.org/pdf/2504.13139", "abs": "https://arxiv.org/abs/2504.13139", "authors": ["Jo\u00e3o Loula", "Benjamin LeBrun", "Li Du", "Ben Lipkin", "Clemente Pasti", "Gabriel Grand", "Tianyu Liu", "Yahya Emara", "Marjorie Freedman", "Jason Eisner", "Ryan Cotterel", "Vikash Mansinghka", "Alexander K. Lew", "Tim Vieira", "Timothy J. O'Donnell"], "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, 4 figures", "summary": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "AI": {"tldr": "This paper develops a sequential Monte Carlo (SMC)-based architecture for controlled language model generation, allowing small models to outperform larger ones in constrained tasks.", "motivation": "Many language model applications require text generation with syntactic or semantic constraints, but exact methods are intractable, necessitating more efficient approaches.", "method": "Proposes an SMC framework that incorporates constraints at inference time and builds on Lew et al. (2023) for flexible and efficient generation.", "result": "Small open-source models outperform models over 8x larger and closed-source ones in domains like code generation and text-to-SQL, due to better posterior distribution approximation.", "conclusion": "Offers a simple, programmable system for applying SMC to various controlled generation problems, enhancing performance and usability."}}
{"id": "2504.13059", "pdf": "https://arxiv.org/pdf/2504.13059", "abs": "https://arxiv.org/abs/2504.13059", "authors": ["Yao Mu", "Tianxing Chen", "Zanxin Chen", "Shijia Peng", "Zhiqian Lan", "Zeyu Gao", "Zhixuan Liang", "Qiaojun Yu", "Yude Zou", "Mingkun Xu", "Lunkai Lin", "Zhiqiang Xie", "Mingyu Ding", "Ping Luo"], "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/", "summary": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.", "AI": {"tldr": "RoboTwin\u6846\u67b6\u4f7f\u7528\u751f\u6210\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u57fa\u51c6\uff0c\u63d0\u9ad8\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u4e2d\uff0c\u53cc\u81c2\u534f\u8c03\u548c\u590d\u6742\u7269\u4f53\u64cd\u4f5c\u80fd\u529b\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u591a\u6837\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165RoboTwin\u6846\u67b6\uff0c\u5229\u75283D\u751f\u6210\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece2D\u56fe\u50cf\u521b\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u751f\u6210\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u5e76\u63d0\u4f9b\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u3002", "result": "\u5728COBOT Magic Robot\u5e73\u53f0\u9a8c\u8bc1\uff0c\u57fa\u4e8eRoboTwin\u6570\u636e\u8bad\u7ec3\u7684\u653f\u7b56\u6210\u529f\u7387\u63d0\u9ad8\uff1a\u5355\u81c2\u4efb\u52a1\u8d85\u8fc770%\uff0c\u53cc\u81c2\u4efb\u52a1\u8d85\u8fc740%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6539\u5584\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u589e\u5f3a\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13180", "pdf": "https://arxiv.org/pdf/2504.13180", "abs": "https://arxiv.org/abs/2504.13180", "authors": ["Jang Hyun Cho", "Andrea Madotto", "Effrosyni Mavroudi", "Triantafyllos Afouras", "Tushar Nagarajan", "Muhammad Maaz", "Yale Song", "Tengyu Ma", "Shuming Hu", "Suyog Jain", "Miguel Martin", "Huiyu Wang", "Hanoona Rasheed", "Peize Sun", "Po-Yao Huang", "Daniel Bolya", "Nikhila Ravi", "Shashank Jain", "Tammy Stark", "Shane Moon", "Babak Damavandi", "Vivian Lee", "Andrew Westbury", "Salman Khan", "Philipp Kr\u00e4henb\u00fchl", "Piotr Doll\u00e1r", "Lorenzo Torresani", "Kristen Grauman", "Christoph Feichtenhofer"], "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Technical report", "summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u611f\u77e5\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u53d1\u5e03\u65b0\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4fc3\u8fdb\u900f\u660e\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u5c01\u95ed\u6e90\u4ee3\u7801\u6a21\u578b\u7684\u900f\u660e\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u4f7f\u7528\u9ed1\u7bb1\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u5bf9\u79d1\u5b66\u8fdb\u6b65\u7684\u963b\u788d\uff0c\u4fc3\u8fdb\u53ef\u8861\u91cf\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u5206\u6790\u65e0\u79c1\u6709\u6a21\u578b\u84b8\u998f\u7684\u6807\u51c6\u8bad\u7ec3\u7ba1\u9053\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bc6\u522b\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5dee\u8ddd\uff1b\u53d1\u5e03280\u4e07\u4eba\u7c7b\u6807\u6ce8\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u95ee\u7b54\u5bf9\u548c\u65f6\u7a7a\u5b9a\u4f4d\u5b57\u5e55\uff1b\u5f15\u5165PLM-VideoBench\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u53d1\u5e03\u4e86280\u4e07\u4eba\u7c7b\u6807\u6ce8\u5b9e\u4f8b\u3001PLM-VideoBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u6570\u636e\u3001\u8bad\u7ec3\u914d\u65b9\u3001\u4ee3\u7801\u548c\u6a21\u578b\u5168\u90e8\u53ef\u590d\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u548c\u53ef\u590d\u73b0\u7684\u6846\u67b6\u63a8\u8fdb\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u7814\u7a76\uff0c\u586b\u8865\u6570\u636e\u548c\u65b9\u6cd5\u4e0a\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u900f\u660e\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068", "abs": "https://arxiv.org/abs/2504.13068", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u4e0e\u4e13\u5bb6\u4e00\u81f4\u6027\u5728\u5d29\u6e83\u53d9\u8ff0\u5206\u7c7b\u4e2d\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u9ad8\u51c6\u786e\u7387\u6a21\u578b\u53ef\u80fd\u4e0e\u4e13\u5bb6\u610f\u89c1\u4e0d\u4e00\u81f4\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u793a\u51fa\u66f4\u597d\u7684\u4e13\u5bb6\u4e00\u81f4\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u5728\u5b89\u5168\u5173\u952e\u7684NLP\u5e94\u7528\u4e2d\uff0c\u4ec5\u4f7f\u7528\u51c6\u786e\u7387\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u6a21\u578b\u51c6\u786e\u7387\u4e0e\u4e13\u5bb6\u4e00\u81f4\u6027\u7684\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bc4\u4f30\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982BERT\u53d8\u4f53\u3001USE\u548c\u96f6-shot\u5206\u7c7b\u5668\uff09\u548c\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\u3001LLaMA 3\u3001Qwen\u3001Claude\uff09\uff0c\u5e76\u4f7f\u7528Cohen's Kappa\u3001PCA\u548cSHAP\u6280\u672f\u91cf\u5316\u6a21\u578b\u4e0e\u4e13\u5bb6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u51c6\u786e\u7387\u6a21\u578b\u5f80\u5f80\u4e0e\u4e13\u5bb6\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c3d\u7ba1\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u5374\u6709\u66f4\u9ad8\u7684\u4e13\u5bb6\u4e00\u81f4\u6027\uff1b\u4e13\u5bb6\u4e00\u81f4\u7684\u6a21\u578b\u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u8bed\u8a00\u7ebf\u7d22\uff0c\u800c\u975e\u4f4d\u7f6e\u7279\u5b9a\u5173\u952e\u8bcd\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6a21\u578b\uff0c\u5e94\u5c06\u4e13\u5bb6\u4e00\u81f4\u6027\u4f5c\u4e3a\u8865\u5145\u6307\u6807\uff0c\u5e76\u6307\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5d29\u6e83\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13078", "pdf": "https://arxiv.org/pdf/2504.13078", "abs": "https://arxiv.org/abs/2504.13078", "authors": ["Riza Velioglu", "Petra Bevandic", "Robin Chan", "Barbara Hammer"], "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/", "AI": {"tldr": "TryOffDiff \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a7f\u7740\u8005\u8eab\u4e0a\u63d0\u53d6\u6807\u51c6\u5316\u670d\u88c5\u56fe\u50cf\uff0c\u63d0\u9ad8\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u8bd5\u7a7f\u548c\u865a\u62df\u8131\u8863\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4eba\u5230\u4eba\u865a\u62df\u8bd5\u7a7f\u4e2d\u4e0d\u60f3\u8981\u7684\u5c5e\u6027\u8f6c\u79fb\u95ee\u9898\uff0c\u5982\u76ae\u80a4\u989c\u8272\u3002", "method": "\u63d0\u51faTryOffDiff\u6a21\u578b\uff0c\u57fa\u4e8e\u6f5c\u6269\u6563\u6846\u67b6\uff0c\u4f7f\u7528SigLIP\u56fe\u50cf\u6761\u4ef6\u548c\u7c7b\u522b\u7279\u5b9a\u5d4c\u5165\uff0c\u652f\u6301\u591a\u670d\u88c5\u63d0\u53d6\u3002", "result": "\u5728VITON-HD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728DressCode\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u670d\u88c5\u865a\u62df\u8131\u8863\uff0c\u5e76\u6539\u5584p2p-VTON\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u65f6\u5c1a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u51cf\u5c11\u5c5e\u6027\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.13079", "pdf": "https://arxiv.org/pdf/2504.13079", "abs": "https://arxiv.org/abs/2504.13079", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Retrieval-Augmented Generation with Conflicting Evidence", "categories": ["cs.CL", "cs.AI"], "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs", "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51faRAMDocs\u6570\u636e\u96c6\u548cMADAM-RAG\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406LLM\u4ee3\u7406\u5728RAG\u4e2d\u7684\u67e5\u8be2\u6b67\u4e49\u3001\u9519\u8bef\u4fe1\u606f\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5b64\u7acb\u5904\u7406\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u540c\u65f6\u8003\u8651\u591a\u4e2a\u56e0\u7d20\uff0c\u4ee5\u6a21\u62df\u66f4\u771f\u5b9e\u7684\u51b2\u7a81\u8bc1\u636e\u573a\u666f\u3002", "method": "\u63d0\u51faRAMDocs\u6570\u636e\u96c6\u6a21\u62df\u590d\u6742\u573a\u666f\uff0c\u5e76\u5f00\u53d1MADAM-RAG\u591a\u4ee3\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u8fa9\u8bba\u548c\u805a\u5408\u5668\u5904\u7406\u6b67\u4e49\u548c\u9519\u8bef\u4fe1\u606f\u3002", "result": "MADAM-RAG\u5728AmbigDocs\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u534711.40%\uff0c\u5728FaithEval\u4e0a\u63d0\u534715.80%\uff0c\u4f46RAMDocs\u5bf9\u57fa\u7ebf\u6a21\u578b\uff08\u5982Llama3.3-70B-Instruct\uff09\u5177\u6709\u6311\u6218\u6027\uff0c\u4ec5\u5f9732.60\u7684\u7cbe\u786e\u5339\u914d\u5206\u3002", "conclusion": "MADAM-RAG\u6709\u6548\u6539\u5584\u4e86\u6027\u80fd\uff0c\u4f46\u5206\u6790\u663e\u793a\u5728\u8bc1\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2504.13102", "pdf": "https://arxiv.org/pdf/2504.13102", "abs": "https://arxiv.org/abs/2504.13102", "authors": ["Wei Huang", "Shumeng Sun", "Junpeng Lu", "Zhenpeng Xu", "Zhengyang Xiu", "Hao Zhang"], "title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.", "AI": {"tldr": "\u63d0\u51faMT-BCA-CNN\u6a21\u578b\uff0c\u901a\u8fc7\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63d0\u9ad8\u5c11\u6837\u672c\u6c34\u4e0b\u58f0\u5b66\u76ee\u6807\u8bc6\u522b\u6027\u80fd\uff0c\u8fbe\u523097%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u58f0\u5b66\u76ee\u6807\u8bc6\u522b\u4e2d\u6837\u672c\u7a00\u7f3a\u548c\u73af\u5883\u5e72\u6270\u95ee\u9898\uff0c\u4fc3\u8fdb\u6d77\u6d0b\u591a\u6837\u6027\u4fdd\u62a4\u548c\u56fd\u9632\u5b89\u5168\u3002", "method": "\u6574\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6784\u5efa\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5668\u548c\u591a\u4efb\u52a1\u5206\u7c7b\u5668\uff0c\u52a8\u6001\u589e\u5f3a\u7279\u5f81\u5e76\u6291\u5236\u566a\u58f0\u3002", "result": "\u5728Watkins Marine Life\u6570\u636e\u96c627\u7c7b\u5c11\u6837\u672c\u573a\u666f\u4e0b\uff0c\u51c6\u786e\u738797%\u3001F1\u5206\u657095%\uff0c\u4f18\u4e8e\u4f20\u7edf\u548c\u5148\u8fdb\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u4e3a\u5c11\u6837\u672c\u6c34\u4e0b\u58f0\u5b66\u8bc6\u522b\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u6d77\u6d0b\u751f\u7269\u58f0\u5b66\u548c\u58f0\u5450\u4fe1\u53f7\u5904\u7406\u7814\u7a76\u3002"}}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120", "abs": "https://arxiv.org/abs/2504.13120", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7IEI\u6846\u67b6\u548cCreativeMashup\u6570\u636e\u96c6\u8bc4\u4f30VLMs\u7684\u7ec4\u5408\u521b\u9020\u529b\uff0c\u53d1\u73b0VLMs\u5728\u7406\u89e3\u4efb\u52a1\u4e2d\u8d85\u8fc7\u5e73\u5747\u4eba\u7c7b\uff0c\u4f46\u751f\u6210\u4efb\u52a1\u4e2d\u6846\u67b6\u53ef\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u63a2\u8ba8VLMs\u8f93\u51fa\u662f\u5426\u4e3a\u771f\u6b63\u7ec4\u5408\u521b\u9020\u529b\u800c\u975e\u6a21\u5f0f\u5339\u914d\uff0c\u53d7\u5230\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u3002", "method": "\u63d0\u51faIEI\u6846\u67b6\uff0c\u6784\u5efa666\u4e2a\u827a\u672f\u751f\u6210\u7684CreativeMashup\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u5b9e\u9a8c\u3002", "result": "VLMs\u5728\u7406\u89e3\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5e73\u5747\u4eba\u7c7b\u4f46\u4e0d\u5982\u4e13\u5bb6\uff1b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cIEI\u6846\u67b6\u663e\u8457\u63d0\u5347\u521b\u9020\u8d28\u91cf\u3002", "conclusion": "\u5efa\u7acb\u4e86\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u521b\u9020\u529b\u7684\u7406\u8bba\u57fa\u7840\u548c\u6539\u8fdb\u751f\u6210\u4efb\u52a1\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2504.13123", "pdf": "https://arxiv.org/pdf/2504.13123", "abs": "https://arxiv.org/abs/2504.13123", "authors": ["Xinsong Zhang", "Yarong Zeng", "Xinting Huang", "Hu Hu", "Runquan Xie", "Han Hu", "Zhanhui Kang"], "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c55\u793a\u4f4e\u5e7b\u89c9\u5408\u6210\u6807\u9898\u53ef\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6570\u636e\u66ff\u4ee3\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u8bc1\u660e\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u9971\u548c\uff0c\u9650\u5236\u4e86\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4f4e\u5e7b\u89c9\u5408\u6210\u6807\u9898\u7684\u7ba1\u9053\uff0c\u4f7f\u7528\u8fde\u7eedDPO\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u975e\u5e7b\u89c9\u6807\u9898\u7387\u4ece48.2%\u63d0\u9ad8\u523077.9%\uff1b\u572835\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u81f3\u5c116.2%\uff1bFID\u5206\u6570\u51cf\u5c1117.1\u548c13.3\uff1b\u53d1\u5e03Hunyuan-Recap100M\u6570\u636e\u96c6\u3002", "conclusion": "\u5408\u6210\u6807\u9898\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u66ff\u4ee3\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13128", "pdf": "https://arxiv.org/pdf/2504.13128", "abs": "https://arxiv.org/abs/2504.13128", "authors": ["Nandan Thakur", "Jimmy Lin", "Sam Havens", "Michael Carbin", "Omar Khattab", "Andrew Drozdov"], "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.", "AI": {"tldr": "FreshStack \u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u793e\u533a\u95ee\u9898\u548c\u7b54\u6848\u81ea\u52a8\u6784\u5efa\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u8bc4\u4f30\u57fa\u51c6\u3002\u5b83\u5305\u62ec\u8bed\u6599\u5e93\u6536\u96c6\u3001\u5173\u952e\u70b9\u751f\u6210\u548c\u68c0\u7d22\u652f\u6301\uff0c\u6784\u5efa\u4e86\u4e94\u4e2a\u6570\u636e\u96c6\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u4e3a\u4e86\u521b\u5efa\u73b0\u5b9e\u3001\u53ef\u6269\u5c55\u4e14\u672a\u53d7\u6c61\u67d3\u7684 IR \u548c RAG \u8bc4\u4f30\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u589e\u957f\u7684\u4e3b\u9898\u4e0a\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6846\u67b6\u6b65\u9aa4\u5305\u62ec\uff1a(1) \u81ea\u52a8\u4ece\u4ee3\u7801\u548c\u6280\u672f\u6587\u6863\u6536\u96c6\u8bed\u6599\u5e93\uff0c(2) \u4ece\u793e\u533a Q&A \u751f\u6210\u5173\u952e\u70b9\uff0c(3) \u4f7f\u7528\u878d\u5408\u68c0\u7d22\u6280\u672f\u548c\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u6587\u6863\u68c0\u7d22\uff0c\u5e76\u6784\u5efa\u4e94\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u663e\u8457\u4f4e\u4e8e\u7406\u60f3\u8868\u73b0\uff0c\u5728\u6240\u6709\u4e94\u4e2a\u4e3b\u9898\u4e0a\u90fd\u6709\u6539\u8fdb\u7a7a\u95f4\uff1brerankers \u5728\u4e24\u4e2a\u4e3b\u9898\u4e0a\u672a\u660e\u663e\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u5e0c\u671b\u4fc3\u8fdb\u672a\u6765\u6784\u5efa\u66f4\u771f\u5b9e\u3001\u53ef\u6269\u5c55 IR \u548c RAG \u57fa\u51c6\u7684\u5de5\u4f5c\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2504.13131", "pdf": "https://arxiv.org/pdf/2504.13131", "abs": "https://arxiv.org/abs/2504.13131", "authors": ["Xin Li", "Kun Yuan", "Bingchen Li", "Fengbin Guan", "Yizhen Shao", "Zihao Yu", "Xijun Wang", "Yiting Lu", "Wei Luo", "Suhang Yao", "Ming Sun", "Chao Zhou", "Zhibo Chen", "Radu Timofte", "Yabin Zhang", "Ao-Xiang Zhang", "Tianwu Zhi", "Jianzhao Liu", "Yang Li", "Jingwen Xu", "Yiting Liao", "Yushen Zuo", "Mingyang Wu", "Renjie Li", "Shengyun Zhong", "Zhengzhong Tu", "Yufan Liu", "Xiangguang Chen", "Zuowei Cao", "Minhao Tang", "Shan Liu", "Kexin Zhang", "Jingfen Xie", "Yan Wang", "Kai Chen", "Shijie Zhao", "Yunchen Zhang", "Xiangkai Xu", "Hong Gao", "Ji Shi", "Yiming Bao", "Xiugang Dong", "Xiangsheng Zhou", "Yaofeng Tu", "Ying Liang", "Yiwen Wang", "Xinning Chai", "Yuxuan Zhang", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song", "Wei Sun", "Kang Fu", "Linhan Cao", "Dandan Zhu", "Kaiwei Zhang", "Yucheng Zhu", "Zicheng Zhang", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Zhi Jin", "Jiawei Wu", "Wei Wang", "Wenjian Zhang", "Yuhai Lan", "Gaoxiong Yi", "Hengyuan Na", "Wang Luo", "Di Wu", "MingYin Bai", "Jiawang Du", "Zilong Lu", "Zhenyu Jiang", "Hui Zeng", "Ziguan Cui", "Zongliang Gan", "Guijin Tang", "Xinglin Xie", "Kehuan Song", "Xiaoqiang Lu", "Licheng Jiao", "Fang Liu", "Xu Liu", "Puhua Chen", "Ha Thu Nguyen", "Katrien De Moor", "Seyed Ali Amirshahi", "Mohamed-Chaker Larabi", "Qi Tang", "Linfeng He", "Zhiyong Gao", "Zixuan Gao", "Guohua Zhang", "Zhiye Huang", "Yi Deng", "Qingmiao Jiang", "Lu Chen", "Yi Yang", "Xi Liao", "Nourine Mohammed Nadir", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Meiqin Liu", "Chao Yao", "Yao Zhao"], "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages", "summary": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u56de\u987e\u4e86NTIRE 2025\u6311\u6218\uff0c\u805a\u7126\u4e8e\u77ed\u89c6\u9891UGC\u8d28\u91cf\u8bc4\u4f30\u548c\u589e\u5f3a\uff0c\u5305\u62ec\u9ad8\u6548VQA\u548c\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002", "motivation": "\u63a8\u52a8\u8f7b\u91cf\u7ea7VQA\u6a21\u578b\u53d1\u5c55\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u63d0\u5347Kwai\u548cTikTok\u7b49\u77ed\u89c6\u9891UGC\u5e73\u53f0\u7684\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u6311\u6218\u8bbe\u7f6e\u4e24\u4e2a\u8d5b\u9053\uff1a\u9ad8\u6548VQA\u6a21\u578b\u5f00\u53d1\u548cKwaiSR\u6570\u636e\u96c6\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u6570\u636e\u96c6\u5305\u62ec1800\u5bf9\u5408\u6210\u56fe\u50cf\u548c1900\u5f20\u771f\u5b9e\u56fe\u50cf\uff0c\u63098:1:1\u5206\u5272\u3002", "result": "\u5438\u5f15266\u540d\u53c2\u4e0e\u8005\uff0c\u6536\u523018\u4e2a\u6709\u6548\u63d0\u4ea4\uff0c\u4fc3\u8fdb\u77ed\u89c6\u9891UGC VQA\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u8fdb\u6b65\u3002", "conclusion": "\u6311\u6218\u6210\u529f\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7814\u7a76\uff0c\u5e76\u516c\u5f00\u8d44\u6e90\u4ee5\u4f9b\u8fdb\u4e00\u6b65\u4f7f\u7528\u3002"}}
{"id": "2504.13143", "pdf": "https://arxiv.org/pdf/2504.13143", "abs": "https://arxiv.org/abs/2504.13143", "authors": ["Siwei Yang", "Mude Hui", "Bingchen Zhao", "Yuyin Zhou", "Nataniel Ruiz", "Cihang Xie"], "title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit", "summary": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165Complex-Edit\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u5f31\u70b9\u3002", "motivation": "\u52a8\u673a\u662f\u7cfb\u7edf\u8bc4\u4f30\u6307\u4ee4\u590d\u6742\u6027\u5bf9\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8986\u76d6\u591a\u6837\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528GPT-4o\u751f\u6210\u539f\u5b50\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7'Chain-of-Edit'\u7ba1\u9053\u6574\u5408\u6307\u4ee4\uff0c\u5e76\u5f00\u53d1\u6307\u6807\u548cVLM-based\u81ea\u52a8\u8bc4\u4f30\u7ba1\u9053\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5f00\u6e90\u6a21\u578b\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u5dee\u8ddd\u6269\u5927\uff1b\u590d\u6742\u6307\u4ee4\u5f71\u54cd\u5143\u7d20\u4fdd\u7559\u548c\u7f8e\u5b66\uff1b\u9010\u6b65\u6267\u884c\u964d\u4f4e\u6027\u80fd\uff1bBest-of-N\u7b56\u7565\u6539\u5584\u7ed3\u679c\uff1b\u89c2\u5bdf\u5230\u5408\u6210\u6570\u636e\u8bc5\u5492\u3002", "conclusion": "\u7ed3\u8bba\u662f\u590d\u6742\u6307\u4ee4\u66b4\u9732\u6a21\u578b\u5c40\u9650\u6027\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u4e0d\u771f\u5b9e\uff0c\u5f3a\u8c03\u6539\u8fdb\u7f16\u8f91\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2504.13165", "pdf": "https://arxiv.org/pdf/2504.13165", "abs": "https://arxiv.org/abs/2504.13165", "authors": ["Anya Zorin", "Irmak Guzey", "Billy Yan", "Aadhithya Iyer", "Lisa Kondrich", "Nikhil X. Bhattasali", "Lerrel Pinto"], "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Website at https://ruka-hand.github.io/", "summary": "Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86RUKA\uff0c\u4e00\u6b3e\u7ecf\u6d4e\u5b9e\u60e0\u3001\u7d27\u51d1\u7684\u808c\u8171\u9a71\u52a8\u673a\u5668\u4eba\u624b\uff0c\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u65b9\u6cd5\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6293\u53d6\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u786c\u4ef6\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u7684\u9014\u5f84\u6765\u5904\u7406\u808c\u8171\u9a71\u52a8\u548c\u4f4e\u6210\u672c\u6750\u6599\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efaRUKA\uff0c\u4f7f\u75283D\u6253\u5370\u90e8\u4ef6\u548c\u73b0\u6210\u7ec4\u4ef6\uff1b\u4eceMANUS\u624b\u5957\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4e2d\u5b66\u4e60\u5173\u8282\u5230\u6267\u884c\u5668\u548c\u6307\u5c16\u5230\u6267\u884c\u5668\u7684\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793aRUKA\u5728\u53ef\u8fbe\u6027\u3001\u8010\u7528\u6027\u548c\u5f3a\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u4eba\u624b\uff1b\u9065\u64cd\u4f5c\u4efb\u52a1\u5c55\u793a\u4e86\u5176\u7075\u5de7\u8fd0\u52a8\u3002", "conclusion": "RUKA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u672c\u6709\u6548\u7684\u7075\u5de7\u673a\u5668\u4eba\u624b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
