<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 34]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.SY](#eess.SY) [Total: 12]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [math.OC](#math.OC) [Total: 2]
- [eess.SP](#eess.SP) [Total: 6]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 本论文提出智能场的理论框架，包括三个原则，并为AI应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 智能场的复杂性使得正式描述和实用应用困难。

Method: 提出完整配置、局部性、目的性原则，并探索AI视角的设计方法。

Result: 建立了初步理论框架，为未来研究和应用奠定基础。

Conclusion: 为理解和利用目标导向动态随机场提供了基础。

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>


### [2] [HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods](https://arxiv.org/abs/2504.16209)
*Paul Zaidins,Robert P. Goldman,Ugur Kuter,Dana Nau,Mark Roberts*

Main category: cs.AI

TL;DR: 本文比较了SHOPFixer、IPyHOPPER和Rewrite三种计划修复算法的理论和实证性能，强调算法差异在选择应用时的关键作用。


<details>
  <summary>Details</summary>
Motivation: 为了理解算法间定义、搜索空间和修复能力的差异，帮助选择适合的具体应用方法。

Method: 通过理论比较算法定义和实证评估基准规划问题。

Result: 理论结果显示算法在搜索空间和修复类型上的差异；实证结果揭示运行时性能和问题覆盖率的insights。

Conclusion: 理解这些区别对选择合适的修复方法至关重要。

Abstract: This paper provides theoretical and empirical comparisons of three recent
hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our
theoretical results show that the three algorithms correspond to three
different definitions of the plan repair problem, leading to differences in the
algorithms' search spaces, the repair problems they can solve, and the kinds of
repairs they can make. Understanding these distinctions is important when
choosing a repair method for any given application.
  Building on the theoretical results, we evaluate the algorithms empirically
in a series of benchmark planning problems. Our empirical results provide more
detailed insight into the runtime repair performance of these systems and the
coverage of the repair problems solved, based on algorithmic properties such as
replanning, chronological backtracking, and backjumping over plan trees.

</details>


### [3] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee,Tianqi Shang,Jae Young Baik,Duy Duong-Tran,Shu Yang,Lingyao Li,Li Shen*

Main category: cs.AI

TL;DR: 本研究调查LLMs在急诊分诊中的鲁棒性和偏差，发现LLMs鲁棒性强，但存在性别和种族交叉偏好。


<details>
  <summary>Details</summary>
Motivation: LLMs在临床决策支持中潜力大，但分诊应用未充分探索。

Method: 通过鲁棒性测试和偏差分析，评估多种LLM方法包括预训练和学习，以及机器学习方法。

Result: LLMs显示优越鲁棒性，并揭示特定性别和种族交叉点上的偏差偏好。

Conclusion: LLMs可能编码人口统计偏好，在特定临床情境中显现。

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>


### [4] [Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems](https://arxiv.org/abs/2504.16622)
*Christoforus Yoga Haryanto,Emily Lomempow*

Main category: cs.AI

TL;DR: 这篇论文提出一种假设的认知计算架构框架'认知硅'，针对2035年的发展，旨在解决AI系统中的基础限制。


<details>
  <summary>Details</summary>
Motivation: 揭示确定性人类编写的计算架构局限性，并探索认知计算系统的发展轨迹。

Method: 通过与大型语言模型的辩证式共同设计，在不对称认知条件下创建结构化摩擦以暴露盲点和权衡。

Result: 提出整合符号支架、治理内存和对齐感知执行的架构，理论上与自由能量原则一致，提供认知系统维护身份的机制。

Conclusion: 目标是构建道德上可处理的认知基础设施，通过硬件约束和身份机制保持人类对齐。

Abstract: Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.

</details>


### [5] [Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models](https://arxiv.org/abs/2504.16635)
*Fredy Pokou,Jules Sadefo Kamdem,François Benhmad*

Main category: cs.AI

TL;DR: 本研究提出结合GARCH和深度强化学习的混合框架，改进波动性市场中的VaR估计。


<details>
  <summary>Details</summary>
Motivation: 传统模型如GARCH假设过于刚性，无法适应复杂市场动态，因此需要更灵活的风险估计方法。

Method: 使用GARCH波动模型与DDQN深度强化学习相结合，将市场方向预测视为不平衡分类问题，实现风险水平动态调整。

Result: 在Eurostoxx 50数据上的实证验证显示，VaR估计准确性提升，违规次数减少，资本要求降低，并符合监管标准。

Conclusion: 模型能实时调整风险水平，提升现代风险管理的实用性。

Abstract: In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.

</details>


### [6] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: 本文引入IRIS系统，使用LLMs辅助科学假设生成，通过HITL方法提升透明度和可操控性，并通过用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在科学发现中的快速发展引发了加速发现的关键问题，特别是假设生成缺乏透明度和可操控性。

Method: IRIS系统采用MCTS的自适应计算扩展、细粒度反馈机制和基于查询的文献合成，以及HITL方法。

Result: 用户研究显示，IRIS系统有效地提升了研究人员的构想能力。

Conclusion: IRIS系统增强了研究人员的控制力和洞察力，并开源了代码。

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>


### [7] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang,Huacan Chai,Yuanyi Song,Siyuan Qi,Muning Wen,Ning Li,Junwei Liao,Haoyi Hu,Jianghao Lin,Gaowei Chang,Weiwen Liu,Ying Wen,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: 这篇论文概述LLM代理通信协议，分类分析性能，并探讨未来挑战。


<details>
  <summary>Details</summary>
Motivation: LLM快速发展，但缺乏标准通信协议，影响代理协作和扩展。

Method: 系统概述现有协议，分类为四类，并进行性能比较分析。

Result: 协议分类、性能分析（安全、可扩展性、延迟），以及未来挑战探讨。

Conclusion: 期望为研究者和工程师提供通信基础设施的实用参考。

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


### [8] [Lightweight Latent Verifiers for Efficient Meta-Generation Strategies](https://arxiv.org/abs/2504.16760)
*Bartosz Piotrowski,Witold Drzewakowski,Konrad Staniszewski,Piotr Miłoś*

Main category: cs.AI

TL;DR: 本文引入LiLaVe，一种轻量级验证器，利用LLM隐藏状态提取正确性信号，提高推理任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统验证器计算成本高，需要更高效的替代方案来支持LLM在推理密集型任务中的应用。

Method: 提出LiLaVe，通过分析基LLM隐藏状态获取正确性信号，并结合或设计新策略如条件自修正和条件多数投票。

Result: 显著提升了生成任务的准确性和效率，尤其适用于较小LLM，并减少了计算资源需求。

Conclusion: 证明了从LLM隐藏状态提取潜在信息的有益性，并为可扩展的推理应用提供资源高效解决方案。

Abstract: Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.

</details>


### [9] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor Gitman*

Main category: cs.AI

TL;DR: 这篇论文介绍了我们赢得AI数学奥林匹克进展奖2的获胜方案，通过构建大规模数据集、集成代码执行和生成解决方案选择来提升数学推理模型。


<details>
  <summary>Details</summary>
Motivation: 动机是开发先进的数学推理模型，以解决包括奥林匹克级难题在内的复杂数学问题。

Method: 方法包括三个支柱：创建54万高质量数学问题和320万长推理解决方案的数据集；开发一种通过迭代训练、生成和质量过滤将代码执行集成到长推理模型中的方法，产生170万高质量工具集成推理解决方案；创建一种训练模型从多个候选方案中选择最有前景方案的管道。

Result: 结果是训练了一系列模型，在数学推理基准测试中达到了最先进的结果，并开源了代码、模型和OpenMathReasoning数据集。

Conclusion: 结论是这种方法显著提高了数学推理性能，并通过发布资源促进进一步研究。

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang,Si-Yang Liu,Hao-Run Cai,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TL;DR: 这篇调查综述系统介绍了表格数据表示学习，包括背景、挑战、模型分类及优缺点。


<details>
  <summary>Details</summary>
Motivation: 表格数据在机器学习中广泛存在，DNNs显示出强大表示学习潜力，需要总结现有方法以推动发展。

Method: 将模型分为专业、可转移和通用类型，并基于特征、样本和目标建立分层分类，探讨集成和扩展策略。

Result: 总结了不同模型的优缺点，并讨论了开放环境、多模态学习和表格理解的应用。

Conclusion: 强调表格学习的重要性，提供资源仓库以支持进一步研究。

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [11] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng,Junhao Song,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Ming Liu*

Main category: cs.LG

TL;DR: 这篇论文概述了Active Learning（AL），一种在少量标注数据下提升机器学习性能的策略，涵盖概念、应用、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据丰富但标注稀缺的矛盾，以推进机器学习发展。

Method: 通过介绍AL基本概念、讨论在计算机视觉、NLP等领域的应用、焦点话题如不确定性估计和类不平衡处理。

Result: AL比被动学习表现更好，尤其在使用良好评价措施时；人类启发的方法可提升数据效率。

Conclusion: 为研究者和从业者提供洞见，提出未来方向，包括重建信任和确保可重复性。

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>


### [12] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman,Lav Varshney*

Main category: cs.LG

TL;DR: SparseJEPA improves JEPA by adding sparsity for better representations, demonstrated on CIFAR-100 and Vision Transformer with theoretical proof.


<details>
  <summary>Details</summary>
Motivation: To address interpretability and efficiency issues in JEPA's dense embeddings.

Method: Uses a penalty method to encourage semantic sharing in latent space, trained on CIFAR-100 and pre-trained Vision Transformer for transfer learning.

Result: Enhanced embeddings improve performance in classification and low-level tasks; theoretically proven to reduce Multiinformation.

Conclusion: Sparsity refines latent space for more interpretable representations; future work includes object-centric learning.

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>


### [13] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.LG

TL;DR: 这篇论文审查了基于过程的模型（PBMs）和深度学习（DL）在农业建模中的应用，介绍了混合框架的优势，并通过案例研究证明了混合模型的优越性。


<details>
  <summary>Details</summary>
Motivation: 整合PBMs和DL的优势，以克服各自的局限性，如PBMs的扩展性和DL的可解释性问题。

Method: 系统审查PBMs、DL和混合框架，分类混合方法，并进行作物干生物量预测的案例研究，比较不同模型的表现。

Result: 混合模型在不同数据质量、样本大小和空间条件下表现出色，优于传统PBMs和DL模型，具有更好的鲁棒性和泛化能力。

Conclusion: 讨论了模型的可解释性、可扩展性和数据需求等挑战，并提供了推进混合建模的建议，以支持可持续农业。

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>


### [14] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang,Yaoyao Ding,Yang Hu,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute 是一种新的基于 tile 的编程语言，用于优化 GPU 上的深度学习混合类型运算符，实现了显著速度提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习量化技术需要新的混合数据类型矩阵乘法运算符，导致 GPU 优化复杂化，现有的编译器要么缺乏表达性，要么需要大量编程努力。

Method: 提出 Hexcute 语言，暴露共享内存和寄存器抽象，支持细粒度优化，并通过任务映射和基于类型推断的算法自动合成布局和映射。

Result: 对多种深度学习运算符通用，比现有编译器快 1.7-11.28 倍，在端到端评估中快达 2.91 倍。

Conclusion: Hexcute 平衡了表达性和工程努力，显著提高了深度学习运算符的优化性能。

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [15] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz,Johannes Schneider*

Main category: cs.LG

TL;DR: 这篇论文比较了在语音到语音翻译中使用文本表示和音素表示，发现音素方法质量相当但具有资源需求更低和更适合低资源语言的优势。


<details>
  <summary>Details</summary>
Motivation: 探索使用音素作为文本表示，以替代传统的文本表示，动机是为多语言同时语音到语音翻译提供更好的低资源语言支持。

Method: 在WMT17数据集上训练了一个开源的序列到序列模型，使用标准文本表示和音素表示两种格式，并使用BLEU指标评估性能。

Result: 结果显示音素方法提供了可比的质量，但具有更低的资源需求和更好的低资源语言适用性。

Conclusion: 结论是音素表示是一种有前景的替代方案，提供实际优势。

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>


### [16] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*Léandre Eberhard,Nirek Sharma,Filipp Shelobolin,Aalok Ganesh Shanbhag*

Main category: cs.LG

TL;DR: 本篇论文提出一种新的后处理公平性调整框架，用于机器学习任务，确保公平性同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习影响信贷、公共政策和人才招聘等关键领域，确保公平性是法律和伦理必要。

Method: 框架将内部处理技术适应为后处理步骤，脱离训练过程，提供灵活性，如无需自定义损失函数、支持不同数据集和黑箱模型。

Result: 与对抗性去偏置方法比较，在真实数据集上实现相似的公平性/准确性权衡。

Conclusion: 框架优势包括消除自定义损失函数需求、启用不同数据集公平调整、适应黑箱模型并提供可解释洞察。

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [17] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad,Mithilesh Kumar Singh,Anthony J. Ripa,Klaus Mueller*

Main category: cs.LG

TL;DR: 本论文介绍了一个名为FairPlay的网络应用软件，允许多方利益相关者协作去偏置数据集，从而在没有统一公平理论的情况下达成共识。用户研究显示，用户可在约五轮内达成一致。


<details>
  <summary>Details</summary>
Motivation: 公平性在决策中至关重要，尤其是在利益相关者对公平有不同和相互冲突的需求时。缺乏系统化的协商过程使得达成共识非常困难。

Method: 开发了一个网络-based软件应用FairPlay，允许用户协商和协作去偏置数据集。

Result: 用户研究表明，用户可以在大约五轮游戏中达成共识，展示了该应用的成功。

Conclusion: FairPlay有潜力提升AI系统中的公平性。

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>


### [18] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo,Michelle Adeline,Julia Kaiwen Lau,Fang Yu Leong,Hwa Hui Tew,Arghya Pal,Vishnu Monn Baskaran,Chee-Ming Ting,Raphaël C. -W. Phan*

Main category: cs.LG

TL;DR: 本文提出了一种名为VPFB的能量-based生成框架，消除了MCMC采样的需求，提高了效率和可解释性，在多种生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 能量-based模型灵活且可解释，但训练不稳定且计算成本高，潜在流与显式EBM的关系尚未充分探索。

Method: 提出VPFB，通过构建流驱动的密度同伦，并通过最小化KL散度的变分损失来匹配数据分布，无需隐式MCMC采样或辅助网络。

Result: 实验结果显示VPFB在图像生成、插值、异常检测和组合生成任务中，与现有方法竞争力相当。

Conclusion: VPFB实现了鲁棒、高效的生成建模，同时保留了EBM的可解释性。

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>


### [19] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding,Nathan Steffen,Kelly Cohen*

Main category: cs.LG

TL;DR: 本文通过基准测试比较梯度优化模糊推理系统（GF）与其他机器学习模型，展示了其在准确性和效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊推理系统依赖无导数优化方法的局限性，探索GF作为高效可解释模型的潜力。

Method: 在五个UCI数据集上评估GF模型，使用梯度下降优化，与随机森林、XGBoost等模型比较。

Result: GF模型在分类准确率、精确度和训练时间上表现出色，具有跨数据集的鲁棒性。

Conclusion: 梯度优化模糊系统可作为可解释、高效的替代方案，适用于监督学习任务。

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>


### [20] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: 这篇论文使用Opposition-Based Learning (OBL) 生成合成样本，提升传统分类算法性能，实验证明准确性和效率均有显著改善。


<details>
  <summary>Details</summary>
Motivation: 提升分类算法性能，尤其在复杂或稀疏数据环境中，通过生成相反样本改善决策边界。

Method: 引入OBL框架，包括Global OBL、Class-Wise OBL和Localized Class-Wise OBL，与KNN、SVM、LR、DT等分类器集成。

Result: 实验在26个数据集上显示，OBL增强分类器在准确性和F1-score上优于标准版本，计算效率特别是SVM和LR得到改善。

Conclusion: OBL作为轻量级策略，具有提升分类性能的潜力，尤其适用于复杂学习环境。

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>


### [21] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo,Ian Yang,Vipul Raheja,Mingyi Hong,Kwang-Sung Jun,Dongyeop Kang*

Main category: cs.LG

TL;DR: 这篇论文通过使用基于可解释性方法的令牌级奖励整形来改善大型语言模型的RLHF，提高性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前RLHF管道中标量奖励导致的反馈稀疏和信用分配次优问题。

Method: 提出使用SHAP和LIME的奖励整形函数，并采用双层优化框架结合贝叶斯优化和策略训练来学习参数。

Result: 实验显示更好的令牌级奖励归因提升下游任务性能，训练更快，并理论证明方法保持最优策略。

Conclusion: 令牌级奖励整形改善RLHF，具有经验和理论支持。

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>


### [22] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born,Filip Skogh,Kahn Rhrissorrakrai,Filippo Utro,Nico Wagner,Aleksandros Sobczyk*

Main category: cs.LG

TL;DR: 这篇论文提出了一种混合经典-量子双重随机Transformer（QDSFormer），通过变分量子电路替换Softmax，以改善视觉任务的性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中Softmax导致的训练不稳定问题，并利用量子电路提供新颖的双重随机矩阵归一化方法来提升模型性能。

Method: 在自注意力层中使用变分量子电路强制注意力矩阵成为双重随机矩阵，并与Sinkhorn算法和基于QR分解的量子启发式Transformer进行比较。

Result: QDSFormer在小规模物体识别任务中优于标准Vision Transformer和其他双重随机Transformer，显示出更好的训练稳定性和较低的性能变异。

Conclusion: QDSFormer可能缓解Vision Transformer在小规模数据上的训练不稳定性，并提供潜在性能提升。

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [23] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana,Moeumu Uili,James Atherton,Mark O'Brien,Joe Wood,Leandra Brickson*

Main category: cs.LG

TL;DR: 本论文提出了一种针对稀有鸟类的自动一击式鸟鸣分类管道，使用嵌入空间和余弦相似度，仅需少量训练数据，并在濒危鸟类上实现了高精度检测。


<details>
  <summary>Details</summary>
Motivation: 现有鸟类分类模型无法有效处理仅有1-3个录音的稀有物种，这对监测濒危鸟类至关重要。

Method: 利用大型鸟类网络的嵌入空间，通过余弦相似度分类，结合预处理过滤和去噪技术，并在模拟和真实场景中使用聚类指标评估。

Result: 在牙喙鸽的真实测试中，召回率达到1.0，准确率达到0.95。

Conclusion: 该开源系统为保护者提供了实用的工具，用于检测和监测濒临灭绝的稀有物种。

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [24] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund,Alaa Maalouf,Levi Cai,Daniel Yang,Tsun-Hsuan Wang,Abigail O'Neil,Timm Haucke,Sandeep Mukherjee,Vikram Ramaswamy,Judy Hansen Shen,Gabriel Tseng,Mike Walmsley,Daniela Rus,Ken Goldberg,Hannah Kerner,Irene Chen,Yogesh Girdhar,Sara Beery*

Main category: cs.LG

TL;DR: 本论文提出DS3问题和DataS^3基准，显示针对部署特定数据集子集选择可显著提升ML模型性能。


<details>
  <summary>Details</summary>
Motivation: 实际ML应用中，模型需在特定部署（如医院或公园）表现良好，但数据分布差异导致性能不佳，因此需要选择部署专用训练数据子集。

Method: 引入DataS^3数据集和基准，评估coresets、data filtering和data curation等算法。

Result: 通用方法失败，手动curation子集可提升准确率高达51.3%。

Conclusion: 强调定制数据集curation在提升部署性能和效率中的关键作用，随着公共数据集增多和模型部署，将更重要。

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>


### [25] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain,Amrit Romana,Vikramjit Mitra,Colin Lea,Shirley Ren*

Main category: cs.LG

TL;DR: 语音异常会影响情感识别模型的表现，通过微调模型可提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语音和声音条件可能改变语音属性，影响对异常语音的情感模型性能，因此需要评估其影响。

Method: 在异常语音数据集上评估情感识别模型，比较典型语音数据集，调查语音可懂度、单调性和粗糙度维度，并通过伪标签数据微调模型。

Result: 模型输出受异常语音影响显著，例如对异常语音预测'悲伤'比例更高；微调后，异常语音性能提升，不影响典型语音。

Conclusion: 强调需要更广泛的训练和评估数据集，以及对语音差异更鲁棒的建模方法。

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>


### [26] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: 这篇论文反思余弦相似度的演变、优势和局限性，并讨论新兴替代方案。


<details>
  <summary>Details</summary>
Motivation: 动机是解决余弦相似度在嵌入范数携带语义信息时的局限性，并为定量科学家提供概念和实用视角。

Method: 方法是非正式的选择性考察，包括回顾演变、分析优势和局限性，以及探索新兴替代方案。

Result: 结果突出了余弦相似度在许多场景中的良好性能、其失效点，以及新兴方法如何开始解决其问题。

Conclusion: 结论是希望通过混合概念清晰和实用视角，帮助科学家更好地理解嵌入。

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [27] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang,Tao Wu,Xingping Xian,Shaojie Qiao,Weina Niu,Canyixing Cui*

Main category: cs.LG

TL;DR: 本文提出GOMKCN方法，通过改进图结构表示提升图神经网络的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法隐式粗糙表征图结构，限制模式分析，需要更精细的解耦合学习。

Method: GOMKCN将图视为节点中心子图，使用GOMK卷积操作符在Hilbert空间计算相似度，并通过梯度下降优化过滤器。

Result: 实验显示GOMKCN在图模式挖掘和预测中实现更高准确性和可解释性。

Conclusion: 框架推进解耦合图表示学习的理论基础。

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>


### [28] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali,Eshika Pathak,Pranay Sharma,Guannan Qu,Gauri Joshi*

Main category: cs.LG

TL;DR: This paper proposes the first model-free policy-based algorithm for non-stationary reinforcement learning and provides a dynamic regret bound.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of policy-based methods in non-stationary RL.

Method: Proposes NS-NAC, a policy gradient method with restart-based exploration, and BORL-NS-NAC, a bandit-over-RL based parameter-free algorithm.

Result: Achieves a dynamic regret of \tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6}).

Conclusion: The regret analysis adapts Lyapunov function methods to handle dynamic environments and simultaneous updates.

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>


### [29] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas,Logan Engstrom*

Main category: cs.LG

TL;DR: 论文提出了一种新方法MAGIC，用于在非凸机器学习设置中更好地估计训练数据对模型预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非凸设置中效果较差，而凸设置中容易，因此需要改进数据归因估计。

Method: 结合经典方法和元微分来优化估计训练数据的添加或移除对预测的影响。

Result: MAGIC方法能近似最优地估计数据影响，提高了准确性。

Conclusion: 该方法显著改进了非凸设置下的预测数据归因性能。

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>


### [30] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang,Shuangfei Zhai,Yizhe Zhang,James Thornton,Zijing Ou,Joshua Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 本文提出TCSM框架，用于训练离散扩散模型，支持预训练和后训练，实验显示性能优越。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在处理离散数据上有前景，但方法有限；TCSM的动机是提供通用框架，支持直接预训练和后训练。

Method: TCSM通过估计目标分布的concrete score训练模型，允许与奖励函数和预训练模型无缝集成。

Result: 在语言建模任务上，TCSM匹配或超过现有方法，提供更好灵活性和样本效率。

Conclusion: TCSM是多功能的框架，提升离散扩散模型的适用性和效率。

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>


### [31] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang,Rui An,Wenqi Fan,Yanghui Rao,Yuxuan Liang*

Main category: cs.LG

TL;DR: This paper proposes iTFKAN, an interpretable time series forecasting model that balances high performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Time series data is predictable, but current deep learning methods lack interpretability, which is essential for safety-critical applications like autonomous driving and healthcare.

Method: iTFKAN uses model symbolization for interpretability and employs prior knowledge injection and time-frequency synergy learning to handle complex time series data.

Result: Experimental results demonstrate that iTFKAN achieves strong forecasting performance with high interpretability.

Conclusion: iTFKAN is an effective and trustworthy model for time series forecasting in critical domains.

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>


### [32] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou,Mei-Yu Wang,Yige Zhu,Daniel Lazar,Giulia Fanti*

Main category: cs.LG

TL;DR: 本文提出POPri算法，使用偏好优化生成高质量差分隐私合成数据，提升联邦学习性能，并在新基准LargeFedBench上取得显著改善。


<details>
  <summary>Details</summary>
Motivation: 最近研究显示差分隐私联邦学习可能被DP合成数据方法超越，本文洞见将客户端反馈视为偏好排名。

Method: 提出POPri算法，通过Direct Preference Optimization (DPO) 等偏好优化算法微调LLMs，生成DP合成数据。

Result: POPri将全隐私与非隐私设置的next-token预测准确率差距缩小多达68%，优于现有合成数据方法的52%和DP联邦学习方法的10%。

Conclusion: POPri显著提升DP合成数据效用，并发布LargeFedBench基准和代码。

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>


### [33] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin,Cheolwoong Kim,Sunwoong Yang,Minseo Lee,Sung Joong Kim,Joongoo Jeon*

Main category: cs.LG

TL;DR: 这篇论文开发了一种基于节点分配的物理信息神经网络（NA-PINN），用于改进核电站严重事故的热液力学模拟，并展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 针对现有热液力学代码的局限性，如不一致的有限差分方案和多物理分析中的单向耦合，提出使用PINN来提升模拟能力。

Method: 提出NA-PINN方法，为系统代码的每个节点分配独立神经网络，排除空间信息，专注于时间解的逼近。

Result: 在6水箱模拟中，NA-PINN的最大绝对误差为0.007，而PINN为1.678，仅NA-PINN达到可接受准确性，这是首次使用PINN实现系统代码。

Conclusion: NA-PINN显示出潜力，未来将扩展到多物理求解器并开发为代理模型。

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>


### [34] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang,Pratik Chaudhari*

Main category: cs.LG

TL;DR: 本论文推导微分方程描述深度网络训练中泛化间隙演化，并通过有效Gram矩阵分析预测测试损失，强调数据与架构匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习中泛化间隙演化机制，以解释为什么某些模型泛化良好或不良。

Method: 推导控制泛化间隙演化的微分方程，计算有效Gram矩阵，分析轨迹和残差对齐。

Result: 准确预测图像分类数据集的测试损失，残差主要位于最小特征值子空间，训练过程良性无显著泛化退化。

Conclusion: 数据与架构的匹配/不匹配是良好/不良泛化的主要因素。

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [35] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi,Sein Kim,Hongseok Kang,Wonjoong Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文提出DITTO框架，处理用户建模中时间感知的持续学习问题，实验显示其在实际场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统用户建模方法任务特定，缺乏泛化；现有持续学习方法忽略时间流逝和项目分布变化。

Method: 引入实际评估场景，并提出DITTO框架，缓解灾难性遗忘并适应项目分布动态变化。

Result: 实验证明DITTO在考虑时间因素的场景中优于最先进方法。

Conclusion: DITTO有效解决用户表示学习问题，适应时间变化，并提供源代码。

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [36] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi,Yili Wang,Mengnan Du,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: 这篇论文是对合成表格数据生成的全面调查，填补了现有调查的空白，包括各种生成方法、管道和未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据在机器学习中的挑战，如数据稀缺、隐私问题和类别不平衡，以及现有调查的碎片化，特别是LLM和扩散模型的不足。

Method: 提出一个全面的分类法、详细描述合成数据生成的管道（包括合成、后处理和评估）、以及识别挑战和未来方向。

Result: 提供了方法比较分析、挑战识别和开放研究问题的概述。

Conclusion: 强调了领域的演变、方法间互动，并为未来工作提供了指导。

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [37] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 本论文提出一种将最小二乘求解器嵌入梯度下降损失函数的混合优化框架，以加速Physics-Informed Neural Networks在求解Helmholtz方程时的训练收敛。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs使用梯度下降训练在高频波场求解时收敛慢且不稳定，需要改进以提高效率和稳定性。

Method: 通过将最小二乘求解器直接嵌入GD损失函数，优化线性输出层，并提供有无完美匹配层(PML)的张量实现。

Result: 数值实验显示比传统方法更快收敛、更准确、更稳定，尤其在标准GD失败时也能有效。

Conclusion: 该方法计算开销小、可扩展，适用于大规模波场模拟。

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>


### [38] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski,Jan Olszewski,Pankhil Gawade,Krzysztof Koras,Serra Korkmaz,Valentin Rauscher,Jakub M. Tomczak,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文介绍了Hyformer，一个基于变换器的联合模型，用于数据生成和属性预测，在分子任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 建模数据样本及其属性的联合分布，可以创建一个单一模型，既能生成数据又能预测属性，比纯生成或预测模型更有协同效应，但训练面临挑战。

Method: 提出Hyformer，使用交替注意力掩码和统一的预训练方案。

Result: Hyformer在分子生成和属性预测中与最先进模型竞争，并在分子表示学习、命中识别和抗菌肽设计等下游任务中显示出益处。

Conclusion: 联合建模通过Hyformer证明是有效的。

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>


### [39] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis,Batuhan Koyuncu,Isabel Valera,Jes Frellsen*

Main category: cs.LG

TL;DR: 本论文提出一种新生成框架，将隐式神经表示(INRs)和Transformer-based超网络集成到潜在变量模型中，使用Transformer-based解码器生成INR参数，提高表示能力和计算效率，并扩展潜在扩散模型(LDMs)。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖MLP-based超网络，存在可伸缩性限制，因此本论文旨在解决这些问题。

Method: 采用Transformer-based解码器从潜在变量生成INR参数，扩展LDMs替换标准解码器，可从头训练或通过hyper-transforming策略微调解码器。

Result: 提高了表示容量和计算效率，实现现有生成模型到INR-based表示的有效适应，无需完整重训练。

Conclusion: 该框架提供了更高效和可伸缩的INR生成方法。

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [40] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu,Rongmei Liang*

Main category: cs.LG

TL;DR: 本文研究了惩罚化逻辑回归（PLR）中手动标注标签噪声对变量选择的有益影响，并提出了一种基于ADMM的并行算法，提高了大尺度数据集的处理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决PLR在变量选择中的过拟合问题，并探讨手动标注导致的标签噪声如何提升系数估计准确性。

Method: 理论证明标签噪声的益处，并开发分区不敏感的ADMM并行算法，具有全局收敛和次线性收敛率。

Result: 实验结果显示，使用手动标注噪声数据的PLR在多个大型数据集上比传统方法有更高的估计和分类准确性。

Conclusion: 标签噪声可改善PLR的变量选择性能，新算法适用于分布式计算环境。

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>


### [41] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry,Thomas Neele,Mohammad Mousavi,Matteo Sammartino*

Main category: cs.LG

TL;DR: 这篇论文提出了一种组合式主动自动机学习方法，用于学习未知分解的同步并行系统，自动细化字母表，并在实验中显示出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有组合式方法在处理并发系统未知分解时的局限性。

Method: 方法包括开发字母表分布理论、处理反例以更新分布，并实现名为CoalA的学习算法，使用LearnLib库。

Result: 结果显示，在630多个系统中，成员查询减少几个数量级，并在高并发系统中等价查询可伸缩性更好。

Conclusion: 结论是，该方法显著提高了并发系统模型学习的效率和可伸缩性。

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>


### [42] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.LG

TL;DR: 本论文引入ParetoHqD方法，通过偏好方向和Pareto前沿数据改善大型语言模型的多目标对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法的偏好表示不当和奖励不平衡问题，以提升性能。

Method: 将偏好表示为目标空间方向，视Pareto前沿数据为高质量，使用两阶段监督微调针对特定偏好。

Result: 实验显示ParetoHqD在两个任务上优于五个基线算法。

Conclusion: ParetoHqD有效提升多目标对齐性能，展示了显著优势。

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [43] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen,Jiapeng Liu,Jiafan Wang,Wenjun Shi*

Main category: cs.LG

TL;DR: 本文提出DAPLSR模型，通过SMOTE和VDM增强数据，并使用流形优化提升PLSR性能，实验显示优越性。


<details>
  <summary>Details</summary>
Motivation: 解决传统PLSR在处理不均衡类别数据时性能不足的问题。

Method: 使用SMOTE过采样和VDM选择邻居生成合成样本，并采用流形优化方法利用约束空间几何属性求解PLSR。

Result: 在各种数据集上，DAPLSR模型的分类性能和评估指标显著优于现有方法。

Conclusion: DAPLSR模型通过数据增强和优化技术有效改善了PLSR的处理不均衡数据能力。

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [44] [BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification](https://arxiv.org/abs/2504.16096)
*Jiaxing Xu,Kai He,Yue Tang,Wei Li,Mengcheng Lan,Xia Dong,Yiping Ke,Mengling Feng*

Main category: q-bio.NC

TL;DR: BrainPrompt integrates Large Language Models with Graph Neural Networks using knowledge-driven prompts to improve neurological disease diagnosis from fMRI data, enhancing prediction and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing brain network methods rely only on imaging data, ignoring non-imaging factors, which limits predictive accuracy and interpretability.

Method: BrainPrompt employs three types of prompts: ROI-level for brain region identity, subject-level for demographic information, and disease-level for disease progression, combined with LLMs and GNNs.

Result: Outperforms state-of-the-art methods on fMRI datasets and extracts interpretable biomarkers consistent with neuroscience knowledge.

Conclusion: The framework improves disease stage prediction and interpretability by leveraging multi-modal information.

Abstract: Neurological conditions, such as Alzheimer's Disease, are challenging to
diagnose, particularly in the early stages where symptoms closely resemble
healthy controls. Existing brain network analysis methods primarily focus on
graph-based models that rely solely on imaging data, which may overlook
important non-imaging factors and limit the model's predictive power and
interpretability. In this paper, we present BrainPrompt, an innovative
framework that enhances Graph Neural Networks (GNNs) by integrating Large
Language Models (LLMs) with knowledge-driven prompts, enabling more effective
capture of complex, non-imaging information and external knowledge for
neurological disease identification. BrainPrompt integrates three types of
knowledge-driven prompts: (1) ROI-level prompts to encode the identity and
function of each brain region, (2) subject-level prompts that incorporate
demographic information, and (3) disease-level prompts to capture the temporal
progression of disease. By leveraging these multi-level prompts, BrainPrompt
effectively harnesses knowledge-enhanced multi-modal information from LLMs,
enhancing the model's capability to predict neurological disease stages and
meanwhile offers more interpretable results. We evaluate BrainPrompt on two
resting-state functional Magnetic Resonance Imaging (fMRI) datasets from
neurological disorders, showing its superiority over state-of-the-art methods.
Additionally, a biomarker study demonstrates the framework's ability to extract
valuable and interpretable information aligned with domain knowledge in
neuroscience.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [45] [One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints](https://arxiv.org/abs/2504.16211)
*Kunpeng Zhang,Lei Xu,Xinlei Yi,Guanghui Wen,Lihua Xie,Tianyou Chai,Tao Yang*

Main category: eess.SY

TL;DR: 这篇论文提出了一种分布式带宽在线算法，处理时间变化约束的凸优化问题，并证明了次线性遗憾和约束违反界。


<details>
  <summary>Details</summary>
Motivation: 动机是解决分布式优化中未知损失函数和时间变化约束的问题，确保代理决策长期满足约束。

Method: 方法是分布式带宽在线原-对偶投影算法，使用一点采样，适用于均匀联合强连通的时间变化有向图。

Result: 结果包括次线性动态网络遗憾和约束违反；静态遗憾为O(T^{3/4 + g})，约束违反为O(T^{1 - g/2})；强凸损失下遗憾为O(T^{2/3 + 4g/3})。

Conclusion: 结论通过数值例子验证了理论结果。

Abstract: This paper considers the distributed bandit convex optimization problem with
time-varying constraints. In this problem, the global loss function is the
average of all the local convex loss functions, which are unknown beforehand.
Each agent iteratively makes its own decision subject to time-varying
inequality constraints which can be violated but are fulfilled in the long run.
For a uniformly jointly strongly connected time-varying directed graph, a
distributed bandit online primal-dual projection algorithm with one-point
sampling is proposed. We show that sublinear dynamic network regret and network
cumulative constraint violation are achieved if the path-length of the
benchmark also increases in a sublinear manner. In addition, an
$\mathcal{O}({T^{3/4 + g}})$ static network regret bound and an $\mathcal{O}(
{{T^{1 - {g}/2}}} )$ network cumulative constraint violation bound are
established, where $T$ is the total number of iterations and $g \in ( {0,1/4}
)$ is a trade-off parameter. Moreover, a reduced static network regret bound
$\mathcal{O}( {{T^{2/3 + 4g /3}}} )$ is established for strongly convex local
loss functions. Finally, a numerical example is presented to validate the
theoretical results.

</details>


### [46] [Nash Equilibrium Learning In Large Populations With First Order Payoff Modifications](https://arxiv.org/abs/2504.16222)
*Matthew S. Hankins,Jair Certório,Tzuyu Jeng,Nuno C. Martins*

Main category: eess.SY

TL;DR: 本文建立了Nash均衡学习，证明了在具有一阶修正的收益动态机制下，种群状态收敛到Nash均衡集。


<details>
  <summary>Details</summary>
Motivation: 动机是模拟代理人的有限理性、预见性或平均项，以及延迟的一阶Padé近似。

Method: 方法是通过结合两种非标准系统理论的钝性概念。

Result: 结果是证实了Nash均衡的收敛。

Conclusion: 结论是成功建立了这种收敛机制，可用于处理现实复杂性。

Abstract: We establish Nash equilibrium learning -- convergence of the population state
to a suitably defined Nash equilibria set -- for a class of payoff dynamical
mechanism with a first order modification. The first order payoff modification
can model aspects of the agents' bounded rationality, anticipatory or averaging
terms in the payoff mechanism, or first order Pad\'e approximations of delays.
To obtain our main results, we apply a combination of two nonstandard
system-theoretic passivity notions.

</details>


### [47] [Distributed Space Resource Logistics Architecture Optimization under Economies of Scale](https://arxiv.org/abs/2504.16385)
*Evangelia Gkaravela,Hang Woon Lee,Hao Chen*

Main category: eess.SY

TL;DR: 这篇论文提出了一种分布式ISRU系统优化框架，支持多任务空间探索，并通过案例研究比较其与集中式ISRU的性能。


<details>
  <summary>Details</summary>
Motivation: 为了支持未来的多任务空间探索，通过分析分布式ISRU系统在空间运输中的性能和影响。

Method: 提出优化框架，包括技术权衡研究、部署策略、设施位置评估和资源物流；开发基于规模经济的分段线性规模和成本估算模型；进行多任务近月点物流案例研究和敏感性分析。

Result: 展示了分布式ISRU的价值，评估了关键权衡，并比较了分布式和集中式ISRU的性能。

Conclusion: 敏感性分析显示，分布式ISRU在某些条件下可能优于集中式ISRU，但需根据具体情况权衡。

Abstract: This paper proposes an optimization framework for distributed resource
logistics system design to support future multimission space exploration. The
performance and impact of distributed In-Situ Resource Utilization (ISRU)
systems in facilitating space transportation are analyzed. The proposed
framework considers technology trade studies, deployment strategy, facility
location evaluation, and resource logistics after production for distributed
ISRU systems. We develop piecewise linear sizing and cost estimation models
based on economies of scale that can be easily integrated into network-based
mission planning formulations. A case study on a multi-mission cislunar
logistics campaign is conducted to demonstrate the value of the proposed method
and evaluate key tradeoffs to compare the performance of distributed ISRU
systems with traditional concentrated ISRU. Finally, a comprehensive
sensitivity analysis is performed to assess the proposed system under varying
conditions, comparing concentrated and distributed ISRU systems.

</details>


### [48] [Hierarchical Distributed Architecture for the Least Allan Variance Atomic Timing](https://arxiv.org/abs/2504.16413)
*Jiayu Chen,Takahiro Kawaguchi,Yuichiro Yano,Yuko Hanado,Takayuki Ishizaki*

Main category: eess.SY

TL;DR: 本论文提出一种基于微型原子钟的分层分布式定时架构，确保在GNSS可用和故障模式下实现高精度同步。


<details>
  <summary>Details</summary>
Motivation: 动机是为GNSS故障提供鲁棒的同步定时系统，使用分布式控制在不同模式下保持准确性。

Method: 方法包括低层分布式控制使用本地信息同步钟表，高层监督器在正常模式锚定GNSS，在紧急模式使用最优浮动控制最小化时标发散。

Result: 结果通过数值例子证明了架构在高精度、抗GNSS故障原子定时中的有效性和稳定性。

Conclusion: 结论是该架构在确保长期准确性和鲁棒性方面是可行且有效的。

Abstract: In this paper, we propose a hierarchical distributed timing architecture
based on an ensemble of miniature atomic clocks. The goal is to ensure
synchronized and accurate timing in a normal operating mode where Global
Navigation Satellite System (GNSS) signals are available, as well as in an
emergency operating mode during GNSS failures. At the lower level, the
miniature atomic clocks employ a distributed control strategy that uses only
local information to ensure synchronization in both modes. The resulting
synchronized time or generated time scale has the best frequency stability, as
measured by the Allan variance, over the short control period. In the upper
layer, a supervisor controls the long-term behavior of the generated time
scale. In the normal operating mode, the supervisor periodically anchors the
generated time scale to the standard time based on GNSS signals, while in the
emergency operating mode, it applies optimal floating control to reduce the
divergence rate of the generated time scale, which is not observable from the
measurable time difference between the miniature atomic clocks. This floating
control aims to explicitly control the generated time scale to have the least
Allan variance over the long control period. Finally, numerical examples are
provided to demonstrate the effectiveness and feasibility of the architecture
in high-precision, GNSS-resilient atomic timing.

</details>


### [49] [Anytime Safe Reinforcement Learning](https://arxiv.org/abs/2504.16417)
*Pol Mestres,Arnau Marzabal,Jorge Cortés*

Main category: eess.SY

TL;DR: 这篇论文提出RL-SGF算法，用于带约束的强化学习问题，确保策略安全并具有anytime保证。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中策略安全问题，特别是算法随时终止时也能返回安全策略。

Method: 引入RL-SGF算法，使用价值函数和梯度估计，通过解决凸二次约束二次规划更新策略参数。

Result: 证明使用足够episode可高概率更新安全策略，迭代收敛到KKT点邻域，并在导航例子中验证性能。

Conclusion: RL-SGF算法提供可靠的理论保证和实际应用潜力。

Abstract: This paper considers the problem of solving constrained
  reinforcement learning problems with anytime guarantees, meaning
  that the algorithmic solution returns a safe policy regardless of
  when it is terminated. Drawing inspiration from anytime constrained
  optimization, we introduce Reinforcement Learning-based Safe
  Gradient Flow (RL-SGF), an on-policy algorithm which employs
  estimates of the value functions and their respective gradients
  associated with the objective and safety constraints for the current
  policy, and updates the policy parameters by solving a convex
  quadratically constrained quadratic program. We show that if the
  estimates are computed with a sufficiently large number of episodes
  (for which we provide an explicit bound), safe policies are updated
  to safe policies with a probability higher than a prescribed
  tolerance. We also show that iterates asymptotically converge to a
  neighborhood of a KKT point, whose size can be arbitrarily reduced
  by refining the estimates of the value function and their gradients.
  We illustrate the performance of RL-SGF in a navigation example.

</details>


### [50] [Power-based control of output oscillations with online estimation of biased harmonics](https://arxiv.org/abs/2504.16445)
*Michael Ruderman,Denis Efimov*

Main category: eess.SY

TL;DR: 本文扩展功率-based控制，添加有限时间估计偏置谐波和改进权重计算，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 减少控制回路通信努力，以补偿边际阻尼或缓慢发散的输出振荡。

Method: 使用有限时间估计偏置谐波、改进脉冲权重因子计算，并在线估计谐波参数。

Result: 在五阶执行器系统上实验评估，涉及重力和测量噪声。

Conclusion: 证明了扩展方法在实际系统中的有效性。

Abstract: The recently introduced discrete power-based control (Ruderman (2024b))
reduces largely the communication efforts in the control loop when compensating
for the marginally damped or even slowly diverging output oscillations. The
control commutates twice per oscillations period (at the amplitude peaks) and
uses the measured harmonic output only. The power-based control scheme requires
the knowledge of the instantaneous frequency, amplitude, and bias parameters of
the harmonic signal. This paper extends the power-based control by the
finite-time estimation of the biased harmonics (Ahmed et al. (2022)). Also an
improved analytic calculation of the impulse weighting factor is provided. The
power-based oscillations control with online estimation of the harmonic
parameters is evaluated experimentally on the fifth-order actuator system with
a free hanging load under gravity and measurement noise.

</details>


### [51] [Distributed Optimization with Efficient Communication, Event-Triggered Solution Enhancement, and Operation Stopping](https://arxiv.org/abs/2504.16477)
*Apostolos I. Rikos,Wei Jiang,Themistoklis Charalambous,Karl H. Johansson*

Main category: eess.SY

TL;DR: 论文介绍了三种高效分布式优化算法，用于传感器网络和IoT设备，节省通信资源，并线性收敛，应用于目标定位。


<details>
  <summary>Details</summary>
Motivation: 在现代大规模系统（如传感器网络和IoT设备）中，需要高效协作解决复杂问题，同时利用网络资源。

Method: 论文呈现了三种分布式优化算法：1. 简单量化平均梯度过程；2. 包含事件触发精化机制的算法，用于提升量化精度；3. 针对低比特消息环境的算法，包含事件触发机制调整量化器基础和级别。

Result: 三个算法均显示线性收敛，在分布式传感器融合目标定位应用中表现出色。

Conclusion: 算法高效，允许基于性能保证终止操作，并优于现有文献中的算法。

Abstract: In modern large-scale systems with sensor networks and IoT devices it is
essential to collaboratively solve complex problems while utilizing network
resources efficiently. In our paper we present three distributed optimization
algorithms that exhibit efficient communication among nodes. Our first
algorithm presents a simple quantized averaged gradient procedure for
distributed optimization, which is shown to converge to a neighborhood of the
optimal solution. Our second algorithm incorporates a novel event-triggered
refinement mechanism, which refines the utilized quantization level to enhance
the precision of the estimated optimal solution. It enables nodes to terminate
their operation according to predefined performance guarantees. Our third
algorithm is tailored to operate in environments where each message consists of
only a few bits. It incorporates a novel event-triggered mechanism for
adjusting the quantizer basis and quantization level, allowing nodes to
collaboratively decide operation termination based on predefined performance
criteria. We analyze the three algorithms and establish their linear
convergence. Finally, an application on distributed sensor fusion for target
localization is used to demonstrate their favorable performance compared to
existing algorithms in the literature.

</details>


### [52] [LiDAL-Assisted RLNC-NOMA in OWC Systems](https://arxiv.org/abs/2504.16498)
*Ahmed A. Hassan,Ahmad Adnan Qidan,Taisir Elgorashi,Jaafar Elmirghani*

Main category: eess.SY

TL;DR: 这篇论文提出了一种结合NOMA、RLNC和LiDAL的双功能光学无线通信系统，用于改善室内多用户干扰管理和通信定位性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集室内光学无线通信中多用户干扰挑战，并实现高效通信和定位。

Method: 使用NOMA和RLNC进行数据传输，结合LiDAL系统被动获取用户位置信息以优化CSI估计。

Result: RLNC-NOMA提高了成功解码概率和系统性能，定位估计准确度高，并减少了CSI不完善的影响。

Conclusion: 所提出的系统显著提升了光学无线通信的整体性能，尤其在解码和定位方面。

Abstract: Optical wireless communication (OWC) is envisioned as a key enabler for
immersive indoor data transmission in future wireless communication networks.
However, multi-user interference management arises as a challenge in dense
indoor OWC systems composed of multiple optical access points (APs) serving
multiple users. In this paper, we propose a novel dual-function OWC system for
communication and localization. Non-orthogonal multiple access (NOMA) with
random linear network coding (RLNC) is designed for data transmission, where
NOMA allows the serving of multiple users simultaneously through controlling
the power domain, and RLNC helps minimize errors that might occur during signal
processing phase. This setup is assisted with a light detection and
localization system (LiDAL) that can passively obtain spatio-temporal indoor
information of user presence and location for dynamic-user grouping. The
designed LiDAL system helps to improve the estimation of channel state
information (CSI) in realistic indoor network scenarios, where the CSI of
indoor users might be noisy and/or highly correlated. We evaluate the
performance of NOMA combined with RLNC by analyzing the probability of
successful decoding compared to conventional NOMA and orthogonal schemes. In
addition, we derive the Cramer-Rao Lower Bound (CRLB) to evaluate the accuracy
of location estimation. The results show that the proposed RLNC-NOMA improves
the probability of successful decoding and the overall system performance. The
results also show the high accuracy of the unbiased location estimator and its
assistant in reducing the imperfection of CSI, leading to high overall system
performance.

</details>


### [53] [Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows](https://arxiv.org/abs/2504.16588)
*Defne E. Ozan,Andrea Nóvoa,Luca Magri*

Main category: eess.SY

TL;DR: 本论文提出一个数据同化模型-based RL框架，用于从部分和噪声测量中控制湍流，在Kuramoto-Sivashinsky方程上验证有效。


<details>
  <summary>Details</summary>
Motivation: 控制湍流在能量和交通领域重要，但因混沌动力学和高维度困难，且强化学习需完整状态信息，但实验中往往不可用。

Method: 提出DA-MBRL框架，使用控制感知Echo State Network预测动态、Ensemble Kalman Filter进行状态估计、off-policy actor-critic算法学习控制策略。

Result: 在Kuramoto-Sivashinsky方程测试中，成功从噪声和部分测量稳定时空混沌流。

Conclusion: 框架在部分可观测和噪声环境下的湍流控制中有效。

Abstract: The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.

</details>


### [54] [Distributed Unknown Input Observers for Discrete-Time Linear Time-Invariant Systems](https://arxiv.org/abs/2504.16815)
*Franco Angelo Torchiaro,Gianfranco Gagliardi,Francesco Tedesco,Alessandro Casavola*

Main category: eess.SY

TL;DR: 这篇论文引入了一种分布式未知输入观测器（D-UIO）设计方法，使用节点检测分解来估计离散时间LTI系统的状态，即使在噪声和未知输入存在的情况下。


<details>
  <summary>Details</summary>
Motivation: 动机是处理分布式系统中状态估计的问题，其中传感器位于通信图节点，每个节点仅能访问本地测量并与邻居共享数据。

Method: 方法包括使用节点检测分解，将问题分为设计本地输出注入增益减轻测量噪声和设计扩散增益通过共识协议补偿信息缺失，并采用线性矩阵不等式和半定编程进行高效合成。

Result: 结果通过两个模拟场景验证了分布式观测器的有效性，在不同节点分解下表现良好。

Conclusion: 结论是，该方法在分布式状态估计中是有效的、可计算的，并能处理噪声和未知输入。

Abstract: This paper introduces a Distributed Unknown Input Observer (D-UIO) design
methodology that uses a technique called node-wise detectability decomposition
to estimate the state of a discrete-time linear time-invariant (LTI) system in
a distributed way, even when there are noisy measurements and unknown inputs.
In the considered scenario, sensors are associated to nodes of an underlying
communication graph. Each node has a limited scope as it can only access local
measurements and share data with its neighbors. The problem of designing the
observer gains is divided into two separate sub-problems: (i) design local
output injection gains to mitigate the impact of measurement noise, and (ii)
design diffusive gains to compensate for the lack of information through a
consensus protocol. A direct and computationally efficient synthesis strategy
is formulated by linear matrix inequalities (LMIs) and solved via semidefinite
programming. Finally, two simulative scenarios are presented to illustrate the
effectiveness of the distributed observer when two different node-wise
decompositions are adopted.

</details>


### [55] [Reconfigurable Intelligent Surface Control for a Moving Receiver](https://arxiv.org/abs/2504.16874)
*Hamed Radpour,Markus Hofer,Thomas Zemen*

Main category: eess.SY

TL;DR: 本论文提出了一种自适应波束形成算法，用于毫米波频段的RIS，在移动场景下实验验证，获得高达24dB的接收功率增益。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段易受视线阻塞，RIS在工业自动化中重要，但现有研究多为理论，实时移动验证不足。

Method: 提出基于低速率反馈的自适应波束形成算法，不需用户位置信息，使用127元素RIS在23.8GHz半消声室环境实验验证。

Result: 实验结果显示，接收功率较未激活RIS基准提升高达24dB。

Conclusion: 突显自适应RIS控制在移动非视线场景中的实际益处。

Abstract: Reconfigurable intelligent surfaces (RISs) are emerging as key enablers of
reliable industrial automation in the millimeter-wave (mmWave) band,
particularly in environments with frequent line-of-sight (LoS) blockage. While
prior works have largely focused on theoretical aspects, real-time validation
under user mobility remains underexplored. In this work, we propose and
experimentally evaluate a self-adaptive beamforming algorithm that enables RIS
reconfiguration based on a low-rate feedback link from the mobile user
equipment (UE) to the RIS controller. The algorithm maintains received signal
power above a predefined threshold without requiring UE position knowledge.
Using a hexagonal RIS with 127 elements operating at 23.8 GHz, we validate our
approach in a semi-anechoic environment over a 60 cm*100 cm observation area.
The results demonstrate up to 24 dB gain in received power compared to the
baseline with inactive RIS elements, highlighting the practical benefits of
adaptive RIS control in mobile non-line-of-sight (NLoS) scenarios.

</details>


### [56] [Learning Verifiable Control Policies Using Relaxed Verification](https://arxiv.org/abs/2504.16879)
*Puja Chaudhury,Alexander Estornell,Michael Everett*

Main category: eess.SY

TL;DR: 本篇论文提出在训练过程中使用可微分可达性分析验证控制策略，确保安全规范，并在机器人模型上实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在训练后验证，易因策略不满足规范或算法保守性而失败；本文旨在训练中持续验证，以获得可在运行时轻量级验证的策略。

Method: 使用可微分可达性分析，并将新组件融入损失函数中。

Result: 在四旋翼和单轮车模型的数值实验中，方法成功学习出满足可达-避开和不变性规范的控制策略。

Conclusion: 该方法可有效生成可在运行时轻松验证的安全控制策略。

Abstract: To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [57] [Heterogeneous networks in drug-target interaction prediction](https://arxiv.org/abs/2504.16152)
*Mohammad Molaee,Nasrollah Moghadam Charkari*

Main category: q-bio.BM

TL;DR: 本调查总结了2020-2024年间使用图机器学习方法预测药物-靶点交互的框架、贡献、数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程耗时且成本高，计算方法可缩小实验搜索空间。

Method: 通过调查图机器学习方法，详细描述框架、主要贡献、常用数据集和性能评估指标。

Result: 方法在药物-靶点交互预测中显示出前景性结果，并讨论了未来挑战。

Conclusion: 探讨了未来挑战和需要探索的关键领域。

Abstract: Drug discovery requires a tremendous amount of time and cost. Computational
drug-target interaction prediction, a significant part of this process, can
reduce these requirements by narrowing the search space for wet lab
experiments. In this survey, we provide comprehensive details of graph machine
learning-based methods in predicting drug-target interaction, as they have
shown promising results in this field. These details include the overall
framework, main contribution, datasets, and their source codes. The selected
papers were mainly published from 2020 to 2024. Prior to discussing papers, we
briefly introduce the datasets commonly used with these methods and
measurements to assess their performance. Finally, future challenges and some
crucial areas that need to be explored are discussed.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [58] [Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model](https://arxiv.org/abs/2504.16093)
*Yurun Ge,Lucas Böttcher,Tom Chou,Maria R. D'Orsogna*

Main category: q-fin.PM

TL;DR: 论文提出基于Bradley-Terry模型的资源分配方法，优化不确定性下的项目选择和排名。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在不确定性下资源分配问题，如创新项目和研究资助，以最大化长期收益。

Method: 方法基于Quicksort和Bradley-Terry模型，通过代理评估项目成对'获胜'概率，聚合排名，并结合采样技术减少比较。

Result: 结果显示新方法优于现有聚合方法，并能显著减少成对比较次数。

Conclusion: 结论讨论Bradley-Terry方法在实际中的实施。

Abstract: How to allocate limited resources to projects that will yield the greatest
long-term benefits is a problem that often arises in decision-making under
uncertainty. For example, organizations may need to evaluate and select
innovation projects with risky returns. Similarly, when allocating resources to
research projects, funding agencies are tasked with identifying the most
promising proposals based on idiosyncratic criteria. Finally, in participatory
budgeting, a local community may need to select a subset of public projects to
fund. Regardless of context, agents must estimate the uncertain values of a
potentially large number of projects. Developing parsimonious methods to
compare these projects, and aggregating agent evaluations so that the overall
benefit is maximized, are critical in assembling the best project portfolio.
Unlike in standard sorting algorithms, evaluating projects on the basis of
uncertain long-term benefits introduces additional complexities. We propose
comparison rules based on Quicksort and the Bradley--Terry model, which
connects rankings to pairwise "win" probabilities. In our model, each agent
determines win probabilities of a pair of projects based on his or her specific
evaluation of the projects' long-term benefit. The win probabilities are then
appropriately aggregated and used to rank projects. Several of the methods we
propose perform better than the two most effective aggregation methods
currently available. Additionally, our methods can be combined with sampling
techniques to significantly reduce the number of pairwise comparisons. We also
discuss how the Bradley--Terry portfolio selection approach can be implemented
in practice.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [59] [Introduction to Quantum Machine Learning and Quantum Architecture Search](https://arxiv.org/abs/2504.16131)
*Samuel Yen-Chi Chen,Zhiding Liang*

Main category: quant-ph

TL;DR: 本教程概述了量子计算和机器学习的最新进展，以及量子机器学习的突破及其在多样领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 整合量子计算和机器学习以提升性能，并通过系统化方法使非量子专家能够有效利用量子工具。

Method: 提供深入教程，涵盖系统化和自动化的量子电路设计方法。

Result: 突出了量子机器学习扩展应用领域的潜力。

Conclusion: 量子机器学习有望在更多领域获得应用。

Abstract: Recent advancements in quantum computing (QC) and machine learning (ML) have
fueled significant research efforts aimed at integrating these two
transformative technologies. Quantum machine learning (QML), an emerging
interdisciplinary field, leverages quantum principles to enhance the
performance of ML algorithms. Concurrently, the exploration of systematic and
automated approaches for designing high-performance quantum circuit
architectures for QML tasks has gained prominence, as these methods empower
researchers outside the quantum computing domain to effectively utilize
quantum-enhanced tools. This tutorial will provide an in-depth overview of
recent breakthroughs in both areas, highlighting their potential to expand the
application landscape of QML across diverse fields.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [60] [Mass-Adaptive Admittance Control for Robotic Manipulators](https://arxiv.org/abs/2504.16224)
*Hossein Gholampour,Jonathon E. Slightam,Logan E. Beaver*

Main category: cs.RO

TL;DR: 本文提出了一种机器人方法，能够在未知负载质量下可靠地跟随路径点，通过整合 admittance 控制和质量估计器来补偿负载。


<details>
  <summary>Details</summary>
Motivation: 处理未知或变化质量的物体是机器人学中的常见挑战，可能导致错误或不稳定性，因此需要实时适应控制系统。

Method: 整合 admittance 控制框架与质量估计器，动态更新激励力以补偿未知负载质量，从而减轻末端执行器下垂并保持稳定性。

Result: 在挑拣和放置任务中实验验证，与基线方案相比，提高了路径点到达精度和顺从运动。

Conclusion: 通过安全处理未知负载，提高了机器人自动化的灵活性，是适应不确定环境控制的重要进展。

Abstract: Handling objects with unknown or changing masses is a common challenge in
robotics, often leading to errors or instability if the control system cannot
adapt in real-time. In this paper, we present a novel approach that enables a
six-degrees-of-freedom robotic manipulator to reliably follow waypoints while
automatically estimating and compensating for unknown payload weight. Our
method integrates an admittance control framework with a mass estimator,
allowing the robot to dynamically update an excitation force to compensate for
the payload mass. This strategy mitigates end-effector sagging and preserves
stability when handling objects of unknown weights. We experimentally validated
our approach in a challenging pick-and-place task on a shelf with a crossbar,
improved accuracy in reaching waypoints and compliant motion compared to a
baseline admittance-control scheme. By safely accommodating unknown payloads,
our work enhances flexibility in robotic automation and represents a
significant step forward in adaptive control for uncertain environments.

</details>


### [61] [Vision Controlled Orthotic Hand Exoskeleton](https://arxiv.org/abs/2504.16319)
*Connor Blais,Md Abdul Baset Sarker,Masudul H. Imtiaz*

Main category: cs.RO

TL;DR: 这篇论文开发了AI视觉控制的手部外骨骼，提升手部障碍者的康复辅助，通过实时物体检测和自动抓取，改善传统EMG系统。


<details>
  <summary>Details</summary>
Motivation: 针对手部运动障碍者，提供无需用户校准的自主系统，提高康复效率和便利性。

Method: 使用Google Coral Dev Board Micro的Edge TPU和定制MobileNet_V2模型，实现实时检测、距离估计和气动致动，设计紧凑，续航8小时。

Result: 推理速度51ms，优于以往，但光照和物体方向变化时模型鲁棒性不足，YOLOv11潜力受量化问题影响。

Conclusion: 证明视觉控制外骨骼在实际辅助中的可行性，平衡便携性和效率，建议未来优化模型和硬件。

Abstract: This paper presents the design and implementation of an AI vision-controlled
orthotic hand exoskeleton to enhance rehabilitation and assistive functionality
for individuals with hand mobility impairments. The system leverages a Google
Coral Dev Board Micro with an Edge TPU to enable real-time object detection
using a customized MobileNet\_V2 model trained on a six-class dataset. The
exoskeleton autonomously detects objects, estimates proximity, and triggers
pneumatic actuation for grasp-and-release tasks, eliminating the need for
user-specific calibration needed in traditional EMG-based systems. The design
prioritizes compactness, featuring an internal battery. It achieves an 8-hour
runtime with a 1300 mAh battery. Experimental results demonstrate a 51ms
inference speed, a significant improvement over prior iterations, though
challenges persist in model robustness under varying lighting conditions and
object orientations. While the most recent YOLO model (YOLOv11) showed
potential with 15.4 FPS performance, quantization issues hindered deployment.
The prototype underscores the viability of vision-controlled exoskeletons for
real-world assistive applications, balancing portability, efficiency, and
real-time responsiveness, while highlighting future directions for model
optimization and hardware miniaturization.

</details>


### [62] [Fast Online Adaptive Neural MPC via Meta-Learning](https://arxiv.org/abs/2504.16369)
*Yu Mei,Xinyu Zhou,Shuyang Yu,Vaibhav Srivastava,Xiaobo Tan*

Main category: cs.RO

TL;DR: 本论文提出快速在线自适应MPC框架，使用MAML和神经网络处理机器人控制中的模型不确定性，提高实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动MPC方法需大量离线数据和计算资源，无法在线适应模型不确定性。

Method: 采用MAML进行少样本适应残差动态，并嵌入L4CasADi-based MPC管道，实现快速模型修正。

Result: 模拟验证在Van der Pol振荡器、Cart-Pole和2D quadrotor上，显示比标准MPC更好的适应速度和预测准确性。

Conclusion: 该框架有效提升实时自适应机器人控制性能。

Abstract: Data-driven model predictive control (MPC) has demonstrated significant
potential for improving robot control performance in the presence of model
uncertainties. However, existing approaches often require extensive offline
data collection and computationally intensive training, limiting their ability
to adapt online. To address these challenges, this paper presents a fast online
adaptive MPC framework that leverages neural networks integrated with
Model-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot
adaptation of residual dynamics - capturing the discrepancy between nominal and
true system behavior - using minimal online data and gradient steps. By
embedding these meta-learned residual models into a computationally efficient
L4CasADi-based MPC pipeline, the proposed method enables rapid model
correction, enhances predictive accuracy, and improves real-time control
performance. We validate the framework through simulation studies on a Van der
Pol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show
significant gains in adaptation speed and prediction accuracy over both nominal
MPC and nominal MPC augmented with a freshly initialized neural network,
underscoring the effectiveness of our approach for real-time adaptive robot
control.

</details>


### [63] [Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms](https://arxiv.org/abs/2504.16916)
*Hsin-Jung Yang,Mahsa Khosravi,Benjamin Walt,Girish Krishnan,Soumik Sarkar*

Main category: cs.RO

TL;DR: 使用强化学习框架实现软连续臂的视觉伺服，支持零样本模拟到真实转移。


<details>
  <summary>Details</summary>
Motivation: 解决软连续臂的无穷自由度和非线性行为在建模和控制中的挑战。

Method: 引入RL-based框架，解耦运动学和机械属性，使用RL控制器和本地控制器，仅依赖视觉反馈，在模拟中训练。

Result: 模拟成功率99.8%，硬件零样本转移成功率67%。

Conclusion: 提供可扩展的解决方案，具有进一步改进和扩展应用的潜力。

Abstract: Soft continuum arms (SCAs) soft and deformable nature presents challenges in
modeling and control due to their infinite degrees of freedom and non-linear
behavior. This work introduces a reinforcement learning (RL)-based framework
for visual servoing tasks on SCAs with zero-shot sim-to-real transfer
capabilities, demonstrated on a single section pneumatic manipulator capable of
bending and twisting. The framework decouples kinematics from mechanical
properties using an RL kinematic controller for motion planning and a local
controller for actuation refinement, leveraging minimal sensing with visual
feedback. Trained entirely in simulation, the RL controller achieved a 99.8%
success rate. When deployed on hardware, it achieved a 67% success rate in
zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This
approach offers a scalable solution for SCAs in 3D visual servoing, with
potential for further refinement and expanded applications.

</details>


### [64] [Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving](https://arxiv.org/abs/2504.16923)
*Jacob Levy,Jason Gibson,Bogdan Vlahov,Erica Tevere,Evangelos Theodorou,David Fridovich-Keil,Patrick Spieler*

Main category: cs.RO

TL;DR: A framework combining Kalman filter-based adaptation and meta-learning improves dynamics models for high-speed off-road autonomous driving, enhancing accuracy and safety.


<details>
  <summary>Details</summary>
Motivation: Challenges in high-speed off-road driving, such as complex terrain and poor model generalization, necessitate real-time adaptation of dynamics models.

Method: Uses offline meta-learning to optimize basis functions and parameters, combined with online Kalman filter adaptation for real-time dynamics model adjustment in model-based control.

Result: Experiments, including real-world tests on a full-scale vehicle, show superior prediction accuracy, performance, and safety compared to baselines.

Conclusion: Meta-learned adaptation effectively advances reliable autonomous systems for diverse and unseen environments.

Abstract: High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [65] [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
*Myunghyun Rhee,Joonseop Sim,Taeyoung Ahn,Seungyong Lee,Daegun Yoon,Euiseok Kim,Kyoung Park,Youngpyo Joo,Hosik Kim*

Main category: cs.AR

TL;DR: 本文提出了一种高带宽处理单元（HPU），通过卸载GPU的内存密集型操作，提高Transformer-based LLM大批量推理的效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型注意力层在当前GPU系统中存在的低运算强度和KV缓存高内存需求导致的效率问题。

Method: 提出并实现基于PCIe FPGA卡的HPU作为GPU系统的协处理器，形成异构系统，卸载内存绑定操作。

Result: 实验显示，GPU-HPU系统比纯GPU系统性能提升高达4.1倍，能源效率提升4.6倍，并支持扩展而不增加GPU数量。

Conclusion: HPU提升了GPU资源利用率，提供更好的可扩展性和效率改进。

Abstract: The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of KV caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [66] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao,Muning Wen,Jun Wang,Weinan Zhang*

Main category: cs.MA

TL;DR: 这篇论文提出了一种针对LLM-based Multi-Agent Systems的Multi-Agent Reinforcement Fine-Tuning（MARFT）范式，旨在通过强化学习提升代理系统的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升代理智能方面有效，但针对LLM-based Multi-Agent Systems的微调研究有限，且直接应用多代理强化学习方法面临独特挑战。

Method: 论文引入了一个通用的算法框架，阐述了MARFT的概念基础、与传统方法的区别、核心算法设计，并提供了开源实现。

Result: 开发了一个鲁棒、可扩展的MARFT框架，并公开了开源代码，讨论了实际应用前景和潜在挑战。

Conclusion: 这项工作为研究者提供了推进MARFT向更坚韧和适应性代理系统解决方案的路线图。

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs](https://arxiv.org/abs/2504.16144)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.IR

TL;DR: 本论文提出一个细粒度分层分类法和Query-Specific Few-shot Learning方法，使用LLM检测和分类灾害相关社交媒体帖子，并评估行动性，以提升人道组织响应效率。


<details>
  <summary>Details</summary>
Motivation: 自然灾害引发社交媒体活动激增，包括求助和提供帮助信息，动机是帮助人道组织更有效地响应灾害。

Method: 提出分层分类法组织请求和提供信息；引入QSF Learning，使用LLM从嵌入数据库检索特定类别示例提升检测和分类性能，并评估消息行动性。

Result: 实验显示，该方法优于基线策略，能够有效识别和优先处理可行动请求和提供。

Conclusion: 该方法显著改善了社交媒体在灾害响应中的信息处理和优先级管理。

Abstract: Natural disasters often result in a surge of social media activity, including
requests for assistance, offers of help, sentiments, and general updates. To
enable humanitarian organizations to respond more efficiently, we propose a
fine-grained hierarchical taxonomy to systematically organize crisis-related
information about requests and offers into three critical dimensions: supplies,
emergency personnel, and actions. Leveraging the capabilities of Large Language
Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)
that retrieves class-specific labeled examples from an embedding database to
enhance the model's performance in detecting and classifying posts. Beyond
classification, we assess the actionability of messages to prioritize posts
requiring immediate attention. Extensive experiments demonstrate that our
approach outperforms baseline prompting strategies, effectively identifying and
prioritizing actionable requests and offers.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [68] [SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation](https://arxiv.org/abs/2504.16122)
*Xuhui Zhou,Zhe Su,Sophie Feng,Jiaxu Zhou,Jen-tse Huang,Hsien-Te Kao,Spencer Lynch,Svitlana Volkova,Tongshuang Sherry Wu,Anita Woolley,Hao Zhu,Maarten Sap*

Main category: cs.CY

TL;DR: 本文介绍了SOTOPIA-S4，一个快速、灵活、可扩展的社会模拟系统，使用LLM代理进行多轮多方交互和假设测试。


<details>
  <summary>Details</summary>
Motivation: 动机是解决当前框架的技术障碍，并为从业者提供自定义评估指标来测试社会科学问题和LLM行为假设。

Method: 方法是通过开发一个pip包，包括模拟引擎、API服务器和网络界面，允许用户无需编程设计、运行和分析模拟。

Result: 结果是通过二元招聘谈判和多方规划场景的用例，展示了系统的实用性。

Conclusion: 结论是SOTOPIA-S4有效提升了社会模拟的可访问性和效率，促进了相关领域的应用。

Abstract: Social simulation through large language model (LLM) agents is a promising
approach to explore and validate hypotheses related to social science questions
and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable
social simulation system that addresses the technical barriers of current
frameworks while enabling practitioners to generate multi-turn and multi-party
LLM-based interactions with customizable evaluation metrics for hypothesis
testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine,
an API server with flexible RESTful APIs for simulation management, and a web
interface that enables both technical and non-technical users to design, run,
and analyze simulations without programming. We demonstrate the usefulness of
SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and
multi-party planning scenarios.

</details>


### [69] [Efficacy of a Computer Tutor that Models Expert Human Tutors](https://arxiv.org/abs/2504.16132)
*Andrew M. Olney,Sidney K. D'Mello,Natalie Person,Whitney Cade,Patrick Hays,Claire W. Dempsey,Blair Lehman,Betsy Williams,Art Graesser*

Main category: cs.CY

TL;DR: 本研究比较了智能辅导系统与人类辅导的有效性，发现两者均显著提升学习成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管辅导对促进学习非常有效，但专家知识对辅导有效性的贡献仍不清楚且存在争议。

Method: 开展了9周的学习效能研究，包括智能辅导系统、领域专家但非辅导专家的人类辅导和无辅导条件，补充课堂教学，通过前后测试和延迟测试，使用logistic混合效应模型分析。

Result: 智能辅导系统和人类辅导在即时后测上均有显著正效应（ITS d=0.71，人 tutor d=0.66），在延迟后测上也有显著正效应（ITS d=0.36，人 tutor d=0.39）。

Conclusion: 讨论了专家知识在辅导中的作用，并为未来研究设计提供了启示。

Abstract: Tutoring is highly effective for promoting learning. However, the
contribution of expertise to tutoring effectiveness is unclear and continues to
be debated. We conducted a 9-week learning efficacy study of an intelligent
tutoring system (ITS) for biology modeled on expert human tutors with two
control conditions: human tutors who were experts in the domain but not in
tutoring and a no-tutoring condition. All conditions were supplemental to
classroom instruction, and students took learning tests immediately before and
after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis
using logistic mixed-effects modeling indicates significant positive effects on
the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which
are in the 99th percentile of meta-analytic effects, as well as significant
positive effects on the delayed post-test for the ITS (d =.36) and human tutors
(d =.39). We discuss implications for the role of expertise in tutoring and the
design of future studies.

</details>


### [70] [A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures](https://arxiv.org/abs/2504.16133)
*Milad Leyli-abadi,Ricardo J. Bessa,Jan Viebahn,Daniel Boos,Clark Borst,Alberto Castagna,Ricardo Chavarriaga,Mohamed Hassouna,Bruno Lemetayer,Giulia Leto,Antoine Marot,Maroua Meddeb,Manuel Meyer,Viola Schiaffonati,Manuel Schneider,Toni Waefler*

Main category: cs.CY

TL;DR: 这篇论文提出一个跨学科的整体概念框架，以整合人类和AI在安全关键系统中的能力，解决透明度、信任和可解释性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能充分处理人类与AI交互中的挑战，包括透明度、信任、可解释性和安全决策的需求。

Method: 通过整合数学、决策理论、计算机科学、哲学、心理学、认知工程等领域，并应用于能源、交通和航空领域；并通过在现有框架上的实例化展示灵活性。

Result: 框架被提出并展示了其灵活性，但未详细说明具体成果。

Conclusion: 需要一个整体框架来桥接安全系统设计、部署和维护的差距，通过跨学科方法实现。

Abstract: The interaction between humans and AI in safety-critical systems presents a
unique set of challenges that remain partially addressed by existing
frameworks. These challenges stem from the complex interplay of requirements
for transparency, trust, and explainability, coupled with the necessity for
robust and safe decision-making. A framework that holistically integrates human
and AI capabilities while addressing these concerns is notably required,
bridging the critical gaps in designing, deploying, and maintaining safe and
effective systems. This paper proposes a holistic conceptual framework for
critical infrastructures by adopting an interdisciplinary approach. It
integrates traditionally distinct fields such as mathematics, decision theory,
computer science, philosophy, psychology, and cognitive engineering and draws
on specialized engineering domains, particularly energy, mobility, and
aeronautics. The flexibility in its adoption is also demonstrated through its
instantiation on an already existing framework.

</details>


### [71] [Trends in Frontier AI Model Count: A Forecast to 2028](https://arxiv.org/abs/2504.16138)
*Iyngkarran Kumar,Sam Manning*

Main category: cs.CY

TL;DR: 本文估计政府AI法规的计算阈值（如EU AI Act的10^25 FLOP和US框架的10^26 FLOP）将捕获多少模型，并预测到2028年模型数量的增长趋势，强调绝对阈值捕获量超线性增加，而相对阈值更稳定。


<details>
  <summary>Details</summary>
Motivation: 政府开始基于AI模型训练计算量施加法规，需要预测这些阈值对模型捕获的影响，以评估政策有效性和潜在挑战。

Method: 通过估计和预测方法，使用数据模型forecast未来模型数量和趋势，分析绝对和相对阈值的捕获效果。

Result: 预测到2028年底，将有103-306个模型超过10^25 FLOP阈值，45-148个超过10^26 FLOP阈值；模型数量每年超线性增加，而相对阈值每年捕获中位数14-16个模型。

Conclusion: 绝对计算阈值将导致捕获模型数量急剧增加，而以最大训练运行为基准的相对阈值能提供更稳定的趋势。

Abstract: Governments are starting to impose requirements on AI models based on how
much compute was used to train them. For example, the EU AI Act imposes
requirements on providers of general-purpose AI with systemic risk, which
includes systems trained using greater than $10^{25}$ floating point operations
(FLOP). In the United States' AI Diffusion Framework, a training compute
threshold of $10^{26}$ FLOP is used to identify "controlled models" which face
a number of requirements. We explore how many models such training compute
thresholds will capture over time. We estimate that by the end of 2028, there
will be between 103-306 foundation models exceeding the $10^{25}$ FLOP
threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding
the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion
Framework (90% CI). We also find that the number of models exceeding these
absolute compute thresholds each year will increase superlinearly -- that is,
each successive year will see more new models captured within the threshold
than the year before. Thresholds that are defined with respect to the largest
training run to date (for example, such that all models within one order of
magnitude of the largest training run to date are captured by the threshold)
see a more stable trend, with a median forecast of 14-16 models being captured
by this definition annually from 2025-2028.

</details>


### [72] [Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts](https://arxiv.org/abs/2504.16139)
*Sridharan Sankaran*

Main category: cs.CY

TL;DR: 这篇论文提出一个比较风险影响评估框架，评估ISO AI标准在全球不同监管环境中的有效性，并建议改进以增强其适用性。


<details>
  <summary>Details</summary>
Motivation: AI的信任性面临全球挑战，包括偏见、不透明和责任缺失，ISO标准旨在促进负责任发展，但其效果因监管差异而异。

Method: 引入比较风险影响评估框架，将ISO标准映射到欧盟AI法案，调查十个地区的监管框架，并应用于欧盟、美国科罗拉多和中国等案例研究。

Result: 框架揭示ISO标准的差距，如在执行力上不足（例如科罗拉多）和低估地区特定风险（如中国的隐私风险）。

Conclusion: 推荐强制风险审计、地区特定附件和隐私模块，以提高ISO标准的适应性，并提供可复制工具来对齐标准与伦理要求，促进全球AI治理。

Abstract: As artificial intelligence (AI) reshapes industries and societies, ensuring
its trustworthiness-through mitigating ethical risks like bias, opacity, and
accountability deficits-remains a global challenge. International Organization
for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to
foster responsible development by embedding fairness, transparency, and risk
management into AI systems. However, their effectiveness varies across diverse
regulatory landscapes, from the EU's risk-based AI Act to China's
stability-focused measures and the U.S.'s fragmented state-led initiatives.
This paper introduces a novel Comparative Risk-Impact Assessment Framework to
evaluate how well ISO standards address ethical risks within these contexts,
proposing enhancements to strengthen their global applicability. By mapping ISO
standards to the EU AI Act and surveying regulatory frameworks in ten
regions-including the UK, Canada, India, Japan, Singapore, South Korea, and
Brazil-we establish a baseline for ethical alignment. The framework, applied to
case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO
standards falter in enforcement (e.g., Colorado) and undervalue region-specific
risks like privacy (China). We recommend mandatory risk audits, region-specific
annexes, and a privacy-focused module to enhance ISO's adaptability. This
approach not only synthesizes global trends but also offers a replicable tool
for aligning standardization with ethical imperatives, fostering
interoperability and trust in AI worldwide. Policymakers and standards bodies
can leverage these insights to evolve AI governance, ensuring it meets diverse
societal needs as the technology advances.

</details>


### [73] [Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room](https://arxiv.org/abs/2504.16148)
*Danial Hooshyar,Gustav Šír,Yeongwook Yang,Eve Kikas,Raija Hämäläinen,Tommi Kärkkäinen,Dragan Gašević,Roger Azevedo*

Main category: cs.CY

TL;DR: 本文识别并分析AI在教育中的九个挑战，并提出使用混合AI方法如神经符号AI来解决，以提升公平性、透明性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI教育系统取得进展，但存在未解决的问题，如忽略学习过程和伦理问题，需要构建负责任的AI。

Method: 通过理论和实证研究识别九个挑战，并建议采用混合AI方法如神经符号AI。

Result: 证明混合AI方法可以有效解决这些挑战，提升AI在教育中的可靠性和有效性。

Conclusion: 混合AI方法应作为教育中负责任、值得信赖的AI系统基础。

Abstract: Despite significant advancements in AI-driven educational systems and ongoing
calls for responsible AI for education, several critical issues remain
unresolved -- acting as the elephant in the room within AI in education,
learning analytics, educational data mining, learning sciences, and educational
psychology communities. This critical analysis identifies and examines nine
persistent challenges that continue to undermine the fairness, transparency,
and effectiveness of current AI methods and applications in education. These
include: (1) the lack of clarity around what AI for education truly means --
often ignoring the distinct purposes, strengths, and limitations of different
AI families -- and the trend of equating it with domain-agnostic,
company-driven large language models; (2) the widespread neglect of essential
learning processes such as motivation, emotion, and (meta)cognition in
AI-driven learner modelling and their contextual nature; (3) limited
integration of domain knowledge and lack of stakeholder involvement in AI
design and development; (4) continued use of non-sequential machine learning
models on temporal educational data; (5) misuse of non-sequential metrics to
evaluate sequential models; (6) use of unreliable explainable AI methods to
provide explanations for black-box models; (7) ignoring ethical guidelines in
addressing data inconsistencies during model training; (8) use of mainstream AI
methods for pattern discovery and learning analytics without systematic
benchmarking; and (9) overemphasis on global prescriptions while overlooking
localised, student-specific recommendations. Supported by theoretical and
empirical research, we demonstrate how hybrid AI methods -- specifically
neural-symbolic AI -- can address the elephant in the room and serve as the
foundation for responsible, trustworthy AI systems in education.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [74] [Eigendecomposition Parameterization of Penalty Matrices for Enhanced Control Design: Aerospace Applications](https://arxiv.org/abs/2504.16328)
*Nicholas P. Nurre,Ehsan Taheri*

Main category: math.OC

TL;DR: 本文提出特征分解参数化惩罚矩阵的方法，以改善控制算法性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 动机是克服传统对角惩罚矩阵的局限性，通过允许非零非对角元素来提升性能和扩展控制范围。

Method: 方法使用特征分解参数化惩罚矩阵，确保正定性，并结合粒子群优化算法优化变量，应用于Zermelo导航、航天器控制和轨迹设计问题。

Result: 结果显示性能提升高达65%，在多个示例中验证。

Conclusion: 结论是该方法提供计算优势和更广控制能力。

Abstract: Modern control algorithms require tuning of square weight/penalty matrices
appearing in quadratic functions/costs to improve performance and/or stability
output. Due to simplicity in gain-tuning and enforcing positive-definiteness,
diagonal penalty matrices are used extensively in control methods such as
linear quadratic regulator (LQR), model predictive control, and Lyapunov-based
control. In this paper, we propose an eigendecomposition approach to
parameterize penalty matrices, allowing positive-definiteness with non-zero
off-diagonal entries to be implicitly satisfied, which not only offers notable
computational and implementation advantages, but broadens the class of
achievable controls. We solve three control problems: 1) a variation of
Zermelo's navigation problem, 2) minimum-energy spacecraft attitude control
using both LQR and Lyapunov-based methods, and 3) minimum-fuel and minimum-time
Lyapunov-based low-thrust trajectory design. Particle swarm optimization is
used to optimize the decision variables, which will parameterize the penalty
matrices. The results demonstrate improvements of up to 65% in the performance
objective in the example problems utilizing the proposed method.

</details>


### [75] [Revisiting Regret Benchmarks in Online Non-Stochastic Control](https://arxiv.org/abs/2504.16581)
*Vijeth Hebbar,Cédric Langbort*

Main category: math.OC

TL;DR: 这篇论文针对在线非随机控制问题，引入最佳固定输入基准，并提出算法实现子线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 原基准（最佳事后线性反馈控制器）对一般凸成本可能不合适，因为线性控制器可能高度次优。

Method: 提出新颖的在线控制算法，实现相对于最佳固定输入的子线性遗憾。

Result: 算法在新基准下实现了子线性遗憾。

Conclusion: 讨论了与Agarwal et al.工作的联系，并通过数值模拟比较性能。

Abstract: In the online non-stochastic control problem, an agent sequentially selects
control inputs for a linear dynamical system when facing unknown and
adversarially selected convex costs and disturbances. A common metric for
evaluating control policies in this setting is policy regret, defined relative
to the best-in-hindsight linear feedback controller. However, for general
convex costs, this benchmark may be less meaningful since linear controllers
can be highly suboptimal. To address this, we introduce an alternative, more
suitable benchmark--the performance of the best fixed input. We show that this
benchmark can be viewed as a natural extension of the standard benchmark used
in online convex optimization and propose a novel online control algorithm that
achieves sublinear regret with respect to this new benchmark. We also discuss
the connections between our method and the original one proposed by Agarwal et
al. in their seminal work introducing the online non-stochastic control
problem, and compare the performance of both approaches through numerical
simulations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [76] [A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis](https://arxiv.org/abs/2504.16097)
*Arthur Buzelin,Pedro Robles Dutenhefner,Turi Rezende,Luisa G. Porfirio,Pedro Bento,Yan Aquino,Jose Fernandes,Caio Santana,Gabriela Miana,Gisele L. Pappa,Antonio Ribeiro,Wagner Meira Jr*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新的LGA-ECG模型，结合局部卷积和全局注意力机制来提升ECG分类性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死亡原因，需要高效ECG诊断工具；传统transformer无法有效捕获局部形态特征。

Method: 提出LGA-ECG模型，通过在重叠卷积窗口上平均嵌入提取查询，并结合全局自注意力机制来捕捉局部和全局ECG特征。

Result: 在CODE-15数据集上，LGA-ECG优于最先进模型，消融研究验证了局部-全局注意力策略的有效性。

Conclusion: 该模型通过捕获ECG信号的层次时间依赖性和形态模式，具有临床部署的潜力。

Abstract: Cardiovascular diseases remain the leading cause of global mortality,
emphasizing the critical need for efficient diagnostic tools such as
electrocardiograms (ECGs). Recent advancements in deep learning, particularly
transformers, have revolutionized ECG analysis by capturing detailed waveform
features as well as global rhythm patterns. However, traditional transformers
struggle to effectively capture local morphological features that are critical
for accurate ECG interpretation. We propose a novel Local-Global Attention ECG
model (LGA-ECG) to address this limitation, integrating convolutional inductive
biases with global self-attention mechanisms. Our approach extracts queries by
averaging embeddings obtained from overlapping convolutional windows, enabling
fine-grained morphological analysis, while simultaneously modeling global
context through attention to keys and values derived from the entire sequence.
Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG
outperforms state-of-the-art models and ablation studies validate the
effectiveness of the local-global attention strategy. By capturing the
hierarchical temporal dependencies and morphological patterns in ECG signals,
this new design showcases its potential for clinical deployment with robust
automated ECG classification.

</details>


### [77] [Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems](https://arxiv.org/abs/2504.16099)
*Luyuan Zhang,Xidong Mu,An Liu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 这篇论文提出了一种针对Pinching Antenna Systems (PASS)的两时间尺度联合发射和捏合波束形成设计，以最大化多用户多输入单输出系统的总和速率。


<details>
  <summary>Details</summary>
Motivation: PASS是一种革命性的灵活天线技术，能够通过低成本的可调节激活位置的捏合天线实现视线链接，因此需要优化波束形成以提升系统性能。

Method: 使用原始对偶分解方法将问题分解为两个子问题：1) 短时间尺度发射波束形成采用Karush-Kuhn-Tucker指导的对偶学习方法；2) 长时间尺度捏合波束形成使用随机连续凸逼近方法。

Result: 模拟结果显示，所提出的两时间尺度算法与基准方案相比取得了显著的性能提升。

Conclusion: 该算法在PASS-based系统中有效提高了总和速率，证明了其优越性。

Abstract: Pinching antenna systems (PASS) have been proposed as a revolutionary
flexible antenna technology which facilitates line-of-sight links via numerous
low-cost pinching antennas with adjustable activation positions over
waveguides. This letter proposes a two-timescale joint transmit and pinching
beamforming design for the maximization of sum rate of a PASS-based downlink
multi-user multiple input single output system. A primal dual decomposition
method is developed to decouple the two-timescale problem into two
sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is
proposed to solve the short-term transmit beamforming design sub-problem; 2)
The long-term pinching beamforming design sub-problem is tackled by adopting a
stochastic successive convex approximation method. Simulation results
demonstrate that the proposed two-timescale algorithm achieves a significant
performance gain compared to other baselines.

</details>


### [78] [Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France](https://arxiv.org/abs/2504.16100)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: eess.SP

TL;DR: 本研究使用机器学习结合空间天气数据预测法国太阳能和风能生产，神经网络模型在中期预测中错误率4%-10%，展示了区域预测潜力。


<details>
  <summary>Details</summary>
Motivation: 准确预测不可调度可再生能源对于电网稳定性和价格预测至关重要，但现有方法未充分利用空间分辨率数据。

Method: 构建2012-2023年数据集，使用RTE电力数据作为目标，ERA5天气数据等作为输入，探索空间平均、PCA和计算机视觉方法，并通过时间序列交叉验证调优机器学习模型。

Result: 时间序列交叉验证最有效，神经网络优于树模型，nRMSE错误率在4%到10%，与单厂模型性能相当。

Conclusion: 这些方法在区域电力供应预测中具有显著潜力。

Abstract: Accurate prediction of non-dispatchable renewable energy sources is essential
for grid stability and price prediction. Regional power supply forecasts are
usually indirect through a bottom-up approach of plant-level forecasts,
incorporate lagged power values, and do not use the potential of spatially
resolved data. This study presents a comprehensive methodology for predicting
solar and wind power production at country scale in France using machine
learning models trained with spatially explicit weather data combined with
spatial information about production sites capacity. A dataset is built
spanning from 2012 to 2023, using daily power production data from RTE (the
national grid operator) as the target variable, with daily weather data from
ERA5, production sites capacity and location, and electricity prices as input
features. Three modeling approaches are explored to handle spatially resolved
weather data: spatial averaging over the country, dimension reduction through
principal component analysis, and a computer vision architecture to exploit
complex spatial relationships. The study benchmarks state-of-the-art machine
learning models as well as hyperparameter tuning approaches based on
cross-validation methods on daily power production data. Results indicate that
cross-validation tailored to time series is best suited to reach low error. We
found that neural networks tend to outperform traditional tree-based models,
which face challenges in extrapolation due to the increasing renewable capacity
over time. Model performance ranges from 4% to 10% in nRMSE for midterm
horizon, achieving similar error metrics to local models established at a
single-plant level, highlighting the potential of these methods for regional
power supply forecasting.

</details>


### [79] [xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM](https://arxiv.org/abs/2504.16101)
*Lei Kang,Xuanshuo Fu,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: eess.SP

TL;DR: 本论文提出xLSTM-ECG方法，使用扩展LSTM网络对ECG信号进行多标签分类，提高诊断效率。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死亡原因，手动ECG解释耗时且易出错，需要更高效的诊断工具。

Method: 使用短时傅里叶变换(STFT)将ECG信号转换为频域，并采用定制xLSTM网络针对12-lead ECG进行多标签分类。

Result: 在PTB-XL数据集上实验显示出色性能，在Georgia 12-Lead数据集上验证了鲁棒性和效率。

Conclusion: 显著提升ECG分类准确性，促进临床诊断和患者护理，代码将于接受后公开。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
worldwide, highlighting the critical need for efficient and accurate diagnostic
tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart
conditions; however, their manual interpretation is time-consuming and
error-prone. In this paper, we propose xLSTM-ECG, a novel approach that
leverages an extended Long Short-Term Memory (xLSTM) network for multi-label
classification of ECG signals, using the PTB-XL dataset. To the best of our
knowledge, this work represents the first design and application of xLSTM
modules specifically adapted for multi-label ECG classification. Our method
employs a Short-Time Fourier Transform (STFT) to convert time-series ECG
waveforms into the frequency domain, thereby enhancing feature extraction. The
xLSTM architecture is specifically tailored to address the complexities of
12-lead ECG recordings by capturing both local and global signal features.
Comprehensive experiments on the PTB-XL dataset reveal that our model achieves
strong multi-label classification performance, while additional tests on the
Georgia 12-Lead dataset underscore its robustness and efficiency. This approach
significantly improves ECG classification accuracy, thereby advancing clinical
diagnostics and patient care. The code will be publicly available upon
acceptance.

</details>


### [80] [A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders](https://arxiv.org/abs/2504.16130)
*Pengju Ren,Ri-gui Zhou,Yaochong Li*

Main category: eess.SP

TL;DR: 本论文提出SMAE（基于Masked AutoEncoder的自监督学习方法），用于Raman光谱分析，提高无标注数据的特征提取和分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖标注数据，成本高且数据有限；标注不足时性能下降，因此需要自监督方法解决特征提取挑战。

Method: 提出SMAE，通过随机masking光谱信息并重建，学习光谱特征，并实现去噪效果。

Result: 重建光谱SNR提高两倍以上；无监督聚类准确率超过80%；微调后识别准确率83.90%，与监督ResNet竞争。

Conclusion: SMAE在Raman光谱分析中表现出色，尤其在数据标注有限的情况下，具有显著优势。

Abstract: Raman spectroscopy serves as a powerful and reliable tool for analyzing the
chemical information of substances. The integration of Raman spectroscopy with
deep learning methods enables rapid qualitative and quantitative analysis of
materials. Most existing approaches adopt supervised learning methods. Although
supervised learning has achieved satisfactory accuracy in spectral analysis, it
is still constrained by costly and limited well-annotated spectral datasets for
training. When spectral annotation is challenging or the amount of annotated
data is insufficient, the performance of supervised learning in spectral
material identification declines. In order to address the challenge of feature
extraction from unannotated spectra, we propose a self-supervised learning
paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.
SMAE does not require any spectral annotations during pre-training. By randomly
masking and then reconstructing the spectral information, the model learns
essential spectral features. The reconstructed spectra exhibit certain
denoising properties, improving the signal-to-noise ratio (SNR) by more than
twofold. Utilizing the network weights obtained from masked pre-training, SMAE
achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in
a pathogenic bacterial dataset, demonstrating significant improvements compared
to classical unsupervised methods and other state-of-the-art deep clustering
methods. After fine-tuning the network with a limited amount of annotated data,
SMAE achieves an identification accuracy of 83.90% on the test set, presenting
competitive performance against the supervised ResNet (83.40%).

</details>


### [81] [A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation](https://arxiv.org/abs/2504.16142)
*Hangxu Liu,Yaojie Sun,Yu Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于动态时间规整(DTW)的非侵入式负载监测(NILM)方法，通过优化特征提取，在边缘微控制器上实现了高准确率和低资源消耗的负载分解。


<details>
  <summary>Details</summary>
Motivation: 现有NILM方法虽准确但计算成本高、内存需求大，难以部署在资源受限的微控制器上。

Method: 提出时频域DTW算法，并比较六种机器学习技术，同时优化频域特征提取过程。

Result: 在边缘MCU上实现95%的识别准确率，运行时间减少55.55%，存储开销减少约34.6%。

Conclusion: 算法将进一步优化，未来焦点是消除电压变压器设计，提供更经济实惠的NILM解决方案。

Abstract: In recent years, non-intrusive load monitoring (NILM) technology has
attracted much attention in the related research field by virtue of its unique
advantage of utilizing single meter data to achieve accurate decomposition of
device-level energy consumption. Cutting-edge methods based on machine learning
and deep learning have achieved remarkable results in load decomposition
accuracy by fusing time-frequency domain features. However, these methods
generally suffer from high computational costs and huge memory requirements,
which become the main obstacles for their deployment on resource-constrained
microcontroller units (MCUs). To address these challenges, this study proposes
an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain
and systematically compares and analyzes the performance of six machine
learning techniques in home electricity scenarios. Through complete
experimental validation on edge MCUs, this scheme successfully achieves a
recognition accuracy of 95%. Meanwhile, this study deeply optimizes the
frequency domain feature extraction process, which effectively reduces the
running time by 55.55% and the storage overhead by about 34.6%. The algorithm
performance will be further optimized in future research work. Considering that
the elimination of voltage transformer design can significantly reduce the
cost, the subsequent research will focus on this direction, and is committed to
providing more cost-effective solutions for the practical application of NILM,
and providing a solid theoretical foundation and feasible technical paths for
the design of efficient NILM systems in edge computing environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram,Xin Zhou,Meenu Ravi,Mohammad Zarei,Anmol Srivastava*

Main category: cs.CV

TL;DR: CAIRO框架通过本体和人类辅助方法检测AI视觉系统故障，提高安全性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉系统对稀有场景的脆弱性带来的安全风险。

Method: 引入CAIRO框架，使用本体和人类-in-the-loop检测及正式化故障案例为知识图谱。

Result: 分析自动驾驶系统故障，生成可共享、可解释的知识图谱。

Conclusion: CAIRO提升AI系统的可扩展性、安全性和问责性。

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>


### [83] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha,Rashid Kisitu,Florence Tushabe*

Main category: cs.CV

TL;DR: 本研究提出混合知识蒸馏框架，将Swin Transformer的知识转移到MobileNetV3上，实现高效的植物病害检测。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在农业IoT中的挑战：大型模型如Swin Transformer准确率高但计算复杂，不适合边缘设备；轻量级模型如MobileNetV3缺乏空间推理能力。

Method: 提出混合知识蒸馏框架，包括自适应注意力对齐和双损失函数，从Swin Transformer教师模型向MobileNetV3学生模型转移logit和注意力知识。

Result: 在lantVillage-Tomato数据集上，蒸馏后MobileNetV3准确率达92.4%，比Swin-L的95.9%略低，但计算量减少95%，延迟降低至PC CPU 23ms和智能手机CPU 86ms/图像。

Conclusion: 此工作推进实时、节能的作物监测，展示了在边缘设备上实现Vision Transformer级别诊断精度的可能性。

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>


### [84] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)
*Jingchao Wang,Hong Wang,Wenlong Zhang,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: This paper proposes the PLVL framework for multi-task visual grounding, which progressively integrates language information into visual learning and uses a multi-task head for collaborative prediction, achieving superior performance on REC and RES tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods, such as insufficient language injection into visual features and ineffective exploitation of relationships between Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES) tasks.

Method: Proposes the PLVL framework that mines inherent visual features, progressively injects language information without additional cross-modal fusion modules, and designs a multi-task head for joint prediction of REC and RES.

Result: Extensive experiments on benchmark datasets demonstrate that PLVL outperforms representative methods in both REC and RES tasks.

Conclusion: The PLVL framework significantly enhances the accuracy and efficiency of multi-task visual grounding by fully utilizing language guidance and task collaborations.

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [Security-First AI: Foundations for Robust and Trustworthy Systems](https://arxiv.org/abs/2504.16110)
*Krti Tallam*

Main category: cs.CR

TL;DR: 这篇论文主张将AI安全作为基础层，优先考虑它来支持AI的安全、透明和责任。


<details>
  <summary>Details</summary>
Motivation: AI安全是AI领域安全、透明、责任等努力的基础，但当前讨论往往忽略它。

Method: 呈现AI挑战的分层视图，区分安全与安全，论证安全优先方法，并讨论威胁模型、攻击向量和防御机制。

Result: 得出结论，AI安全需要采用度量驱动的方法。

Conclusion: 以度量为基础的AI安全方法对实现稳健的AI系统至关重要。

Abstract: The conversation around artificial intelligence (AI) often focuses on safety,
transparency, accountability, alignment, and responsibility. However, AI
security (i.e., the safeguarding of data, models, and pipelines from
adversarial manipulation) underpins all of these efforts. This manuscript
posits that AI security must be prioritized as a foundational layer. We present
a hierarchical view of AI challenges, distinguishing security from safety, and
argue for a security-first approach to enable trustworthy and resilient AI
systems. We discuss core threat models, key attack vectors, and emerging
defense mechanisms, concluding that a metric-driven approach to AI security is
essential for robust AI safety, transparency, and accountability.

</details>


### [86] [AI-Based Vulnerability Analysis of NFT Smart Contracts](https://arxiv.org/abs/2504.16113)
*Xin Wang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文使用决策树和随机森林模型检测智能合约缺陷，如Risky Mutably Porxy等。


<details>
  <summary>Details</summary>
Motivation: 智能合约存在常见缺陷，需要高效检测方法以提升安全性。

Method: 收集并分类智能合约代码，使用Python处理数据，构建CART决策树和随机森林模型。

Result: 比较决策树、随机森林和自建模型，分析性能差异。

Conclusion: 得出模型比较的一般结论，可能推荐随机森林在缺陷检测中的优势。

Abstract: In the research experiment of this article, our research work is divided into
several stages. Firstly, we collected a large number of smart contract codes
and classified them, identifying several common defects, including Risky
Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and
Public Burns. Secondly, we used Python to process the smart contracts. On the
one hand, we modified the file names, and on the other hand, we batched the
process of the content for analysis and application. Next, we built a model of
the decision tree. Firstly, we carried out the feature extraction. We selected
the algorithm and divided the data. After comparing and processing, we chose
the CART classification tree to process. By gene coefficient, we analyzed and
sorted the data, and got the initial model of the decision tree. Then, we
introduced the random forest model on the basis of the decision tree. From
abstracting the same amount of samples to selecting features randomly.From
adjusting and optimizing parameters to completing the construction of the
forest model. Finally, we compared and analyzed the decision tree, random
forest, and self-built model in the paper and drew general conclusions.

</details>


### [87] [DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain](https://arxiv.org/abs/2504.16116)
*Miracle Master,Rainy Sun,Anya Reese,Joey Ouyang,Alex Chen,Winter Dong,Frank Li,James Yi,Garry Zhao,Tony Ling,Hobert Wong,Lowes Yang*

Main category: cs.CR

TL;DR: 本文引入DMind Benchmark框架，评估LLM在Web3领域的性能，涵盖九个类别的主观任务，并公开数据集。


<details>
  <summary>Details</summary>
Motivation: LLM在一般NLP任务上进步显著，但对Web3等专业领域的探索不足。

Method: 开发DMind Benchmark框架，包括区块链基础、DeFi等九个类别，并使用主观任务如智能合约审计和数值推理。

Result: 评估15个LLM，发现模型在Web3特定推理上存在差距，尤其在代币经济学和安全漏洞识别方面。

Conclusion: 公开基准数据集、评估管道和结果，以促进Web3-enabled LLM的发展。

Abstract: Recent advances in Large Language Models (LLMs) have led to significant
progress on a wide range of natural language processing tasks. However, their
effectiveness in specialized and rapidly evolving domains such as Web3 remains
underexplored. In this paper, we introduce DMind Benchmark, a novel framework
that systematically tests LLMs across nine key categories encompassing
blockchain fundamentals, infrastructure, smart contract analysis, decentralized
finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible
tokens (NFTs), token economics, meme concepts, and security vulnerabilities.
  DMind Benchmark goes beyond conventional multiple-choice questions by
incorporating domain-specific subjective tasks (e.g., smart contract code
auditing and repair, numeric reasoning on on-chain data, and fill-in
assessments), thereby capturing real-world complexities and stress-testing
model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,
Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in
Web3-specific reasoning and application, particularly in emerging areas like
token economics and meme concepts. Even the strongest models face significant
challenges in identifying subtle security vulnerabilities and analyzing complex
DeFi mechanisms. To foster progress in this area, we publicly release our
benchmark dataset, evaluation pipeline, and annotated results at
http://www.dmind.ai, offering a valuable resource for advancing specialized
domain adaptation and the development of more robust Web3-enabled LLMs.

</details>


### [88] [Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks](https://arxiv.org/abs/2504.16118)
*Milad Rahmati*

Main category: cs.CR

TL;DR: 本文提出了一种可解释且轻量级的AI框架（ELAI），用于边缘网络的实时网络威胁检测，解决了传统AI模型的可解释性和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 网络威胁不断演变，边缘网络因分布式特性和资源限制而难以安全；AI驱动的威胁检测系统缺乏可解释性和高计算成本，限制了实际部署。

Method: 引入ELAI框架，整合决策树等可解释机器学习、基于注意力的深度学习和联邦学习，实现了透明且高效的实时威胁检测。

Result: 在CICIDS和UNSW-NB15数据集上评估，实现了高检测率、低误报率，并显著降低了计算需求。

Conclusion: 主要贡献包括：针对边缘计算的创新可解释AI模型、优化轻量级深度学习方法，以及对AI安全应用中可解释性技术的全面分析。

Abstract: As cyber threats continue to evolve, securing edge networks has become
increasingly challenging due to their distributed nature and resource
limitations. Many AI-driven threat detection systems rely on complex deep
learning models, which, despite their high accuracy, suffer from two major
drawbacks: lack of interpretability and high computational cost. Black-box AI
models make it difficult for security analysts to understand the reasoning
behind their predictions, limiting their practical deployment. Moreover,
conventional deep learning techniques demand significant computational
resources, rendering them unsuitable for edge devices with limited processing
power. To address these issues, this study introduces an Explainable and
Lightweight AI (ELAI) framework designed for real-time cyber threat detection
in edge networks. Our approach integrates interpretable machine learning
algorithms with optimized lightweight deep learning techniques, ensuring both
transparency and computational efficiency. The proposed system leverages
decision trees, attention-based deep learning, and federated learning to
enhance detection accuracy while maintaining explainability. We evaluate ELAI
using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing
its performance across diverse cyberattack scenarios. Experimental results
demonstrate that the proposed framework achieves high detection rates with
minimal false positives, all while significantly reducing computational demands
compared to traditional deep learning methods. The key contributions of this
work include: (1) a novel interpretable AI-based cybersecurity model tailored
for edge computing environments, (2) an optimized lightweight deep learning
approach for real-time cyber threat detection, and (3) a comprehensive analysis
of explainability techniques in AI-driven cybersecurity applications.

</details>


### [89] [A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content](https://arxiv.org/abs/2504.16120)
*Chaima Njeh,Haïfa Nakouri,Fehmi Jaafar*

Main category: cs.CR

TL;DR: 这篇论文引入BART-Corrective Model，通过生成后修正机制减少LLM中的偏差和有害内容，并在多个模型上显示出显著安全提升。


<details>
  <summary>Details</summary>
Motivation: 解决LLM潜在偏差和有害内容的担忧，提供一种实用数据中心替代方案，而非仅依赖模型微调或提示工程。

Method: 使用BART-Corrective Model的生成后修正机制，这是一种数据中心的方法。

Result: 实验在多个有毒数据集上显示：GPT-4毒性和越狱分数减少15%和21%，PaLM2减少28%和5%，Mistral-7B减少约26%和23%，Gemma-2b-it减少11.1%和19%。

Conclusion: 该方法提升LLM的安全性，使其更适合真实世界应用。

Abstract: Large Language Models (LLM) have made remarkable progress, but concerns about
potential biases and harmful content persist. To address these apprehensions,
we introduce a practical solution for ensuring LLM's safe and ethical use. Our
novel approach focuses on a post-generation correction mechanism, the
BART-Corrective Model, which adjusts generated content to ensure safety and
security. Unlike relying solely on model fine-tuning or prompt engineering, our
method provides a robust data-centric alternative for mitigating harmful
content. We demonstrate the effectiveness of our approach through experiments
on multiple toxic datasets, which show a significant reduction in mean toxicity
and jail-breaking scores after integration. Specifically, our results show a
reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,
a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately
26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.
These results demonstrate the potential of our approach to improve the safety
and security of LLM, making them more suitable for real-world applications.

</details>
